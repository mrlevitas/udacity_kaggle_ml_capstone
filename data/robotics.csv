"id","title","content","tags"
"1","What is the right approach to write the spin controller for a soccer robot?","<p>Imagine programming a 3 wheel soccer robot. What type of controller would you use for spinning it? P? PID?</p>

<p>The goal for this controller is that it should make the robot stand in a defined angle ( 0 degree ) and turn back if rotated by hand or other robot. </p>

<p>I use stepper motors for my robot and not servos so I need to implement this in my software!</p>

<p>I have written a sample P type controller already and the movement is fairly good. But I would like to make it better if possible. The code is as follows:</p>

<pre><code>void spinSpeed(int devidedValue, int addedValue, int correction) {

    if(degree&lt;correction &amp;&amp; degree&gt;-correction) {
        motorSpeed = 0;
    } else {
        if(degree &gt; 0) {
            motorSpeed = ((degree)/(devidedValue) + (addedValue));
        } else {
            motorSpeed = ((degree)/(devidedValue) - (addedValue));  
        }
    }
}
</code></pre>

<p><code>correction</code> is a range , in which robot has no movement. 
<code>degree</code> is a number between -127 and 128 which is returned from the compass.
<code>motorSpeed</code> is a number between 0 and 255 which is applied to the PWM.</p>
","soccer control"
"2","How can I modify a low cost hobby servo to run 'freely'?","<p>I've got some hobby servos (<a href=""http://www.servodatabase.com/servo/power-hd/hd-1501mg"">Power HD 1501MGs</a>) and I'd like to be able to control them (via an Arduino) so they will either go to the angle I set, or put them in a 'free running' mode, where the load will take them wherever it goes.</p>

<p>Is this even possible, or am I just going to end up stripping the gears?</p>

<p>My first thought is to simply kill the power to the servo, but the force required to move them in that state is more than I'd like.</p>

<p>If it is possible, am I looking at a hardware change, or could I do it in software?</p>
","control rcservo"
"3","What useful gaits exist for a six legged robot, and what are their pros and cons?","<p><a href=""http://www.oricomtech.com/projects/leg-time.htm"" rel=""nofollow"">http://www.oricomtech.com/projects/leg-time.htm</a> lists three gaits:</p>

<ul>
<li>the tripod</li>
<li>wave, and</li>
<li>ripple.</li>
</ul>

<p>Can these be improved, or can their relative pros and cons be altered, and are there other gaits worth considering?</p>
","gait walk"
"4","Good Microcontrollers/SOCs for a Robotics Project","<p>I am looking for a starting point for my project, preferably using popular systems (ones there is a lot of support for). I have an Arduino Uno, a Raspberry Pi, and a lot of willpower :) Anyone here built a project using the systems above?</p>

<p>Observation: I'd like to start with a simple line-following vehicle and build up afterwards.</p>
","microcontroller arduino raspberry-pi"
"5","Nearest-neighbor data structure for non-Euclidean configuration space","<p>I'm trying to implement a nearest-neighbor structure for use in an RRT motion planner. In order to do better than a linear brute-force nearest-neighbor search, I'd like to implement something like a kd-tree. However, it seems like the classical implementation of the kd-tree assumes that each dimension of the space can be split into ""left"" and ""right"". This notion doesn't seem to apply to non-Euclidean spaces like SO(2), for instance.</p>

<p>I'm working with a serial manipulator arm with fully rotational links, meaning that each dimension of the robot's configuration space is SO(2), and therefore non-Euclidean. Can the kd-tree algorithm be modified to handle these kinds of subspaces? If not, is there another nearest-neighbor structure that can handle these non-Euclidean subspaces while still being easy to update and query? I also took a look at <a href=""http://www.cs.ubc.ca/~mariusm/index.php/FLANN/FLANN"">FLANN</a>, but it wasn't clear to me from their documentation whether they can handle non-Euclidean subspaces.</p>
","motion-planning rrt"
"6","What good robotics software platforms / operating systems are available?","<p>My company will soon be starting a brand new robotics project, and we are still trying to decide whether we should design and code a robotics software platform from scratch, or if there are any good existing ones.</p>

<p>It would be very useful if there was a software platform which was commonly used among both academics and industry so that our robotic system was generally compatible with others, and so that people were already familiar with it.</p>

<p>We would like the software platform to be able to:</p>

<ul>
<li>Integrate new robotic hardware components easily.</li>
<li>Already contain a wide array of useful data processing and visualisation tools</li>
<li>Make efficient use of computing hardware</li>
</ul>
","software platform"
"11","What software do you use to design your PCB in the robotics field?","<p>What is the best software (despite the price) for designing the circuits and PCB boards for robots?</p>

<p>I mean having lots of components, different designing methods, best accuracy, ...</p>

<p>I myself use the Altium Designer which I think answers my needs, but maybe there are better ones in the market i don't know about!</p>
","software circuit"
"18","What are good methods for tuning the process noise on Kalman filters?","<p>Most often tuning the Kalman filter noise matrices is done by trial and error or domain knowledge. Are there more principled ways for tuning all the Kalman filter parameters?</p>
","odometry localization kalman-filter"
"19","Keyboard control map for scalar based movement?","<p>I'm working with a Wild Thumper 6 wheel chasis that is designed for use with an RC controller. However I'd like to have a mapping to a keyboard for control as well. Can you suggest a set of keys and behaviors that you've used to deal with the continuous value control normally offered by a joystick or pair of joysticks? The standard wasd keys + an accelerate/decelerate pair? I'd also take a pointer to a videogame that you think does this well.</p>
","untagged"
"20","Ideas for shooting the ball in a soccer robot","<p>What is the best option to use for the shooting system of a soccer robot?</p>

<p>I have already implemented a solenoid-based system for shooting and it works perfectly. 
However, I'd like some other methods to check if they are better than mine. </p>
","soccer mechanism"
"23","F/OSS Optical Object Avoidance","<p>I am beginning work on a larger scale, 250-350 lbs wheeled robot and looking to use both optical and other means of object avoidance. I am concerned with a robot this large causing issues with running into things, including people as it has a top speed of 15mph and that would cause issues with safety. I am starting out with a remote control but am looking to have the robot become self contained. I have been loosely following the DARPA driver-less car project but will not have anywhere near the fiscal or power budget that they do for sensors and computers. Am I thinking to far afield with my idea of having a self-contained robot in the 250-300 lbs range that does not break the bank on optical object avoidance? Any comments or experiences will be greatly appreciated.</p>
","computer-vision wheeled-robot"
"25","How to choose the right propeller/motor combination for a quadcopter?","<p>There are many sites which explain briefly this problem and even propose combinations. I however would like a much more detailed explanation. What is going to give my quad the most agility? Do I need bigger motors/props in a heavy quad to achieve the same level of agility than in a lighter quad?</p>

<p><strong>EDIT:</strong>
Here is what I have understood on the subject:</p>

<ul>
<li>A quadcopter doesn't need high revving motors as there are 4 propellers providing thrust and high revving motors require more battery power.</li>
<li>Larger propellers give more thrust per revolution from the motor.</li>
</ul>

<p>The question is focused more on the general characteristics of various combinations but some specific questions do spring to mind: </p>

<ol>
<li>For a given combination what would be the effect of upgrading propeller size in comparison to installing higher revving motors?</li>
<li>What changes would need to be made to lift a heavier quad?</li>
<li>How can I achieve more agility in my quad?</li>
</ol>
","quadcopter"
"26","Cheap web to buy robotic pieces from","<p>I love computer programming, and if I can interact with the world programming, it's even better. I used to steal them from school, and make little robots that bounce when collided with a wall, but I want to go further, but I don't have the needed pieces, and I don't know any good place to buy pieces , like servos, etc. either.</p>

<p>Also, I'm Spanish, so if the page can sell to places in Spain, even better.</p>

<p>I would highly appreciate your help,</p>

<p>Aritzh</p>
","servos"
"37","Mobile robot localization in a known map","<p>I want to localize a mobile robot equipped with a 2D laser scanner in a known indoor environment.  The map is a 2D occupancy grid, but is not perfect.</p>

<p>What algorithms are appropriate for mobile robot localization?</p>
","localization mobile-robot"
"39","I'd like to use gesture based input for my robot. What are the pros and cons between the Xtion Live and the Kinect?","<p>As in the title, I'd like to implement gesture recognition on my robot and I'm looking for the pros and cons between the Kinect and the Xtion - and also if there are any other sensible options available.</p>

<p>I'm thinking of the following, but open to other suggestions:</p>

<ul>
<li>Accuracy</li>
<li>Price</li>
<li>Driver quality</li>
<li>Power draw</li>
</ul>
","kinect input"
"42","What tyre tread would be best suited to an off road robot expected to deal with frequently muddy conditions?","<p>I'm looking to potentially build an autonomous robot that will frequently venture off road, and remain autonomous for up to 6 hours at a time. I've found limited information however about the best tyre tread for this purpose, what could be most suitable?</p>

<p>I'm especially looking for a tread pattern that won't need regular cleaning, to save setting this up automatically (a tread that gets ""clogged"" very quickly clearly won't be that effective at tackling tough terrain autonomously.)</p>
","wheel"
"43","What algorithm should I use for balancing a two wheeled robot using a gyroscope?","<p>Is there a good, popular and reliable algorithm I can use by taking input from a gyroscope and using this to control two independant wheels to keep such a balanced robot reliably upright? I'm looking for an algorithm that will let me use it to drive a robot around as well as keep it upright when stationary. The ability to deal with inclines and people <em>nudging</em> it would also be a bonus, but not essential.</p>
","control gyroscope balance two-wheeled"
"44","Choosing the right dimensions for an underwater glider","<p>I'm looking to potentially build an <a href=""http://en.wikipedia.org/wiki/Underwater_glider"">underwater glider</a>, a type of submarine that's slow but can operate on extremely low power draw. However, in order for it to work effectively I've found several sources hinting that the dimensions of the components, especially the wings, are critical to its success.</p>

<p>However, I've found very sparse information about what these dimensions should be! I'm happy to do a bit of trial and error if it comes down to it, but to save some work does anyone have any information on what the critical dimensions should be?</p>
","design underwater auv"
"46","What's the most effective type of rechargeable battery when taking into account size / weight / Ah?","<p>I'm looking to build an underwater glider robot that will need to remain autonomous for long periods of time, perhaps even months. Power draw should be minimal, and I'm thinking of including some form of charging device (such as a solar charger) however I'd also like the battery capacity to be large enough so I don't hugely need to worry about this. Large current draw isn't really needed, but the battery does need to hold its charge effectively for long periods of time. Considering this is an underwater vehicle, weight and size are also a concern.</p>

<p>Cost isn't too much of an issue, as long as it's within reason of a hobbyist project.</p>

<p>I am looking to understand the pros and cons of each technology (Lead acid, LiPo, NiCad, fuel cell?), so I can decide what type of battery would be best suited to my purpose. As such, I'm looking at battery technology rather than looking for a specific shopping recommendation.</p>
","underwater battery auv"
"48","How can I best protect sensitive components against damage through vibration?","<p>It's common for components on some types of robots to experience large environmental stresses, one key one being vibration. Is this something I need to worry about with typical electronics and other sensitive components, or not really? If it is, then how do I secure such components? </p>

<p>I've heard of two main philosophies behind this, the first being that you should use a damping system such as with springs to absorb the shock. The second is that you should keep everything rigidly in place so it can't move, and therefore can't hit against anything else and break.</p>

<p>Which one should I follow, or if the answer is ""it depends"" what should I use as a guide as to best protect sensitive components?</p>
","electronics protection"
"49","What's the most accurate way to obtain a locational fix using GPS?","<p>Obviously GPS is the most obvious and accessible technology for obtaining a locational ""fix"" for a robot at any particular time. However, while it's great sometimes, in other locations and situations it's not as accurate as I'd like, so I'm investigating whether there's a relatively easy way to improve on this accuracy (or not, if that turns out to be the case.)</p>

<p>I've considered the following options, but found limited information online:</p>

<ul>
<li><p>Would using a much better antenna help, especially for low signal areas? I'm thinking yes to this, but if so how would I construct such an antenna and know that it's an improvement? Are there any good guides on how to do this? I could use a ready made antenna if they're not too expensive.</p></li>
<li><p>Would using multiple separate receivers in tandem help, or would they likely all be off by a similar amount, or would I not be able to extract a meaningful average with this approach?</p></li>
<li><p>What sort of characteristics should I look for when choosing a good GPS receiver to help accuracy?</p></li>
<li><p>Is there anything else I should consider which I've missed?</p></li>
</ul>
","localization gps"
"53","What algorithm can I use for constructing a map of an explored area using a number of ultrasound sensors?","<p>Ultrasound sensors are incredibly cheap these days which makes them a popular choice for many hobbyist robotic applications, and I'd like to use a bunch of them (say 10) around a robot with an algorithm to build a rough map of an area (as the robot explores it.) I'm not interested in dealing with moving objects at this stage, just pinpointing stationary ones, and I'll be using GPS for location. I realise that other components such as a laser scanner would produce much more accurate results, however such devices are also astronomically more expensive.</p>

<p>Does an algorithm exist for this purpose?</p>
","slam localization gps mapping acoustic-rangefinder"
"55","Adding external heat sinking to a Dynamixel RX-24F servo?","<p>Hobby servos are generally not sufficient for real robotics for a number of reasons: Quality, precision, range of motion, torque, etc.</p>

<p>Meanwhile, industrial servos, such as ABB, Emerson, GE, etc, are generally both heavy and expensive, and not suitable for small-humanoid scale actuation. Similarly, building your own servo from motors, gearboxes, and encoders, is akin to trying to design your own CPU just to control a motor -- too much detail getting in the way of real work.</p>

<p>There exists an in-between level of servos -- reasonably priced, reasonable performance, and reasonably controllable -- in the form of the competing brands of Dynamixel and HerculeX servos.</p>

<p>The smallest offerings in those lines generally are not strong enough for real-world interaction, but the next step up hold a lot of promise. For the Robotis Dynamixel line, this is the RX-24F servo (priced between the cheap AX-12F and the next step up the MX-28R.) Asking around, it seems that the specs and interface on that servo is great, but that it shuts down from thermal overload if you actually try to run it at rated load -- something that I'd expect from a hobby servo, but not a robotics servo.</p>

<p>Now, stepping up to the MX-28R doubles the price. Thus, if the RX-24F heat flaw could be fixed, it would be positioned at a nice price/performance point.</p>

<p>Does anyone have experience in providing additional cooling for this servo? Anything from thermal-gluing heat sinks to the case, to drilling holes and running cooling fluid tubing to any hot parts on the interior would be reasonable approaches. However, before I spend significant time and effort investigating this, I'd like a second opinion -- is it possible, does anyone have experience doing this, is it worth it?</p>
","servos heat-management cooling"
"57","How can I detect if a DC motor on a robot is starting to fail?","<p>What characteristics can I look for which could be reliable early warning signs that a DC motor on my robot, say one used for the drive, could be failing? I'm looking for an answer that deals in terms of sensors rather than manual inspection, so a circuit could be constructed to warn of a potential failure before it happens.</p>

<p>I have a few ideas such as an increase in current draw or decrease in rotation speed / voltage, but I want to guard against false warnings caused by reasonable wear and tear, or just the robot struggling on tough terrain.</p>

<p>Obviously such a system will never be foolproof, but are there any points I can look out for?</p>
","sensors failure motor"
"60","Is it possible to both move and stabilize a two wheeled robot with no gyroscopes?","<p>With two wheeled robot <a href=""http://www.pages.drexel.edu/~dml46/Tutorials/BalancingBot/files/nxt_icon.jpg"">like this one</a>, I have managed to stabilize it while keeping it stationary. This was done using a digital feedback control system by reading the position of the wheels to determine position, and the natural back electromotive force from the wheel motors was used in the feedback loop to determine velocity. It was kept stable with a PID controller, which was designed using a root locus algorithm to keep it stable and modulate the performance parameters (such as percent overshoot, settling time, etc.). I wanted to attempt to keep it stable while simultaneously propelling it forward, but I couldn't figure out how to go about designing a linear controller that could do that. Is it possible to both propel the robot forward <strong>and</strong> keep it stable using a feedback controller on the wheels, or is a gyroscope necessary?</p>
","two-wheeled stability"
"65","Calculating the efficiency of Mecanum wheels","<p>I'm part of a <a href=""http://usfirst.org"">FIRST Robotics</a> team, and we're looking into using <a href=""http://en.wikipedia.org/wiki/Mecanum_wheel"">Mecanum wheels</a> for our robot.</p>

<p>What are the advantages and disadvantages of using Mecanum wheel versus regular ones? From looking through Google, it looks like Mecanum wheels give more mobility but don't have as much traction. Are there any other advantages or disadvantages?</p>

<p>Compared to regular wheels, are Mecanum wheels less efficient or more efficient in any way? And if so, is there a quantifiable way to determine by how much?</p>

<p>Are there equations I can use to calculate efficiency (or inefficiency) and/or speed of moving forwards, sideways, or at arbitrary angles?</p>

<p><em>A picture of a robot with mecanum wheels:</em></p>

<p><img src=""http://i.stack.imgur.com/kLSuG.png"" alt=""robot using mecanum wheels""></p>
","mobile-robot design movement wheel first-robotics"
"75","Using an Arduino to control an ON / OFF connection between two pins","<p>I've got this driver: <a href=""http://www.pololu.com/catalog/product/1182"" rel=""nofollow"">http://www.pololu.com/catalog/product/1182</a></p>

<p>... a A4988 stepper motor driver carrier</p>

<p><img src=""http://i.stack.imgur.com/MPEI5.png"" alt=""enter image description here""></p>

<p>I'm attempting to control a connection between the RESET and SLEEP pins with logic ( code ) running on my Arduino. The motor runs perfectly when these two pins are connected however I'd like to control when the stepper is powered off from my Arduino ( and thus not generating extra heat )</p>

<p>I'd like to:</p>

<ol>
<li>designate a pin to control the connection between these two pins</li>
<li>use a ""digitalWrite"" to the above pin with a HIGH or LOW to
switch power on and off from the stepper</li>
</ol>

<p>NOTE: The <a href=""http://www.pololu.com/file/0J450/a4988_DMOS_microstepping_driver_with_translator.pdf"" rel=""nofollow"">data sheet</a> mentioned that for the driver to be powering the stepper both RESET and SLEEP needed to be in switched on ( HIGH )</p>
","arduino logic-control stepper-motor stepper-driver"
"85","Shape Memory Alloy wire for robot gripper arm actuation: How to vary grip pressure?","<p>For a robotic gripper arm we are designing for factory floor use on very small components, we propose to use electrically activated Shape Memory Alloy (SMA) wire harnesses for actuation. </p>

<p>The device being designed is akin to Pick &amp; Place machines used for circuit assembly, but moves over an aircraft-hanger sized work surface on wheels. It manipulates irregular shaped and porous objects between 0.5 cu.cm and 8 cu.cm each - hence the traditional vacuum P&amp;P mechanism does not appeal. Also, individual objects in the assembly line have varying hardness and weights.</p>

<p>Our design constraints are: </p>

<ul>
<li>Ensuring minimal to zero vibration and sound </li>
<li>Using minimal volume within the mechanism (batteries are at the wheelbase, providing stability, so their weight is not a concern)</li>
<li>Fine variation of gripper pressure</li>
</ul>

<p>We believe SMA meets the first two constraints well, but need some guidance on achieving constraint 3, i.e. different levels of pressure of the gripper controlled electronically.</p>

<p>My questions:</p>

<ul>
<li>Can PWM of a current above the activation threshold (320 mA for <a href=""http://www.dynalloy.com/TechDataWire.php"">0.005 inch Flexinol HT</a>) provide variable, repeatable actuation force? </li>
<li>Would we need pressure sensors on each fingertip and a closed loop control for grip, or can the gripper be calibrated periodically and maintain repeatable force?</li>
<li>Is there any well-documented precedent or study we should be referring to?</li>
</ul>
","mobile-robot"
"88","Mechanical design for motorized spherical caster wheels","<p>Design goal is to have a mobile robot that operates on 3 large casters, essentially 2 to 4 inch diameter steel ball bearings, that are motorized. No other mechanism would touch the surface. The robot should thus be able to move in any XY direction on a flat surface, with steering being achieved by varying the speed and rolling direction of these wheels. The robot has no designated ""front"" side, so it does not need to (and should not have to) bodily turn, in order to move off in any given direction. </p>

<p>Conventional wheels or tracks are not the preferred approach. </p>

<p>Looking for suggested mechanical layouts of multiple rubber wheels, pressing down onto the steel ball from within the castor housing, to drive the ball in any direction. A single wheel on a stepper, rotated around the vertical axis using a sail-winch servo, is one approach under consideration. Would this be ideal, or are there any serious flaws in this approach?</p>

<p>Alternatively, is there any other suggested method of driving such a steel ball in any arbitrary direction under electronic control?</p>
","servos mobile-robot stepper-motor"
"91","What determines the amount of noise an actuator produces?","<p><a href=""http://www.youtube.com/watch?v=ZHJf365p_zw"">Many robots</a> and other mechanical devices produce the signature whirring noise as they move, <a href=""http://www.youtube.com/watch?v=67CUudkjEG4"">some</a> produce less. What makes the difference? What restrictions a silence requirement places on a robot?</p>
","motor actuator noise"
"93","What BLDC servo drive takes sinusoidal hall sensor signals?","<p>I am looking for a servo drive to control a brushless DC motor, with at least 10A, 30V rating. However, I want to know if any exist which take sinusoidal hall sensor signals directly.</p>

<p>I already know there are servo drives taking hall sensor pulses (with 6 different phases), but that is trapezoidal control.</p>

<p>Note: a servo drive includes the driving electronics (no additional transistors required).</p>
","brushless-motor hall-sensor"
"94","Developing for 8-bit AVR-s, what are the current, open and free libraries out there?","<p>I would be very interested to ask for a list of repos of free open code, applicable to 8-bit avr-s and having relation to robotics - object avoidance, process controllers, battery management, etc. This would be of huge help for me, preventing me from wasting weeks and months to invent the wheel.</p>
","software microcontroller"
"99","How to manage interrupts on an AVR?","<p>I have a number of interrupt service routines on an <a href=""https://en.wikipedia.org/wiki/Atmel_AVR"" rel=""nofollow"">AVR</a>. These include interrupts for <a href=""https://en.wikipedia.org/wiki/Universal_asynchronous_receiver/transmitter#Synchronous_transmission"" rel=""nofollow"">USART</a> serial communication, timers, and <a href=""http://en.wikipedia.org/wiki/Serial_Peripheral_Interface_Bus"" rel=""nofollow"">SPI</a> communication.</p>

<p>For all of these, I use circular queues (using a start and end pointer, without boundary checking).</p>

<p>Some problems start to occur if the AVR starts to get overloaded. The circular queues will lose chunks of data. To solve this, I can reduce the load on the AVR (for example, by skipping some cycles in the timer). However, this is a manual process, where it is reduced if the AVR appears to have problems. This is partly because I do want relatively consistent timer periods.</p>

<p>However, even at 70% average processor load, the queues can fill up randomly by chance.</p>

<p>In case of spurious overloading at times, how can I make this more adaptive to avoid queue overflows?</p>
","microcontroller avr interrupts"
"100","What connectors are most reliable?","<p>If you have used connectors for signal wiring for any length of time, you may find that they are unreliable.</p>

<p>Specifically, I find these to be unreliable when used for a long time, with a number of disconnections and re-connections:</p>

<p><img src=""http://i.stack.imgur.com/RDJQV.jpg"" alt=""enter image description here""></p>

<p>This is due to the loss of springy-ness of the crimped metal end on the wire, which causes contact problems.</p>

<p>Which connectors (with rapid connection time) are reliable for multiple re-connections for simple signal wiring?</p>

<p>This excludes screw terminals and connectors with screws (eg. D-subminiature connectors), because they are not simple plug-in connectors.</p>
","wiring"
"106","What is a suitable model for two-wheeled robots?","<p>What is a suitable model for two-wheeled robots? That is, what equations of motion describe the dynamics of a two-wheeled robot.</p>

<p>Model of varying fidelity are welcome. This includes non-linear models, as well as linearized models.</p>
","mobile-robot two-wheeled"
"112","How should emergency stops be wired?","<p>Emergency stops are obviously a good idea on most robots, how should they be wired?  What systems should be killed immediately, and what should stay working?</p>
","mobile-robot errors"
"113","How to model the noise in a range sensor's return?","<p>Range sensors (for example sonar, infrared, and lidar) are notoriously noisy.  How can I characterize the noise characteristics to include these in a probabilistic localization sensor model?</p>
","sensors noise"
"117","What is the difference between 4-point and 8-point connectivity in graph based planning?","<p>In graph-based planning (say, A*), states are connected to their neighbors.  How should one decide whether to connect to the 4 neighbors or the 8 neighbors?  What are the pros and cons of each approach?</p>
","motion-planning artificial-intelligence planning"
"118","Properly flashing the firmware on a Lego Mindstorms NXT","<p>I am attempting to upload a custom firmware to a Lego Mindstorms NXT and am having issues.</p>

<p>First of all, I'm attempting to use <a href=""http://lejos-osek.sourceforge.net/"">nxtOSEK</a>, which would let me run C++ programs on it.  The problem is, everytime I put it into firmware update mode, the download doesn't seem to actually occur.</p>

<p>What I mean by this is that, according to the output in my terminal (both Mac and Windows), the download was successful, however when the NXT reboots, I still see the normal logo (not nxtOSEK).</p>

<p>So, what I'm doing is first holding down the <code>Reset</code> button for a few seconds, then hitting the orange button, giving me that tic-tic-tic sound.  Then I run the firmware update (either using the Windows NextTool or Mac OSX GUI NextTool) and attempt the download.  I get a success message, yet the robot is still using the old firmware.</p>

<p>What could be the cause of this problem and how can I solve it?</p>
","mindstorms nxt"
"119","Is it better to have weight distributed over the wheels or the center of the robot?","<p>When designing a standard 4 or 6 wheel robot, is it better to have the weight distributed primarily in the center of the robot, or over the wheels, or is there no difference?</p>

<p>Specifically, which weight distribution will make the robot less likely to tip over?</p>
","mobile-robot design stability wheeled-robot"
"128","How mature is real-time programming in robotics?","<p><strong>Edit:</strong> I don't know why, but this question seems to be confusing many people. I am aware of when/where/why/how to use real-time. I am interested in knowing whether people who have a real-time task would actually care enough to implement it in real-time or not.</p>

<p>There's no need to mention why real-time operations are important for a robot. My question is however, how much is it actually used in robotics?</p>

<p>Take <a href=""http://robotics.stackexchange.com/q/6/158"">this question</a> for example. Only one answer mentions any platform with real-time capabilities, and it is far from the top too. ROS apparently, being a very popular platform which is not real-time.</p>

<p>In the real-time world however, RTAI<sup>1</sup> seems to be the only workable <em>free</em> real-time platform of use. It is however, limited to Linux (no problem), badly documented and slowly developed.</p>

<p><del>So, how much is real-time behavior sought after among robotics developers?</del> The question is, how much are developers inclined to write real-time applications when real-time behavior is actually needed? If not much, why?</p>

<p>For example, reflexive behavior based on tactile data, cannot go through ROS because it would lose its real-time property. But do people really come up with a real-time solution or use ROS anyway, ignoring the real-time property?</p>

<p><sup>1</sup> or similarly Xenomai</p>
","software platform real-time"
"131","Connecting two microcontrollers using I2C","<p>I Have an ATmega16 mc which is master on the i2c and a ATMega8 mc which is slave on the i2c.</p>

<p>I have connected the two mcs' sda and scl ports to each other alongside a pullup resistor.</p>

<p>Now I want to read a register from the ATMega8 using the ATMega16. </p>

<p>The problem is that I don't want to assign all the variables manually. Is there any libs or headers that will do this thing for me?</p>
","software microcontroller i2c"
"138","Mathematical prerequisites for beginning graduate student in robotics","<p>A beginning graduate student in robotics asked me the areas of mathematics that he should brush up on (prerequisites) to begin a masters research program in robotics. What are some good materials/books that are indispensable for a research student? Which ones should we suggest in order that the student develops a solid foundation in robotics?</p>
","research"
"142","Technology behind Kiva Systems mobile robots","<p>What kind of sensors and algorithms are the mobile robots of <a href=""http://www.kivasystems.com/"">Kiva Systems</a> equipped with? </p>
","mobile-robot industrial-robot"
"143","How do I calculate the required loop frequency for a servo controller?","<p>I have a motor which drives a string connected to a load cell. I would like to implement a closed loop controller to control the load applied by the motor to the string.</p>

<p><img src=""http://i.stack.imgur.com/wWerj.png"" alt=""Motor driving string connected to a load cell""></p>

<p>How do I go about determining the required loop frequency in order to create a stable control system? Is it something like the Nyquist frequency, where the loop speed should be at least twice the highest frequency inherent in the mechanical system?</p>
","control motor force"
"146","What rail material is best used for linear bearings?","<p>For the 3d printer <a href=""http://www.reprap.org/wiki/Prusa_Mendel_Assembly"">RepRap Prusa</a> there are several rails (smooth rods) that guide the printer head on the different axises. The printer head uses several linear bearings to glide along the rails. </p>

<p>There isn't any specification on what kind of material would be best suited for this purpose with the linear bearings. My first assumption would be for stainless steel because it won't corrode (rust) on the surface, but I'm not sure if this is true for all printers (whether they are 3D printers or not) as a different material may allow the linear bearings to glide more easily. Aluminum would have been my second choice but I have the same reservations of which grade would be least resistant. </p>

<p>This <a href=""http://www.lm76.com/linear_shaft_selector.htm"">resource</a> has some limited information but does not help with which would be best suited for this particular application.</p>

<p>What material is best suited for this purpose?</p>
","reprap 3d-printing linear-bearing"
"148","Detect Nao robot in Kinect","<p>I am not sure if this has been tried before but I am trying to use <a href=""http://en.wikipedia.org/wiki/Kinect"" rel=""nofollow"">Kinect</a> and detect gestures made by the <a href=""http://en.wikipedia.org/wiki/Nao_%28robot%29"" rel=""nofollow"">Nao robot</a>.</p>

<p>I have made a Kinect application, <a href=""http://youtu.be/v8SumS-I1qo"" rel=""nofollow"">a gesture based picture viewer</a> and it detects humans fine(Obviously it does!) What I wanted to try was (lazy as I am), to see if I could use some (say, voice) command to tell the Nao to do a Swipe Right gesture and have my application identify that gesture. The Nao can easily identify my command and do some gesture. The problem however is, when I put the Nao in front of the Kinect sensor, the Kinect does not track it. </p>

<p>What I want to know is, are there some basics behind Kinect's human body motion tracking that essentially fails when a robot is placed in front of it instead of a human?</p>

<p>PS: I have kept the Nao at the right distance from the sensor. I have also checked if the entire robot is in the field of view of the sensor.</p>

<p>EDIT: This has been posted <a href=""http://stackoverflow.com/q/13068945/1473556"">on stackoverflow</a> and <a href=""http://social.msdn.microsoft.com/Forums/en-US/kinectsdk/thread/723a1072-f637-466f-a721-196984e7763a/#723a1072-f637-466f-a721-196984e7763a"" rel=""nofollow"">on msdn</a> by me so as to target a large audience as this problem has not been encountered by anyone in the past.</p>
","kinect"
"150","Preventing leaks in motor shafts for underwater bots","<p>Whenever building an aquatic bot, we always have to take care to prevent leakages, for obvious reasons. Now, holes for wires can be made watertight easily--but what about motors? We can easily seal the casing in place (and fill in any holes in the casing), but the part where the axle meets the casing is still left unprotected. </p>

<p><img src=""http://i.stack.imgur.com/xahVT.jpg"" alt=""enter image description here""></p>

<p>Water leaking into the motor is still quite bad. I doubt there's any way to seal up this area properly, since any solid seal will not let the axle move, and any liquid seal (or something like grease) will rub off eventually.</p>

<p>I was thinking of putting a second casing around the motor, maybe with a custom rubber orifice for the shaft. Something like (forgive the bad drawing, not used to GIMP):</p>

<p><img src=""http://i.stack.imgur.com/z0E4K.png"" alt=""enter image description here""></p>

<p>This would probably stop leakage, but would reduce the torque/rpm significantly via friction.</p>

<p>So, how does one prevent water from leaking into a motor without significantly affecting the motor's performance?</p>

<p>(To clarify, I don't want to buy a special underwater motor, I'd prefer a way to make my own motors watertight)</p>
","motor underwater auv protection"
"154","Fixed point arithmetic on microcontrollers","<p>Often we use microcontrollers to do things in our robots, but need to make some calculations in decimal. Using floating point variables is <strong>very</strong> slow, because a software floating point library is automatically included (unless you have a high-end microcontroller). Therefore, we generally use fixed point arithmetic.</p>

<p>Whenever I do this, I just use an integer, and remember where the decimal place is. However, it does take some care to ensure that everything is consistent, especially when calculations involve variables where the decimal point is in a different place.</p>

<p>I have implemented a fixed point atan2 function, but because I was trying to squeeze every last drop of limited precision (16 bits), I would often change the definition of where the decimal point is, and it would change as I tweaked it. In addition, I would have some constants, as a quasi look-up table, which themselves have an implied decimal point somewhere.</p>

<p>I want to know if there is a better way. Is there a library, or set of macros, that can simplify the use of fixed point variables, making multiplication and division between mixed variables easier, and allowing declaration of decimal numbers or constant expressions, but automatically converting to the desired fixed point representation at compile time?</p>
","microcontroller c"
"155","How to choose a good IMU for a wheeled robot?","<p>At our lab, we have a several ""Kurt"" type robots (about the size of a Pioneer, six wheels, differential drive). The built-in gyroscopes are by now really outdated; the main problem is that the gyroscopes have a large drift that increases as the gyro heats up (the error is up to 3°/s). We mainly use the IMU to get initial pose estimates that are later corrected by some localization algorithm, but even so the large initial pose error caused by the IMU is often annoying.</p>

<p>We've temporarily used an Android phone (Galaxy S2) as a replacement IMU, and the results are so much better compared to the old IMUs. However, I don't like depending on a WiFi connection between IMU and the control computer (a laptop running ROS/Ubuntu), so we're looking to buy a new IMU.</p>

<p>What IMU should we choose? What criteria are important to consider for our application? Please share your experiences! :-)</p>
","ros imu odometry gyroscope ugv"
"167","What are good strategies for tuning PID loops?","<p>Tuning controller gains can be difficult, what <strong>general</strong> strategies work well to get a stable system that converges to the right solution?</p>
","control pid tuning"
"172","Absolute positioning without GPS","<p>Using an IMU a robot can estimate its current position relative to its starting position, but this incurs error over time. GPS is especially useful for providing position information not biased by local error accumulation. But GPS cannot be used indoors, and even outdoors it can be spotty.</p>

<p>So what are some methods or sensors that a robot can use to localize (relative to some frame of reference) without using GPS? </p>
","localization gps sensors slam"
"178","HMMs vs. CRFs to model time-series force data of robots interacting with environment?","<p>I have a time-series of force data of robots interacting with environment objects with various textures. I would like to develop models of various textures using the time-series data to classify textures into smooth, rough, moderate, etc. categories. For this purpose, will Hidden Markov Models be sufficient or should i use Conditional Random Fields? If I decide to classify into more categories and the distinction between each of are categories are very subtle, in that case what would be a good choice? Will force-data be sufficient to capture all the information I need to classify textures into these categories?</p>

<p>Thanks for your replies :)</p>
","artificial-intelligence"
"182","Suitable control algorithm for Air Muscle based joint?","<p>I have a joint actuated by an antagonistic pair of Pneumatic Muscles.</p>

<p><img src=""http://i.stack.imgur.com/7FNlN.png"" alt=""Pneumatic Muscle Joint""></p>

<p>There are two valves per muscle, one to fill and one to empty each muscle. The joint has an angle sensor, and each muscle also contain an air pressure sensor.</p>

<p>What is a suitable control algorithm set up?</p>

<ul>
<li>A PID controller controlling the valve orifice sizes?</li>
<li>A PID controller controlling the mass flow rate?</li>
<li>A PID controller controlling the pressure using two PID pressure controllers?</li>
<li>A Fuzzy Logic controller?</li>
<li>A Neural Network?</li>
</ul>
","control pid air-muscle"
"184","Robotics Trends","<p>Robotics has always been one of those engineering fields which has promised so much, but is taking a long time to deliver all the things that people imagine.</p>

<p>When someone asks: ""How long before we have [X] type of robots?"" Are there any resources we can call upon to try to calculate a rough answer. These resources might include:</p>

<ul>
<li>Rate of progress of computational power, and some estimate of how much will be needed for various types of AI.</li>
<li>Rate of progress of electrical energy storage density, and some estimate of how much will be needed for various types of robot.</li>
<li>Rate of progress of actuation systems, and some estimate of what would be needed for various types of robot.</li>
<li>Lists of milestones towards various types of robot, and which ones have been achieved and when.</li>
</ul>

<p>Are these types of studies performed, and are the results published?</p>

<hr>

<p>Added:</p>

<p>In response to Jakob's comment, I am not looking for opinions or discussions on this subject. What I am looking for are published studies which might shed light on this question.</p>
","research"
"205","Creating a fast, uniform, linear actuator","<p>Most of the linear actuators I've seen are nonuniform and/or slow. Those using a cam or crankshaft-like mechanism (and nearly anything hydraulic/pneumatic) cannot be moved at a constant speed without some programming. Those using a screw-like mechanism are uniform, but slow. </p>

<p>Aside from a rack and pinion/rope wound around a stick, what other fast, uniform linear actuators exist? By uniform, I mean that the speed is uniform (Or the distance moved is linearly dependant on the angle rotated by the motor)</p>
","actuator"
"209","How to determine quality of ICP matches?","<p>In <a href=""http://ais.informatik.uni-freiburg.de/teaching/ws11/robotics2/pdfs/rob2-13-frontends.pdf"">SLAM frontends</a> which use the Iterative Closest Point (ICP) algorithm for identifying the association between two matching point clouds, how can you determine if the algorithm is stuck in a local minimum and returns a wrong result? </p>

<p>The problem is defined as matching two pointclouds which are both samples of some arbitrary surface structure, and the sampled areas have an overlap of 0-100% which is unknown. I know the <a href=""http://glorfindel.mavrinac.com/~aaron/school/pdf/chetverikov02_tricp.pdf"">Trimmed ICP</a> variant works by iteratively trying to determine the overlap, but even this one can be stuck in a local minimum. </p>

<p>A naive approach would be to look a the mean square error of the identified point pairs. But without some estimate of the sampling this seems a risky thresholding. In the manual for the <a href=""http://hds.leica-geosystems.com/en/Leica-Cyclone_6515.htm"">Leica Cyclone</a> they suggest manual inspection of the pair error histogram. If it has a Gaussian shape the fit is good. If there is a linear fall-off the match is probably bad. This seems plausible for me, but I've never seen it used in an algorithm.</p>
","slam"
"210","How can I automatically adjust PID parameters on the fly?","<p>I have a simple servo system that uses a PID controller implemented in an MCU to perform the feedback. However, the properties of the system change dynamically, and so the PID parameters can never be tuned for all circumstances.</p>

<p>My robot is a light weight arm with back-drivable electric motors, similar to this one:</p>

<p><img src=""http://i.stack.imgur.com/sp7VH.jpg"" alt=""Lightweight robot arm""></p>

<p>The arm performs several tasks, including picking up heavy weights, pushing and pulling objects across the desk. Each of these tasks requires different PID tuning parameters which I cannot easily predict.</p>

<p>What I would really like is for some higher level function which can carefully adjust the parameters in response to the arm's behaviour. For example, if it notices that the arm is oscillating, it might reduce P and increase D. Or if it noticed that the arm wasn't reaching its target, it might increase I.</p>

<p>Do such algorithms exist? I would be happy even if the algorithm didn't perfect the parameters immediately. E.G. the arm could oscillate a few times before the parameters were adjusted to their new values. </p>
","control pid automatic tuning"
"213","How can I integrate a smart phone with my robotics project?","<p>Smart phones these days typically come with a gyroscope, accelerometer, compass, camera, and GPS sensor all on board. They also usually have a connection to the internet with Wifi and mobile data networks. I've seen many cases of using a phone as a remote control for a robot, but to me, it seems like the phone itself is a perfect lightweight computing and sensing platform for an autonomous robot.</p>

<p>The main obstacle I see is interfacing with actuators. Being able to control motors to steer even a table-top robot, or control servos, for example. Connecting and communicating to a microcontroller could be an obstacle as well. </p>

<p>As a robot hobbyist, I'd like to know how I can overcome these and other obstacles to be able to harness the power of my smart phone with my robotics projects.</p>
","actuator"
"215","Starting out advice on making robots and tinkering with microcontrollers","<p>I'd like to start making robots and tinkering with microcontrollers. Where do I start, and what do I need?</p>

<p>I'd like to make my own robots. I'm comfortable with programming (assembly and C) so I've got that part covered, but my electronics/circuits knowledge is a little weak. I have no idea what material to start with and which tools I need, nor how to put stuff together.</p>

<p>For the microcontroller, I'm thinking about going with the Pololu Orangutan LV-168 or the Arduino Duemilanove, although I'm leaning more towards the Orangutan because of the built-in LCD and pushbuttons (which I would expect to use, especially for debugging and user interaction). Am I on the right track? It seems to me like the number of I/O ports is small, but is that the case in practice?</p>
","arduino microcontroller beginner"
"222","Floor Segmentation to Determine Navigable Paths","<p>In my application, my robot has the following physical setup:</p>

<ul>
<li>Differential drive mechanics with feedback (wheel encoders)</li>
<li>Commercially available webcam mounted with a known transform to the base of the robot (RGB, no depth)</li>
</ul>

<p>The robot will be navigating through a structured, indoor type environment (think office, home, or university), and I would like to be able to determine the navigable paths through the environment using my vision sensor.</p>

<p>What is the best way to approach the problem of finding safe paths to travel when given a single vision sensor?</p>

<p><strong>Edit:</strong>  I think that I am more interested in the vision processing techniques than the actual path-planning mechanics.</p>
","computer-vision"
"229","Red [ERROR] output in python in ROS","<p>In ROS, I cannot get [ERROR] logs to print in red when I use python. How can I make them appear in red instead of black?</p>

<p><br></p>

<p>For example, the following Python:</p>

<pre><code>rospy.logerr(""No analog input received."")
</code></pre>

<p>produces this output in <strong>black</strong>:</p>

<blockquote>
  <p>[ERROR] [WallTime: 135601422.876123] No analog input received.</p>
</blockquote>

<p><br></p>

<p>whereas the following C++:</p>

<pre><code>ROS_ERROR(""No analog input received."");
</code></pre>

<p>produces the following output in <strong>red</strong>.</p>

<blockquote>
  <p>[ERROR] [135601551.192412]: No analog input received.</p>
</blockquote>
","ros python"
"230","Can ROS run on a Raspberry Pi?","<p>Can <a href=""http://www.ros.org/wiki/"">ROS</a> run on a Raspberry Pi?</p>

<p>ROS is resigned to run on a network of machines, with different machines, even different cores on the same machine doing different jobs. Can one of those machines be a Raspberry Pi?</p>

<p>I am considering using an R-Pi as the <a href=""http://en.wikipedia.org/wiki/EtherCAT"">EtherCAT</a> master on a mobile robot, communicating with the main PC over WiFi, using a dongle.</p>

<ul>
<li>Can an R-Pi even run ROS at all?</li>
<li>Would an R-Pi have enough processing power to do some 1kHz servoing?</li>
<li>Would it be possible to run some servoing on the host through the WiFi connecion?</li>
</ul>
","ros raspberry-pi wifi"
"231","What frequency does my quadcopter output-sense-calculate-output update loop need to stay stable?","<p>With a 600 mm (2 foot) motor-to-motor quadcopter, what frequency does my output-sense-calculate-output update loop need to stay stable?</p>

<p>I'm estimating a total takeoff weight of very roughly 2 pounds ( 0.9 kg ),
which I expect to be mostly motors and batteries.</p>
","stability quadcopter"
"235","Is it better to have batteries distributed at the rotors or the center of the multicopter?","<p>I've seen 3 approaches to mounting batteries on a multicopter:</p>

<ul>
<li>All the batteries rigidly mounted near the center of the airframe</li>
<li>All the batteries in a bag hanging under the center of the airframe</li>
<li>Each rotor has its share of the batteries rigidly mounted near/under it. (For example, a quadcopter with 1/4 of all the batteries mounted underneath each motor).</li>
</ul>

<p>Which design is the best, and why?
If there is no one best design, what are the advantages/tradeoffs between the designs?
Is there some other design I'm overlooking that is better in some way?</p>

<p>(This question focuses on multirotor flying machines.
For ground vehicles, see "" <a href=""http://robotics.stackexchange.com/questions/119/is-it-better-to-have-weight-distributed-over-the-wheels-or-the-center-of-the-rob"">Is it better to have weight distributed over the wheels or the center of the robot?</a> "").</p>
","design stability quadcopter"
"237","How to calculate serial speed and buffer requirements for PC to microcontroller communications?","<p>A common scenario is to have a PC that sends commands to a microcontroller via RS232.  My PC program is sending commands (each of which are composed of multiple bytes) as fast as it can to a small robot.  The microcontroller on the robot is a Parallax Propellor.</p>

<p>I have noticed that if I don't process bytes quickly enough on the microcontroller side of things, it can very quickly overflow the default buffers in the popular serial port drivers that are available for the Propellor. (The buffers are generally anywhere from 16 to 256 bytes).  I can arbitrarily increase these buffers or create my own larger circular buffer, but I would like to have a more methodical approach to determining appropriate size requirements and/or the minimal amount of time I can wait before pulling bytes out of the serial port driver buffer.</p>

<p>At 1st glance:</p>

<ul>
<li>115200 == 115.2 bits per millisecond == ~12.8 bytes per millisecond (assuming 1 stop bit)</li>
</ul>

<p><strong>1) Is that a valid way to calculate timing for serial transmissions?</strong></p>

<p>Also, given my specific setup:</p>

<ul>
<li>PC Program &lt;--> Bluetooth Serial Profile Driver &lt;--> Bluetooth Transceiver &lt;-<em>*</em>-> BlueSMIRF Wireless Modem &lt;--> Parallax Propellor Program</li>
</ul>

<p><strong>2) What is the maximum amount of data I can send for a given period of time consistently without eventually running in to problems?</strong></p>

<p>Maybe I'm over complicating things, but it seems like there are potentially multiple buffers involved in the transmission chain above.  How do others commonly deal with this? Do they throttle the PC sending to a known safe rate? Implement flow control?  If implementing flow control, how does that affect bandwidth and response times?</p>

<p>(If it matters, my experiment is to use a joystick on the PC to control multiple servos with instant reaction to the joystick movements. So every small movement of the joystick results in multiple commands being sent to the microcontroller. The commands are not just simple positional commands though, they also involve acceleration/deacceleration of servos over time and this is the reason that the microcontroller spends a significant amount of clock cycles before processing new bytes.)</p>
","microcontroller serial rs232"
"249","How does a six-axis force/torque sensor work?","<p>I would really like a six-axis force/torque sensor for my robot, but I just can't afford one. I was thinking about making one of my own.</p>

<p>I have experience using strain gauges, but I can't work out how to arrange them so as to create a six-axis force/torque sensor.</p>

<ul>
<li>Is this something I could feasibly make myself?</li>
<li>How do they work? What is the theory behind them?</li>
</ul>

<p>I'm curious to know how they work, even if it's not feasible to make one myself.</p>

<hr>

<p>Added:</p>

<p>Just to be clear, I'm talking about <em>force / torque</em> sensors, like this <a href=""http://www.ati-ia.com/products/ft/ft_models.aspx?id=Nano17"" rel=""nofollow"">ATI Nano 17</a>:</p>

<p><img src=""http://i.stack.imgur.com/QwUvX.jpg"" alt=""ATI Nano""></p>

<p>I am not talking about accelerometers or gyros, or MEMS IMUs.</p>
","sensors force-sensor"
"254","Should I switch my servo system from brushed to brushless motors?","<p>I have a robot that uses brushed motors in its servo system. These are Maxon 3W motors, with 131:1 planetary gearboxes. The motors are controlled by a PIC microcontroller, running a 1kHz PID controller. The servos are for a low speed high torque application. There is significant backlash between the sensor and the motor.</p>

<p>Maxon offer 12W brushless motors which are the same size. These are better in many ways: double the torque, better heat dissipation, higher efficiency.</p>

<p>The problem, obviously, is that they require more complex drive electronics. Also, I have heard a couple of people mention that brushed motors are better for servo applications, though they never explained why.</p>

<ul>
<li>Has anyone else implemented this kind of system?</li>
<li>Are there any gotchas when using brushed motors for servos?</li>
<li>Is it possible to servo it at low speeds if I only have the 3 integral digital Hall sensors, and no encoder? (I would prefer not to add an encoder because of the money and space cost)</li>
<li>It torque ripple likely to be a problem?</li>
</ul>
","brushless-motor servomotor"
"255","What UAV kit(s) would be suitable for a beginner roboticist with programming experience?","<p>I'm really new to robotics, however I am a programmer familiar with several different languages. I don't have a ton of money to spend and I was wondering what is a really good starter kit. </p>

<p>My criteria is for the kit to be inexpensive and <strong>powerful</strong>, in that its functionality is extensible -- something that would allow the builder to be creative and possibly invent new ways to use it, not just a glorified model kit.  </p>

<p>Being extendable to smartphones is a plus.</p>

<p>I'm not looking for something easy or introductory, just something powerful, flexible, and cost effective.</p>
","uav kit"
"256","Quadcopter Localization Beacon","<p>I want to use an RF beacon to localize my quadcopter for autolanding, when GPS is not precise enough, for example, when my driveway is only 10 feet wide, and the GPS is only showing 20-30 ft. accuracy (with a proverbial lake of lava on either side). The quadcopter would use the GPS to fly to the rough location until it had a strong enough signal off the beacon, when it would begin to use that signal to come to a landing in a precise location, referenced off said beacon. Can someone please explain to me the concepts and theories behind building the beacon and it's accompanying receiver (suitable for connection to an Arduino via any digital or analog method) and achieving, say, a 4"" or better horizontal and vertical accuracy within a 50' sphere? Minimally, the quad should have range and altitude, i.e. ""I am 10 feet away from the beacon and 2 feet above it"". How much added complexity would it take to make the robot fully position aware about the beacon, i.e. ""x ft. South, y ft. West and z ft. above it"", where the coordinate system is determined by the beacon and not linked to any sort of geographic coordinate system? If the beacon is mounted on a, say, 10 ft pole, are there any changes to be made versus having it on the ground and presuming that all activity takes place above it's x-y plane?</p>

<p>Last note-
This thing would prefferably operate in the 72MHz band, please presume that where I'm operating, there are not other devices operating on the same band.</p>
","localization quadcopter gps"
"261","What do the commutation waveforms look like for a brushless motor?","<p>I have seen waveforms for driving a brushless motor.</p>

<p><img src=""http://i.stack.imgur.com/UHO04.jpg"" alt=""Brushless motor waveform""></p>

<p>I guess this is the waveform used for the simpler block commutation. But if I want to do sinusoidal waveforms, what does the PWM signal look like now? Is there a need to carefully synchronise the edges on the three phases?</p>
","brushless-motor pwm"
"262","Effectiveness of a Mobile Robot In Relation To Mass","<p>Do mobile and/or autonomous robots become more or less effective the bigger they get? For example, a bigger robot has bigger batteries, and thus bigger motors, whereas a smaller robot has the exact opposite, making it need less energy, but also have smaller motors. Is there any known theorem that models this?</p>
","mobile-robot design dynamics"
"267","Why are capacitors added to motors (in parallel); what is their purpose?","<p>I've seen many motors having capacitors attached in parallel in bots. Apparently, this is for the ""safety"" of the motor. As I understand it, all these will do is smoothen any fluctuations--and I doubt that fluctuations can have any adverse effects on a motor. Apparently these protect the motor if the shaft is being slowed/blocked, but I fail to see how.</p>

<p>What exactly is the function of such a capacitor? What does it prevent, and how?</p>
","motor protection"
"271","Spatial tracking between two UAVs","<p>I have two Unmanned Aerial Vehicles (planes) which work well. They can fly to various waypoints automatically using GPS.</p>

<p>Now I would like them to fly together in formation. I would like them to fly side-by-side fairly close. This is too close to reliably use GPS to guarantee that they keep the correct relative positions safely, and so I am looking for another way.</p>

<p>Somehow the UAVs need to be able to measure their position and orientation in space relative to the other one. How can I do this? Is there some kind of sensor which can do this? It would need to have the following properties:</p>

<ul>
<li>6 axes (position and orientation)</li>
<li>Range 0m - 5m, (from between plane centres, but planes won't actually ever touch wingtips)</li>
<li>Works in day or night and all weather conditions</li>
<li>Light weight (This is for 1.5m wingspan RC planes, so max extra weight of about 100g)</li>
<li>Probably need about 50Hz - 100Hz refresh rate, but might get away with less, using the IMU to fill in the gaps</li>
</ul>
","sensors uav multi-agent"
"274","Low-cost servo with digital control interfaces?","<p>Some years ago, there where some projects that provided hardware and software to perform modifications on standard hobby servos to convert them to digital servos, with all the advantages that come with it. </p>

<ul>
<li><a href=""http://openservo.com/"" rel=""nofollow"">OpenServo</a> is a little outdated, and does not seem to be worked on anymore, and there is no hardware to buy.</li>
<li>Sparkfun has its <a href=""https://www.sparkfun.com/products/9014"" rel=""nofollow"">own version</a> of the OpenServo, which at least is available for buying.</li>
</ul>

<p>Do you know if there are other mods, or even complete low cost digital servos? I am mostly interested in position feedback, and servo chaining.</p>
","servos i2c"
"277","Why do I need a Kalman filter?","<p>I am designing an unmanned aerial vehicle, which will include several types of sensors: </p>

<ul>
<li>3-axis accelerometer</li>
<li>3-axis gyroscope </li>
<li>3-axis magnetometer</li>
<li>horizon sensor</li>
<li>GPS </li>
<li>downward facing ultrasound.</li>
</ul>

<p>A friend of mine told me that I will need to put all of this sensor data through a Kalman filter, but I don't understand why. Why can't I just put this straight into my micro controller. How does the Kalman filter help me about my sensor data?</p>
","kalman-filter uav"
"284","Which type of actuator will be suitable for a very strong robot arm","<p>I wish to build a robotic arm that can lift a useful amount of weight (such as 3-6kg on an arm that can extend to approx 1.25 meters). What actuators are available to accomplish this. The main factors and design points are:</p>

<ul>
<li>Not Expensive</li>
<li>5 to 6 d.o.f.</li>
<li>to be mounted on a yet to be designed mobile platform</li>
<li>battery powered</li>
<li>stronger than hobby servos (at least for the 'shoulder' and 'elbow' joints)</li>
<li>not slow to actuate</li>
</ul>
","mobile-robot robotic-arm actuator"
"287","Programming Robots with JavaScript","<p>As somebody who is spending the majority of his time programming in JavaScript, what's the best route to get into small-robotics without needing to deviate too much from my current language focus?</p>

<p>Are there any project kits or tools that make use of the JavaScript language that might make the field more approachable for developers like myself? I would even be interested in virtual environments where all code is executed in a simulation.</p>
","software programming-languages"
"299","How can the inverse kinematics problem be solved?","<p>The forward kinematics of a robot arm can be solved easily. We can represent each joint using <a href=""http://en.wikipedia.org/wiki/Denavit%E2%80%93Hartenberg_parameters"">Denavit–Hartenberg</a> transformation matrices.</p>

<p>For example, if the $i^{th}$ joint is a linear actuator, it may have the transformation matrix:</p>

<p>$T_i = \left[\begin{matrix}
1&amp;0&amp;0&amp;0\\
0&amp;1&amp;0&amp;0\\
0&amp;0&amp;1&amp;d_i\\
0&amp;0&amp;0&amp;1
\end{matrix} \right]$
where the extension length is defined by $d_i$</p>

<p>whereas, a rotating link may be:</p>

<p>$T_i = \left[\begin{matrix}
1&amp;0&amp;0&amp;L\\
0&amp;\cos\alpha_i&amp;-\sin\alpha_i&amp;0\\
0&amp;\sin\alpha_i&amp;\cos\alpha_i&amp;0\\
0&amp;0&amp;0&amp;1
\end{matrix} \right]$ where $\alpha$ is the angle, and $L$ is the length of the link.</p>

<p>We can then find the position and orientation of the end effector by multiplying all the transformation matrices: $\prod{T_i}$.</p>

<p>The question is, how do we solve the inverse problem?</p>

<p>Mathematically, for a desired end effector position $M$, find the parameters $d_i$, $\alpha_i$ such that $\prod{T_i} = M$. What methods exist to solve this equation?</p>
","inverse-kinematics kinematics joint arm"
"301","Which spline function would be best suited for the trajectory of a differential drive","<p>What's the best kind of spline that can be used for generating trajectory that can be adapted during execution time?</p>

<p>The use case is having a differential drive which has to move towards a point (x,y,theta) without stopping during the movement (e.g. no, turn toward the goal, straight move to the goal position, turn to the goal orientation). The robot is provided with a laser scanner for detecting dynamic obstacles which have to be avoided.</p>

<p>What's the best kind of controller in this case?</p>
","control motion-planning"
"309","Correct way to use Subsumption Architecture with Robot C","<p>I've been doing a lot of reading lately about <a href=""http://en.wikipedia.org/wiki/Subsumption_architecture"" rel=""nofollow"">Subsumption Architecture</a> and there are a few different ways people seem to advocate. </p>

<p>For instance some people use a global ""flag"" variable to have a task take control. Others use the <code>endTimeSlice()</code> and allow the arbiter to really choose. And I think this is correct. </p>

<p>I have this small section of <a href=""http://en.wikipedia.org/wiki/Robotc"" rel=""nofollow"">RobotC</a> code that I'm working on for a line following robot but am not sure I am doing it right as currently the track method will always take over the find method. The correct flow should be that find should guide the robot to the line using a spiral path to find the line. Once the line is found track should take over. </p>

<pre><code>task evade(){
    if(SensorValue(forwardSonarSensor) &gt; threshold){
            //box the obstruction
    }
}

task find(){
    if(SensorValue(lightSensor) &gt; threshold){
            //spiral the robot
    }
}

task track(){

    if(SensorValue(lightSensor) &lt; threshold){
            //go straight
    }else{
                //execute turns to follow the line
    }
}

task main(){
    while(true){
        StartTask(evade,9);
        StartTask(track,8);
        StartTask(find,7);
        wait1Msec(250);
    }
}
</code></pre>

<p>I've just used some comments here rather than the actual code to keep it brief. Are my if statements not good enough as conditions because when the robot is off the line, <code>track()</code> takes over. Is this due to the else statement within track? If so, how to have <code>track()</code> perform turns when it looses the line without taking over from forage at the start of the program? </p>
","mobile-robot software two-wheeled robotc"
"317","How much can working with CNC machines teach you about robotics?","<p>I have built a few simple X/Y/Z CNC machines.  I've learned about G-Code, motor control, firmware and open loop systems.  I see machines like rovers, big dog and factory arms that seem incredibly complex by comparison, yet they don't seem that magical any more.  </p>

<p>What are the important skills to pick up from working with CNC machines?  What's the next logical thing to learn?  What things would CNC machines never teach me?</p>
","control"
"323","IPC-Bridge problem","<p>Is anyone able to help me out getting IPC-bridge working on my ubuntu lucid installation (with matlab 2012a)? I'm not being able to finish the last step on here (Compiling the messages folders): <a href=""https://alliance.seas.upenn.edu/~meam620/wiki/index.php?n=Roslab.IpcBridge#Installation"" rel=""nofollow"">https://alliance.seas.upenn.edu/~meam620/wiki/index.php?n=Roslab.IpcBridge#Installation</a></p>

<p>I'm able to rosmake the ipc_bridge_ros, however when i enter this ""roscd ipc_roslib &amp;&amp; make"", it seems meX does not recognize the commands. Here is what i get (screen shot): <a href=""http://img13.imageshack.us/img13/6031/screenshot20121108at191.png"" rel=""nofollow"">http://img13.imageshack.us/img13/6031/screenshot20121108at191.png</a></p>

<p>NOTE: i'm going to use IPC-bridge so that i can control a pioneer 3DX and implement a Fast-slam algorithm in matlab.</p>
","mobile-robot software slam ros"
"327","Learning Algorithms for Walking Quadruped","<p>I'm building a 4 legged robot (quadruped) with 3 Degrees of freedom per leg.
The goal of my project is to make this robot able to learn how to walk.
What learning algorithms will I need to implement for it to work?</p>

<p>I'm using an <a href=""http://www.arduino.cc/"" rel=""nofollow"">Arduino Uno</a> for the microcontroller.</p>
","arduino microcontroller machine-learning walking-robot"
"331","Robot Serial Communication Error","<p>We are using a Koro robot for our PC based automation solution. But sometimes the robot is getting the command but refuses to respond. Then I get a serial communication timeout error. The error is happening for a random type of commands and it is also not happening all the time making the troubleshooting difficult.</p>

<p>I doubt the driver problem. How do you approach this problem.?</p>

<p>Thanks</p>
","logic-control"
"338","Is there a list of maneuvers related to control of a tracked platform?","<p>I have a platform with two tracks and two motors. Each one uses an electronic speed control with ""double tap to reverse"". Each ESC takes an input pulse train frequency from at 1500 neutral +/-700.</p>

<p><strong>I'm interested in learning if there are algorithms or a list of commands I can use to control how such platform executes maneuvers.</strong> For example:</p>

<ul>
<li>Lock one thread and have the platform rotate by using the other one</li>
<li>Have two treads rotate in opposite directions</li>
<li>Execute a U turn</li>
</ul>

<p>I'm struggling with expressing in code how such maneuvers should be executed. There's a ""dead"" zone around the 1500 pulse train frequency where the ESC output is too weak to cause the platform to move. The double tap to reverse also makes it tough to understand for how long each track should be turned off.</p>

<p>Thank you for your input</p>
","control motion-planning tracks"
"341","Tendon longevity","<p>I am thinking of developing a tendon driven robot manipulator for an industrial application that requires a high level of reliability. However, I am aware that tendons in a robot are prone to wear and tear, and failure.</p>

<p>How can I go about selecting a suitable tendon material (steel, kevlar, spectra, etc.) and use it appropriately?</p>

<p>Have any studies been undertaken to examine longevity and failure patterns in robotic tendon materials?</p>

<p>If I were to perform tests on materials myself, how can I perform those tests efficiently, and make best use of the testing time (learn as much as possible about tendon failure in a reasonable length of time).</p>
","failure reliability"
"342","What are some common mistakes that robots make?","<p>Is there a taxonomy of errors that are common in robotics? Things that come to mind but I don't have names for are:</p>

<ul>
<li>Getting stuck in a stable infinite loop</li>
<li>Going into an unstable feedback loop (A balancing robot overcompensating more with each correction)</li>
<li>An inability to generalize between tasks (Pick up a bowl vs pick up a glass)</li>
<li>An inability to generalize between 'similar' sensory inputs.</li>
<li>Causing damage to itself or its environment.</li>
</ul>

<p>These would be things that make a robot look 'stupid' to a non-roboticist. If you're curious I want to have this list so I can then prepare a clear answer ready for people who don't know why these various things are hard.</p>
","errors"
"344","With a 6-axis robot, given end-effector position and range of orientations, how to find optimal joint values","<p>Given a six-axis articulated robot arm holding a tool at its end-effector,  if I have a desired tool position and tool orientation,  there will be exactly 1 solution to the inverse kinematics equation for the robot to reach that position.<br>
(or rather up to 16 different solutions, depending on range of the joints)</p>

<p><img src=""http://i.stack.imgur.com/CkYqu.gif"" alt=""http://en.wikipedia.org/wiki/Robotic_arm""></p>

<p>But if the robot is holding something like a pen, and I want the robot to mark a specific point with that pen on the target, then I do not care how the pen is oriented, as long as it is perpendicular to the marked surface.</p>

<p>So the inverse-kinematics equation will have infinitely many solutions.</p>

<p>How can I pick among these solutions the joint configuration that is closest to the current configuration: the one that will require the least amount of movement to reach?<br>
(or the joint configuration that is optimal according to some other similar criterion, such as that all joint angles are furthest from their maximum and minimum?)</p>
","localization motion-planning industrial-robot inverse-kinematics kinematics"
"354","Is subsumption architecture still an active area of research?","<p>I am interested in learning more about <a href=""http://en.wikipedia.org/wiki/Subsumption_architecture"" rel=""nofollow"">subsumption architecture</a>. I have read a number of books that talk about the idea but none of them go into great detail. I have also read a fair number of Dr. Brooks papers on the topic however he hasn't published much on the topic in recent years.</p>

<p>Is this still an active area of research? Are there are any must read papers on the topic?</p>
","control research"
"358","restricting range of motion with complex constraints","<p>I am looking for a way to restrict a robot's range of motion, using complex constraints such as not tearing of a cable attached to the robot.</p>

<p>Take an articulated 6-axis robot arm as shown below, with attached cable (red), fixed at points X (before axis A4) and Y (after axis A6).</p>

<p><img src=""http://i.stack.imgur.com/ipApa.png"" alt=""enter image description here""></p>

<p>The cable will limit the range of movement for the robot. It can stretch and bend only to some extend, but something like a full 360° turn of axis A4, with all other axes remaining as they are in the picture, will tie the cable around the arm and rip it off.<br>
If joint A5 is at 0°, then A4 and A6 can still move the full 360°, but they cannot diverge too much from each other, as that would twist the cable. If A5 is tilted, the relationship becomes even more complicated.</p>

<p>How can you express such a constraint?  </p>

<p>It is not a simple joint constraint, where you can independently limit the range of the joints, and it is also not a positional constraint, where you can define a region the robot must not enter. Checking a start and a goal posture is not sufficient, since along the path from start to goal posture there may still be a posture that puts too much strain on the cable.</p>

<p>Without limiting the robot to a small set of pre-tested paths, how can you limit the robot to movements that will not rip off the cable?</p>

<p>What are the standard techniques used for this sort of problem?</p>
","motion-planning industrial-robot joint"
"359","What kind of performance can I expect when using an Extended Kalman Filter for calibration and localization?","<p>Currently I have a tricycle style robot that uses an extended kalman filter in order to track 6 state variables. The inputs to the system are a steer encoder, a distance encoder, and a rotating laser that returns bearing only information to known landmarks. Currently both encoders are located on the main wheel (The one that steers, and is also powered).</p>

<p>The 6 variables tracked by the Kalman Filter are X, Y, Heading, Distance Scaling (calibration of the distance encoder), Steer Calibration (offset of the steer encoder), and finally a bearing calibration of a rotating laser. </p>

<p>With this kind of system we put together a vehicle give it a known good location with plenty of landmarks, drive it around a bit, and end up with a well calibrated vehicle that can drive extended distances reliably with few landmarks. Its simple and it works great. Over time if an encoder drifts it will automatically follow the drift and adjust. </p>

<p>We are now attempting to apply the same principles to a robot with multiple steer and drive wheels. In this case the vehicle will be able to move in any direction, spin in place, etc. . Each steer/drive wheel will have its own steer and distance encoder that each need to be calibrated. </p>

<p>Can I expect to get the same kind of reliability and performance out of the more complex system? Are there any common pitfalls to look out for when expanding a kalman filter to include more variables? Is there a risk of it settling on sub-optimal values?</p>
","localization kalman-filter"
"361","Programming a line following robot with reinforcement learning","<p>I am considering programming a line following robot using reinforcement learning algorithms. The question I am pondering over is how can I get the algorithm to learn navigating through any arbitrary path?</p>

<p>Having followed the <a href=""http://webdocs.cs.ualberta.ca/~sutton/book/the-book.html"" rel=""nofollow"">Sutton &amp; Barto Book</a> for reinforcement learning, I did solve an exercise problem involving a racetrack where in the car agent learnt not to go off the track and regulate its speed. However, that exercise problem got the agent to learn how to navigate the track it trained on.</p>

<p>Is it in the scope of reinforcement learning to get a robot to navigate arbitrary paths? Does the agent <strong>absolutely</strong> have to have a map of the race circuit or path? What parameters could I possibly use for my state space?</p>
","machine-learning artificial-intelligence reinforcement-learning line-following"
"369","Are inverse kinematics and reinforcement learning competitive techniques?","<p>Are <a href=""http://en.wikipedia.org/wiki/Inverse_kinematics"" rel=""nofollow"">inverse kinematics</a> and <a href=""http://en.wikipedia.org/wiki/Reinforcement_learning"" rel=""nofollow"">reinforcement learning</a> techniques contending techniques to solve the same problem viz. movement of robotic manipulators or arm?</p>

<p>By a glance through the wikipedia article, it appears that inverse kinematics seems to attempt to achieve a <a href=""http://en.wikipedia.org/wiki/Equation_solving"" rel=""nofollow"">solution</a>  as opposed to reinforcement learning which attempts to <a href=""http://en.wikipedia.org/wiki/Optimization"" rel=""nofollow"">optimizes</a> the problem. Have I misunderstood anything?</p>
","inverse-kinematics reinforcement-learning machine-learning"
"380","Processor and command interface preference for a robot arm","<p>I want to build a robot arm that'll be approximately 1.25 meter long and will be able to lift up to 2 kilograms. It'll have 6 dof and it is an expensive project. And most importantly, i am the only programmer in this brand new robotics facility of ours. :)</p>

<p>The robot that i want to build will be led by Inverse Kinematics, so with all these parameters and matrices, i think that i'll need a tough processor (Not so sure).</p>

<p>Assuming that my robots control interface will be on an Android tablet, i thought that i also could develop my program for Android, and send necessary commands to the control chip via RS-232 interface.</p>

<p>So, my question is, are standart 1 GHz Android tablets suitable for these tasks? If not, has anybody got an advice for me?</p>
","software inverse-kinematics arm rs232"
"382","How to fuse linear and angular data from sensors","<p>My team and I are setting up an outdoor robot that has encoders, a commercial-grade <a href=""http://en.wikipedia.org/wiki/IMU"">IMU</a>, and <a href=""http://en.wikipedia.org/wiki/Global_Positioning_System"">GPS</a> sensor. The robot has a basic tank drive, so the encoders sufficiently supply ticks from the left and right wheels. The IMU gives roll, pitch, yaw, and linear accelerations in x, y, and z. We could later add other IMUs, which would give redundancy, but could also additionally provide angular rates of roll, pitch, and yaw. The GPS publishes global x, y, and z coordinates.</p>

<p>Knowing the robot's x y position and heading will useful for the robot to localize and map out it's environment to navigate. The robot's velocity could also be useful for making smooth movement decisions. It's a ground-based robot, so we don't care too much about the z axis. The robot also has a <a href=""http://en.wikipedia.org/wiki/LIDAR"">lidar</a> sensor and a camera--so roll and pitch will be useful for transforming the lidar and camera data for better orientation. </p>

<p>I'm trying to figure out how to fuse all these numbers together in a way that optimally takes advantage of all sensors' accuracy. Right now we're using a <a href=""http://en.wikipedia.org/wiki/Kalman_filter"">Kalman filter</a> to generate an estimate of <code>[x, x-vel, x-accel, y, y-vel, y-accel]</code> with the simple transition matrix:</p>

<pre><code>[[1, dt, .5*dt*dt, 0,  0,        0],
 [0,  1,       dt, 0,  0,        0],
 [0,  0,        1, 0,  0,        0],
 [0,  0,        0, 1, dt, .5*dt*dt],
 [0,  0,        0, 0,  1,       dt],
 [0,  0,        0, 0,  0,        1]]
</code></pre>

<p>The filter estimates state exclusively based on the accelerations provided by the IMU. (The IMU isn't the best quality; within about 30 seconds it will show the robot (at rest) drifting a good 20 meters from its initial location.) I want to know out how to use roll, pitch, and yaw from the IMU, and potentially roll, pitch, and yaw rates, encoder data from the wheels, and GPS data to improve the state estimate. </p>

<p>Using a bit of math, we can use the two encoders to generate x, y, and heading information on the robot, as well as linear and angular velocities. The encoders are very accurate, but they can be susceptible to slippage on an outdoor field. </p>

<p>It seems to me that there are two separate sets of data here, which are difficult to fuse:</p>

<ol>
<li>Estimates of x, x-vel, x-accel, y, y-vel, y-accel</li>
<li>Estimates of roll, pitch, yaw, and rates of roll, pitch, and yaw</li>
</ol>

<p>Even though there's crossover between these two sets, I'm having trouble reasoning about how to put them together. For example, if the robot is going  at a constant speed, the direction of robot, determined by its x-vel and y-vel, will be the same as its yaw. Although, if the robot is at rest, the yaw cannot be accurately determined by the x and y velocities. Also, data provided by the encoders, translated to angular velocity, could  be an update to the yaw rate... but how could an update to the yaw rate end up providing better positional estimates?</p>

<p>Does it make sense to put all 12 numbers into the same filter, or are they normally kept separate? Is there already a well-developed way of dealing with this type of problem?</p>
","sensors kalman-filter sensor-fusion"
"390","Can I use IMUs to improve the position/posture measurement of fingers in a ""data glove""?","<p>I have been using the Cyberglove to control a humanoid robot hand, but found it disappointing as it doesn't measure the posture of the human hand very accurately.</p>

<p><img src=""http://i.stack.imgur.com/T7TYQ.png"" alt=""Cyberglove""></p>

<p>I have been wondering about the possibility of using Inertial Measurement Units (<a href=""http://en.wikipedia.org/wiki/Inertial_measurement_unit"" rel=""nofollow"">IMU</a>s) mounted on the fingers to track position and measure posture. But I'm not sure how feasible it is.</p>

<ul>
<li>Would an IMU return enough data to make tracking reliable in all circumstances?</li>
<li>Would it be possible to fool the system into incorrectly tracking the fingers?</li>
<li>Might it be possible to get away with using simple 3-axis accelerometers, or would it need 9-axis (accelerometer, gyro, and magnetometer)?</li>
</ul>
","imu sensor-fusion hri"
"397","Resources for learning basics of Robotics","<p>I am interested in Robotics. Practically I have no idea about Robotics. I want to start learning the basics of Robotics. But I am confused what to start with. So I need suggestions about what will be the best resources to start Robotics with. That may be books, sites, or others. Please help me with this.</p>
","books"
"403","What is the current state of the Google Self Driving Car Project?","<p>I am aware of the legislation's in Nevada, but what is happening with the technology currently. When is it expected to be commercialized ?</p>
","ugv"
"405","Selecting an accelerometer for Deduced Reckoning","<p>I have never used an accelerometer before, but I am aware that they come with I2C, SPI and analog outputs. If I choose to use an I2c or SPI, device, will I accumulate errors due to communication time?</p>

<p>Is the fast sampling of an analog signal likely to get me a more accurate deduced position than using am I2C?</p>

<p>Will this be true for </p>

<ol>
<li>A robot moving in a room </li>
<li>A robot moving in an outdoor terrain and is likely to slip and roll down a slope.</li>
</ol>

<p>Also, I have no sense of Gs. I tried to move my hand around fast with my phone running andro-sensor in my fist and saw that the readings saturated at 20m/s<sup>2</sup>. What G can I expect my robot to experience if it is hit by another fat moving bot or bumped by a fast walking human?</p>
","sensors deduced-reckoning navigation accelerometer"
"413","Questions about quadcopter and radio controller","<p>I have not bought any parts yet, but I am making my own quadcopter. I have done the research and know all about the parts that I need, but many guides are sponsored and cost thousand(s) of euros/dollars while not explaining things entirely clearly.</p>

<p>Firstly, I have found this <a href=""http://www.hobbyking.com/hobbyking/store/__24723__Hobbyking_KK2_0_Multi_rotor_LCD_Flight_Control_Board.html"">flight control board</a>. Would I need another microcontroller (such as the Arduino nano) for it to work? (IF ANYONE has experience with this board, let me know!).</p>

<p>Secondly, would the above board work with this <a href=""http://www.hobbyking.com/hobbyking/store/uh_viewitem.asp?idproduct=9042&amp;aff=655899"">radio controller</a>. Are controllers universal?</p>

<p>(Please tell me if I'm not in the right section here, or if this doesn't count as a relevant topic).</p>
","microcontroller quadcopter radio-control"
"416","What is the best way to power a large number (27) servos at 5 V?","<p>I apologize if this question may sound a little vague. I am working on a robotics project that will contain 27 servos of various sizes and I am having trouble figuring it how they should be powered.</p>

<p>I was hoping to use several (3-6) 5 W 18650 battery boxes to power them, but the smallest motors would use 2.5 W each, so 1 battery box can only power two. The larger servos, obviously, use even more current, so this plan of using a small number of 18650's becomes infeasible.</p>

<p>There is not enough room on the robot for a 12 V car battery, and adding one would require recalculating the sizes of the servomotors that would be needed. Furthermore, I am not sure how to convert the 12 V it gives down to 5 V for the servomotors.</p>

<p>P.S. What about the stall current of the motors? Should the power supply be able to supply the stall current of all the motors it supplies (at the same time) or just the working current? Should I use a fuse to handle when (if?) the servomotors stall? Should I use a fuse or a circuit breaker? Do they make 5 V fuses? If so, where can I get one?</p>

<p>Something like a larger version of the 18650 box would be most preferable.</p>
","design servos power"
"419","How do you implement an INS from an accelerometer and (optionally) gyros and a magnetometer?","<p>I'm building a walking robot that will need to know when it moves forward. I'm using on-board intelligence and I plan on using accelerometers, gyros, and magnometers (if needed) to be able to detect if the robot moves forward. The problem is, I dont know how to program an Internal Navigation System or an IMU.  What software algorithms are needed?</p>

<p>To clarify my problem, I need to know how to program the micro controller to read the sensors and be able to tell if the robot has displaced itself <em>forward</em> since a previous measurement.
Also if I used <a href=""http://store.diydrones.com/ArduIMU_V3_p/kt-arduimu-30.htm"" rel=""nofollow"">this sensor board</a> (or similar) could I use it to determine the displacement. </p>
","software imu deduced-reckoning artificial-intelligence"
"431","Inverse Kinematics in Java","<p>I'm planning to write an Inverse Kinematics controlled 6 dof virtual robot for Android. I did some research on packages avaliable and couldn't choose the right one which will satisfy my needs on this project. I've seen a work with Eigen in C++, and used it, it was just fine. But since i'm not so experienced in Java, i wanted to ask before i start, if someone knows some appropiate packages for these operations.</p>

<p>Here is what i found so far:</p>

<p>JAMA,
Vecmath,
Jmathtools,
EJML,
JAMPACK
I ask this because i really dont want to get stuck in the middle of my project. Thanks in advance.</p>
","inverse-kinematics programming-languages"
"433","Visual Odometry options?","<p>What are the pros/cons of the different <a href=""http://en.wikipedia.org/wiki/Visual_odometry"" rel=""nofollow"">visual</a> <a href=""http://en.wikipedia.org/wiki/Odometry"" rel=""nofollow"">odometry</a> options? </p>

<ul>
<li>Stereo Camera</li>
<li>Optical Flow</li>
<li><a href=""http://en.wikipedia.org/wiki/Simultaneous_localization_and_mapping"" rel=""nofollow"">SLAM</a></li>
<li>other?</li>
</ul>

<p>Criteria:</p>

<ul>
<li>how well it performs vs other odometry options/sensors (lidar, radar)</li>
<li>sensor fidelity</li>
<li>computation</li>
<li>accuracy</li>
<li>precision</li>
<li>drift</li>
<li>native resilience and repeadability in sensor noise or vehicle speed</li>
<li>ease of integrating with <a href=""http://en.wikipedia.org/wiki/Inertial_measurement_unit"" rel=""nofollow"">IMU</a>/GPS</li>
<li>etc</li>
</ul>

<p>In general, of course, because there are a lot of different ways the trade-offs go when we get into specifics about applications and hardware.  I'm asking out of curiosity, not for designing anything in particular.</p>
","mobile-robot localization computer-vision odometry"
"436","How do I model a robot?","<p>The answers I received to the question on <a href=""http://robotics.stackexchange.com/questions/361/programming-a-line-following-robot-with-reinforcement-learning"">training a line following robot using reinforcement learning techniques</a>, got me to think on how to train a robot. I believe there are essentially two ways -</p>

<ol>
<li>Train the physical robot.</li>
<li>Model the robot and simulate the training.</li>
<li>Did I miss something?</li>
</ol>

<p>Approach 2 is definitely the better approach. However, a priori knowledge of the motion (response), a certain PWM signal (stimulus) would cause when the robot is in a given state is required. The motion caused by a PWM signal may depend on the (<strong>1</strong>) current battery voltage, (<strong>2</strong>) the mass of the robot and the (<strong>3</strong>) current velocity (did I miss something?).</p>

<p>How do I model such a robot? And how do I model it quick? If I change the battery or add a few boards and other peripherals and change the mass of the robot, I would have to remodel and retrain the robot. Can I do this by providing some random stimulus PWMs and measuring the response?</p>

<p><strong>added:</strong> My related <a href=""http://dsp.stackexchange.com/questions/6006/how-do-i-compute-the-imuplse-response-of-a-robot"">question in dsp.SE</a></p>

<p><strong>Update:</strong> A suggested edit to the title by <a href=""http://robotics.stackexchange.com/users/350/ian"">Ian</a> worth mentioning - ""<em>How do I model train a robot so that if its dynamics change, it does not need complete re-training?</em>"" I think this is a good question too but different from the one I am asking here. I am okay with re-training for now.</p>
","reinforcement-learning pwm"
"442","How can the dynamic effects of motor current on a digital compass be characterized and compensated for?","<p>Digital compasses (magnetometers) require a hard/soft iron calibration in order to be accurate.  This compensates for the magnetic disturbances caused by nearby metal objects -- the robot's chassis.</p>

<p><img src=""http://i.stack.imgur.com/vrsV8.png"" alt=""enter image description here""></p>

<p>(image from <a href=""http://diydrones.com/forum/topics/magnetometer-soft-and-hard-iron-calibration"">http://diydrones.com</a>)</p>

<p>However, digital compasses are also susceptible to the electric fields caused by the relatively high amount of current drawn by motors.  </p>

<p>In order to get an accurate compass reading, what is the best way to measure (and compensate for) the interference caused by changing motor current levels?</p>
","mobile-robot sensors"
"445","How to obtain dense point clouds from stereo cameras?","<p>I am trying to use a stereo camera for scene reconstruction, but I can usually only obtain sparse point clouds (i.e. over half the image does not have any proper depth information). </p>

<p>I realize that stereo processing algorithms rely on the presence of texture in the images and have a few parameters that can be tweaked to obtain better results, such as the disparity range or correlation window size. As much as I tune these parameters, though, I am never able to get results that are even remotely close to what can be obtained using an active sensor such as the Kinect.</p>

<p>The reason why I want that is because very often point clouds corresponding to adjacent regions don't have enough overlap for me to obtain a match, so reconstruction is severely impaired.</p>

<p>My question to the Computer Vision experts out there is the following: <strong>what can I do to obtain denser point clouds in general</strong> (without arbitrarily modifying my office environment)?</p>
","slam computer-vision"
"446","How much torque do I need for a CNC machine?","<p>I have a handful of 31.2oz-in stepper motors (<a href=""http://www.mouser.com/Search/ProductDetail.aspx?R=HT17-268Dvirtualkey55010000virtualkey834-HT17-268D"">Mouser.com - Applied Motion: HT17-268D</a>), and I was curious if they would be big enough to run a 3D printing/cutting/etching type (think RepRap) of machine. I had in mind to attach them via  a simple gear to a screw-type drive to run the tool head back and forth. </p>

<ul>
<li>Maximum bed size would probably be ~1.5'<sup>3</sup>. </li>
<li>Heaviest tool head would be something about half the weight of a Dremel tool.</li>
<li>Hardest substances I would use it on would probably be hardwoods (with high speed cutter) and copper (for PCB etching).</li>
</ul>

<p>How do I figure the amount of torque needed to drive the head, and would the motors that I already have be big enough to do the job?</p>
","stepper-motor reprap"
"453","How can one determine whether a LiPo battery is going bad?","<p>In our lab we use LiPo batteries to power our quadrotors. Lately we have been experiencing stability issues when using certain batteries. The batteries seem to charge and balance normally and our battery monitor indicates they are fine even when putting them under load. However when we attempt to fly the quadrotor with one of these batteries, manually or autonomously, it has a severe tendency to pitch and/or roll. My guess is that the battery is not supplying sufficient power to all the motors which brings me to my question. Is this behavior indicative of a LiPo going bad? If so what is the best way to test a battery to confirm my suspicions?</p>
","battery troubleshooting"
"461","In PID control, what do the poles and zeros represent?","<p>Whenever I read a text about control (e.g. PID control) it often mentions 'poles' and 'zeros'. What do they mean by that?  What physical state does a pole or a zero describe?</p>
","control pid"
"463","Connecting a 6 pole motor to a motor driver?","<p>We are trying to power <a href=""http://www.e-fliterc.com/Products/Default.aspx?ProdID=EFLM3032DFA"" rel=""nofollow"">this motor</a> with <a href=""http://www.coolcomponents.co.uk/catalog/vnh2sp30-motor-driver-carrier-md01b-p-228.html"" rel=""nofollow"">this motor driver</a> , using a 11.1V 2.2Ah lithium-ion polymer battery.</p>

<p>(We're in over our heads with this and really need the help) </p>

<p>We checked with the company (E-flite) and the motor is definitely DC -- we're a bit confused as to the purpose of three wires, and how we should connect them to the motor. </p>

<p>Any help would be appreciated.</p>
","brushless-motor driver"
"469","Is there a working implementation of ""Navigation Among Movable Obstacles"" for a bi-pedal robot?","<p>I would like to have a better understanding of work in the field of ""Navigation Among Movable Obstacles"". I started off with <a href=""http://www.ri.cmu.edu/pub_files/pub4/stilman_michael_2007_1/stilman_michael_2007_1.pdf"" rel=""nofollow"">Michael Stilman's thesis under James Kuffner</a>, but that has not yet sated my appetite.</p>

<p>I am currently trying to simulate a scenario where debris (Tables and Table parts) from a disaster scenario block pathways. The debris forms part of a movable obstacle. The robot which will be used is a bipedal humanoid.</p>

<p>The thesis describes an approach to define the search space of possible actions leading from the start point to the goal. However, it assumes a mobile robot which works via gliding. </p>

<p>I think the state space definitions would change for a bi-pedal robot. Why is why I wonder what other work is being done in this field. Perhaps the work of other research groups could give me clues as to how to design and perhaps reduce the search space for a bipedal humanoid robot.</p>

<p>An implementation of Navigation among Movable Obstacles would also aid me in understanding how to reduce the search space of possible actions.</p>

<p>So does anyone know of a working implementation of Navigation among movable obstacles? </p>

<p>Any supporting information about other professors or research groups working on similar problems would also be very useful.</p>

<p>I hope this edit is sufficient for the problem description.</p>
","navigation"
"472","Humble beginnings","<p>I want to learn robotics and build my first robot. I am looking for a well supported kit that is simple enough and can walk me through, the initial stages of my intellectual pursuit in Robotics. I want to be able to do the basic things first and build a solid foundation in robotics. And then I want to be able to use the solid foundation, to gain confidence in my ability to build new and interesting robotic contraptions. In other words, I want to be able to follow the rules off the game to gain a solid foundation and then once I'm comfortable with what I know, I want to break free of the rules and start making my own robots. </p>

<p>I would like help with 2 things,</p>

<ol>
<li><p>I would like to begin my robotics learning with a good kit that can walk me through my initial stages. I expect that this initial stage might take quite a while. So, any recommendations for how I can start and/or what kit I can buy, to get my feet wet, would be helpful.</p></li>
<li><p>I would like suggestions for ""other"" actions I can take, that will set me on a path to gain confidence in my knowledge of robotics.</p></li>
</ol>

<p>A little bit about myself. I have a BS and MS in IT. So I am not new to programming. I like to code in golang and haskell. I do not know if it is possible, but it would be awesome if I can write the software aspect of all my robotic projects in haskell.</p>

<p>Thanks</p>
","kit"
"474","USB instead of RS232","<p>RS232 is not popular as it used to be and is mainly replaced by USB [<a href=""http://en.wikipedia.org/wiki/RS-232#Role_in_modern_personal_computers"">wikipedia</a>]. Problems such as mentioned in <a href=""http://robotics.stackexchange.com/q/331/158"">this question</a> doesn't help its reputation either.</p>

<p>In a new system design therefore, one could think of using USB instead of Serial Port for communication. However, it still seems like RS232 is the serial communication protocol/port of choice.</p>

<p>Why is that? I understand changing old machinery that work with RS232 is costly, but what prevents <em>new</em> system designers from using USB instead of RS232?</p>
","rs232 usb"
"478","How can I optimise control parameters for a stepper motor?","<p>As an industrial roboticist I spent most of my time working with robots and machines which used brushless DC motors or linear motors, so I have lots of experience tuning PID parameters for those motors.</p>

<p>Now I'm moving to doing hobby robotics using stepper motors (I'm building my first <a href=""http://tvrrug.org.uk/"">RepRap</a>), I wonder what I need to do differently. </p>

<p>Obviously without encoder feedback I need to be much more conservative in requests to the motor, making sure that I always keep within the envelope of what is possible, but how do I find out whether my tuning is optimal, sub optimal or (worst case) marginally unstable?</p>

<p>Obviously for a given load (in my case the extruder head) I need to generate step pulse trains which cause a demanded acceleration and speed that the motor can cope with, without missing steps.</p>

<p>My first thought is to do some test sequences, for instance:</p>

<ul>
<li>Home motor precisely on it's home sensor.</li>
<li>Move $C$ steps away from home slowly.</li>
<li>Move $M$ steps away from home with a conservative move profile.</li>
<li>Move $N$ steps with the test acceleration/speed profile.</li>
<li>Move $N$ steps back to the start of the test move with a conservative move profile.</li>
<li>Move $M$ steps back to home with a conservative move profile.</li>
<li>Move $C$ steps back to the home sensor slowly, verifying that the sensor is triggered at the correct position.</li>
<li>Repeat for a variety of $N$, $M$, acceleration/speed &amp; load profiles.</li>
</ul>

<p>This should reliably detect missed steps in the test profile move, but it does seem like an awfully large space to test through however, so I wonder what techniques have been developed to optimise stepper motor control parameters.</p>
","control stepper-motor tuning"
"479","Particle filters: How to do resampling?","<p>I understand the basic principle of a particle filter and tried to implement one. However, I got hung up on the resampling part. </p>

<p>Theoretically speaking, it is quite simple: From the old (and weighted) set of particles, draw a new set of particles with replacement. While doing so, favor those particles that have high weights. Particles with high weights get drawn more often and particles with low weights less often. Perhaps only once or not at all. After resampling, all weights get assigned the same weight.</p>

<p>My first idea on how to implement this was essentially this:</p>

<ol>
<li>Normalize the weights</li>
<li>Multiply each weight by the total number of particles</li>
<li>Round those scaled weights to the nearest integer (e.g. with <code>int()</code> in Python)</li>
</ol>

<p>Now I should know how often to draw each particle, <em>but</em> due to the roundoff errors, I end up having <em>less particles</em> than before the resampling step. </p>

<p>The Question: How do I ""fill up"" the missing particles in order to get to the same number of particles as before the resampling step? Or, in case I am completely off track here, how do I resample correctly?</p>
","localization particle-filter"
"483","Strategies for managing power on electrical systems for mobile robots","<p>What are some good strategies to follow while designing power supply for electrical systems on mobile robots?</p>

<p>Such robots typically comprise of systems with</p>

<ul>
<li>microprocessor, microcontroller, DSP, etc units and boards along with immediate peripherals</li>
<li>Motor control </li>
<li>Analog Sensors(proximity, audio, voltage, etc)</li>
<li>Digital Sensors (Vision, IMU, and other exotica)</li>
<li>Radio comm circuits (Wifi, Bluetooth, Zigbee, etc)</li>
<li>Other things more specific to the purpose of the robot being designed.</li>
</ul>

<p>Are there unified approaches/architectural rules to designing power systems which can manage clean power supply to all these various units which may be distributed across boards, without issues of interference, common ground, etc? Furthermore, also including aspects of redundancy, failure management, and other such 'power management/monitoring' features?</p>

<p>well explained examples of some such existing power systems on robots would make for excellent answers.</p>
","mobile-robot electronics"
"488","Matlab 'system' function with ROS","<p>Is it possible to use the matlab's ""system"" function to call ROS commands?</p>

<p>For example, using:
system('rostopic pub /cmd_vel geometry.msgs.Twist {....}
or system('rospack find ipc_bridge)</p>

<p>I'm trying to send some commands to ROS without using something like IPC-Bridge.</p>

<p>PS: I know, however, that I need to use IPC-Bridge to subscribe to topics.</p>
","mobile-robot software ros"
"494","More powerful alternatives to Lego Mindstorm NXT 2.0?","<p>I'm interested to build Robot from my imagination, and I was looking to purchase a robotic kit.</p>

<p>I find the Lego Mindstorm NXT 2.0 really interesting for many reasons : You can plug whatever brick you want, and you can develop in the language you want.</p>

<p>I am a developer, and the use of this kind of robotic would be interaction mostly (not moving, so the servo motors are useless to me, at least now).</p>

<p>But regarding the spec of the NXT main component, I feel it's a bit low (proc, ram &amp; rom).</p>

<p>That made me wonder if any of you know something similar (where I can plug whatever I want on it, and most importantly, program the reaction), but with a more powerful hardware ?</p>

<p>Price will also be a limitation : I like the NXT also because I can build what I want under 300 USD. I don't want to spend 10k USD on my first kit, but I would appreciate buying a better piece of robotic if the price isn't too distant from the NXT's.</p>

<p>Do you have some alternatives to check out ?</p>

<p>Thanks for your help !  :)</p>
","nxt"
"497","Collaborative Behavior: Implementing a Bucket Brigade With Robot Arms","<p>I was wondering whether something like this is possible: A block of ice(say) needs to be transferred piece by piece from a source to a destination with the help of 5 robots standing in a straight line between the source and destination. The first robot picks up a piece of the block from the source and checks if the next robot in line is busy. If yes, it waits for it to complete its task and proceeds, otherwise it transfers the piece and goes back to collect another piece. Please help me on implementing this if it is possible, as I am thinking to make it a project topic.</p>

<p>to clear out the confusions, here's a smaller prototype of the project i'm thinking,</p>

<p>i have two cars, one wired, another wireless. the wired car is the master here, the wireless, the slave. through a remote, i send a command to the wired car to instead command the wireless car to move forward. the wired car will then check if the wireless slave is already executing some previously given command or no, and accordingly send the command.</p>

<p>conversely, the master may send the command as soon as it receives it, it's on the slave now to complete the task it's doing, and then execute the command it just received.</p>
","mobile-robot multi-agent"
"499","I don't understand Integral part of PID controller","<p>I dont understand integral part of PID controller. Let's assume this pseudocode from Wikipedia:</p>

<pre><code>previous_error = 0
integral = 0 
start:
  error = setpoint - measured_value
  integral = integral + error*dt
  derivative = (error - previous_error)/dt
  output = Kp*error + Ki*integral + Kd*derivative
  previous_error = error
  wait(dt)
  goto start
</code></pre>

<p>Integral is set to zero in the beginning. And then in the loop it's integrating the error over the time. When I make a (positive) change in setpoint, the error will become positive and integral will ""eat"" the values over the time (from the beginning). But what I dont understand is, when error stabilizes back to zero, the integral part will still have some value (integrated errors over time) and will still contribute to the output value of controller, but it should not, because if error is zero, output of PID should be zero as well, right?</p>

<p>Can somebody explain me that please?</p>
","control pid"
"502","Is there a place for posting ""look at what I did"" videos?","<p>Robots are somewhat videogenic, and the old saying ""show me, don't tell me"" is especially applicable.</p>

<p>But of course, a video is not a <em>question</em>, so it doesn't fit the Stack Exchange format.  Maybe video links would be more suitable in a CodeProject post.  It just seems like this board hits the right cross-section of people, whose projects I would be interested in seeing.</p>
","untagged"
"504","Choosing motors for 2 wheel drive robot","<p>I am making a 2 wheel drive robot.</p>

<p>Suppose I know that my robot is going to weight x kg when finished and I know the diameter of the wheels y (geared motors will be connected directly to the wheels). I can choose from several geared motors and I know the peak torque of each motor and the idling speed.</p>

<p>How can I calculate the load that a specific motor can take? I.e. will a motor with a given torque be able to move my robot without being too overloaded? What rpm will the motor have when it has load?</p>
","motor"
"511","Where can I find a tutorial or sample code for the Juniper WiFi Arduino Shield?","<p>I recently got an arduino wifi shield known as ""juniper"" (I believe it was by cutedigi). I've tried to find code examples, but when I saw code, it was un-commented and very little explained, I could really use a tutorial or some sample code with a good explanation, can anyone help me find a place to start? I found a piece of code here: <a href=""http://arduino.cc/forum/index.php?action=printpage;topic=103582.0"" rel=""nofollow"">http://arduino.cc/forum/index.php?action=printpage;topic=103582.0</a>
and I just want to connect to a network, maybe send some get requests, or open a socket.</p>

<p>EDIT:
after poking around for a while, i found documentation, but I still can't get it to work.
my code:
<a href=""http://pastie.org/5455603"" rel=""nofollow"">http://pastie.org/5455603</a>
I can't seem to get any input at all from the wifi shield.</p>
","arduino electronics wifi"
"512","How can I start learning robotics?","<p>For someone interested in robotics but do not know the ABC of robotics or mechanical/electronic engineering .What's a good roadmap for becoming an amateur roboticist . I'm studying theoretical physics so that I have no problems on the physics/math . If the question is too broad and doesn't meet the criteria of posting on this site . Please inform me of any helpful advice/study material etc. before the question get closed .
Thanks in advance.</p>
","books"
"519","EKF-SLAM Update step, Kalman Gain becomes singular","<p>I'm using an EKF for SLAM and I'm having some problem with the update step.  I'm getting a warning that K is singular, rcond evaluates to near eps or NaN. I think I've traced the problem to the inversion of Z.  Is there a way to calculate the Kalman Gain without inverting the last term? </p>

<p>I'm not 100% positive this is the cause of the problem, so I've also put my entire code here <a href=""https://github.com/jdowns/EKF-SLAM"">https://github.com/jdowns/EKF-SLAM</a>.  The main entry point is slam2d.</p>

<pre><code>function [ x, P ] = expectation( x, P, lmk_idx, observation)
    % expectation
    r_idx = [1;2;3];
    rl = [r_idx; lmk_idx];

    [e, E_r, E_l] = project(x(r), x(lmk_idx)); 
    E_rl = [E_r E_l];
    E = E_rl * P(rl,rl) * E_rl';

    % innovation
    z = observation - e;
    Z = E;

    % Kalman gain
    K = P(:, rl) * E_rl' * Z^-1;

    % update
    x = x + K * z;
    P = P - K * Z * K';
end


function [y, Y_r, Y_p] = project(r, p)     
    [p_r, PR_r, PR_p] = toFrame2D(r, p);
    [y, Y_pr]   = scan(p_r);
    Y_r = Y_pr * PR_r;
    Y_p = Y_pr * PR_p;    
end


function [p_r, PR_r, PR_p] = toFrame2D(r , p)
    t = r(1:2);
    a = r(3);
    R = [cos(a) -sin(a) ; sin(a) cos(a)];
    p_r = R' * (p - t);
    px = p(1);
    py = p(2);
    x = t(1);
    y = t(2);
    PR_r = [...
        [ -cos(a), -sin(a),   cos(a)*(py - y) - sin(a)*(px - x)]
        [  sin(a), -cos(a), - cos(a)*(px - x) - sin(a)*(py - y)]];
    PR_p = R';    
end


function [y, Y_x] = scan(x)
    px = x(1);
    py = x(2);
    d = sqrt(px^2 + py^2);
    a = atan2(py, px);
    y = [d;a];
    Y_x =[...
    [     px/(px^2 + py^2)^(1/2), py/(px^2 + py^2)^(1/2)]
    [ -py/(px^2*(py^2/px^2 + 1)), 1/(px*(py^2/px^2 + 1))]];
end
</code></pre>

<p>Edits:
project(x(r), x(lmk)) should have been project(x(r), x(lmk_idx)) and is now corrected above.  </p>

<p>K goes singular after a little while, but not immediately.  I think it's around 20 seconds or so.  I'll try the changes @josh suggested when I get home tonight and post the results.</p>

<p><strong>Update 1:</strong></p>

<p>My simulation first observes 2 landmarks, so K is 7x2.  (P(rl,rl) * E_rl') * inv( Z ) results in a 5x2 matrix, so it can't be added to x in the next line.  </p>

<p>K becomes singular after 4.82 seconds, with measurements at 50Hz (241 steps).  Following the advice here (http://www.mathworks.com/help/matlab/ref/inv.html), I tried K = (P(:, rl) * E_rl')/Z which results in 250 steps before a warning about K being singular is produced.  </p>

<p>This tells me the problem isn't with matrix inversion, but it's somewhere else that's causing the problem.    </p>

<p><strong>Update 2</strong></p>

<p>My main loop is (with a robot object to store x,P and landmark pointers):</p>

<pre><code>for t = 0:sample_time:max_time
    P = robot.P;
    x = robot.x;
    lmks = robot.lmks;
    mapspace = robot.mapspace;

    u = robot.control(t);
    m = robot.measure(t);

    % Added to show eigenvalues at each step
    [val, vec] = eig(P);
    disp('***')
    disp(val)

    %%% Motion/Prediction
    [x, P] = predict(x, P, u, dt);

    %%% Correction
    lids = intersect(m(1,:), lmks(1,:));  % find all observed landmarks
    lids_new = setdiff(m(1,:), lmks(1,:));
    for lid = lids
        % expectation
        idx = find (lmks(1,:) == lid, 1);
        lmk = lmks(2:3, idx);
        mid = m(1,:) == lid;
        yi = m(2:3, mid);

        [x, P] = expectation(x, P, lmk, yi);
    end  %end correction

    %%% New Landmarks

    for id = 1:length(lids_new)
    % if id ~= 0
        lid = lids_new(id);
        lmk = find(lmks(1,:)==false, 1);
        s = find(mapspace, 2);
        if ~isempty(s)
            mapspace(s) = 0;
            lmks(:,lmk) = [lid; s'];
            yi = m(2:3,m(1,:) == lid);

            [x(s), L_r, L_y] = backProject(x(r), yi);

            P(s,:) = L_r * P(r,:);
            P(:,s) = [P(s,:)'; eye(2)];
            P(s,s) = L_r * P(r,r) * L_r';
        end
    end  % end new landmarks

    %%% Save State
    robot.save_state(x, P, mapspace, lmks)
    end  
end
</code></pre>

<p>At the end of this loop, I save x and P back to the robot, so I believe I'm propagating the covariance through each iteration.  </p>

<p><strong>More edits</strong>
The (hopefully) correct eigenvalues are now here: <a href=""http://pastebin.com/Vn4NzkQy"">http://pastebin.com/Vn4NzkQy</a></p>

<p>There are a number of eigenvalues that are negative.  Although their magnitude is never very large, 10^-2 at most, it happens on the iteration immediately after the first landmark is observed and added to the map (in the ""new landmarks"" section of the main loop).</p>
","slam kalman-filter"
"520","Is it practical to 3D print a refractive lens?","<p>A lot of awesome optics projects like hacking cameras and projectors become possible with CAD lens modelling software<sup>1</sup>, if we can also easily prototype the lenses we design.</p>

<p>What are some materials and additive or subtractive 3D fabrication strategies that can make a clear lens with strong refraction and the ability to be polished?</p>

<p><sup>1</sup> <a href=""http://www.optenso.com/links/links.html#lds"" rel=""nofollow"">Here is a helpful list of 37 different lens design &amp; simulation programs</a>.</p>
","3d-printing manufacturing"
"521","Computing the Jacobian matrix for Inverse Kinematics","<p>When computing the Jacobian matrix for solving an Inverse Kinematic analytically,I read from many places that I could use this formula to create each of the columns of a joint in the Jacobian matrix:</p>

<p><img src=""http://i.stack.imgur.com/EYHBM.png"" alt=""enter image description here""></p>

<p>Such that $a'$ is the rotation axis in world space, $r'$ is the pivot point in world space, and $e_{pos}$ is the position of end effector in world space.</p>

<p>However, I don't understand how this can work when the joints have more than one DOFs. Take the following as example:</p>

<p><img src=""http://i.stack.imgur.com/7mVwI.png"" alt=""enter image description here""></p>

<p>The $\theta$ are the rotational DOF, the $e$ is the end effector, the $g$ is the goal of the end effector, the $P_1$, $P_2$ and $P_3$ are the joints.</p>

<p>First, if I were to compute the Jacobian matrix based on the formula above for the diagram, I will get something like this:</p>

<p>$$J=\begin{bmatrix}
((0,0,1)\times \vec { e } )_{ x } &amp; ((0,0,1)\times (\vec { e } -\vec { P_{ 1 } } ))_{ x } &amp; ((0,0,1)\times (\vec { e } -\vec { P_{ 2 } } ))_{ x } \\ ((0,0,1)\times \vec { e } )_{ y } &amp; ((0,0,1)\times (\vec { e } -\vec { P_{ 1 } } ))_{ y } &amp; ((0,0,1)\times (\vec { e } -\vec { P_{ 2 } } ))_{ y } \\ ((0,0,1)\times \vec { e } )_{ z } &amp; ((0,0,1)\times (\vec { e } -\vec { P_{ 1 } } ))_{ z } &amp; ((0,0,1)\times (\vec { e } -\vec { P_{ 2 } } ))_{ z } \\ 0 &amp; 0 &amp; 0 \\ 0 &amp; 0 &amp; 0 \\ 1 &amp; 1 &amp; 1 
\end{bmatrix} $$</p>

<p>This is assumed that all the rotation axes are $(0,0,1)$ and all of them only have one rotational DOF. So, I believe each column is for one DOF, in this case, the $\theta_\#$.</p>

<p>Now, here's the problem: What if all the joints have full 6 DOFs? Say now, for every joint, I have rotational DOFs in all axes, $\theta_x$, $\theta_y$ and $\theta_z$, and also translational DOFs in all axes, $t_x$, $t_y$ and $t_z$.</p>

<p>To make my question clearer, suppose if I were to ""forcefully"" apply the formula above to all the DOFs of all the joints, then I probably will get a Jacobian matrix like this:</p>

<p><a href=""http://i.stack.imgur.com/f6Fm7.png""><img src=""http://i.stack.imgur.com/f6Fm7.png"" alt=""enter image description here""></a></p>

<p>(click for full size)</p>

<p>But this is incredibly weird because all the 6 columns of the DOF for every joint is repeating the same thing.</p>

<p>How can I use the same formula to build the Jacobian matrix with all the DOFs? How would the Jacobian matrix look like in this case?</p>
","inverse-kinematics kinematics"
"524","Compatable ESC's with brushless 3 phase motors","<p>Am trying to find the right ESC for the following two motors</p>

<p><a href=""http://www.e-fliterc.com/Products/Default.aspx?ProdID=EFLM30180MDFA#quickFeatures"" rel=""nofollow"">http://www.e-fliterc.com/Products/Default.aspx?ProdID=EFLM30180MDFA#quickFeatures</a></p>

<p><a href=""http://www.e-fliterc.com/Products/Default.aspx?ProdID=EFLM3032DFA"" rel=""nofollow"">http://www.e-fliterc.com/Products/Default.aspx?ProdID=EFLM3032DFA</a></p>

<p>Can't figure out which of the ESC's listed on the site would be best? Are there alternative (cheaper or better?) options?</p>
","motor brushless-motor"
"530","Rainbowduino 3.0 - Arduino IDE fails to upload","<p>OK, not really robotics, but has anyone been able to upload to a Rainboduino v3.0 using the Arduino IDE? I can't seem to figure it out, and there is virutally no documentation online. I followed <a href=""http://www.anyware.co.uk/2005/2012/01/17/getting-started-with-arduino-rainbowduino/"" rel=""nofollow"">this blog entry</a>, but got no connection to the board. </p>

<p>If anyone can give me some suggestions, I would appreciate it! </p>
","software arduino programming-languages"
"531","Using an IMU to build an INS","<p>What's needed to utilize an IMU such as the <a href=""http://store.diydrones.com/ArduIMU_V3_p/kt-arduimu-30.htm"" rel=""nofollow"">ArduIMU+ V3</a> to be used in an INS. Is there any other hardware needed? </p>
","arduino slam imu deduced-reckoning"
"533","What was the earliest concept of a robot?","<p>I'm a highschool student studying electronics and for an assessment task on the history of electronics I have decided to focus on the history of robotics. I want to begin with the earliest possible concept of a robot and progress through major developments in robotics to the current day. Where should I begin my research?</p>
","electronics research"
"535","How computationally powerful is an Arduino Uno board?","<p>What can an Arduino board such as the Uno really do? Of course simple things like controlling a couple servos is very easy for it. However, I don't think an Uno board would be able to preform real-time 3D SLAM from point cloud data gathered from a Kinect sensor on a mobile robot, right? If the robot had any speed at all the Arduino wouldn't be able to keep up, correct? Could it do 2D SLAM while moving and be able to keep up? What about taking 1/10 of the points from the Kinect sensor and processing only those?</p>

<p>Basically, what are some examples of the resource limitations of such an Arduino board?</p>
","arduino slam kinect"
"541","Wheels vs Continuous Tracks (Tank Treads)","<p>I'm building a small robot using some cheap Vex Robotics tank treads. However, my choice of picking tank treads is almost purely based on the fact that they seem like more fun than wheels. I don't actually know if they really have much of an advantage or disadvantage when compared to wheels.</p>

<p>What are the pros and cons of both wheels and continuous tracks?</p>
","mobile-robot design wheeled-robot tracks"
"543","Why are quadcopters more common in robotics than other configurations?","<p>I've noticed that almost all research being done with helicopter robots is done using quadcopters (four propellers). Why is there so little work done using tricopters in comparison? Or a different number of propellers? What about four propellers has made quadcopters the most popular choice?</p>
","quadcopter design uav"
"550","How To Determine Heading Without Compass","<p>Lets say I drop a robot into a featureless environment and any magnetic field based sensors (magnetometer/compass) are not allowed.</p>

<p>What methods are there of determining where north is?</p>

<p>Tracking the sun/stars is an option but not reliable enough when the weather is considered.
Can you pick up the rotation of the earth using gyros?</p>

<p>Are there any more clever solutions?</p>
","localization"
"554","Quadcopter liPo battery weight/capacity trade off","<p>I'm trying to find where additional battery capacity becomes worthless in relation to the added weight in terms of a quadcopter. Currently with a 5500 mAh battery, 11.1V, I can get between 12 minutes and 12:30 flight time out of it. My question, then, is this - within the quads lifting capability of course, is there any way to find out where the added weight of a larger battery (or more batteries) cancels out any flight time improvement? Obviously it's not going to be as long as two separate flights, landing and swapping batteries; I'm just trying to maximize my continuous 'in air' time. I'm trying to figure out where the line is (and if I've already crossed it) with tacking bigger batteries onto the quad and seeing diminishing returns. Thanks!</p>

<p>(Again, for now presume that the quad is strong enough to lift whatever you throw at it. With one 5500mAh, ~ 470 grams, my max throttle is about 70%)</p>
","battery quadcopter power"
"556","Is an accelerometer sufficient to detect displacement, or do I need an INS?","<p>Do I need a complex system (of gyros, accelerometers etc.) to detect if a robot has moved forward or can I simply use an accelerometer. </p>

<p>I'm building a robot that learns to walk and I need to detect displacement for machine learning. Can I use an accelerometer or will I need a complicated/expensive Internal Navigation System?</p>
","slam machine-learning deduced-reckoning gyroscope accelerometer"
"558","Why do Mars rovers designers prefer wheels over tracks?","<p>Typically Mars rovers use wheels and not tracks. I guess <a href=""http://en.wikipedia.org/wiki/Spirit_rover"">Spirit</a> would have better chances getting out of that soft soil should it have tracks. In general, Mars surface structure is not known in advance, so it seems wiser to be prepaired for difficult terrain and so use tracks.</p>

<p>Why do Mars rovers typically use wheels and not tracks?</p>
","mobile-robot design"
"565","Kinect - Libfreenect vs OpenNI+SensorKinect","<p>What are the pros and cons of each? Which is better maintained? Which allows for more functionality? Which utilizes the hardware more efficiently? Etc.</p>
","software sensors kinect"
"566","Decision trees for solving 2D inverse kinematics?","<p>While experimenting with the OpenCV Machine Learning Library, I tried to make an example to learn the inverse kinematics of a 2D, 2 link arm using <a href=""http://docs.opencv.org/modules/ml/doc/decision_trees.html"">decision trees</a>. The forward kinematics code looks like this:</p>

<pre><code>const float Link1 = 1;
const float Link2 = 2;

CvPoint2D32f forwardKinematics(float alpha, float beta) 
{
    CvPoint2D32f ret;

    // Simple 2D, 2 link kinematic chain
    ret.x = Link1 * std::cos(alpha) + Link2 * std::cos(alpha - beta);
    ret.y = Link1 * std::sin(alpha) + Link2 * std::sin(alpha - beta);

    return ret;
}
</code></pre>

<p>I generate a random set of 1000 (XY -> alpha) and (XY -> beta) pairs, and then use that data to train two decision tree models in OpenCV (one for alpha, one for beta). Then I use the models to predict joint angles for a given XY position. </p>

<p>It seems like it sometimes gets the right answer, but is wildly inconsistent. I understand that inverse kinematic problems like this have multiple solutions, but some of the answers I get back are just wrong.</p>

<p>Is this a reasonable thing to try to do, or will it never work? Are there other learning algorithms that would be better suited to this kind of problem than decision trees?</p>
","inverse-kinematics machine-learning"
"568","Is it possible to run a neural network on a microcontroller","<p>Could you implement a simple neural network on a microprocessor such as the Arduino Uno to be used in machine learning?</p>
","microcontroller machine-learning"
"578","Which programming language should I use with the NXT?","<p>We have an optional course in our high-school which is about robotics. We're using the Lego Mindstorms NXT and program it with the original Mindstorms-software.
However, we want to advance and use a major programming-language. We have tried NXC and LeJos. Plus, I tried out the Microsoft Robotics Development Studio, but with all these different possibilities we are a little bit overwhelmed.</p>

<p>Because of that (now it becomes interesting), I want to ask, what technology is the best for NXT and especially: What is easy to use? I don't want to need 14 steps just to compile a program and get it running on the NXT. Also, it would be nice, if it's an extend-able language, like using C#, but are there some better or easier possibilities?</p>
","programming-languages nxt"
"583","Inverse kinematics with joint contraints","<p>I have a manipulator having 4 revolute joints with some movement limitations. So, when I apply inverse kinematics, I'm getting results which are out of limits. Please provide me an algorithm that implements inverse kinematics considering joint limitations.</p>
","inverse-kinematics"
"585","King Robota: Does he speak for himself?","<p>I want to know if its currently possible for a robot to speak by it self as <code>King Robota</code> does, or is just someone speaking on his behalf?</p>

<p><a href=""https://www.youtube.com/watch?v=UYI2UDpQtv4"" rel=""nofollow"">Youtube video</a></p>
","control software"
"599","Who coined (or popularized) the term ""SLAM""?","<p>According to <a href=""http://en.wikipedia.org/wiki/Simultaneous_localization_and_mapping"">Wikipedia's article on SLAM</a>, the original idea came from Randal Smith and Peter Cheeseman (<em><a href=""http://www.frc.ri.cmu.edu/~hpm/project.archive/reference.file/Smith&amp;Cheeseman.pdf"">On the Estimation and Representation of Spatial Uncertainty</a></em> [PDF]) in 1986, and was refined by Hugh F. Durrant-Whyte and J.J. Leonard (<em><a href=""http://ieeexplore.ieee.org/xpl/login.jsp?tp=&amp;arnumber=174711&amp;url=http%3A%2F%2Fieeexplore.ieee.org%2Fxpls%2Fabs_all.jsp%3Farnumber%3D174711"">Simultaneous map building and localization for an autonomous mobile robot</a></em>) in 1991. </p>

<p>However, neither paper uses the term ""SLAM"".  Where (and when) did that term come from?  Was there a particular author or whitepaper that popularized it?</p>
","slam"
"602","Vex motors and tank treads drained 9-volt battery more quickly than expected","<p>I've got a couple <a href=""http://www.vexrobotics.com/276-2181.html"" rel=""nofollow"">Vex 269 Motors</a> hooked up to an <a href=""http://www.arduino.cc/en/Main/ArduinoBoardDuemilanove"" rel=""nofollow"">Arduino Duemilanove</a>. These motors run a some <a href=""http://www.vexrobotics.com/276-2168.html"" rel=""nofollow"">Vex Tank Treads</a>. I powered the whole setup with an off-brand 9-volt battery. Everything seems to run great, except that it is only able to run for about 30 seconds worth of motor movement. Then the battery quickly isn't able to pump out the energy needed to move the treads and the whole thing quickly slows to being unusable.</p>

<p>What's my problem here? The tank treads seem loose enough that I don't think they're so restricting the motor has to pump out too much energy to move them. There's nothing else being powered except the Arduino and the motors. Is it because this Enercell 9-volt (alkaline) is just a terrible battery choice? Should I only expect that long of battery life for this robot on a 9-volt? Or is there something else I'm missing? Thank you much!</p>
","battery tracks troubleshooting"
"603","How to get two continuous tracks (tank treads) to move at the same rate?","<p>I've got a couple <a href=""http://www.vexrobotics.com/276-2181.html"">Vex 269 Motors</a> hooked up to an <a href=""http://www.arduino.cc/en/Main/ArduinoBoardDuemilanove"">Arduino Duemilanove</a>. These motors run a some <a href=""http://www.vexrobotics.com/276-2168.html"">Vex Tank Treads</a>. The two motors are run as servos on the Arduino using the Servo Library. The problem I'm having is that the two tracks don't turn at the same speed when sent the same servo angle. This is clearly due to the fact that the continuous tracks have so many moving parts that having identical friction forces on each track is hard to get.</p>

<p>How do I get them to move the same speed? Should they be moving the same speed given the same servo angle regardless of the friction and the Vex 269 Motors just aren't strong enough (meaning I should use the Vex 369 or some other more powerful motor)? Is it best to just doing trial and error long enough to figure out which servo angle results in equal speeds on each? Should I tinker with the tracks until they have nearly identical frictions? Thank you much!</p>
","mobile-robot motor tracks"
"609","Including a RaspberryPi within a robot... Does this allow for a ""universal API""?","<p>I know this is a <em>broad</em> statement, but when you've got support for both TCP as well as a full fledged computer on board (to integrate/run an arduino), does this essentially allow for anything that would run on a linux box (raspberryPi) to run and operate your robot?</p>

<p>I know clock speed as well as the dependency libraries for a given code base (on the Pi) would add some complexity here, but what are some of the big issues that I'm overlooking in such a vertically-integrated control system?</p>

<p>Including a RaspberryPi within a robot... Does this allow for a ""universal API""?</p>
","software arduino raspberry-pi"
"613","What is stall current and free current of motors?","<p>What are the stall and free currents of an electric motor? For example, <a href=""http://www.vexrobotics.com/276-2181.html"">this Vex motor</a> lists its stall and free currents at the bottom of the page.</p>

<p>I think I understand the general idea, but a detailed description would be helpful.</p>
","motor current"
"616","Arduino Vin Current Limit","<p>I've found that Arduino (Duemilanove) has a current limit of 40mA per pin. Does this include the V<sub>in</sub> pin? Or does the V<sub>in</sub> pin have some sort of work around in place on the board to allow for higher currents?</p>

<p>If this is the limit on the V<sub>in</sub>, is there good way of using still using the power supply jack on the board and allowing other sources to draw on that supply without it needing to pass through the chip first?</p>

<p>Thank you much.</p>

<p>EDIT: For the second part, what should I do if I wanted to get up to something like 2 amps?</p>
","arduino current"
"619","Programming an ESC to have reverse mode","<p>How do you program an <a href=""http://en.wikipedia.org/wiki/Electronic_speed_control"" rel=""nofollow"">ESC</a> to have a reverse mode? We're looking to control an ESC from a servo board (for a <a href=""http://greymatterrobotics.com/2012/09/18/the-hoverbot/"" rel=""nofollow"">robotics project</a>).</p>

<p>Assuming that the input will be between 0 and 255, we're looking for 127 as off, 255 as fully forward and 0 as full reverse, so how do we achieve that?</p>
","control motor"
"623","When taking VCC power from an arduino to a 12v regulator, then to a 5v, do I need two sets of capacitors?","<p>I'm building an open-source bio-research hardware (ask me how you can help!) and I've got this guy here:</p>

<p><img src=""http://i.stack.imgur.com/MZw6q.jpg"" alt=""enter image description here""></p>

<p>My big questions are:</p>

<ul>
<li><p>Can I get away with all the ground being common? (I've got a 12v and 5v needing to be grounded)</p></li>
<li><p>Do I need two sets of capacitors? There are 2 wired up to the 12v regulator and 2 wired to the 5v regulator. (These are shown in blue)</p></li>
</ul>

<p>I've generally denoted connections which go UNDER the shield as orange, and those above as green.</p>

<p>If anyone happens to see something which might backfire, feel free to point it out. As this is also my first time making anything quite like this!</p>

<ul>
<li>I've verified the regulator positions and they are correct.</li>
<li>This is a <a href=""http://www.nkcelectronics.com/Protoshield-KIT-for-Arduino-UNO-R3_p_308.html"" rel=""nofollow"">proto-shield</a> for an Arduino R3 Uno.</li>
</ul>

<p>A larger version of the image can be seen here: <a href=""http://i.imgur.com/BPXjn.jpg"" rel=""nofollow"">http://i.imgur.com/BPXjn.jpg</a></p>
","arduino electronics"
"627","Formatting an SD card for Panda Board ES","<p>I have a Panda Board ES. I am not able to get it to boot. I sent it back to SVTronics to get it checked and they said that the board is OK; I am the one who is not able to configure it properly.</p>

<p>After doing a little research and following all the directions on the Panda Board and Ubuntu website, I am still not able to get the board to boot. I think the problem is how I am formatting the SD card. I am using disk utility for Mac to format the SD card to ""MSDOS(FAT)"" partition.</p>

<p>I would like to know how to format an ""SD Card"" on a Macintosh to install Ubuntu on it for Panda Board ES.</p>
","electronics operating-systems"
"628","What algorithm should I implement to program a room cleaning robot?","<p>For this question assume that the following things are unknown:</p>

<ul>
<li>The size and shape of the room</li>
<li>The location of the robot</li>
<li>The presence of any obstacles</li>
</ul>

<p>Also assume that the following things are constant:</p>

<ul>
<li>The size and shape of the room</li>
<li>The number, shape and location of all (if any) obstacles</li>
</ul>

<p>And assume that the robot has the following properties:</p>

<ul>
<li>It can only move forward in increments of absolute units and turn in degrees. Also the operation that moves will return true if it succeeded or false if it failed to move due to an obstruction</li>
<li>A reasonably unlimited source of power (let's say it is a solar powered robot placed on a space station that faces the sun at all times with no ceiling)</li>
<li>Every movement and rotation is carried out with absolute precision every time (don't worry about unreliable data)</li>
</ul>

<p>Finally please consider the following properties of the robot's environment:</p>

<ul>
<li>Being on a ceiling-less space station the room is a safe but frustratingly close distance to passing comets, so the dust (and ice) are constantly littering the environment.</li>
</ul>

<p>I was asked a much simpler version of this question (room is a rectangle and there are no obstacles, how would you move over it guaranteeing you could over every part at least once) and after I started wondering how you would approach this if you couldn't guarantee the shape or the presence of obstacles. I've started looking at this with <a href=""http://en.wikipedia.org/wiki/Dijkstra%27s_algorithm"">Dijkstra's algorithm</a>, but I'm fascinated to hear how others approach this (or if there is a well accepted answer to this? (How does Roomba do it?)</p>
","mobile-robot artificial-intelligence algorithm coverage theory"
"634","How to charge a LiFePO4 battery?","<p>From what I've seen, LiFePO4 batteries seem like one of the top battery choices for robotics applications. However, I've seen people mentioning that you can't use a charger for a different battery to charge these, but I haven't seen why. If I were to build my own setup to charge LiFePO4 batteries what would it specifically need to do? What kind of voltages or current rates does it need to supply to charge these?</p>

<p>More specifically, I was think about setting up a solar charger for these batteries. Is there any immediate reason why this is a bad solution? Such as, the battery needs to charge with a current above some amount for it to work properly?</p>

<p>If you're ambitious enough to provide an example along with your explanation, I'm specifically thinking of having <a href=""http://www.batteryspace.com/lifepo426650cell32v3300mah16.5arate10whunapproved.aspx"">4 of these batteries</a> with 2 pairs of 2 in series in parallel. </p>
","battery"
"636","Will turning an NXT motor by hand damage it?","<p>Many people claim that turning an NXT motor by hand will damage it. Is this true? Does it matter if the motor is idle or set on break? Are there any facts to confirm or refute this argument? I know that some projects (e.g. etch-a-sketch) use the built-in rotation sensor to measure how much the motor has turned. Does this indicate that hand-turning NXT motors is okay? Do they need to be put in a special 'rotation sensor' mode?</p>
","motor nxt mindstorms"
"637","Protecting electronics against voltage/current extremes and bad polarity","<p>I have built a robot from a wheelchair that has worked very well thus far. It is now time for me to take the next step. I need to implement a permanent power circuit with proper protection. </p>

<p>The lowest level of protection I can think of is a fuse, but I would like to take a step further (current/voltage/direction/switches/High/Low voltages). If some one could give some insight on this project of mine any info will be greatly appreciated.</p>

<hr>

<p><strong>Moderator comment:</strong>  Please see <a href=""http://meta.robotics.stackexchange.com/questions/5/how-do-we-address-questions-about-related-subject-areas"">How do we address questions about related subject areas?</a> before answering.  This question is close to the boundary, but is on-topic here.</p>
","mobile-robot wheeled-robot protection circuit"
"641","Good book on mechanisms","<p>I am working with students (9th &amp; 10th grade) on robotics and wanted to get a good book which covers basic mechanisms.  Does anyone have any recommendations.  Searching Google or Amazon yields many results, however, I thought the community might have a standard book to use.  </p>
","design mechanism"
"642","MMA7361 Accelerometer Always Displays Same Values","<p>I recently purchased a 3-axis accelerometer from Amazon, and can't seem to find how it works. I've been looking for quite a while now, and haven't found any real clues. The x, y, and z values always seem to return the same values. They change when I tilt or move the accelerometer, but revert to about 120 for each reading. I am currently using this device with the Arduino Uno, using the following code: </p>

<pre><code>int x=1,y=2,z=3;
void setup() {
  pinMode(x, INPUT);
  pinMode(y, INPUT);
  pinMode(z, INPUT); 
  Serial.begin(9600);
}
void loop() {
 Serial.println();
 Serial.print(analogRead(x));
 Serial.print("", "");
 Serial.print(analogRead(y));
 Serial.print("", "");
 Serial.print(analogRead(z));
}
</code></pre>

<p>Also, how would I go about converting this to tilt?</p>
","arduino sensors accelerometer"
"646","MEMS accelerometer calibration","<p>I am trying to calibrate a MEMS accelerometer. I was able to calibrate it for the current axis which is parallel to gravity and shows correctly, 1g. But the other two axes which should be 0.00g are showing +-0.02g instead. 
So, e.g., when the accelerometer's x axis is parallel to gravity, it should show (1g, 0g, 0g) and not (1g, 0.02g, -0.01g) like now.</p>

<p>How could I eliminate those values, e.g. further calibrate accelerometer? </p>

<p><strong>EDIT:</strong> The <a href=""http://www.st.com/internet/com/TECHNICAL_RESOURCES/TECHNICAL_LITERATURE/DATASHEET/CD00091417.pdf"" rel=""nofollow"">acelerometer's datasheet</a> says nothing about calibrating except that <em>The IC interface is factory calibrated for sensitivity (So) and Zero-g level (Off)</em> (page 20).</p>
","design electronics accelerometer calibration"
"649","Does RRT* guarantee asymptotic optimality for a minimum clearance cost metric?","<p>The optimal sampling-based motion planning algorithm $\text{RRT}^*$ (described <a href=""http://sertac.scripts.mit.edu/web/wp-content/papercite-data/pdf/karaman.frazzoli-ijrr11.pdf"" rel=""nofollow"">in this paper</a>) has been shown to yield collision-free paths which converge to the optimal path as planning time increases. However, as far as I can see, the optimality proofs and experiments have assumed that the path cost metric is Euclidean distance in configuration space. Can $\text{RRT}^*$ also yield optimality properties for other path quality metrics, such as maximizing minimum clearance from obstacles throughout the path?</p>

<p>To define minimum clearance: for simplicity, we can consider a point robot moving about in Euclidean space. For any configuration $q$ that is in the collision-free configuration space, define a function $d(q)$ which returns the distance between the robot and the nearest C-obstacle. For a path $\sigma$, the minimum clearance $\text{min_clear}(\sigma)$ is the minimum value of $d(q)$ for all $q \in \sigma$. In optimal motion planning, one might wish to <strong>maximize</strong> minimum clearance from obstacles along a path. This would mean defining some cost metric $c(\sigma)$ such that $c$ increases as the minimum clearance decreases. One simple function would be $c(\sigma) = \exp(-\text{min_clear}(\sigma))$.</p>

<p>In the <a href=""http://sertac.scripts.mit.edu/web/wp-content/papercite-data/pdf/karaman.frazzoli-rss10.pdf"" rel=""nofollow"">first paper</a> introducing $\text{RRT}^*$, several assumptions are made about the path cost metric so that the proofs hold; one of the assumptions concerned additivity of the cost metric, which doesn't hold for the above minimum clearance metric. However, in the more recent <a href=""http://sertac.scripts.mit.edu/web/wp-content/papercite-data/pdf/karaman.frazzoli-ijrr11.pdf"" rel=""nofollow"">journal article</a> describing the algorithm, several of the prior assumptions weren't listed, and it seemed that the minimum clearance cost metric might also be optimized by the algorithm.</p>

<p>Does anyone know if the proofs for the optimality of $\text{RRT}^*$ can hold for a minimum clearance cost metric (perhaps not the one I gave above, but another which has the same minimum), or if experiments have been performed to support the algorithm's usefulness for such a metric?</p>
","motion-planning algorithm rrt theory"
"650","Can I make a simple Bluetooth receiver?","<p>I can control a relay from an Android smartphone using Arduino and Bluetooth as seen <a href=""http://bellcode.wordpress.com/2012/01/02/android-and-arduino-bluetooth-communication/"" rel=""nofollow"">here</a>.</p>

<p>However, it seems too costly to be using Arduino and a Bluetooth receiver for driving a switch. As long as Bluetooth is a radio frequency, is it possible to make a <strong>simple</strong> Bluetooth receiver which can output 1 or 0 to drive a relay? If yes, how tough that is going to be?</p>

<p>The main factor here is the <strong>cost</strong>, which should be \$1-$5. </p>
","sensors circuit"
"653","What are the notable limitations on using Java with Mindstorms NXT 2.0?","<p>I'm a long time <a href=""http://en.wikipedia.org/wiki/Java_%28programming_language%29"" rel=""nofollow"">Java</a> developer who is starting to learn on the <a href=""http://en.wikipedia.org/wiki/Lego_Mindstorms_NXT_2.0"" rel=""nofollow"">Lego Mindstorms NXT 2.0</a>. Are there any limitations to using the Java API? Which language is the most robust on the platform?</p>

<p>I found a post, <a href=""http://robotics.stackexchange.com/questions/578/lego-nxt-programming-tip%29""><em>Which programming language should I use with the NXT?</em></a> which mentions many of the alternatives. The answer is helpful, but doesn't mention the different languages' limitations.</p>
","nxt programming-languages mindstorms"
"654","What is the difference between Kinect for Windows and Kinect for XBox?","<p>As I see there is a huge price gap between the two \$223 vs \$99 (at amazon).</p>

<p>My intention is to use one of those from Ubuntu linux to perform depth sensing, navigation etc. and naturally I prefer the cheaper. 
However I am not sure if I miss some important point while betting on the Kinect for Xbox version. </p>

<p>As it seems the Windows version is <a href=""http://www.pcworld.com/article/247724/kinect_for_windows_available_february_1_but_overpriced_at_249.html"">overpriced</a> because it has the license for development. <a href=""http://gaming.stackexchange.com/questions/73290/can-kinect-for-windows-work-on-an-xbox-360"">Here</a> it is stated that there are internal differences but without exact details (The minimum sensing distance seems to be better for Windows version.).</p>

<p>Could anyone give a comparison chart?
It would be good to know about</p>

<ul>
<li>Connectivity: USB, special connector, ... .</li>
<li>Hardware differences: are they the same or do they really differ in weight, energy consumption, speed, sensing range, ...?</li>
<li>Driver: could I use Xbox version under Ubuntu?</li>
<li>API usage: could I develop on Xbox version, could I use the same/similar API on both, is the API for Xbox mature enough?</li>
<li>License: is it against the license of Xbox version to develop for home/hobby/educational use?</li>
</ul>

<p>Thanks.</p>
","sensors kinect"
"656","Lightweight, commercially available robotic arms","<p>I was wondering what options are there in terms of lightweight (&lt; 5 lbs) robotic arms. I see <a href=""http://www.robai.com/products.php?prdt_id=1"" rel=""nofollow"">Robai Cyton Gamma 300</a>, and <a href=""http://www.crustcrawler.com/products/AX-18F%20Smart%20Robotic%20Arm/"" rel=""nofollow"">CrustCrawler AX18</a> look like interesting options. What lightweight arms do people use/like? </p>
","mobile-robot arm"
"667","Raspberry Pi operating system for robotics","<p>Is there an operating system for the Raspberry Pi that is specifically made for running robotics applications? Or an operating system whose purpose is to optimized just to run a few specific programs?</p>

<p>I've been working with an Arduino for a while now. As far as efficiency goes, it makes sense to me to just upload a specific set of commands and have the hardware only need to handle that, and not have to worry about running a full fledged operating system. Is something like this possible to do on a Raspberry Pi?</p>
","raspberry-pi operating-systems"
"671","Check if task exists in Not eXactly C","<p>Is there a way to check if a task, function or variable exists in Not eXactly C?</p>

<p>I know that in PHP you can use <code>isset()</code> to check if a variable exists and <code>function_exists()</code> to do the same for a function, but is there a way to do that in NXC?</p>

<p>I am specifically interested in checking whether a task exists or it is alive.</p>
","nxt programming-languages mindstorms not-exactly-c"
"672","PID line follow with three sensors in Not eXactly C","<p>I'm currently working on a line-following robot which uses three sensors to follow a black line. The sensors are pretty much in line and right next to each other.</p>

<p>Right now, I'm doing a simple line follow: if on the line go forward, otherwise turn left or right to regain the line. This means that the robot is wiggling along the line most of the time.</p>

<p>I'm looking for a better way for this robot to follow the line using three sensors. The program I'm writing is in Not eXactly C code. I'm trying to get the robot to utilize the power of PID control, but I'm not sure how one would go about writing a three-sensor PID line-follower program in NXC.</p>
","nxt programming-languages mindstorms algorithm not-exactly-c"
"679","Why are Mars rovers so slow?","<p>Mars rovers are typically very slow. Curiosity, for example, has average speed of about 30 meters per hour.</p>

<p>Why is it designed so slow? Is it because of some specific power restrictions or for other reasons? What is the top reason why it is so slow?</p>
","mobile-robot design"
"684","Why does our LM2576 circuit suddenly cut down the power?","<p>I have an LM2576 circuit plus an adjuster to adjust the output voltage, for controlling motor speed in a line follower robot. The circuit works great when adjusted to give out low voltages, but when I adjust it to higher voltages for my motors to go faster, it works great for 1-2 minutes, then suddenly cuts down the power and motors start to go extremely slow.</p>

<p>Even when I decrease or increase the output voltage, it won't respond until I turn off the power and turn it back on again. There is something mentioned in the LM2576 datasheet that if we overload the IC it will cut down the power until the load comes lower, so I think it might be a problem with that.</p>

<p>Since this problem has already caused us to lose the competitions with 5+ teams, I would like to solve it for our next competition, so why does our LM2576 circuit suddenly reduce the power?</p>
","motor electronics power"
"687","Robotics with Kinect","<p>I want to learn robotics and really interested in making a robot based on Kinect sensor.
I see so many projects like this one : <a href=""http://www2.macleans.ca/2011/11/03/the-150-robot-revolution/"" rel=""nofollow"">http://www2.macleans.ca/2011/11/03/the-150-robot-revolution/</a>
and just wondering how it works on top level. I downloaded Kinect SDK and did some basic tutorials, but I just don't think that Microsoft SDK is the library to use for real robotics projects. Any suggestions where to start and what library to use? Any good books in particular or online resources? Any help is appreciated, thank you.</p>
","kinect"
"689","Cable routing in theta, x, y motion control system. Better inside or outside?","<p>I'm building a motion control platform with 3 DoF: 1 axis of rotation (theta) and 2 cartesian (x,y). In most applications, like wrist actuation, you have an X-Y stage with a rotating servo as the stage's payload. This configuration works well since little of the power and data wiring needs to transit to the non-linear moving portion of the platform. </p>

<p>For my inverted application, the stackup is reversed. The rotating axis comes first (from the mounting plane) with the stage connected as the rotating platform's payload. Now nearly all of the wiring (power, command, sensor, and otherwise) must be routed to the non-linearly moving section.</p>

<p>I can see two broad approaches: </p>

<ol>
<li><p>The inside track, I route the cabling through the center of rotation.</p></li>
<li><p>The outside track, I route the cabling around outside the outer diameter of the rotating platform.</p></li>
</ol>

<p>Mathematically, I can see that (1) results in minimum cable length, but maximum torsional loading, while (2) results in maximum cable length, but minimum torsional loading on the wires.</p>

<p>Having limited experience with cable routing (and the associated carriers, strategies, and products) in non-linear applications, my question is...</p>

<h3>...which approach is better in practice?</h3>

<p>Cost isn't really the issue here. I'm more interested in reliability, ease of construction, availability of commercial components (says something about the popularity of the technique), etc...</p>

<p>e.g. the generic concepts behind why you pick one over the other. </p>

<p>...of course, if you have some part numbers for me I wouldn't be upset &lt;-- I know I'm not supposed ask that here ;-)</p>
","control wiring routing motion"
"690","Noise in motion and measurement models","<p>When using an EKF for SLAM, I often see the motion and measurement models being described as having some noise term.  </p>

<p>This makes sense to me if you're doing a simulation, where you need to add noise to a simulated measurement to make it stochastic.  But what about when using real robot data?  Is the noise already in the measurement and thus does not need to be added, or does the noise matrix mean something else?</p>

<p>For example, in Probabilistic Robotics (on page 319), there is a measurement model: $z_t^i = h(y,j) + Q_t$, where $Q_t$ is a noise covariance.  Does $Q_t$ need to be calculated when working with real data?</p>
","slam kalman-filter"
"693","Can I use ROS with a Roomba?","<p>Is there anything different between a iRobot Roomba and the Create?  I want go start building my own turtlebot and playing with ROS but with the cost of all the parts I'm going to have to do it piece by piece.  It's pretty easy to find cheap used Roombas.  </p>
","ros roomba irobot-create"
"696","Are there working instances of Kilobot projects?","<p>The interesting <a href=""http://www.eecs.harvard.edu/ssr/projects/progSA/kilobot.html"" rel=""nofollow"">Kilobot project</a> from Harvard for investigating multi-robot behavior with masses of small dumb robots has been made <a href=""http://ssr.wikidot.com/kilobot-documents"" rel=""nofollow"">open hardware</a> for a year now. </p>

<p>However I cannot find so much activity about robot creation and movies about results. Is it too hard to create the robots, the programmer, the charger or isn't the project interesting enough?</p>
","multi-agent"
"697","Standalone (or capable of being) Robotics Simulator","<p>I'm a software engineer who volunteers with a non-profit that introduces young girls to technology. We have recently been talking about methods of introducing these children to the world of robotics, and I am curious what types of low-cost options we have.</p>

<p>One very appealing idea would be to have an online simulator, or (more preferable) an off-line standalone-simulator that we can build and program simple robots with. Perhaps nothing more than dragging components together, and then programming the interactions between those components.</p>

<p>What solution(s) exist that I might be able to make use of in our outreach?</p>
","software simulator children"
"704","Wifly Shield Not Connecting","<p>I recently asked a question about the juniper WiFi shield, and am now working with wifly from spark fun. I've been using an updated version of their experimental library, and have been attempting to set up a webserver. Unfortunately, when I attempt to connect through a web browser, I get an error saying that the page sent no data. Here's my code:</p>

<pre><code>#include &lt;SPI.h&gt;
#include &lt;WiFly.h&gt;

WiFlyServer s(80);
boolean current_line_is_blank=true;
void setup() {
  Serial.begin(9600);
  WiFly.begin();
  if(!WiFly.join(placeholderssid, placeholderpass,WPA_MODE)) {
    Serial.println(""Connection Failed."");
  } else {
    Serial.println(""Connection Succesful!"");
    Serial.println(WiFly.ip());
    Serial.println(""Receving Client Input..."");
    s.begin();

  }
}
void loop() { 
   WiFlyClient c = s.available();
   if(c) {
   Serial.println(""Server Ready."");
   current_line_is_blank=true;
   while(c.connected()) {
     Serial.println(""Client Connected."");
     if(c.available()) {
       Serial.println(""Client Available for data."");
       char tmp = c.read();
       Serial.println(tmp);
       if(tmp == '\n' &amp;&amp; current_line_is_blank) {
         Serial.println(""Sent OK Response."");
         c.println(""HTTP/1.1 200 OK"");
         c.println(""Content-Type: text/html"");
         c.println();
         c.print(""WiFly Webserver Running!"");
         c.println(""&lt;br /&gt;"");
         break;
       }
      if (tmp == '\n') {
          // we're starting a new line
          current_line_is_blank = true;
        } else if (tmp != '\r') {
          // we've gotten a character on the current line
          current_line_is_blank = false;
        }
       }
   }
   }
    delay(2000);
    c.stop();
}
</code></pre>

<p>I am using Arduino Uno, and the serial monitor looks like this:</p>

<pre><code>Connection Succesful!
10.100.1.173
Receving Client Input...
</code></pre>

<p>Is there anything obviously wrong with my code?</p>

<p>EDIT:
I now have a new shield, but I'm still working with the same problem. Is it a malfunction in the hardware? I just can't figure this out!</p>
","arduino software wifi c"
"709","Why are Servo Motors so noisy?","<p>I was working on a project to make a bedside night light out of a stuffed butterfly or bird. I was making a mechanism to make the wings flap with a servo motor and some small gears. The <a href=""http://www.vexrobotics.com/276-2162.html"">servo motor</a> was very loud as it moved. And this was whether or not the servo was moving large amounts, small amounts, fast or slow. </p>

<p>I've worked with small servos before and realized they usually are pretty noisy machines, but I can't really explain why.</p>

<p>Why are small servo motors noisy when they move? Is it backlash in the internal gearing?</p>
","rcservo"
"712","Quadruped Learning Simulator","<p>I'm currently building a robot with four legs (<a href=""http://en.wikipedia.org/wiki/Quadrupedalism/"">quadruped</a>), 3 DOF (Degrees of Freedom) and Its been suggested <a href=""http://robotics.stackexchange.com/questions/327/learning-algorithms-for-walking-quadruped/"">here</a> that I use a simulator to do the learning on a computer and then upload the algorithms to the robot. I'm using an <a href=""http://arduino.cc"">Arduino Uno</a> for the robot and what software could I use to simulate the learning and then be able to upload to the Arduino board?</p>
","mobile-robot arduino microcontroller machine-learning simulator"
"716","C++ Robust Model Fitting Library","<p>Often when I need to perform model fitting I find myself looking for a decent C++ library to do this. There is the RANSAC implementation in <a href=""http://reference.mrpt.org/svn/classmrpt_1_1math_1_1_model_search.html"" rel=""nofollow"">MRPT</a>, but I was wondering if there are alternatives available.</p>

<p>To give an example for the type of problems I would like to solve: For a set $A$ of (approx 500) 3D point pairs $(a, b)$ I would like to find the Isometry transform $T$, which maps the points onto each other so that $|(a - Tb)| &lt; \epsilon$. I would like to get the largest subset of $A$ for a given $\epsilon$. Alternatively I guess I could have the subset size fixed and ask for the lowest $\epsilon$.</p>
","c++ ransac"
"718","How can computer vision distinguish one object being contained by another vs being on top of it?","<p>How do we know that an object is contained inside another object or is just lying on top of it? </p>

<p>Lets take an example of a cup-plate-spoon. The cup is lying on top of the plate. But the spoon is inside the cup. How do we distinguish between the 2 situations? What are the criteria to decide whether A is contained inside B or just lying above B?</p>

<p>I am trying to solve it using kinect.</p>
","kinect computer-vision algorithm"
"725","Least squares map joining","<p><strong>There is a lot of background here, scroll to the bottom for the question</strong></p>

<p>I am trying out the map joining algorithm described in <a href=""http://services.eng.uts.edu.au/~sdhuang/Shoudong_IROS_2010.pdf"">How Far is SLAM From a Linear Least Squares Problem</a>; specifically, formula (36).  The code I have written seems to always take the values of the second map for landmark positions.  My question is, am I understanding the text correctly or am I making some sort of error. I'll try to explain the formulas as I understand them and show how my code implements that.  I'm trying to do the simple case of joining just two local maps. </p>

<p>From the paper (36) says joining two local maps is finding the a state vector $X_{join,rel}$ that minimizes:</p>

<p>$$
\sum_{j=1}^{k}(\hat{X_j^L} - H_{j,rel}(X_{join,rel}))^T(P_j^L)^{-1}(\hat{X_j^L} - H_{j,rel}(X_{join,rel}))
$$</p>

<p>Expanded for two local maps $\hat{X_1^L}$ and $\hat{X_2^L}$ I have:</p>

<p>$$
(\hat{X_1^L} - H_{j,rel}(X_{join,rel}))^T(P_1^L)^{-1}(\hat{X_1^L} - H_{j,rel}(X_{join,rel})) + (\hat{X_2^L} - H_{j,rel}(X_{join,rel}))^T(P_2^L)^{-1}(\hat{X_2^L} - H_{j,rel}(X_{join,rel}))
$$</p>

<p>As I understand it, a submap can be viewed as an integrated observation for a global map, so $P^L_j$ is noise associated with the submap (as opposed to being the process noise in the EKF I used to make the submap, which may or may not be different). </p>

<p>The vector $X_{join,rel}$ is the pose from the first map, the pose from the second map and the union of the landmarks in both maps.</p>

<p>The function $H_{j,rel}$ is:</p>

<p>$$
\begin{bmatrix} X_{r_{je}}^{r_{(j-1)e}}\\
                 \phi_{r_{je}}^{r_{(j-1)e}}\\
                 R(\phi_{r_{(j-1)e}}^{r_{m_{j1}e}})
                        (X^{r_{m_{j1}e}}_{f_{j1}} -
                         X^{r_{m_{j1}e}}_{r_{(j-1)e}})\\.\\.\\.\\
                 R(\phi_{r_{(j-1)e}}^{r_{m_{jl}e}})
                        (X^{r_{m_{jl}e}}_{f_{jl}} -
                         X^{r_{m_{jl}e}}_{r_{(j-1)e}})\\
                         X_{f_{j(l+1)}}^{r_{j-1e}}\\
                         .\\.\\.\\
                         X_{f_{jn}}^{r_{j-1e}}
\end{bmatrix}
$$</p>

<p><strong>I'm not convinced that my assessment below is correct:</strong></p>

<p>The first two elements are the robot's pose in the reference frame of the previous map.  For example, for map 1, the pose will be in initial frame at $t_0$; for map 2, it will be in the frame of map 1.</p>

<p>The next group of elements are those common to map 1 and map 2, which are transformed into map 1's reference frame.</p>

<p>The final rows are the features unique to map 2, in the frame of the first map.</p>

<p><strong>My matlab implementation is as follows:</strong></p>

<pre><code>function [G, fval, output, exitflag] = join_maps(m1, m2)
    x = [m2(1:3);m2];
    [G,fval,exitflag,output] = fminunc(@(x) fitness(x, m1, m2), x, options);
end

function G = fitness(X, m1, m2)
    m1_f = m1(6:3:end);
    m2_f = m2(6:3:end);
    common = intersect(m1_f, m2_f);
    P = eye(size(m1, 1)) * .002;
    r = X(1:2);
    a = X(3);
    X_join = (m1 - H(X, common));
    Y_join = (m2 - H(X, common));
    G = (X_join' * inv(P) * X_join) + (Y_join' * inv(P) * Y_join);
end

function H_j = H(X, com)
    a0 = X(3);
    H_j = zeros(size(X(4:end)));
    H_j(1:3) = X(4:6);
    Y = X(1:2);
    len = length(X(7:end));
    for i = 7:3:len
        id = X(i + 2);
        if find(com == id)
            H_j(i:i+1) = R(a0) * (X(i:i+1) - Y);
            H_j(i+2) = id;
        else  % new lmk
            H_j(i:i+2) = X(i:i+2);
        end
    end
end

function A = R(a)
    A = [cos(a) -sin(a); 
         sin(a)  cos(a)];
end
</code></pre>

<p>I am using the <a href=""http://www.mathworks.com/help/optim/ug/fminunc.html"">optimization toolbox</a> to find the minimum of the fitness function described above.  The fitness function itself is pretty straightforward I think.  The function H returns the vector H described above.</p>

<p><strong>The result is:</strong>
When I run join_maps on the two vectors</p>

<pre><code>map_1 = [3.7054;1.0577;-1.9404; %robot x, y, angle
      2.5305;-1.0739;81.0000]; % landmark x, y, id
map_2 = [3.7054;1.0577;-1.9404;
         2.3402;-1.1463;81.0000]; % note the slightly different x,y

[G,fv,output,exitflag] = join_maps(map_1, map_2)
</code></pre>

<p>The output is:</p>

<pre><code>Warning: Gradient must be provided for trust-region algorithm;
  using line-search algorithm instead. 
&gt; In fminunc at 341
  In join_maps at 7

Local minimum found.

Optimization completed because the size of the gradient is less than
the default value of the function tolerance.

&lt;stopping criteria details&gt;


Local minimum possible.

fminunc stopped because it cannot decrease the objective function
along the current search direction.

&lt;stopping criteria details&gt;

G = 
      3.7054
      1.0577
     -1.9404
      3.7054
      1.0577
     -1.9404
      2.3402
     -1.1463
      81.0000

 fv =
     1.3136e+07
  output = 
     iterations: 1
      funcCount: 520
       stepsize: 1.0491e-16
  firstorderopt: 1.6200e+05
      algorithm: 'medium-scale: Quasi-Newton line search'
        message: [1x362 char]
  exitflag =
   5
</code></pre>

<p><strong>The question:</strong></p>

<p>My program gives map 2 is the minimum of the map joining function.  It seems like the minimum should be somewhere between map 1 and map 2.  I'm pretty sure the problem is with the matrix H.  What am I doing wrong?</p>
","slam"
"726","Guiding a Quadrotor Towards a Target","<p>I am working on a quadrotor.  I know its position -- $a$, where I would like to go -- target position $b$, and from that I calculate a vector $c$ -- a unit vector that will take me to my target:</p>

<pre><code>c = b - a
c = normalize(c)
</code></pre>

<p>Since a quadrotor can move in any direction without rotation, what I have tried to do is </p>

<ol>
<li>rotate $c$ by the robots yaw angle</li>
<li>split it into its $x, y$ components </li>
<li>pass them to the robot as roll and pitch angles.  </li>
</ol>

<p>The problem is that if the yaw is 0&deg; &plusmn;5 then this works, but if the yaw is near +90 or -90 it fails and steers to wrong directions. My question is am I missing something obvious here?</p>
","quadcopter uav navigation"
"730","Good method for building a pan and tilt controller?","<p>Have you ever seen one those video games that has headset/goggles you stand in and look around the virtual scene with? I'm building one of those, and I'm trying to design a simple controller. I need the output of the controller to emulate a mouse input. So if you look to the left, it's as if you were moving the mouse to the left. Supposing I use optical encoders, the pan and tilt will need to be in separate locations (a couple of inches apart). It seems that many mouse hacks online have the components very close together.</p>

<p>Do you think it's possible to have one of the encoders some distance away from the controller chip? For OEM purposes, is there a good mouse controller chip that will output USB protocol mouse movements that I could buy in bulk?</p>

<p>Many thanks for any suggestions. Cheers</p>
","microcontroller"
"734","Comparing maps to groundtruth","<p>When you've created a map with a SLAM implementation and you have some groundtruth data, what is the best way to determine the accuracy of that map?  </p>

<p>My first thought is to use the Euclidean distance between the map and groundtruth. Is there some other measure that would be better?  I'm wondering if it's also possible to take into account the covariance of the map estimate in this comparison. </p>
","slam mapping"
"736","Inter-processor communication for robotic arm","<p>I'm building a hobby 6-DOF robotic arm and am wondering what the best way is to communicate between the processors (3-4 AVRs, 18 inches max separation). I'd like to have the control loop run on the computer, which sends commands to the microprocessors via an Atmega32u4 USB-to-??? bridge.</p>

<p>Some ideas I'm considering:</p>

<ul>
<li>RS485
<ul>
<li>Pros: all processors on same wire, differential signal more robust</li>
<li>Cons: requires additional chips, need to write (or find?) protocol to prevent processors from transmitting at the same time</li>
</ul></li>
<li>UART loop (ie, TX of one processor is connected to RX of next)
<ul>
<li>Pros: simple firmware, processors have UART built in</li>
<li>Cons: last connection has to travel length of robot, each processor has to spend cycles retransmitting messages</li>
</ul></li>
<li>CANbus (I know very little about this)</li>
</ul>

<p>My main considerations are hardware and firmware complexity, performance, and price (I can't buy an expensive out-of-box system).</p>
","microcontroller electronics arm"
"738","What are methods for dealing with compass lag (rate dependent hysteresis)?","<p>I've got a tread-driven robot, with low precision wheel encoders for tracking distance and an electronic compass for determining heading.  The compass has significant (> 1 second) lag when the robot turns quickly, e.g. after reaching a waypoint &mdash; pivoting in place to point to its new heading.  </p>

<p>What are ways for dealing with the lag?  I would think one could take a lot of measurements and model the compass response.  However, this seems problematic since it's rate-dependent and I don't know the instantaneous rate.</p>

<p>As a simple-but-slow approach, I have the robot turn until it's very roughly pointed in the right direction, then make very small incremental turns with brief measurement pauses until it's pointed the right way.  Are there other ways of dealing with this? </p>
","sensors compass"
"741","How do you determine EKF process noise for pre-recorded data sets?","<p>I've seen <a href=""http://robotics.stackexchange.com/questions/18/what-are-good-methods-for-tuning-the-process-noise-on-kalman-filters"">this question</a>, which asks about determining the process noise for an EKF.  I don't see anything there about pre-recorded data sets.  </p>

<p>My thought on how to determine the noise parameters, assuming ground truth is available, would be to run the data several times with the EKF and minimize the mean square error, while varying the noise parameters.</p>

<p>Is this an acceptable way to determine noise for a pre recorded data set?  Are there better (or just other) ways from determining the optimal noise values based just on the data set?</p>
","noise ekf"
"748","How many amps do I want my Switching BEC to provide?","<p>I'm trying to power 7-12 <a href=""http://en.wikipedia.org/wiki/Servo_%28radio_control%29"" rel=""nofollow"">servos</a>, and I was under the impression that each one would need about an amp, but in looking around for an appropriate <a href=""http://en.wikipedia.org/wiki/Battery_eliminator_circuit"" rel=""nofollow"">BEC</a> to supply them, I notice that most seem to output around 1-3.5 amps.</p>

<p>They won't all be running at once, but often, say 4 will be drawing enough juice to move.</p>

<p>Obviously, I'm missing some link in my understanding.  How do I determine how many amps will be needed from the power supply?</p>
","design power rcservo bec"
"751","Confused about the variables in RobotC?","<p>I'm trying to program advanced functions in <a href=""http://www.robotc.net/wiki/VEX2_Functions_Motors_and_Servos#motorType"" rel=""nofollow"">RobotC</a> but I'm not too sure I'm doing it right. I want to specify the motor port I'm using, but I assigned names to all the motors. Funny thing though, they don't exactly work the same as regular variables.</p>

<p>For instance, motor[port7]'s alternate name is light_blue.</p>

<pre><code>#pragma config(Motor,  port7,           light_blue,    tmotorVex393, openLoop)
</code></pre>

<p>I'm not really sure if these are new variables, or just specifications.  Anyway, here is the variable's signature:</p>

<pre><code>int motor[tMotor motor]
</code></pre>

<p>My code plans on doing something similar to this:</p>

<pre><code>void testThing (Motor motorName)
{
  motorName = someValue;
}

testThing(light_blue);
</code></pre>

<p>But with the int/motor hybrid variable/unidentified I'm not sure how well that would work out. Or at all.</p>
","robotc"
"757","How can I upgrade an existing robot with a higher torque, sensored motor @ ~100 watts?","<p>I would like a high torque motor (37 oz-in @ 5760 rpm) for souping up a <a href=""http://www.theoldrobots.com/scorbot.html"" rel=""nofollow"">Scorbot 3</a> I bought. I really need it to have an encoder to count the number of revolutions and to allow high start-up torque. So far, I'm having difficulty finding a suitable motor.</p>

<p>The closest I've found are:</p>

<ol>
<li><a href=""http://holmeshobbies.com/product.php?productid=467"" rel=""nofollow"">Revolver S Stubby</a>
(still not ready for purchase)</li>
<li><a href=""http://v2.teamnovak.com/products/index.php?main_page=product_info&amp;cPath=1_126&amp;products_id=79"" rel=""nofollow"">Team Novak Ballistic 25.5T</a></li>
</ol>

<p>I've found other RC car motors, but they are usually too big.</p>

<p>Some alternatives I thought about are:</p>

<ol>
<li>adding hall sensors to an existing motor - how hard is this?</li>
<li>rewinding a motor with more turns to increase torque (decrease Kv)</li>
</ol>

<p>Does anybody know of any motors that fit these requirements or modifications I can make to existing ones?</p>

<hr>

<p><em><strong>Update:</em></strong> I had almost given up hope, until someone at Homebrew Robotics suggested using the <a href=""http://www.maxonmotorusa.com/maxon/view/msp/"" rel=""nofollow"">Maxon motor finder</a>.</p>

<p>If you just type in my given torque and speed, it returns 3 motors, but they're all over powered because the search interprets your specs as a continuous operating point, whereas my robot will only need that much power 20% of the time, and maybe for 1 second max.</p>

<p>If I type in 12V, 5000rpm, and 15 oz-in, then it returns 2 brushless motors, of which, the <strong>Motor EC 45</strong> is the best fit, which has this operating curve:</p>

<p><img src=""http://msp.maxonmotor.com/camosHtml/i?SIG=fb9a5d91198caf381122a3d6eab8b1bda3877f30_fa_1e0.png"" alt=""""></p>

<p>However, I don't want to pay what Maxon is charging, so instead, I've contacted the guy who makes the yet to be released Revolver Stubby and he has kindly offered to build a custom high torque, low RPM motor for me.</p>

<p>Can anyone comment on why high torque, low RPM motors like the one I want seem so rare? Is due to lack of applications (robotics) or is there some intrinsic difficulty in making them?</p>
","motor brushless-motor"
"758","In HRI, how is the ""uncanny valley"" experienced by people on the autism spectrum?","<p>I'm familiar with the idea of the <a href=""http://en.wikipedia.org/wiki/Uncanny_valley"" rel=""nofollow"">uncanny valley</a> theory in human-robot interaction, where robots with <em>almost</em> human appearance are perceived as creepy. I also know that there have been research studies done to support this theory using MRI scans. </p>

<p>The effect is an important consideration when designing robotic systems that can successfully interact with people. In order to avoid the uncanny valley, designers often create robots that are very far from humanlike. For example, many therapeutic robots (Paro, Keepon) are designed to look like animals or be ""cute"" and non-threatening.</p>

<p>Other therapeutic robots, like <a href=""http://kaspar.herts.ac.uk/"" rel=""nofollow"">Kaspar</a>, look very humanlike. Kaspar is an excellent example of the uncanny valley, since when I look at Kaspar it creeps me out. However, people on the autism spectrum may not experience Kaspar the same way that I do. And according to Shahbaz's comment, children with autism have responded well to Kaspar.</p>

<p>In the application of therapeutic robots for people on the autism spectrum, some of the basic principles of human-robot interaction (like the uncanny valley) may not be valid. I can find some anecdotal evidence (with Google) that people on the autism spectrum don't experience the uncanny valley, but so far I haven't seen any real studies in that area.</p>

<p>Does anyone know of active research in human-robot interaction for people on the autism spectrum?  In particular, how does the uncanny valley apply (or doesn't it apply) when people on the autism spectrum interact with a humanlike robot?</p>
","research hri uncanny-valley"
"763","Differences between Ackermann steering and standard bi/tricycles concerning kinematics?","<p>I got the following homework question:</p>

<blockquote>
  <p>What are the general differences between robots with Ackermann steering and standard bicycles or tricycles concerning the kinematics?</p>
</blockquote>

<p>But, I don't see what differences there should be, because a car-like robot (with 2 fixed rear wheels and 2 <strong>dependent</strong> adjustable front wheels) can be seen as a tricycle-like robot (with a single adjustable front wheel in the middle).</p>

<p>Then, if you let the distance between the two rear wheels approach zero, you get the bicycle.</p>

<p>So, I can't see any difference between those three mobile robots. Is there something I am missing?</p>
","mobile-robot design kinematics theory"
"764","The relationship between point cloud maps and graph maps","<p>I am most familiar with SLAM maps that are point clouds, usually in the form of a vector like $&lt;x,y,\theta,f_{1x},f_{1y},...,f_{nx},f_{ny}&gt;$.  I also understand how to create a map like this using an EKF.</p>

<p>Today I came across a <a href=""http://www.mrpt.org/Graph-SLAM_maps"">.graph file format</a>, which as you would expect consists of vertices and edges in the format:</p>

<p><code>VERTEX2 id x y orientation</code></p>

<p><code>EDGE2 observed_vertex_id observing_vertex_id forward sideward rotate inf_ff inf_fs inf_ss inf_rr inf_fr inf_sr</code></p>

<p>I know that there's a connection between matrices and graphs (an adjacency matrix for example).  But it's not clear to me how this graph format of a map is equivalent to a point cloud map that I'm familiar with.  </p>

<p>What is the relationship?  Are the vertices both poses and landmarks? Are they in a global reference frame? How is this created from say velocity information and a range/bearing sensor?  Is there a transformation between a graph map and a point cloud?  </p>
","slam mapping"
"768","How to balance a flying quadcopter?","<p>Im using my own code to create a quadcopter robot. The hardware part is done but I need to balance the copter. </p>

<p>This is the video of its current status: 
<a href=""https://www.dropbox.com/s/53tpf1jzaly6m33/Movie%20on%202013-01-10%20at%2006.36.mov"">https://www.dropbox.com/s/53tpf1jzaly6m33/Movie%20on%202013-01-10%20at%2006.36.mov</a></p>

<p>I have tried to play with the speed of each motor to get it balanced. It didnt go. 
I actually have a gyro and accelerometer onboard. But how shall I adjust the motor speed based on these values? What are the rules that I should beware of?</p>

<p>Is there any better solution other that try and error? Where shall I begin? Any tips? </p>
","balance quadcopter"
"775","Getting started with robotic arm design","<p>I would like to design a robotic arm to hold a weight X at length Y (in my case I want to hold X=2.5 lbs at Y = 4 inches). Starting out simply, I would like try building an arm with a gripper plus one servo joint. </p>

<p>[Servo Joint] ----- Y ------ [Gripper]  </p>

<p>When designing an arm, would I want to say that the gripper has to have enough torque to hold the desired weight (e.g. 2.5 lbs) at a minimal distance (however long the fingers are) then design the servo joint to bear the weight of the gripper + the load?</p>

<p>I would like to be able to hold the object at full extension</p>
","design servos arm"
"777","Building a controllable ""knob""","<p>I am trying to build a semi-analog timer. Something like those old egg timers that you rotate the face of. I want a knob that I can turn that can be read by a microcontroller, and I also want the microcontroller to be able to position the knob. I'd like to implement ""stops"" by letting the microcontroller push the knob towards certain positions. As it runs down, the knob should turn. This is my first project of this kind; I've built small robots in the past, but it's been many years.</p>

<p>I've considered <a href=""http://forums.trossenrobotics.com/tutorials/how-to-diy-128/get-position-feedback-from-a-standard-hobby-servo-3279/"">hacking a servo motor</a> to read its position, but the small hobby servos I've tried are too hard to turn, very noisy, and pick up too much momentum when turned. They don't act like a good knob.</p>

<p>I'm now considering a rotary encoder connected to a motor, but after hunting at several sites (SparkFun, ServoCity, DigiKey, Trossen, and some others), I haven't been able to find anything that seemed appropriate. I'm not certain how to find a motor that's going to have the right kind of low torque.</p>

<p>This seems like it shouldn't be a really uncommon problem. Is there a fairly normal approach to creating a knob that can be adjusted both by the user and a microcontroller?</p>
","motor servos"
"780","What are the reasons for not having autonomous robots in our daily activities?","<p>The fact is that the more I search the less I find autonomous (real) robots in use. The companion robots are all toys with limited useless functionality. Whenever there is a natural disaster you don’t see operational search and rescue robots in the news. Even military robots in service are all remotely controlled machines. They are not intelligent machines. Industrial robotic arms are deterministic machines. The only robots with some levels of autonomous functionality are cleaning bots, warehouse operations bots and farming robots.</p>

<p>On the other hand, today:</p>

<ul>
<li>the artificial intelligence algorithms are very good in making decisions </li>
<li>the sensing technologies are very sophisticated</li>
<li>the communication technologies are very fast</li>
<li>we can manufacture cheap parts</li>
<li>people are extremely gadget savvy</li>
</ul>

<p>So, why there is no real robot in our day to day life? No investment in the domain? No market yet? Not enough knowledge in the domain? A missing technology? Any idea?</p>
","mobile-robot"
"783","Would ROS benefit from a multicore processor like Epiphany or XMOS?","<p>I am looking for a good embedded PC to run ROS on. I recently came across a couple of little machines using new very multi-core processors, such as the <a href=""http://www.kickstarter.com/projects/adapteva/parallella-a-supercomputer-for-everyone"" rel=""nofollow"">Epiphany</a> and the <a href=""http://www.xmos.com/"" rel=""nofollow"">XMOS</a>.</p>

<p>Since the one thing that ROS really seems to want is cores, would ROS be able to take advantage of all of these cores? Or are they all just too feeble with too little RAM to be of any use?</p>

<p>Would it make more sense to focus on machines with fewer, more powerful cores?</p>
","ros"
"788","Do ""nano bots"" (that can fit inside the human body) actually exist?","<p>I was wondering, do we have real nano bots, like the ones in the movies? </p>

<p>I think we have bots which can move through the blood vessels, am I right?</p>
","mobile-robot"
"790","Do ""toy"" robots move technology forwards?","<p>Over the last month, I saw many robots that don't have any real purpose, which made me ask myself: ""Does this have any value?"" I saw dancing robot on CES, advanced lego based robots and also robots combined for very limited purpose. I saw ten year old children playing with robots, and competitions for them. Someone has told me that this is just for education and logic spreading. </p>

<p>In other cases, there were arguments like, ""this is for informing people that everything is going forwards"". I know that people will buy robotic vacuum cleaners because they think that they'll save some time, but these robotic cleaners are not very reliable and I see it only as marketing. </p>

<p>Do these things (children's education, dancing robots, and other instances of <a href=""http://en.wikipedia.org/wiki/Pig_in_a_poke"">selling a pig in a poke</a>) have any value in terms of robotics, and are really advancing the field as manufacturers say?</p>
","research"
"795","How can Microhard 920 series modems be made compatible with Microhard 910 series?","<p>Microhard Systems currently sells several types of 900MHz modems, which are mostly used in robotics and SCADA applications.  One of their product lines, the 910 series (MHX910, n910, Spectra 910), is obsolete and no longer sold.  However, some older equipment is built with OEM versions of the 910 series soldered in place.</p>

<p>Microhard currently sells a 920 series (MHX920, n920, Spectra 920) that shares many of the specs with the 910 series, but cannot establish a radio link with a 910 series modem due to differences in encryption and hopping pattern.  Therefore, if you want to make new equipment communicate with equipment using the 910 series, your options are:</p>

<ol>
<li>De-solder the old 910 modem and replace it with the footprint-compatible 920 modem, or</li>
<li>Reconfigure a 920 series modem to communicate with the 910 series modem.</li>
</ol>

<p>Option 1 is undesirable, since I don't have access to the firmware on the older equipment in question.  Does anyone know how to accomplish option 2?</p>
","radio-control"
"797","Plans to use Vendor ID to identify EtherCAT devices?","<p>I also asked this question on <a href=""http://answers.ros.org/question/52182/plans-to-use-vendor-id-to-identify-ethercat-devices/"" rel=""nofollow"">ROS Answers</a>, but it's not getting much interest there.</p>

<p>Currently the EtherCAT package in ROS uses the slaves' Product IDs to identify the devices, and load the correct drivers. This works great when all of the devices are manufactured by a single vendor, but are there any plans to prevent Product ID collisions when multiple vendors make ROS compatible EtherCAT devices?</p>

<p>We manufacture our own EtherCAT devices, and are just using some large values for Product ID, just hoping that these don't collide with anyone else's. Ideally, ROS would concatenate the vendor and product IDs into a single 64-bit value, and use that to identify the correct driver.</p>
","ros"
"801","Simple Neural Network with hardcoded positions for walk optimisation","<p>I'm building a quadrupedal robot that will learn how to walk. From the <a href=""http://robotics.stackexchange.com/questions/568/is-it-possible-to-run-a-neural-network-on-a-microcontroller"">responses</a> I got from asking if  its possible to run a NN on a micro controller I realised I needed to think of a clever system that wouldn't take 1000 years to be effective and would still be able to demonstrate onboard learning. I've designed a system but I'm not sure how effective it will be.</p>

<p>Firstly I hardcode 5-20 positions for the legs.</p>

<p>I set up a (simple) neural network where each node is a different set of positions for the legs, which I will write.</p>

<p>The robot moves from one node to another and the weight of the joint is determined by how far forward the robot moves.</p>

<p>Eventually there will be strong connections between the best nodes/positions and the robot will have found a pattern of moves that are most successful in walking.</p>

<blockquote>
  <p>How effective would this be in learning to walk?</p>
</blockquote>

<p>Note: instead of positions I could write short gaits and the process would work out which sets work best when combined.</p>
","microcontroller machine-learning walk"
"807","How can I control a fast (200Hz) realtime system with a slow (30Hz) system?","<p>We are currently designing a mobile robot + mounted arm with multiple controlled degrees of freedom and sensors. </p>

<p>I am considering an architecture in two parts:</p>

<ol>
<li><p>A set of realtime controllers (either Raspeberry Pis running an RTOS such as Xenomai or bare metal microcontrollers) to control the arm motors and encoders.  Let us call these machines RTx, with x=1,2,3… depending on the number of microcontrollers.  This control loop will run at 200Hz.</p></li>
<li><p>A powerful vanilla linux machine running ROS to compute SLAM, mocap, and execute high-level logic (decide the robot’s task and compute the motors' desired position and speed).  This control loop will run at 30Hz.</p></li>
</ol>

<p>I know my framework needs to be scalable to account for more motors, more sensors, more PCs (eg. for external mocap). </p>

<p>My main problem is to decide how to have the different RTx communicate with PC1. I have looked at papers related to robots architecture (e.g. <a href=""http://global.kawada.jp/mechatronics/hrp2.html"">HRP2</a>), most often they describe the high level control architecture but I have yet to find information on how to have the low level communicate with the high level and in a scalable way. Did I miss something?</p>

<p>In order to connect the fast RT machines ensuring the motor control with PC1, I have considered TCP/IP, CAN and UART:</p>

<ul>
<li>TCP/IP: not deterministic but easy to put in place. Is non determinism a real issue (as it will only be used at at slow speed 30Hz anyways)?</li>
<li>CAN: slow, very reliable, targeted to cars ( have seen there are some exemples using CAN with robots but it looked exotic)</li>
<li>UART: if I had only had one RT machine for motor control I would have considered UART but I guess this port does not scale well with many RTx
Is TCP/IP really a no go because of its non-deterministic characteristics? It is so easy to use…</li>
</ul>

<p>At the moment no solution really seems obvious to me. And as I can find no serious robot example using a specific reliable and scalable solution, I do not feel confident to make a choice. </p>

<p>Does anyone have a clear view on this point or literature to point to?  Are there typical or mainstream communication solutions used on robots?</p>
","control design communication"
"810","Can ultrasonic and button sensors be run in a VEX analog port?","<p>I'm running out of digital ports, and have no sensors that fit the definition 'analog'. Would it be possible to run a <a href=""http://www.robotc.net/support/vex/WebHelpCortex/index.htm#page=functions_vex/sensors/VEX_2.0_Cortex_-_Sensors.htm"">touch sensor, a quadrature encoder, or an ultrasonic sensor on an analog port?</a> </p>

<p>I'm thinking not, but I didn't run across anything that said otherwise.</p>
","sensors"
"812","How to find out how far a motor has taken a vehicle?","<p>I have a small motorized vehicle with gears as wheels running up and down a track made of gear racks. How can this robot know when it has run half the track? And what's the best method to keep it from running off its track at the end and then return to start.</p>

<p>The robot is carrying water, not exactly the same amount each time, so it will not weigh the same. Therefore it might not be the same amount of steps in the stepper-motor each time.</p>

<p>Here I have some ideas that might work, though I am a beginner, and don't know what's the best solution.</p>

<ul>
<li>GPS tracking it (overkill on such a small scale?)</li>
<li>Some kind of distance measurer</li>
<li>Have a knob it will hit at the middle of the track, telling program to delay for a given time</li>
<li>Track amount of steps the motor has performed (won't be as accurate?)</li>
</ul>
","mobile-robot arduino sensors"
"815","Arduino Motor control","<p>I'm working on a rather low budget project, and need some way to control four or more motors using one Arduino. I've looked at motor shields a little, but I have a shield on top of it already, It does have female input on the top though, so a motor shield may work. Any suggestions?</p>
","control arduino microcontroller motor power"
"819","Adding Rotary Encoders to an Electronic Wheel Chair","<p>We have an electric wheel chair, and are looking to add a rotary encoder to each wheel.  We don't want to hack the motor itself, so want to add the encoder without harming the motor-to-wheel connection.  We will be using an arduino to read the signal.</p>

<p>Does anyone have any experience adding rotary encoders to already assembled wheel assemblies?  </p>
","arduino microcontroller"
"826","Can I use digital animation software to define the movements of humanoid robots?","<p>I'm working with a lifesize (~130cm) humanoid robot (Hubo+) and looking for a way to easily program new motions and gestures into him. Obviously, I could write my own tool, but I am looking for a solution that can leverage existing tools or standards for robot motion. My first thought was trying to use animation software like Blender or Maya, and writing a script to extract the joint angles at keyframes. However, few robotics researchers are probably proficient with Maya. (I know I'm not!)</p>

<p>Is there already some kind of 3D posing tool for robotics that is a standard? The only things I have seen so far that comes close is the <a href=""http://support.robotis.com/en/software/roboplus/roboplus_motion/motionedit/poseedit/roboplus_motion_poseutility.htm"" rel=""nofollow"">Pose Utility</a> in RoboPlus and <a href=""http://web.eecs.utk.edu/~parker/Courses/CS494-529-fall11/NAOs/creatingmovement.html"" rel=""nofollow"">Choregraphe</a> for the Nao, but both programs seem limited to particular robots and don't appear to be extendable to Hubo.</p>

<p>So my questions are:</p>

<ul>
<li><em>Are there standard file formats for robot motion?</em> Not 2D wheeled robot motion. Arm and leg motion! Something equivalent to the .bvh file format used in motion capture.</li>
<li><em>Do you know of any WYSIWYGish tool for creating robot motion using keyframes and inverse kinematics?</em></li>
</ul>
","software motion"
"829","Which is model is best for feedback control of robotic manipulators: MIMO or parallel SISO?","<p>I'm currently designing a robotic arm with 6-DOF, and my goal is to be able to give setpoints for 3d position, velocity and orientation ($x,y,z,\dot{x},\dot{y},\dot{z},\theta,\alpha,\gamma$).</p>

<p>I only had feedback-control for <a href=""http://en.wikipedia.org/wiki/Single-input_single-output_system"">SISO</a> systems so far in College, so, taking the learning curve of multivariable control in consideration, should I approach this problem trying to model the system as a <a href=""http://en.wikipedia.org/wiki/MIMO"">MIMO</a> or multiple SISOs?</p>

<p>If possible please mention possible disadvantages and advantages in each strategy.</p>
","control manipulator robotic-arm"
"832","How do I interpret this data, received by the I2C controller on an NXT 2 brick?","<p>I have been trying to write code to connect a <a href=""http://www.hitechnic.com/cgi-bin/commerce.cgi?preadd=action&amp;key=SPR2010"" rel=""nofollow"">HiTechnic prototype board</a> to my lego brick. Although I am using MSRDS studio, that isn't the issue; reading and writing to the serial port that the device is connected to works fine. </p>

<p>Where I am lacking is that I don't understand the data is that is being sent and received. It goes out and comes back in the form of a byte array. For example:
[128]
[15]
[0]
[2]
[16]
[2]
[8]</p>

<p>Is this byte array converted from hex? What is this response telling me? </p>

<p>Obviously I am a total newbie at this, I can program but I don't really understand electronics and I am trying to make that connection between what I have read about how an I2C controller works and what is happening when I send and receive data over a serial port. </p>
","nxt i2c"
"835","CNCing an injection mold","<p>I want to injection-mold several thousand of a part that fits in a 6"" x 6"" x 2"" bed.</p>

<p>I would like to be able to use only tooling that I can make myself, so I can rapidly iterate on the tooling as production problems are discovered.</p>

<p>I know that typical injection-mold ""hard tooling"" is created using <a href=""http://en.wikipedia.org/wiki/Electrical_discharge_machining"" rel=""nofollow"">electrical discharge machining</a>, which requires first CNCing a carbon positive and then using that as an electrode to spark-burn out a negative mold from hard steel.</p>

<p>However, I do not have the equipment for EDM. Instead, I would prefer to directly CNC the negative mold. I know that a soft enough steel to be CNCed will not last very long as an injection mold, but like I said, my run size is tiny, and I am ok with making a new mold every 500 units or so if necessary.</p>

<p>I am open to buying an endmill that is diamond-tipped, to work with harder steel, but then the limitation will probably be how much torque the CNC can produce on the endmill.</p>

<p>What are some recommendations or links to helpful resources? In particular, what is a good CNC with enough torque, and what blend of steel should I use? Thanks!</p>
","cnc"
"838","What microcontroller should be used for QuadCopter flight control and ESC?","<p>I am working on building my own quadcopter from scratch. I noticed that many solutions available online use arduino, but I am not a fan of arduino. So my questions are: what microcontrollers should be used, what are the crucial features of those microcontrollers etc. I would like to build it from total scratch. I was thinking about PIC microcontrollers.</p>

<p>Also what should be used for ESC, since I would build that from scratch too.</p>

<p>Summing it all up:</p>

<ul>
<li>4 ESCs</li>
<li>Gyro,acceloremeter,gps</li>
<li><p>transceiver</p>

<p>which is about 8 slaves and one master microcontroller.</p></li>
</ul>
","microcontroller quadcopter esc"
"839","Sonar for obstacle avoidance: how many sensors and where to place them?","<p>For avoiding obstacles during 2D robot navigation what is the best position/angle to place the sonar sensors? How many should there be?</p>

<p>I would like to know if there is some theory or examples for the problem of placing. I realize that it depends on the way that the robot moves and its geometry, but I am searching for general answers.</p>
","mobile-robot sensors navigation acoustic-rangefinder"
"840","Is there a tool for building and analysing robots (kinematics, control) visually?","<p>I am reading research papers about robotics and many of them follow the same pattern:</p>

<ol>
<li>some construction is established</li>
<li>kinematical formulas are read from the mechanical structure</li>
<li>the state space is analysed (e.g. how far the robot can reach, what the maximum speed can be, what is left underspecified and how to handle such mathematically incorrect systems and so on)</li>
</ol>

<p>Is there some tool or software product that can receive (as input) the mechanical structure and then output the kinematical formulas?  Preferably, it would provide some kind of plots, analysis, suggestions for optimal design parameters (e.g. length, angles of the sturcture, optimum parameters of motors and so on).  Does this exist?</p>
","software design inverse-kinematics research kinematics"
"845","How to Identify Objects in Space","<p>Using a depth sensing camera like Kinect, I would like to retrieve the position of an predetermined object (e.g. a cup, fork etc so that I would ultimately be able to grab the object). What would be a way to achieve this?</p>
","computer-vision algorithm"
"848","Why must I loop 127 times for a ""7-bit"" address in this example?","<p>I am learning about I2C on the Arduino. I was looking at a sample program to scan for I2C devices and saw this:</p>

<pre><code>// This sketch tests the standard 7-bit addresses
// from 0 to 127. Devices with higher bit address
// might not be seen properly.
</code></pre>

<p>With the following code. </p>

<pre><code>    for(address = 0; address &lt;= 127; address++ ) 
      {
        // The i2c_scanner uses the return value of
        // the Write.endTransmisstion to see if
        // a device did acknowledge to the address.
        Wire.beginTransmission(address);
        error = Wire.endTransmission();

        if (error == 0)
        {
          Serial.print(""I2C device found at address 0x"");
          if (address&lt;16) 
            Serial.print(""0"");
          Serial.print(address,HEX);
          Serial.println("" !"");
    }
}
</code></pre>

<p>As far as I understand it, a bit is just 1.  So, why how do 7 bits loop from 0 - 127? </p>
","arduino i2c"
"851","Optimal Control for a simple pendulum","<p>I'm studying various optimal control methods (and implements them in Matlab), and as test case I choose (for now) a simple pendulum (fixed to the ground), which I want to control to the upper position.</p>

<p>I managed to control it using ""simple"" feedback method (swing-up based on energy control + LQR stabilization for the upper position), and the state trajectory is show in figure (I forgot the axis description: x is theta, y is theta dot.</p>

<p><img src=""http://i.stack.imgur.com/rZAcD.png"" alt=""Swing-up + LQR control state trajectory""></p>

<p>Now I want to try a ""full"" optimal control method, starting with an iterative LQR method (which I found implemented here <a href=""http://homes.cs.washington.edu/~todorov/software/ilqg_det.m"">http://homes.cs.washington.edu/~todorov/software/ilqg_det.m</a>)</p>

<p>The method requires one dynamic function and one cost function (<code>x = [theta; theta_dot], u</code> is the motor torque (one motor only)):</p>

<pre><code>function [xdot, xdot_x, xdot_u] = ilqr_fnDyn(x, u)
    xdot = [x(2);
        -g/l * sin(x(1)) - d/(m*l^2)* x(2) + 1/(m*l^2) * u];
    if nargout &gt; 1
        xdot_x = [ 0, 1;
            -g/l*cos(x(1)), -d/(m*l^2)];
        xdot_u = [0; 1/(m*l^2)];
    end
end

function [l, l_x, l_xx, l_u, l_uu, l_ux] = ilqr_fnCost(x, u, t)
    %trying J = x_f' Qf x_f + int(dt*[ u^2 ])
    Qf = 10000000 * eye(2);
    R = 1;
    wt = 1;
    x_diff = [wrapToPi(x(1) - reference(1)); x(2)-reference(2)];

    if isnan(t)
        l = x_diff'* Qf * x_diff;
    else
        l = u'*R*u;
    end

    if nargout &gt; 1
        l_x = zeros(2,1);
        l_xx = zeros(2,2);
        l_u = 2*R*u;
        l_uu = 2 * R;
        l_ux = zeros(1,2);

        if isnan(t)
            l_x = Qf * x_diff;
            l_xx = Qf;
        end
    end
end
</code></pre>

<p>Some info on the pendulum: the origin of my system is where the pendulum is fixed to the ground. The angle theta is zero in the stable position (and pi in the unstable/goal position).
<code>m</code> is the bob mass, <code>l</code> is the rod length, <code>d</code> is a damping factor (for simplicity I put <code>m=1</code>, <code>l=1</code>, <code>d=0.3</code>)</p>

<p>My cost is simple: penalize the control + the final error.</p>

<p>This is how I call the ilqr function</p>

<pre><code>tspan = [0 10];
dt = 0.01;
steps = floor(tspan(2)/dt);
x0 = [pi/4; 0];
umin = -3; umax = 3;
[x_, u_, L, J_opt ] = ilqg_det(@ilqr_fnDyn, @ilqr_fnCost, dt, steps, x0, 0, umin, umax);
</code></pre>

<p>This is the output</p>

<blockquote>
  <p>Time From 0 to 10. Initial conditions: (0.785398,0.000000). Goal: (-3.141593,0.000000)
   Length: 1.000000, mass: 1.000000, damping :0.300000</p>
  
  <p>Using Iterative LQR control</p>
  
  <p>Iterations = 5;  Cost = 88230673.8003</p>
</blockquote>

<p>the nominal trajectory (that is the optimal trajectory the control finds) is </p>

<p><img src=""http://i.stack.imgur.com/tf9gp.png"" alt=""ILQR optimal trajectory""></p>

<p>The control is ""off""... it doesn't even try to reach the goal...
What am I doing wrong? (the algorithm  from Todorov seems to work.. at least with his examples)</p>
","control"
"854","Sensors for differential drive","<p>I have the following chassis along with an Arduino and a motor shield. <img src=""http://i.stack.imgur.com/DgPKZ.jpg"" alt=""Robot Chassis""></p>

<p>I'm in the process of developing a tracking mechanism for use with differential drive.</p>

<p>Normally, a photo reflector can be placed adjacent to the wheel that will reflect when each <em>spoke</em> passes through therefore allowing code to be written that will accurately measure each wheels position.</p>

<p>The problem I have is that you cannot see the wheels from inside the chassis, only small holes for the driveshaft. Placing sensors on the outside would look ridiculous and a wall crash would cause havoc.</p>

<p>Would I be able to use a photo reflector on the gears (as shown) if I accurately placed it to count each spoke on the gear itself? I'm a bit hesitant though because even a small bump could misalign the sensor - again causing havoc.</p>

<p>So does any one have an idea on how to track the wheel movements?</p>
","arduino two-wheeled"
"857","Can you seed a Kalman filter with a particle filter?","<p>Is there a way of initializing a Kalman filter using a population of particles that belong to the same ""cluster""? How can you determine a good estimate for the mean value (compute weighted average ?) and the covariance matrix ? Each particle is represented as $[ x , y , θ , weight]$.</p>
","localization kalman-filter particle-filter"
"863","How do I simulate an assembly line?","<p>I need to simulate a stream of vehicles, such as on an assembly line. Automatons are performing operations on the vehicles when they come within reach.  The automatons do not keep track of the individual vehicles, they simply collect data.</p>

<p>We need to choose a method of matching the data gathered by each automaton with the vehicle it belongs to.  For example, we could guess the identity of a vehicle using its timing when arriving in the operation range (sensors) of an automaton.</p>

<p>I have to check the possible problems we will face, so I would like a little (hopefully simple) video/simulation tool that I could play with.</p>

<ul>
<li>vehicles could be symbolized has moving black squares</li>
<li>automatons/sensors could be static points or circles.</li>
<li>it should be possible to change the time interval between two vehicles, and their speed, and add some random delays.</li>
</ul>

<p>What kind of software should I search for, or where should I look?</p>

<p>Should I consider to developing it from scratch? </p>
","simulator"
"865","How to tell a stepper motor's position, or detect slippage","<p>I am creating a CNC machine on a budget, using old motors out of printers/scanners/etc. </p>

<p>I am limited to about 650mA for the whole system, so my fear is that when the cutting bit touches the material, the stepper might be moving too quickly and won't have enough torque.  This would mean it will become one rotation behind, which could really mess up a CNC project.</p>

<p>Detecting when the motor ""misses"" a step would allow me to readjust the motor speed until it reaches a balance between working quickly and having adequate torque.  How can I achieve this?</p>
","arduino stepper-motor current cnc"
"869","Building Robotic arm joint","<p>I am very new to robotic design and I need to determine what parts I will need to assemble an arm joint.  The joint will contain one timing belt pulley which a remote motor will be turning, a forearm that the pulley will be rotating and an upper-arm piece that will actually be two parallel arms that will grip the pulley on top and bottom in order to brace the pulley from off axis torque from the timing belt.</p>

<p>I am kind of at a lost as to how to mount all of these together.  I would like to mount the forearm directly to the pulley and then the two parallel arms (comprising the upper-arm) sandwich the top of the pulley and the lower part of the forearm.  This would be attached using a turn table.  Any ideas on how a shaft would mount to these?  Or how to attach the pulley to the arms themselves?</p>

<p>Any kind of direction or links would be greatly appreciated, I don't even know the names of the parts I would be looking for.</p>

<p>In this ASCII art model the dashed lines (-) are the arms.  The arm on the left is the forearm and the two arms on the right are the two parallel parts of the upper arm.  The stars are the belt and the bars (||) are the pulleys at the elbow |E| and shoulder |S|.  </p>

<pre><code>              -----------------
              |E|***********|S|
-----------------
              -----------------
</code></pre>

<p>I am thinking of mounting the pulley to the left arm directly (a bushing?) and then maybe using turntables to mount the pulley to the top arm and another turn table to mount the left arm to the bottom arm.</p>

<p>Here is a picture of the design to help you visualize:</p>

<p><a href=""http://imgur.com/ci9BuKL""><img src=""http://i.stack.imgur.com/qyAQ9.png"" alt=""Double Joint Arm assembly""></a></p>
","design arm joint"
"872","Do I really need a gyro for an airplane flight stabilization system?","<p>I'm working on a basic airplane flight stabilization system, as the precursor to a full autopilot system. I'm using a salvaged Wii Motion Plus and Nunchuk to create a 6DOF IMU. The first goal is to keep the wings level, then mix in the users commands. Am I correct in saying that this would not require a gyro, just a 3 (2?) axis accelerometer, to detect pitch and roll, then adjust the ailerons and elevator to compensate?</p>

<p>Secondly, if we extend my design goal from ""keeping the wings level"" to ""flying in a straight line"" (obviously two different things, given wind and turbulence), does the gyro become necessary, insofar as this can be accomplished without GPS guidance?</p>

<p>I've tried integrating over the gyro values to get roll, pitch &amp; yaw from that, however (as evidenced by this question), I'm at a level in my knowledge on the topic where I'd prefer simpler mathematics in my code. Thanks for any help!</p>
","uav accelerometer imu gyroscope"
"873","3D Mapping from a quadcopter with KINECT","<p>I have a quadcopter robot that has a KINECT on it and i want to do 3D mapping with it. </p>

<ol>
<li>Is KINECT reliable on a moving robot (i.e., can it give me stable images and maps with this movement)?</li>
<li>Is there an SDK for producing 3D maps from KINECT data?  Will SLAM algorithms work?</li>
<li>Is the arduino board on the copter (ATmega 2560) powerful enough to handle this?</li>
</ol>
","arduino slam kinect quadcopter"
"876","Remote car controlling","<p>Before I start asking you for help let you know that I am newbie in electronic field.</p>

<p>All I want to know is the principle of wheel rotation (left-right) from remote car gadget. I am not talking about changing the spin rotation of DC motor (up,down buttons from remote), I am asking about left and right movement of wheel.</p>

<p>I know that spin change depends on polarity of DC motor, so changing polarity changes spin, but what is the principle of changing the left and right positions of front wheels.</p>
","control wheel"
"878","Accurate 3D Printing W/Sketchup","<p>I have 3D printers at my school, but unfortunately they are not super high quality. I want to try 3D printing a model I made on google sketchup, but I would like for it to be fairly accurate.</p>

<p>What measures can I take to prevent error in the model? I understand that I need to export the file as an STL; is there anything I can do to the model before hand to ensure accuracy? </p>

<p>What can I do to calibrate a 3D printer for best results?  </p>
","3d-printing"
"882","Force measurement on grab bars","<p>I recently start a project to measure the force on a bathroom grab bar. The force/load is applied by the person who need to the grab bar for assistant. What I want to measure is the load against the wall and do the the real-time monitoring of the load for further analysis to improve the design.</p>

<p>I am not quite sure about what kind of sensor would be suitable to do the measurement. I am looking at different load cells but cannot get the idea how to mount commercial load cells to do the measurement. What I am trying right now is using strain gauge to measure the strain near the end of the bar(wall side) and roughly calculate the load. I think (might be wrong) there may exists some kind of force/load sensors that can clamp on the bar to do the measurement.</p>

<p>Any sensor types/models or suggestion are welcome.</p>

<p>I also posted this question to EE forum
<a href=""http://electronics.stackexchange.com/questions/57197/how-to-measure-force-that-applied-on-grab-bar"">http://electronics.stackexchange.com/questions/57197/how-to-measure-force-that-applied-on-grab-bar</a></p>
","sensors force"
"883","Measuring speed of movement in Webots","<p>I have been experimenting with different fitness functions for my <a href=""http://www.cyberbotics.com/overview"" rel=""nofollow"">Webots robot simulation</a> (in short: I'm using genetic algorithm to evolve interesting behaviour).</p>

<p>The idea I have now is to reward/punish Aibo based on its speed of movement. The movement is performed by setting new joint position, and currently it results in jerky random movements. I have been looking at the nodes available in Webots, but apart from GPS node (which is not available in Aibo) I couldn't find anything relevant.</p>

<p>What I want to achieve is to measure the distance from previous location to current location after each movement.  How can I do this?</p>
","mobile-robot reinforcement-learning simulator"
"884","Using a Sick laser with Matlab in Windows","<p>Is there a Matlab toolbox available to use <a href=""http://www.sick.com/us/en-us/home/products/product_portfolio/laser_measurement_systems/Pages/indoor_laser_measurement_technology.aspx"" rel=""nofollow"">Sick lasers</a> in Windows?</p>

<p>I found <a href=""http://sourceforge.net/projects/sicktoolbox/"" rel=""nofollow"">one toolbox for Matlab in GNU/Linux</a>.  Is there another way to use Sick laser via Matlab in Windows?</p>
","mobile-robot localization"
"885","Would is be possible to connect a HiTechnic prototype board to an Arduino?","<p>Does anyone know if this is possible? It's just an i2c device right? I mean you would have to cut the cable and make it so you could plug into the pins on the Arduino but you should just be able to use the wire library and say something like. </p>

<pre><code>Wire.beginTransmission(0x10);
</code></pre>

<p>the NXT hardware developers kit tells you what pins are which <a href=""http://mindstorms.lego.com/en-us/support/files/default.aspx"" rel=""nofollow"">http://mindstorms.lego.com/en-us/support/files/default.aspx</a> </p>

<p>Thanks</p>

<p>EDIT. Turns out this is very possible. The main problem was that HiTechnic says the address is 0x10 and it is actually 0x08 but here is a short sketch that reads and prints some into about the device, i.e. the manufacturer and version. </p>

<pre><code>#include &lt;Wire.h&gt;

#define ADDRESS 0x08

void setup()
{
  Wire.begin();
  Serial.begin(9600);
}

void loop()
{
  readCharData(0, 7);
  Serial.println();    
  readCharData(8, 8);
  Serial.println();
  readCharData(16, 8);
  Serial.println();

  Serial.println(""-----------------------------"");

  delay(1000);
}

void readCharData(int startAddress, int bytesToRead)
{
  Wire.beginTransmission(ADDRESS);
  Wire.write(startAddress);
  Wire.endTransmission();

  Wire.requestFrom(ADDRESS, bytesToRead);

  while(Wire.available()) 
  {
     char c = Wire.read();
     Serial.print(c);  
  }
}
</code></pre>
","arduino"
"891","Arduino Nano + Raspberry Pi = UAV Ground Station?","<p>I'm a programmer by trade, and an amateur aerospace nut, with some degree-level training in both fields. I'm working on a UAV project, and while the good people over at <a href=""http://www.diydrones.com/"">DIY Drones</a> have been very helpful, this question is a little less drone-related and a little more general robotics/electronics. Essentially, I'm looking at options for ground stations, and my current rough plan is something like this:</p>

<p>I have a PC joystick with a broken sensor in the base, which I plan to dismantle, separate the handle from the base, insert an Arduino Nano into the (mostly hollow) handle and hook it up to all the buttons and the hat thumbstick. Then, where the hole is that used to accept the stem to the base, I fit a bracket that runs horizontally to hold a smallish touchscreen (think <a href=""http://cdn11.mobilemag.com/wp-content/uploads/2012/12/Razer-Fiona.jpg"">Razer's Project Fiona tablet</a> with only one stick), behind which is mounted a Raspberry Pi. The Nano talks to the RPi over USB as a HID input. The RPi will be running some custom software to display telemetry and other data sent down from the UAV.</p>

<p>My main question whether that Nano would have enough power to run the XBee that provides the telemetry link without causing lag in the control inputs. It's worth mentioning that the UAV will be doing fly-by-wire moderation, so slight stutters won't result in wobbly flying, but serious interruptions will still be problematic - and annoying. It's also worth mentioning that this will only be used as a simplified ""guiding hand"" control; there will ALWAYS be a regular remote control available (not least because of EU flight regulations) so this is just for when I don't want to use that. If that Nano won't do, what are my options? My first thought is to get a second Nano and get that to drive the XBee (the RPi has two USB ports after all) but there may well be a better way.</p>
","arduino control uav raspberry-pi radio-control"
"892","Is it possible to achieve arbitrary precision in camera calibration?","<p>Is it possible to achieve arbitrary precision to the calibration of the extrinsic parameters of a camera or is there a minimum error wich can not be compensated (probably dictated by the camera's resolution)?</p>
","computer-vision calibration"
"896","How to select cameras for a stereo vision system?","<p>I am in the process of building a stereo vision system to be used on a UGV. The system is for a robot that will be used in a competition wherein the robot is teleoperated to find relatively small colored rocks in a large outdoor field. I understand how to calibrate such a system and process the data for a stereo vision system. I do not however know how to select cameras for such a system. What are the best practices for picking cameras for a stereo vision system?</p>
","computer-vision stereo-vision cameras"
"897","High voltage motor control with arduino","<p>I'm trying to control a higher voltage motor than an arduino can source with a pin, with an arduino. I am trying to hook it up to a transistor. The battery pack is not supposed to be 4.8V, it's 6V, 4 D batteries.</p>

<p>Here is the setup:</p>

<p><img src=""http://i.stack.imgur.com/m9QPz.png"" alt=""Circuitry setup""></p>

<p>Here is the arduino code I'm trying to run to it:</p>

<pre><code>int motorpin = 2;

void setup()
{
    pinMode(motorpin, OUTPUT);
}

void loop()
{
    digitalWrite(motorpin, HIGH);
    delay(500);
    digitalWrite(motorpin, LOW);
    delay(500);
}
</code></pre>

<p>Code gives me no errors, but no motor movement happens. What would make this work? Thanks.</p>
","arduino motor"
"900","How do I select the best configuration for a known workspace, load and task?","<p>Given workspace constraints, load and task to be done, how do I select the best configuration of my robot? How do I select between a cartesian or Scara robot for instance? How do I select a manipulator? How do I determine how many axes that I need?</p>

<p>Most of what I have seen is based on experience, rules of thumb and readily available standard devices, but I would like a more formal answer to quantify my choice. Is there some technique (genetic algorithm?) which describes the task, load, workspace, budget, speed etc. and rates and selects an optimal robot configuration or maybe even multiple configurations? How can I be mathematically ensure I ultimately chose the optimal solution?</p>

<p>The only thing I found online was a thesis from 1999 titled <a href=""http://darwin2k.sourceforge.net/thesis.pdf"" rel=""nofollow""><em>Automated Synthesis and Optimization of Robot Configurations: An Evolutionary Approach</em></a> (pdf, <a href=""http://scholar.google.co.uk/scholar?hl=en&amp;q=CMU-RI-TR-99-43&amp;btnG=&amp;as_sdt=1,5&amp;as_sdtp="" rel=""nofollow"">CMU-RI-TR-99-43</a>). It is a synthesis and optimization tool called <a href=""http://darwin2k.sourceforge.net/"" rel=""nofollow"">Darwin2K</a> presented in a thesis written by Chris Leger at CMU. I am surprised no one has updated it or created a tool similar to it.</p>

<p>To provide some context for my question, we are developing a robot to assist the elderly with domestic tasks. In this instance, the robot identifies and picks food items from a previously stored and known location. The hand opens the package and place it in the oven. The pick and place locations are fixed and nearby so the robot is stationary.</p>
","design algorithm industrial-robot theory manipulator"
"907","Calibrate a 2d scanner mounted on a rotary axis","<p>A 2d laser scanner is mounted on a rotary axis. I wish to determine the transformation matrix from the center of the axis to the center of the scanner, using only the input from the scanner and the angle of rotation.</p>

<p><img src=""http://i.stack.imgur.com/bygWj.png"" alt=""enter image description here""></p>

<p>The 2d scanner itself is assumed to be calibrated, it will accurately measure the position of any object inside the plane of the laser, in regards to the scanner origin.</p>

<p>The rotary axis is calibrated as well, it will accurately measure the angle of its own movement.</p>

<p>The scanner is aligned and mounted close to the center of rotation, but the exact offset is unknown, and may drift over time.</p>

<p>Assume it is impractical to measure the position and orientation of the scanner directly. 
I am looking for a way to determine the exact values for the 6 degrees of offset the scanner may have in relation to the axis, determined solely on the 2d information from the scanner and the rotation angle from the axis.</p>

<p><img src=""http://i.stack.imgur.com/JtBDH.png"" alt=""enter image description here""></p>

<p>I am mainly interested in the 4 offsets depicted here, since the other two do not matter in regard to generating a consistent 3d point cloud from the input data.</p>

<p>By scanning a known calibration object, it should be possible to determine these offsets. What are the mathematical formulas for this? </p>

<p>What sort of calibration information is required at a minimum?
Is it for example possible to determine all parameters simply by scanning a flat surface, knowing nothing about the surface except that it is flat?</p>

<p>(The transformation matrix from rotation axis to world is unknown as well, but that one is trivial to determine once the transformation from axis to camera is known.)</p>

<hr>

<p><strong>Example</strong></p>

<p><img src=""http://i.stack.imgur.com/TQh9z.png"" alt=""enter image description here""></p>

<p>On the left the camera is placed exactly on the rotational axis.  The camera scans a planar object with reference points A B and C. Based on the laser distance measurements and the angle of the axis, this planar object can be reconstructed.</p>

<p>On the right, the camera has an unknown offset to the axis. It scans the same object. If the point cloud is constructed without knowing this offset, the planar surface maps to a curved surface. </p>

<p>Can I calculate the offset based on the surface curvature?</p>

<p>If I know the real-world distances and angles between A, B and C, how can I calculate the camera offsets from that? What would be the minimum number of reference points I need for all 4 offsets?</p>
","calibration"
"908","How does the Makeblock threaded slot work?","<p>I've been looking into a <a href=""http://www.makeblock.cc/"" rel=""nofollow"">Makeblock robotics kit</a> but have found no information on the web that comes from end-users, and one of the main advertised features is not clear to me:  The slot threads shown below are straight, while the screw thread that will mate with them is angled.  Is there just very little contact between screw thread and rail thread vs. regular screw hole threads?  Or would the screw want to rest angled somewhat- and then the head would not be flush with the rim of the rail?  Or would the screw deform the aluminum rail if over-torqued?</p>

<p>This is a close up picture of the slot with screws:
<img src=""http://www.wired.com/wiredenterprise/wp-content/uploads//2012/12/makeblock-threaded-slot.jpg"" alt=""makeblock closeup""></p>
","mechanism kit"
"909","GPS tracking device","<p>I'm looking for a GPS tracking device without screen or apps. I just need it to look for the current position of a bus and send it to a server through TCP/IP protocol. This process must be constant so I can have a real-time tracking. The bus already has a wireless access point. </p>

<p>What device can be useful? Do I need another piece of hardware to send the coordinates to the server? I have no experience but... can something like an arduino connected to the gps send the data?</p>
","gps"
"913","Are power and torque required related in some way?","<p>I am designing a new platform for outdoor robotics and I need to calculate the power and/or torque that is needed to move the platform. I have calculated that I need about 720 W of total power to move it (360W per motor), but I don't know how to calculate the torque that I need. </p>

<p>Is it really just about having the required power and ignoring the torque or is there a way to calculate it easily?</p>

<p>Already known parameters of the platform are:</p>

<ul>
<li>Weight of the whole platform: 75 kg.</li>
<li>Number of wheels: 4.</li>
<li>Number of powered wheels: 4.</li>
<li>Diameter of wheels: 30 cm.</li>
<li>Number of motors: 2.</li>
<li>Wanted speed: 180 RPM (3 m/s).</li>
<li>Wanted acceleration: > 0.2 m/s^2</li>
</ul>
","mobile-robot design motor"
"918","Storing Kinect Data on a USB Drive","<p>Does anybody know if Kinect Data can be stored directly onto a USB Drive?? 
I have a Kinect for Windows that i cannot use on Linux(ROS). However what i plan is to mount the Kinect on my robot, store the captured frames on a USB and then un mount the USB ,transfer to Linux and process them on ROS.</p>

<p>Is this possible?? Any suggestions.  </p>
","kinect ros"
"921","How can I send video from my Arduino camera module video to my Android screen?","<p>I'm trying to connect a camera module to my Arduino Mega, connect my Mega to my Android phone (throught BlueTooth or other), and send the live view of the camera to the mobile phone.</p>

<p>I saw a <a href=""http://www.youtube.com/watch?v=qEVXqOJz-GY"">video online</a> that showed this for still images -- an image captured by the camera module on the Arduino was sent to Android and the output image was viewed after a couple of seconds (the time to send image by BT).</p>

<p>Is this doable with live video instead of image?  If yes, please guide me; if no, please suggest some workarounds.</p>
","arduino cameras"
"922","How do I design for a target speed?","<p>I need to make an omni wheeled robot platform (4 wheels), which should go at a minimum speed of 15 cm/s.  I have an idea for the design, but since this is my first time doing something like this I have made <strong>a lot</strong> of assumptions. </p>

<p>I decided to choose the <a href=""http://www.hobbyking.com/hobbyking/store/__13360__TGY_S4505B_40g_4_8kg_0_10sec_Dual_Bearing_Analog_Servo.html"" rel=""nofollow"">TGY-S4505B</a> servos as my motor system. I intend to attach these servos to <a href=""http://store.kornylak.com/ProductDetails.asp?ProductCode=FXA308B"" rel=""nofollow"">FXA308B</a> wheels. Finally, I intend to power my servos with one <a href=""http://www.hobbyking.com/hobbyking/store/__25030__Turnigy_LSD_6_0V_2300mAh_Ni_MH_Flat_Receiver_Pack.html"" rel=""nofollow"">Turnigy LSD 6.0V 2300mAh Ni-MH Flat Receiver Packs</a> (not sure if LiPo is a better choice). I need to be able to run the servos continuously for roughly 8 minutes. You can ignore the microcontroller and other stuff, relatively speaking they will consume much less power. The robot will have four wheels (thus, four servos).</p>

<p>The basic specifications of each servo is:</p>

<ul>
<li>Type: Analog</li>
<li>Gear train: Plastic</li>
<li>Bearings: Dual</li>
<li>Motor Type: Carbon Brushed</li>
<li>Weight: 40g (1.41oz)</li>
<li>Lead: 30cm</li>
<li>Torque: 3.9kg.cm @ 4.8v / 4.8kg.cm @ 6v</li>
<li>Speed: 0.13sec 60°@ 4.8v / 0.10 60° @ 6v</li>
</ul>

<p>So based on my battery pack, I will be running the servos at 6V. That gives me a speed of 60 degrees per 0.10 seconds. I plan on modifying these servos for continuous rotation, and connected them directly to the wheel. Since the wheel has a diameter of ~5 cm, it has a circumference of ~15 cm. Based on these specs, it seems to me that my robot can move at roughly 15 cm/0.6 seconds, or 25 cm/s (quite fast actually). I don't intend to run it constantly at that speed, so in the 8 minute run, assume my average speed to be 20 cm/s.</p>

<p>Are these assumptions reasonable, and are the calculations correct?  I would really appreciate any insight, advice, recommendations, and criticisms you may have.</p>
","mobile-robot wheel rcservo"
"924","TI ARM with stacked RAM","<p>Do any of the TI <a href=""http://en.wikipedia.org/wiki/ARM_system-on-chip_architecture"" rel=""nofollow"">ARM SOC</a>s, e.g. <a href=""http://www.ti.com/lsds/ti/arm/sitara_arm_cortex_a_processor/sitara_arm_cortex_a8/omap3503_15_arm_cortex_a8/products.page?paramCriteria=no"" rel=""nofollow"">OMAP</a> or <a href=""http://www.ti.com/lsds/ti/dsp/video_processors/overview.page"" rel=""nofollow"">Da Vinci</a>, have a version with stacked RAM? (e.g. DDR2 or mDDR) For miniature robots like micro drones, it would be really nice to not need to spend board area on an external RAM chip. Thanks!</p>
","arm"
"939","Will connecting two servo motors double the torque?","<p>For my robot, I am using two continuous rotation servos to spin a threaded rod. I am trying to make this project as cheap as possible. Here are the servos that I can find:</p>

<ul>
<li>Servo #1: This is a very cheap option and it has half of the torque I need.</li>
<li>Servo #2: This has all of the torque my project requires, but it is much more expensive that two of servo #1.</li>
</ul>

<p>Can I hook up two of servo #1 to each end of the rod and have them move synchronized? I can spare a few extra pins on my microprocessor that I am using; that isn't a issue. I know hooking two together will increase torque, but I don't want 75% of the torque I want in this situation. Also, I don't care if I only have 98% of my torque ""goal"" with the extra weight (which probably won't happen) but I don't want to, like I said earlier, have 70, 80, 90% of my ""target goal"" of torque if possible.</p>

<p>Any help appreciated. Thanks in advance.</p>
","motor rcservo"
"940","How do I convert link parameters and angles (in kinematics) into transformation matrices in programming logic?","<p>I'm doing robotics research as an undergraduate, and I understand the conceptual math for the most part; however, when it comes to actually implementing code to calculate the forward kinematics for my robot, I am stuck. I'm just not getting the way the book or websites I've found explain it.</p>

<p>I would like to calculate the X-Y-Z angles given the link parameters (Denavit-Hartenberg parameters), such as the <a href=""http://i.stack.imgur.com/j6Cf6.png"" rel=""nofollow"">following</a>:</p>

<p>$$\begin{array}{ccc}
\bf{i} &amp; \bf{\alpha_i-1} &amp; \bf{a_i-1} &amp; \bf{d_i} &amp; \bf{\theta_i}\\
\\ 
1 &amp; 0 &amp; 0 &amp; 0 &amp; \theta_1\\
2 &amp; -90^{\circ} &amp; 0 &amp; 0 &amp; \theta_2\\
3 &amp; 0 &amp; a_2 &amp; d_3 &amp; \theta_3\\
4 &amp; -90^{\circ} &amp; a_3 &amp; d_4 &amp; \theta_4\\
5 &amp; 90^{\circ} &amp; 0 &amp; 0 &amp; \theta_5\\
6 &amp; -90^{\circ} &amp; 0 &amp; 0 &amp; \theta_6\\
\end{array}$$</p>

<p>I don't understand how to turn this table of values into the proper transformation matrices needed to get $^0T_N$, the Cartesian position and rotation of the last link. From there, I'm hoping I can figure out the X-Y-Z angle(s) from reading my book, but any help would be appreciated.</p>
","kinematics forward-kinematics"
"946","How can I detect the edge of a table?","<p>I'm new to robot making and just got my first arduino to play around.</p>

<p>I want to make a robot that will wander on a table, and it will last longer I think if I could make it avoid falling from the table.</p>

<p>What will be the best way to make it detect the edge of a table so I can make it stop and turn around ? It have to be something reliable and preferably cheap.</p>

<p>It will also be better if I don't need to add extra stuff to the table so I can use it on any surface (my first idea was to draw path lines on the table and make a line follower robot, but I don't like this idea very much).</p>
","sensors"
"948","What is the opposite of 'Antagonistic'?","<p>A robotic joint is connected to two actuators, e.g. air muscles. One flexes the joint, while the other extends it. This arrangement is called 'antagonistic'.</p>

<p><img src=""http://i.stack.imgur.com/7FNlN.png"" alt=""Pneumatic Muscle Joint""></p>

<p>But what if I had an electric motor instead of the air muscles? In that case it can only pull on one tendon at a time, and it's not antagonistic. What it the arrangement called in this case? Untagonistic?</p>
","motor air-muscle"
"952","What's an efficient way to visit every reachable space on a grid with unknown obstacles?","<p>I'm trying to create a map of the obstacles in a fairly coarse 2D grid space, using exploration.  I detect obstacles by attempting to move from one space to an adjacent space, and if that fails then there's an obstacle in the destination space (there is no concept of a rangefinding sensor in this problem).</p>

<p><img src=""http://www.eriding.net/resources/general/prim_frmwrks/images/asses/asses_y3_5d_3.gif"" alt=""example grid""> (for example)</p>

<p>The process is complete when all the reachable squares have been visited.  In other words, some spaces might be completely unreachable even if they don't have obstacles because they're surrounded.  This is expected.</p>

<p>In the simplest case, I could use <a href=""http://stackoverflow.com/a/11556238/2063546"">a DFS algorithm</a>, but I'm worried that this will take an excessively long time to complete &mdash; the robot will spend more time backtracking than exploring new territory.  I expect this to be especially problematic when attempting to reach the unreachable squares, because the robot will exhaust every option.</p>

<p>In the more sophisticated method, the proper thing to do seems to be <a href=""http://en.wikipedia.org/wiki/Boustrophedon_cell_decomposition"">Boustrophedon cell decomposition</a>.<br>
<img src=""http://planning.cs.uiuc.edu/img2836.gif"" alt=""Boustrophedon cell decomposition""></p>

<p>However, I can't seem to find a good description of the Boustrophedon cell decomposition algorithm (that is, a complete description in simple terms).  There are resources like <a href=""http://planning.cs.uiuc.edu/node352.html"">this one</a>, <a href=""http://planning.cs.uiuc.edu/node262.html"">or this more general one on vertical cell decomposition</a> but they don't offer much insight into the high-level algorithms nor the low-level data structures involved.</p>

<p>How can I visit (map) this grid efficiently?  If it exists, I would like an algorithm that performs better than $O(n^2)$ with respect to the total number of grid squares (<em>i.e.</em> better than $O(n^4)$ for an $n*n$ grid).</p>
","algorithm coverage planning"
"953","Dropping PWM on Ardrone Parrot 2.0","<p>I am having some issues with the <a href=""http://ardrone2.parrot.com/usa/"">ARDrone Parrot 2.0</a> and hope someone else may be running into the same thing.</p>

<p>While hovering, the drone is (seemingly) randomly losing altitude then recovering . It is doing so while not being commanded any velocity inputs and should hold altitude. </p>

<p>We are using the drivers from the <a href=""https://github.com/AutonomyLab/ardrone_autonomy/tree/dev-unstable"">ardrone_autonomy (dev_unstable branch) on github</a>. We are able to watch the PWM outputs being sent to the motor and they are dropping from the hover command do a small value before exponentially returning to the hover value when this drop occurs.</p>

<p>The issue could be a communication between the IMU and the onboard controller or on our software control implementation.</p>

<p>Has anyone seen a similar problem or suggestions to test/troubleshoot what is happening?</p>
","ros quadcopter pwm"
"954","Controlling more than 12 Servos with the Arduino Servo library","<p>I'm using Teensy hardware specifically.  I have a Teensy 2.0 and a Teensy 3.0, and from the documentation it seems like there are two 16 bit timers available, and each should be able to control 12 servos.  However, I've attached a logic analyzer and have confirmed that only the first 12 servos attached ever function.</p>

<p>Is there anything special I have to do with my sketch in order to convince the Servo library to allocate the second timer for servos attached beyond number 12?</p>

<p>This works:</p>

<pre><code>#define NUM_SERVOS 12

Servo servos[NUM_SERVOS];

// teensy 2.0 pins
int pin_assignments[NUM_SERVOS] = {0, 1, 2, 3, 4, 5, 20, 19, 18, 17, 16, 15};

void setup() { 
  for(int i = 0; i &lt; NUM_SERVOS; i++) {
    servos[i].attach(pin_assignments[i]) ;
  }
}
</code></pre>

<p>But this, below, only ever shows activity on the first twelve pins attached:</p>

<pre><code>#define NUM_SERVOS 24

Servo servos[NUM_SERVOS];

// teensy 3.0 pins
int pin_assignments[NUM_SERVOS] = {0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23};


void setup() { 
  for(int i = 0; i &lt; NUM_SERVOS; i++) {
    servos[i].attach(pin_assignments[i]) ;
  }
}
</code></pre>
","arduino rcservo"
"957","Adhesion for a heavy wall-climbing robot","<p>I have come across a number of methods for developing wall-climbing robots.</p>

<ul>
<li>Suction</li>
<li>Chemical Adhesion</li>
<li>Gecko like hair adhesion</li>
<li>Electroadhesion</li>
</ul>

<p>Which method would be the best for heavy robots (5kg+)? Are there any other methods that I have missed?</p>
","mobile-robot"
"963","Simulated kinect rotation around X [gazebo bug?]","<p>I asked this question on <a href=""http://answers.ros.org/question/53784/simulated-kinect-rotation-around-x-bug/"">answers.ros.org</a> and <a href=""http://answers.gazebosim.org/question/1177/simulated-kinect-rotation-around-x-bug/"">gazebo.ros.org</a> but still haven't got any answer. I'm posting my question here with the hope someone can help me.</p>

<p>In our robot, the Kinect can be mounted on the side of the arm, as shown in the screenshot below. When running the simulation in Fuerte, I found this weird behaviour. As you can observe on the image, the point cloud does not match the robot model (we see a partial image of the hand/arm at the bottom left of the screenshot, which should be on the robot model).</p>

<p><img src=""http://answers.ros.org/upfiles/13597242445897604.png"" alt=""Rotate Kinect""></p>

<p>As soon as I rotate the kinect against its X axis (so that the kinect is horizontal as you can see on the second screenshot), then the point cloud and robot model are aligned properly.</p>

<p><img src=""http://answers.ros.org/upfiles/13596283675382491.png"" alt=""Horizontal Kinect""></p>

<p>The kinect xacro and dae are the one from the turtlebot. I'm simply attaching them with a rotation:</p>

<pre><code>&lt;joint name=""base_camera_joint"" type=""fixed""&gt;
  &lt;origin  xyz=""0.01216 0.1713 0.433""
       rpy=""-${M_PI/2} ${M_PI/4} -${M_PI/12}"" /&gt; 
  &lt;!-- This -pi/2 in origin rpy is the offending parameter --&gt;

  &lt;parent link=""shadowarm_trunk""/&gt;
  &lt;child link=""camera_link"" /&gt;
&lt;/joint&gt;
</code></pre>

<p>The code can be seen on github.</p>

<p>Any help is greatly appreciated!</p>
","kinect ros simulator"
"964","Extended Kalman Filter using odometry motion model","<p>In the prediction step of EKF localization, linearization must be performed and (as mentioned in <a href=""http://books.google.co.uk/books/about/Probabilistic_Robotics.html?id=k_yOQgAACAAJ&amp;redir_esc=y"" rel=""nofollow"">Probabilistic Robotics [THRUN,BURGARD,FOX]</a> page 206) the Jacobian matrix when using velocity motion model, defined as</p>

<p>$\begin{bmatrix} x \\ y \\ \theta \end{bmatrix}' = \begin{bmatrix} x \\ y \\ \theta \end{bmatrix} + \begin{bmatrix} \frac{\hat{v}_t}{\hat{\omega}_t}(-\text{sin}\theta + \text{sin}(\theta + \hat{\omega}_t{\Delta}t)) \\ \frac{\hat{v}_t}{\hat{\omega}_t}(\text{cos}\theta - \text{cos}(\theta + \hat{\omega}_t{\Delta}t)) \\ \hat{\omega}_t{\Delta}t \end{bmatrix}$</p>

<p>is calculated as </p>

<p>$G_{T}= \begin{bmatrix}
  1 &amp; 0 &amp; \frac{υ_{t}}{ω_{t}}(-cos {μ_{t-1,θ}} + cos(μ_{t-1,θ}+ω_{t}Δ{t})) \\
  0 &amp; 1 &amp; \frac{υ_{t}}{ω_{t}}(-sin {μ_{t-1,θ}} + sin(μ_{t-1,θ}+ω_{t}Δ{t})) \\
  0 &amp; 0 &amp; 1
 \end{bmatrix}$.</p>

<p>Does the same apply when using the odometry motion model (described in the same book, page 133), where robot motion is approximated by a rotation $\hat{\delta}_{rot1}$, a translation $\hat{\delta}$ and a second rotation $\hat{\delta}_{rot2}$ ? The corresponding equations are:</p>

<p>$\begin{bmatrix} x \\ y \\ \theta \end{bmatrix}' = \begin{bmatrix} x \\ y \\ \theta \end{bmatrix} + \begin{bmatrix} \hat{\delta}\text{cos}(\theta + \hat{\delta}_{rot1}) \\ \hat{\delta}\text{sin}(\theta + \hat{\delta}_{rot1}) \\ \hat{\delta}_{rot1} + \hat{\delta}_{rot2} \end{bmatrix}$.</p>

<p>In which case the Jacobian is</p>

<p>$G_{T}= \begin{bmatrix}
  1 &amp; 0 &amp; -\hat{\delta} sin(θ + \hat{\delta}_{rot1}) \\
  0 &amp; 1 &amp; -\hat{\delta} cos(θ + \hat{\delta}_{rot1}) \\
  0 &amp; 0 &amp; 1
 \end{bmatrix}$.</p>

<p>Is it a good practise to use odometry motion model instead of velocity for mobile robot localization?</p>
","localization kalman-filter"
"965","Firmata over nRF24","<p>I'm having some technical problems... I'm trying to use Firmata for arduino but over nrf24, not over Serial interface. I have tested nRF24 communication and it's fine. I have also tested Firmata over Serial and it works. </p>

<p>Base device is simple ""serial relay"". When it has data available on Serial, read it and send it over <a href=""http://maniacbug.github.com/RF24Network/index.html"" rel=""nofollow"">nRF24 network</a>. If there is data available from network, read it and send it through Serial.</p>

<p>Node device is a bit complex. It has custom <a href=""https://github.com/arduino/Arduino/tree/master/libraries/Firmata"" rel=""nofollow"">Standard Firmata</a> where I have just added write and read override. </p>

<p><img src=""http://i.stack.imgur.com/nEj0s.png"" alt=""Diagram""></p>

<p>Read override id handeled in <code>loop</code> method in this way:</p>

<pre><code>while(Firmata.available())
    Firmata.processInput();

// Handle network data and send it to Firmata process method
while(network.available()) {
    RF24NetworkHeader header;
    uint8_t data;
    network.read(header, &amp;data, sizeof(uint8_t));
    Serial.print(data, DEC); Serial.print("" "");
    Firmata.processInputOverride(data);
    BlinkOnBoard(50);
}

currentMillis = millis();
</code></pre>

<p>Firmata <code>processInputOverrride</code> is little changed method of <code>processInput</code> where <code>processInput</code> reads data directly from <code>FirmataSerial</code>, and in this method we pass data down to method from network. This was tested and it should work fine.</p>

<p>Write method is overloaded in a different way. In <code>Firmata.cpp</code> I have added an method pointer that can be set to a custom method and used to send data using that custom method. I have then added custom method call after each of the <code>FirmataSerial.write()</code> call:</p>

<pre><code>Firmata.h
...
size_t (*firmataSerialWriteOverride)(uint8_t);
...

void FirmataClass::printVersion(void) {
  FirmataSerial.write(REPORT_VERSION);
  FirmataSerial.write(FIRMATA_MAJOR_VERSION);
  FirmataSerial.write(FIRMATA_MINOR_VERSION);
  Firmata.firmataSerialWriteOverride(REPORT_VERSION);
  Firmata.firmataSerialWriteOverride(FIRMATA_MAJOR_VERSION);
  Firmata.firmataSerialWriteOverride(FIRMATA_MINOR_VERSION);
}
</code></pre>

<p>I have then set the overrided write method to a custom method that just writes byte to network instead of <code>Serial</code>.</p>

<pre><code>size_t ssignal(uint8_t data) {
    RF24NetworkHeader header(BaseDevice);
    network.write(header, &amp;data, sizeof(uint8_t));
}

void setup() {
...
Firmata.firmataSerialWriteOverride = ssignal;
...
}
</code></pre>

<p>All stages pass right (I guess) and then I don't get any response from Node when I request pin states</p>

<pre><code>...
&lt; f0 6a 7f 7f 7f ... 7f 0 1 2 3 4 5 6 7 8 9 a b c d e f f7 // analog mapping
&gt; f0 6d 0 f7 // sysex request pin 0 state and value
&gt; f0 6d 1 f7 
&gt; f0 6d 2 f7
...
&gt; f0 6d 45 f7
// And I wait for response...
</code></pre>

<p>There is no response. Any ideas why would that happen? Node receive all messages correctly and code for handling pin states exist.</p>
","arduino serial c++"
"970","Do magnets affect IMU values?","<p>Im in the process of making a robot which requires 12 3x10mm cylindric magnets for the construction. They are 30mm from the center of the robot where I plan to have the IMU. </p>

<p>I was thinking about using MPU-6050. Do magnets affect the values? If yes, is there a solution for it? like maybe I could have a shield or something around the IMU?</p>
","sensors imu"
"975","Why do space probes need heating?","<p>I know that temperature influences the characteristics of semiconductors and other materials, but we know how and can take that into account. Furthermore, lower temperatures makes electronics more efficient, sometimes even superconducting.</p>

<p>I remember reading somewhere that engineers building Curiosity even considered low temperature electronics for the motors driving the wheels but still decided against it in the end.</p>

<p>Why is it, apparently, so hard to build components with operating temperatures matching those on Mars, Europa, or in space?</p>

<p><strong>Edit:</strong> None of the answers address my question thus far. I know that all parts, both electronic and mechanical, and greases and so on have relatively narrow working temperatures. My question is, why don't we build special cold metals and cold greases and cold chips that have their narrow operating temperature band at -100 C or whatever?</p>

<p>Valid answers could be: it's too expensive, insufficient science has been done to determine materials appropriate for such cold, such cold materials cannot be manufactured in the sweltering heat of planet Earth.</p>
","heat-management cooling"
"976","What is a good approach to a Quadruped Gait?","<p>I have a small quadruped with three degree of freedom legs which I have been working on: <a href=""http://www.thingiverse.com/thing:50125"">3DOF Mini Quadruped</a>.</p>

<p>My original code for it was a simple servo controller on the arduino, and Scala code which would send servo commands over the wire.  I did all the Inverse Kinematics and Gait logic in Scala, and got it to walk: <a href=""https://www.youtube.com/watch?v=VsHNA3oLXC8"">3dof quadruped first gait</a>.</p>

<p>My Gait logic in Scala was somewhat naive; it depended on the legs being in the right position at the beginning (one side extended fore and aft, the other side in toward each other).  The logic was simply translate all four feet backward by 1mm along y, and whenever a coxa angle became excessively rearward, stop and perform a little routine where that foot is lifted 10mm in z, then translated forward 60mm along y, and set back down.  Naive, but effective.</p>

<p>Now, I have rewritten my IK code in arduino C, and I'm trying to decide how to move forward with the Gait dynamics.  I've had a hard time finding good, easy to understand resources about gaits.  I do have some knowledge about the difference between dynamically stable gaits (like creep gaits) where the body is a stable tripod at all times and dynamically unstable gaits (walking, trotting), where two legs are off the ground at a time and the body is essentially falling forward into the advancing leg.</p>

<p>I had some thoughts about state machines and trying to calculate whether the body center falls within a triangle made by the remaining feet to decide which foot was safe to lift, but I'm not sure if these are ideas worth exploring.</p>

<p>I know this is kind of an overly general question, but I'm interested to see how other people have attacked this problem, and about all I've been able to find are research papers.</p>
","mobile-robot walk"
"979","One BEC for multiple ESC (Quadcopter)","<p>I'm building a quadcopter and have discovered that most ESC have a built-in BEC, but I was wondering if it wouldn't be better to use only one.</p>

<p>What if I delivered power to my four ESC with a unique BEC ? Would that work ?
I think this would be easier to configure (you have to set it up only once for the four ESC) and it would prevent each ESC from having it's own behavior.</p>

<p>Am I doing it wrong ?</p>

<p>Here is an image of what I'm talking about :</p>

<p>Edit : trying to find the original image and upload it.</p>

<p>Given the <a href=""http://robotics.stackexchange.com/a/982/37"">answer by Ian McMahon</a> it appears that this schema is not the right thing to do, since I had misunderstood the role of BECs.</p>

<p>So would the right schema would look like this ?
Edit : trying to find the original image and upload it.
I'm still not sure if I'm getting it.</p>

<p>Do I need 4 ESCs with integrated BECs and connect all three cables to flight controller ?</p>
","electronics quadcopter bec esc"
"984","Does anyone know what might be giving me this error coming from an i2c device","<p>Here is the background. I am trying to write a service for the HiTechnic prototype board. </p>

<p>Using the Appendix 2 from the blue tooth developers kit from Lego's site I am able to understand what is going on with this service I am trying to build however the response I get is always 221 = 0xDD = ""Communication Bus Error"" or 32 = 0x20 = ""Pending communication transaction in progress"". </p>

<p>I figured out that the HiTechnic prototype board is using <a href=""http://refriedgeek.blogspot.com/2013/02/connecting-to-hitechnic-prototype-board.html"" rel=""nofollow"">i2c address 0x08</a> so I modified the brick code to use that address instead of the standard 0x02. It goes out and configures the device, I get a response and then it does an LSWrite which seems OK then I get a get an error when it does the LSGetStatus. </p>

<p>I know this thing works - I can bit bang it all day long with an Arduino but I only did that to test it out - <a href=""http://refriedgeek.blogspot.com/2013/02/connecting-to-hitechnic-prototype-board.html"" rel=""nofollow"">see this link</a></p>

<p>I am not sure what else to try. Here is how I am setting it up in the connect to brick handler. </p>

<pre><code> pxbrick.Registration registration = new pxbrick.Registration(
                new LegoNxtConnection(LegoNxtPort.Sensor1),
                LegoDeviceType.DigitalSensor,
                Contract.DeviceModel,
                Contract.Identifier,
                ServiceInfo.Service,
                _state.Name);
        Debugger.Break();
        // Reserve the port
        LogInfo(""ConnectToBrickHandler"");
        yield return Arbiter.Choice(_legoBrickPort.ReserveDevicePort(registration),
            delegate(pxbrick.AttachResponse reserveResponse)
            {
                Debugger.Break();
                if (reserveResponse.DeviceModel == registration.DeviceModel)
                {
                    registration.Connection = reserveResponse.Connection;
                }
            },
            delegate(Fault f)
            {
                Debugger.Break();
                fault = f;
                LogError(""#### Failed to reserve port"");
                LogError(fault);
                registration.Connection.Port = LegoNxtPort.NotConnected;
            });
</code></pre>

<p>I have also tried setting AnyPort as well so that it will hit the TestPortForI2CSensorHandler that just does what I explained before - it seems to set the mode fine and then gets an error when it tries to read the device information. </p>

<p>Here is the data. - this first part is the set input more - both the message and response - You can see it is totally fine. </p>

<p>Send command data. </p>

<pre><code>0 
5
0 
11 
0 
receive command data. (_commState.SerialPort.Read(receiveData, 0, packetSize);) 
2 
5 
0
</code></pre>

<p>Then it does an LSWrite - everything still seems fine... You can see I have modified the NxtComm code to use 0x08 instead of 0x02 which it would normally use, then the last byte is also 0x08 which is the starting address of the manufacturer. It's asking for 16 bytes which would be the manufacturer and sensor type. like I said - I know that works I can print that info out using the Arduino.  </p>

<pre><code>128 
15 
0 
2 
16 
8 // i2c address
8 //I assume this is what address I want to read from? 
Got response: True Error code Success [02/25/2013 02:20:31]
-- SendCommandHandler (NxtComm) [02/25/2013 02:20:31]
--- RequestResponseHandler (NxtComm) [02/25/2013 02:20:31]
--- CommSendImmediateHandler (NxtComm) [02/25/2013 02:20:31]
Send command data.
</code></pre>

<p>Then it tries to get the status</p>

<pre><code>0 
14 
0 
</code></pre>

<p>Here is the response... </p>

<pre><code> 2 
14 
32 
 0 
</code></pre>

<p>It's either 32 or 221. It's making me nuts... </p>

<p>If anyone has anything that might help me out I would so much appreciate it. At this point I am running out of ideas. I can see what is going on, I can understand the entire transaction but can't seem to figure out why it just errors out like that. </p>

<p>Also - just for grins I tried 0x10 which is what they tell you on the HiTechnic website. That gets a response of 2,14,0,0 from the NXT brick - that would indicate there is no data but as I pointed out I can get data using the Arduino. How could I have two different I2C device addresses? </p>
","nxt i2c"
"985","Which is easier/cheaper: Hbridge vs ESC for controlling a motor?","<p>I was able to find a small ESC for about $12 off of ebay.  If you were designing a robot, would you see that and think?  </p>

<blockquote>
  <p>\$12 bucks for an ESC that connects to simple pulse-wave interface -
  sign me up!</p>
</blockquote>

<p>Or would you think:</p>

<blockquote>
  <p>\$12 just to control a motor? I could throw together an H-bridge for
  $0.50 and be done with it.</p>
</blockquote>

<p>My robot in particular actually has two motors and therefor $24 to control the two of them.  But the interface is really easy (plus has the added advantage of being R/C vs computer controlled with a simple change of connectors.</p>

<p>Which way would you go?</p>
","design electronics wheeled-robot"
"987","problem in serial communication between PC and ATMEGA 8535(AVR)","<p>I have written code to send data from controller to pc through serialport using interrupt
but it echos garbage value exactly 3 times back.</p>

<pre><code>    ISR(USART_RX_vect)
{
    unsigned char index = UDR;

    UDR = index;        
}

void uartInit()
{
    UCSRA=0x00;
    UCSRB=0x18;
    UCSRC=0x86;
    UBRRH=0x00;
    UBRRL=0x67;
    UCSRB |= (1 &lt;&lt; RXCIE); // Enable the USART Receive Complete interrupt (USART_RXC)
    _delay_ms(10);
}

int main(void)
{
    uartInit();
    lcd_init(); 
    sei();

   while(1)
    {
     }
}
</code></pre>

<p><strong>EDIT:</strong>
Function used to set baud rate..</p>

<pre><code>#define FOSC 16000000// Clock Speed
#define BAUD 9600
#define MYUBRR FOSC/16/BAUD-1


void USART_Init( unsigned int baud )
{
/* Set baud rate */
UBRRH = (unsigned char)(baud&gt;&gt;8);
UBRRL = (unsigned char)baud;
/* Enable receiver and transmitter */
UCSRB = (1&lt;&lt;RXEN)|(1&lt;&lt;TXEN);
/* Set frame format: 8data, 1stop bit */
UCSRC = (0&lt;&lt;USBS)|(3&lt;&lt;UCSZ0);
}
</code></pre>
","serial avr"
"989","Local Localisation with particle filter","<p>I am doing Local Localisation with sonar, particle filter (i.e all particles are initially with robot pose).</p>

<p>I have grip map of environment. When I execute algorithm in environment (where doors are closed/open), particles are not able to followup the robot.</p>

<p>I don't have random particles since I know the initial position of the robot exactly.</p>

<p>Adding random particle will change the pose of robot (i am find median of particles as robot pose).</p>

<p>Any idea/methods how to improve local localisation?</p>

<p>I want to know, do I need random variable if I am doing local localisation? And how do I improve localisation if there are many changes in the map without adding random particles?</p>
","mobile-robot localization odometry particle-filter"
"993","Conventional Land Vehicle Dynamic Models for GPS/INS augmentation","<p>I am looking to augment a GPS/INS solution with a conventional land vehicle (car-like) model. That is, front-wheel steered, rear wheels passive on an axle.  I don't have access to odometry or wheel angle sensors.</p>

<p>I am aware of the Bicycle Model (e.g. Chapter 4 of <a href=""http://link.springer.com/book/10.1007/978-3-642-20144-8/page/1"" rel=""nofollow"">Corke</a>), but I am not sure how to apply the heading/velocity constraint on the filter.</p>

<p>So my questions are:</p>

<ol>
<li>Are there any other dynamic models that are applicable to the land vehicle situation, especially if they have the potential to provide better accuracy?</li>
<li>Are there any standard techniques to applying such a model/constraint to this type of filter, bearing in mind I don't have access to odometry or wheel angle?</li>
<li>Are there any seminal papers on the topic that I should be reading?</li>
</ol>
","gps dynamics"
"994","Chaining Kalman filters","<p>My team is building a robot to navigate autonomously in an outdoor environment. We recently got a new integrated IMU/GPS sensor which apparently does some extended Kalman filtering on-chip. It gives pitch, roll, and yaw, north, east, and down velocities, and latitude and longitude.</p>

<p>However, we also have some encoders attached to our wheels, which provide linear and angular velocities. Before we got this new IMU/GPS sensor, we made our own EKF to estimate our state using the encoders and some other low-cost sensors. We want to use this new sensor's on-chip filter, but also incorporate our encoders into the mix.</p>

<p>Is there any problem with chaining the filters? What I mean is, we'd use the output of the IMU/GPS sensor's on-chip EKF as an update to our own EKF, just as we use the data read from the encoders as an update to our EKF. It seems reasonable to me, but I was wondering what is usually supposed to be done in this case. </p>
","kalman-filter imu navigation"
"997","How do we make our robot work?","<p>As a holiday project we are building a surveillance robot that is capable of transmitting live images using a webcam and is also capable of lifting small objects.  </p>

<p>It uses a <em>CC2500 module</em> for communicating with the robot.  The interface is designed in <em>Visual Basic 6</em> and it allows us to set the port of the computer to which the transreceiver is connected. It is connected via a USB to RS232 port (USB side is connected to the computer).</p>

<p>We tried the settings as shown below and we get an error that the config is unsuccessful. We have tried the same settings in 4 different computers so far and it did not work.  </p>

<p><img src=""http://i.stack.imgur.com/BJRwL.png"" alt=""Screenshot of the GUI ""></p>

<p>Circuit diagram for the robot:</p>

<p><img src=""http://i.stack.imgur.com/VDb5R.jpg"" alt=""x""></p>

<p>It is designed using an Atmel 89S52.</p>

<p><strong>Please tell us what settings to try to make it work</strong></p>
","electronics computer-vision wheeled-robot robotic-arm"
"998","Motors response different with high-frequency PWM","<p>We are making a junior soccer robot and we just got our brilliant motors from Maxon.
Setting the PWM timer to low-frequencies (around 39kHz or 156 kHz ) the robot acts as expected. But this produces some problems. </p>

<ol>
<li>It puts a heavy current on batteries (around 1.5A for 3 motors which is far too high).</li>
<li>The high current causes our motor drivers (L6203) to heat up very quickly and even heat-sinks won't help them.</li>
<li>The motors make such a bad sound as they are screaming and this is not normal.</li>
</ol>

<p>In contrast when I configure the timer on high-frequencies (such as 1250 kHz or 10000 kHz) the current drops off to 0.2A which is ideal and the sounds quit down.
But this causes a problem that our 3 motors when set to run on their highest speed (PWM set to 255) don't run by the same rpm. like one of them runs slower than others making robot turn to a specific side and so our handling functions fail to work correctly.</p>

<p>Asking someone he told me that the drivers don't respond the same to frequencies thus resulting in different speeds and because on low frequencies the difference is very small I won't notice it but on higher frequencies the difference becomes bigger and noticeable.</p>

<p>So is there any workaround for this problem? or I should continue using low frequencies? </p>

<p>PS: I'm using ATMEGA16 as the main controller with a 10 mHz external crystal. </p>
","pwm avr"
"999","For servos, can it be implied that `Holding Torque = Operating Torque * 2` like with steppers?","<p>I am following a guide that recommends using stepper motors and it has an approximate holding and operating torque. It says that if you don't know the operating torque, it is often half of the holding torque. I am adapting this to use with a servo and I was wondering can this same formula be used with a servo. My servo has approximately <code>1.98 kg/cm</code> of torque so does that mean that I can estimate the operating torque would be <code>~1 kg/cm</code>?</p>

<p><strong>A couple of things:</strong></p>

<ul>
<li>I know operating torque and holding torque are different. This is just a estimate-it isn't an exact science.</li>
<li>I know servos are harder to find their location (75 degrees, etc.) than to use a stepper and assume that it worked. I have external means of finding the location.</li>
</ul>
","rcservo stepper-motor"
"1005","Controlling the Power of a Solenoid","<p>I am trying to control the force of a solenoid. My current system has a bank of capacitors connected to a relay. In order to control the force (how hard I am trying to hit the object) I am increasing or decreasing the the time the relay is on. The problem is this works but it either hits with too much force or way too much force. I can turn the relay on for 5 ms or more. If I try to turn it on for 1 ms it does not even respond. (I am using a mechanical relay.)</p>

<p>I would like to have more control on how much of the energy I discharge so I can control how hard/soft the solenoid moves (say discharge only 10 percent of the total energy stored so it hits slower). While searching I found out about Solid State Relays which according to wikipedia can be switched on an off way faster that mechanical relay (of the order of microseconds to milliseconds).</p>

<p>So my question is am I on the right track? or is there something better to achieve what I am trying to achieve?</p>
","control"
"1006","Taylor Series expansion for EKF","<p>In <a href=""http://www.probabilistic-robotics.org/"" rel=""nofollow"">Probablistic Robotics by S. Thrun</a>, in the first section on the Extended Kalman Filter, it talks about linearizing the process and observation models using first order Taylor expansion.  </p>

<p>Equation 3.51 states:</p>

<p>$g(u_t,x_{t-1}) \approx g(u_t,\mu_{t-1}) + g\prime(u_t, \mu_{t-1})(x_{t-1} - \mu_{t-1})$</p>

<p>I think $\mu_{t-1}$ is the state estimate from the last time step.  My question is: what is $x_{t-1}$?  </p>

<p>Also, the EKF algorithm following this (on table 3.3) does not use the factor $(x_{t-1} - \mu_{t-1})$ anywhere, only $g\prime(u_t, \mu_{t-1})$.  So after being confused about $x_{t-1}$, I'm left wondering where it went in the algorithm.</p>
","kalman-filter theory ekf"
"1011","Charging multiple LiFePO4 batteries at the same time?","<p>I'm looking to use 4 of <a href=""http://www.batteryspace.com/lifepo426650cell32v3300mah16.5arate10whunapproved.aspx"" rel=""nofollow"">these 3.2V LiFePO4 batteries</a>. I intend to have 2 pairs of 2 in series in parallel. So two 6.4V battery packs in parallel. Because my setup will be run off of this, it will also be easiest to recharge the batteries using the same setup. To accomplish this, I'm looking to charge all the batteries at once using <a href=""http://www.batteryspace.com/Smart-Charger-0.7-A-for-6.4V-2-cells-LiFePO4-Battery-Pack-100-240VAC.aspx"" rel=""nofollow"">this 6.4V LiFePO4 smart charger</a>. From a simplistic standpoint, the resulting voltage should be correct and this should work fine. However, I know (from a <a href=""http://robotics.stackexchange.com/questions/634/how-to-charge-a-lifepo4-battery"">previous question</a>) that LiFePO4 battery chargers are a bit more complex then a basic voltage supply and check. Would the setup I've described work correctly? And in general, will a LiFePO4 smart charger be able to charge several batteries of the correct voltage at the same time so long as it doesn't try to charge them at too high an amperage? Or does a LiFePO4 battery also have a minimum amperage cutoff point to charge such that trying to charge more than one battery at a time will cause problems? Any other issues I didn't mention? Thank you much!</p>
","battery"
"1014","How would I go about making an art drawing robot like this?","<p>I saw this art drawing robot on youtube:</p>

<p><a href=""http://www.youtube.com/watch?v=Wo15zXhFdzo"">http://www.youtube.com/watch?v=Wo15zXhFdzo</a></p>

<p>What do I need to learn in order to build something like that? What are some beginner oriented projects that could lead up to building something like this?</p>

<p>I'm an experienced programmer but I have very little hardware experience.</p>
","robotic-arm"
"1018","Connecting multiple different voltage servos to the same controller","<p>I am using the <a href=""http://www.pololu.com/file/0J37/ssc03a_guide.pdf"" rel=""nofollow"">Pololu Micro Serial Servo Controller</a> connected to an Arduino and multiple other servos (4 total) to make a robot arm.  Two of the four servos require 4-6 volts, while the other 2 require 7-10 volts, so I am planning on powering all the servos separate from the Pololu.</p>

<p>I have the Arduino and Pololu connecting to each other correctly (flashing green led), but the servo(s) don't move when plugged in to the control pins.  All the servos work correctly when plugged into a servo-tester.</p>

<p>I think that this problem could be fixed by connecting the grounds of the servos to the ground of the Pololu, but would like advice because I am not sure if it will work, or will end up frying one of the parts (We already fried a pololu).</p>

<p>Would connecting the grounds of the batteries to the ground of the Pololu help, or damage the parts?</p>

<p><img src=""http://i.stack.imgur.com/Gf2t4.jpg"" alt=""Wiring Diagram"">, but I couldn't figure out how to show the micro serial servo controller.</p>
","rcservo"
"1020","How to make an ""invisible line following robot""?","<p>I would like to build a robot which follows a virtual path (Not a visible path like a 'black line on a white surface', etc).</p>

<p>I'm just enthusiastic by seeing some sci-fi videos which show robots carry goods and materials in a crowded place. And they really don't follow a physical line. They sense obstacles, depth, etc.  </p>

<p>I would like to build one such robot which follows a specific (virtual)  path from point A to B.</p>

<p>I have tried a couple of things:</p>

<ol>
<li><p>Using a magnetic ""Hall effect"" sensor on the robot and wire carrying current (beneath the table). The problem here was that the vicinity of the hall effect sensor is so small (&lt; 2cms) that it is very difficult to judge whether robot is on the line or off the line. Even using series of magnets couldn't solve this issue, as my table is 1 inch thick. So this idea flopped :P</p></li>
<li><p>Using an ultraviolet paint (on a line) and using UV leds on the robot as sensors. This will give more Zig-Zag motion for the robot. And due to potential threats of using UV light source, even this idea flopped :P</p></li>
</ol>

<p>I finally thought of having a camera on top and using image processing algorithms to see whether robot is on the line or diverging.</p>

<p>Is there any better solution than this? Really looking for some creative and simple solutions. :)</p>
","mobile-robot localization wheeled-robot industrial-robot line-following"
"1021","looking for a miniature joystick, but in reverse","<p>Does anyone know if small mechanical actuators exist which can be controlled electrically, sort of like a miniature joystick, but in reverse.  Instead of it picking up mechanical movement and outputting electrical signals, I want it to generate mechanical movement controlled via my electrical input signals.  I’ve searched for : electromechanical actuators, not finding what I need. Think of a pencil attached to a surface which can pivot to point anywhere in its half dome.  I’m thinking small, on the order of an inch.  It will not be load bearing. </p>

<p>My goal is to programmatically control the normal pointed to by a small flat surface attached to the end of each joystick rod.  Accuracy is more important than speed.  From across a small room, say 10' by 10', I'd like the surface normal to accurately point to arbitrary objects in the room, say a person walking across the room.  If I can cheaply buy/build such mechanisms to control the movement of these small flat surfaces, I would like dozens places across the walls of the room.</p>

<p>Its for an electromechanical sound project I’m planning. </p>
","control actuator"
"1023","Servo motor considerations for a quadruped","<p>I'm building a quadruped and I'm not sure of the features I should be looking for in a servo motor. e.g. digital vs analog, signal vs dual bearings. Some of the ones I'm considering are <a href=""http://www.servodatabase.com/compare?servos=806,1660,1664,1666http://www.servodatabase.com/compare?servos=806,1660,1664,1666"" rel=""nofollow"">here</a></p>
","rcservo walking-robot"
"1025","Cant see Kinect Data in ROS","<p>I am working on this project that involves using the Kinect for XBOX 360S with ROS.</p>

<p>I did all the steps mentioned in the ROS Tutorials to have Openni Installed and the Prime sense and other drivers. and when i go to Openni samples i see a output. But in ROS i do a roscore and in another terminal do a roslaunch openni_launch openni.launch. And it loads with the regular calibration warnings and service already registered errors. Then in another terminal i open Rviz which gives a error /.rviz/display_config does not exist. And even though i accept the error and go ahead i see a black window which shows no output ,even if i do all tasks mentioned at the RVIZ Tutorials. Also i tried running ""rosrun image_view image_view image:=/camera/rgb/image_color"" and it shows up a blank window with no output. How do i resolve this and get ros to show my kinect data??</p>

<p>I need to run RGBDSLAM and use this kinect later.</p>

<p>I am on Ubuntu 12.04 and ROS-Fuerte.</p>

<p>Well when i launch the openni.launch it starts as usual except for the errors ¨Tried to advertise a service that is already advertised in this node.</p>

<p>And when i run a rostopic it just says subscribed to the /camera/depth_registered/points and cursor keeps blinking.
Even subscribing to the rectified topics just says subscribed and nothing more happens.</p>
","kinect ros"
"1032","How is PIV control performed?","<p>I'm considering experimenting with PIV control instead of PID control. Contrary to PID, PIV control has very little explanation on the internet and literature. There is almost a single source of information explaining the method, which is a <a href=""http://parkermotion.com/whitepages/ServoFundamentals.pdf"">technical paper by Parker Motion</a>. </p>

<p>What I understand from the control method diagram (which is in Laplace domain) is that the control output boils down to the sum of:</p>

<ul>
<li>Kpp*(integral of position error)</li>
<li>-Kiv*(integral of measured velocity)</li>
<li>-Kpv*(measured velocity)</li>
</ul>

<p>Am I correct? Thank you. </p>
","control servos pid"
"1033","What is the actual physical actuated quantity when controlling the position of a servo?","<p>I'm trying to learn about servo control. I have seen that the most generic position control method for servos is PID, where the control input is position error. However, I am not sure about what is the actuated quantity. I am guessing that it is one of:</p>

<ul>
<li>Voltage applied to the motor</li>
<li>Current applied to the motor</li>
</ul>

<p>I am then guessing that the actuated quantity gets turned into one of:</p>

<ul>
<li>Torque that the motor exerts</li>
<li>Angular velocity that the motor runs at</li>
</ul>

<p>I haven't been able to get my hands on and explicitly control a physical servo so I cannot confirm that the actuated quantity is any of these. I know very little of the electronics that controls the motor. It might well be that the controlled quantities are different for different series servos. </p>

<p>My bet is on torque control. However, assume that the servo is holding a weight at a distance (so it is acting against gravity), which means an approximately constant torque load. In this case, if the position error is zero and the servo is at rest, then each of P, I and D components are zero, which means the exerted torque is zero. This would cause the weight to sink, which is countered by the error in its position causing P,I components to increase. Wouldn't this situation cause the lifted weight to oscillate and balance at a constant position which is significantly different from the goal position? This isn't the case with the videos of servos I have seen lifting weights. Or is this the case and friction is smoothing everything out? Please help me understand. </p>
","servos pid"
"1036","Web mapping that can be used for autonomous vehicles/robots","<p>Is there web mapping tool that allows developers to use it to plot GPS data of autonomous vehicles/robots?</p>

<p><code>Google Maps</code> forbids it. See <a href=""https://developers.google.com/maps/terms"">10.2.C</a>. <code>Google Earth</code> terms of use link jumps to the same page. <code>Bing</code> Maps looks the similar (see <a href=""http://www.microsoft.com/maps/product/terms.html"">3.2.(g)</a>).</p>

<p>What I want is a internet-based tool that shows either/both satellite images and/or map, which can overlay plot using its API. <a href=""https://github.com/ros-visualization/rqt_common_plugins/issues/41"">I'm making a generic GPS plotter</a> on <code>ROS</code> that could be used both for slow robots or fast vehicles/cars.</p>

<p>Thanks!</p>
","gps visualization"
"1037","Building a balancing robot with differential drive","<p>I've already built a two wheeled balancing robot using some continuous rotation servos and an accelerometer/gyroscope.  I upgraded the servos to some geared DC motors with 8-bit encoders with the goal having the robot drive around while balancing.   </p>

<p>I'm kind of stuck on how to program it to drive around while still balancing.  I think one way would be to just have the control input to the motors act sort of like pushing it.  So the robot would be momentarily unbalanced in the direction I want it to travel.  That seems kind of clumsy to me though.  There must be a better way of doing?  I think I need to combine the dynamic model for the balancer with the differential drive but this is a bit beyond the control theory that I know.  </p>

<p><strong>Update</strong> 
From Anorton's answer I have a good looking state matrix now. </p>

<p>Now about pole placement:  The A matrix will will have to be 4x4 based on the new state vector.  And B will then have to be a 4x2 matrix since I can only control the left/right wheel torque (u = 2x1 vector).  </p>

<p>I may need to read more about this but is there a systematic way to determine the A matrix by pole placement?  It seems to me for this example and even more complicated examples, determining A by guess and check would be very difficult.  </p>

<p><strong>Update #2</strong>
After a bit of reading I think I understand it now.  I still need the dynamics of the robot to determine the A matrix.  Once I have that I can do the pole placement using matlab or octave.   </p>
","mobile-robot control dynamics"
"1039","Resetting position of e-puck in Webots using Supervisor node - problem with getting a handle to the robot","<p>I am writing a method (Java) that will reset the position of e-puck in Webots. I have been following tutorial on <a href=""http://www.cyberbotics.com/guide/section6.3.php"" rel=""nofollow"">Supervisor approach</a>. I have two controllers in my project:</p>

<ul>
<li>SupervisorController extends <a href=""http://www.cyberbotics.com/reference/section3.47.php"" rel=""nofollow"">Supervisor</a> - responsible for genetic algorithm and resetting e-puck's position</li>
<li>EpuckController extends <a href=""http://www.cyberbotics.com/reference/section3.41.php"" rel=""nofollow"">Robot</a> - drives the robot</li>
</ul>

<p>Robots are communicating via Emitter and Receiver, and everything works fine but the position reset.
This is what I'm doing in SupervisorController:</p>

<pre><code>412 Node epuck = getFromDef(""epuck"");    
413 Field fldTranslation = epuck.getField(""translation"");
</code></pre>

<p>And as a result I get this exception:</p>

<pre><code>[SupervisorController] Exception in thread ""main"" java.lang.NullPointerException
[SupervisorController]  at SupervisorController.initialise(SupervisorController.java:413)
[SupervisorController]  at SupervisorController.main(SupervisorController.java:497)
</code></pre>

<p>epuck variable is null. I tried calling different methods on epuck, and they all resulted in NullPointerException. The name of e-puck matches the world file. </p>

<pre><code>DEF EPUCK DifferentialWheels {
  translation 0.134826 -0.000327529 0.107963
  rotation 0.0244439 0.999246 -0.0301538 1.95838
  children [
  (........)
  ]
  name ""epuck""
  controller ""EpuckController""
  axleLength 0.052
  wheelRadius 0.0205
  maxSpeed 6.28
  speedUnit 0.00628
}
</code></pre>

<p>I would appreciate any advice on how to get a handle to the robot or where to look for issues in simulation/code.</p>
","mobile-robot simulator"
"1046","Overview - what skills are needed for sensor fusion?","<p>I want to make a list of what knowledge is necessary for <a href=""http://en.wikipedia.org/wiki/Sensor_fusion"" rel=""nofollow"">sensor fusion</a>. Since it has a wide array of possible applications, it is not clear where to begin studying. Can <em>we</em> please verify add topics that are in-scope, and specify to what extent?:</p>

<ol>
<li>Digital Signal Processing - course: <a href=""https://www.coursera.org/course/dsp"" rel=""nofollow"">https://www.coursera.org/course/dsp</a></li>
<li>Probability - course <a href=""http://ocw.mit.edu/courses/electrical-engineering-and-computer-science/6-041-probabilistic-systems-analysis-and-applied-probability-spring-2006/lecture-notes/"" rel=""nofollow"">http://ocw.mit.edu/courses/electrical-engineering-and-computer-science/6-041-probabilistic-systems-analysis-and-applied-probability-spring-2006/lecture-notes/</a></li>
<li>Machine Learning - course at Coursera from Stanford University</li>
<li>Programming robotic car - course at Udacity</li>
<li>Knowledge of Matlab and Simulink - tutorials on mathworks webpage and offline help.</li>
<li>Basic knowledge about integrals, matrices operations, differential equations.</li>
</ol>
","sensors sensor-fusion"
"1047","Source to learn Kalman Fusion, explanatory code snippets","<p>Currently I am reading a book of Mr. Thrun: <a href=""http://www.probabilistic-robotics.org/"" rel=""nofollow"">Probabilistic Robotics</a>. I find it really helpfull to understand concept of filters, however I would like to see some code in eg. Matlab. Is the book ""Kalman Filter for Beginners: with MATLAB Examples"" worth buying, or would you suggest some other source to learn the code snippets from?</p>
","kalman-filter books"
"1049","Is it possible to interface android mobile as GSM and GPS module with arduino based robotic applications?","<p>I want to built a robot and i need bunch of modules to track it like GSM/GPS Wifi and Camera
If i try to buy each of module separately it will cost me 300 dollar each aprox in Pakistan. On the other hand an android enable phone can be purchased on just 250$ having all of them. I was wondring if it is possible to interface android phones like (Huawaii or google nexus) with 8-bit microcontrollers or Arduino? The only port available with android phones are USB and Arduino supports USB. It is possible to some how attach both of them?</p>
","arduino usb"
"1050","How do space rovers survive at very low temperatures?","<p>For example, if a rover has working temperature range of -70 to +120 Celsius, how does it survive and then restore itself if the temperature drops to -150 degrees for several months?</p>
","electronics ugv reliability"
"1051","What are human-friendly terms for mobile-robot orientation and relative direction of non-robot objects?","<p>Within robotics programming, orientation is primarily given in terms of x, y, &amp; z coordinates, from some central location.  However x, y, z coordinates aren't convenient for rapid human understanding if there are many locations from which to select (e.g., {23, 34, 45}, {34, 23, 45}, {34, 32, 45}, {23, 43, 45} is not particularly human friendly, and is highly prone to human error).  Yet more common English orientation descriptors are frequently either too wordy or too imprecise for rapid selection (e.g.,  ""front-facing camera on robot 1's right front shoulder"" is too wordy; but ""front""/ ""forward"" is too imprecise - is the camera on the leading edge or is it pointing forward?)</p>

<p>In the naval and aeronautical fields vehicle locations are generically talked about as fore, aft (or stern), port, and starboard. While, direction of movement that is relative to the vehicle is frequently given in reference to a clockface (e.g., forward of the the fore would be ""at 12"", rear of the aft would be ""at 6"", while right of starboard and left of port would be ""at 3"" and ""at 9"", respectively).  This language supports rapid human communication that is more precise than terms such as ""front"" and ""forward"".  Are there equivalent terms within mobile robotics?</p>
","mobile-robot control design localization navigation"
"1056","Perfecting Tripod Gait - Building a R-Hex Robot","<p>I need help with figuring out the following things:</p>

<p>I'm developing a hexapod R-Hex type model with a tripod gait. However, the angles obtained during the robot's walking in real life are not perfectly aligned. Because of this the robot often collapses and falls even on perfectly straight terrain.</p>

<p>My configuration is: </p>

<ul>
<li>60 rpm dc motors for each leg</li>
<li>H-bridge motor drivers for each dc motor</li>
<li>Atmega 8</li>
</ul>

<p>Should I change the gait type?</p>

<p>Or is Tripod sufficiently stable?</p>

<p>Are DC motors providing fine enough control or do I need servos?</p>

<p>Do I need a DC motor with an Encoder? 
What will be its benefits?</p>

<p>What could be done to improve performance of the robot?</p>

<p><strong>Added: Would a Stepper motor work as well, instead of a servo?</strong></p>
","motor wheeled-robot gait"
"1060","Self learning maze solving robot using 8bit microcontroller?","<p>I want to know if there is best algorithm and technique to implement self learning maze solving robot in 8 bit limited resource micro-controller? I was looking for some well optimized algorithm and/or technique. Maze can be of any type. Of-course first time it has to walk all the way and keep tracking obstacles it found. </p>

<p>I think best technique would be neural networks but is it possible to do this in such a short resources of 8bit? Any example online with similar kind of problem? </p>

<p>My wall detection is based on units, well, I have counted the wheel turns and it is almost 95% accurate in integers. For sensing the walls Ultrasonic range finding is used. Wheel can remeber its current position in let say, 3 feet staight, 2 feet right, etc.</p>
","algorithm machine-learning mapping micromouse"
"1064","Prototyping a device with 25-100 small DC 3.0V motors, is Arduino a good fit?","<p>I want to prototype a therapeutic device that will have a lot of tiny mobile-phone type vibration motors <a href=""http://www.cutedigi.com/robotics/vibration-motor.html"" rel=""nofollow"">like this one</a> in it, and I want to be able to activate them in any configuration I want.  I'm going to need analogue control, and support for logic like perlin noise functions and so on.  I'm not really going to need sensor data or any other kind of feedback beyond a few buttons for control.  I just need fine control over lots of little motors.  Depending on what results I can get out of, say, 25 motors on the initial prototype, I may decide that I'm done, or that it needs more motors.  I also don't have an enormous budget.</p>

<p>So the question is, is Arduino a good fit for a project like this?  Is it feasible to get that many motors working off the same controller?  I know some of the Arduino boards have up to 50-something serial outputs, but from what I can tell, that may only translate to 25 or so motors, so I'd need a way to extend the board with more serial outputs if I wanted to try more.</p>

<p>Additionally, if Arduino isn't a good fit, what would be better?  Could I try something directly out of a serial port on my PC?  I've never tried to home-cook a robotics application before, so I'm not really aware of the options.</p>
","arduino motor"
"1066","How to invert D-H parameters","<p>I currently have a working kinematic chain made by a set of ten links in <a href=""http://en.wikipedia.org/wiki/Denavit%E2%80%93Hartenberg_parameters"" rel=""nofollow"">D-H convention</a> (with usual parameters [ $A_i, D_i, \alpha_i, \theta_i$]).</p>

<p>But my task currently requires the inversion of some of them. Basically, I would have a part of the chain that is read from the end-effector to the origin, using the same links (and thus the same parameters). Is it possible? How to do so?</p>

<hr>

<p>Please notice that this is not related to the <a href=""http://robotics.stackexchange.com/q/299/37"">inversion of the kinematic chain</a>. It's more basic: <em>I want to find the dh parameters of the inverted forward kinematic chain</em>.</p>

<p>Let's put it simple: I have dh parameters for a 2 link planar chain from joint 0 to joint 1, so I can compute its direct kinematics. But what if I want to compute the direct kinematics from joint 1 to joint 0?</p>

<p>Given DH parameters [ $A_i, D_i, \alpha_i, \theta_i$], I can retrieve the transform matrix with this formula:</p>

<p>$G = \left[\begin{matrix}
cos(\theta) &amp; -sin(\theta)*cos(\alpha) &amp;  sin(\theta)*sin(\alpha) &amp; cos(\theta)*a \\
sin(\theta) &amp;  cos(\theta)*cos(\alpha) &amp; -cos(\theta)*sin(\alpha) &amp; sin(\theta)*a \\
          0 &amp;              sin(\alpha) &amp;              cos(\alpha) &amp;             d \\
          0 &amp;                        0 &amp;                        0 &amp;             1
\end{matrix} \right]$</p>

<p>This is the transform matrix from the i-th link to the (i+1)-th link. Thus, I can invert it to obtain the transform matrix from the (i+1)-th link to the i-th link, but the problem is that this is not working. I believe that the reason is related to the fact that the DH convention doesn't work any more <strong>as it is</strong>.</p>

<p>Any help?</p>
","control inverse-kinematics kinematics"
"1068","Mindsensor Motor Multiplexer jump on run_unlimited","<p>I am trying to run a nxt motor using the mindsensors motor multiplexer at a slow speed.  When I turn it on, it tends to jump approx 20 to 40 degrees before moving at a slow speed.  Has anyone seen this behavior?</p>

<p>I am using NXT 1.0 with firmware down loaded from <code>lms_arm_mindsensors_129.rfw</code>.  Sample code in NXC (I am using Bricx Command Center as my IDE) is as follows:</p>

<pre><code>MMX_Run_Degrees (SensorPort, Addr, MMX_Motor_2, MMX_Direction_Reverse,
     MMX_Speed_Slow, 220, MMX_Completion_Wait_For, MMX_Next_Action_Brake);
Wait(500);

MMX_Run_Unlimited( SensorPort, Addr, MMX_Motor_2,MMX_Direction_Forward, 5);
// The jump happens here.
while(Sensor(IN_2)&lt; SENSORTHRESHOLD);
</code></pre>
","nxt mindstorms not-exactly-c"
"1069","Is there a way to use a single dc motor output for two different loads?","<p>Is there a way to use a single dc motor output for two different loads (by using gears, relays etc) ? Please see the illustration below:</p>

<p><img src=""http://oi50.tinypic.com/2z5sjdx.jpg"" alt=""""></p>

<p>To clarify the illustration, I get the dc motor power at ""output 1"" gear which is extended over an idler gear to output 2 gear. All three are in contact (though the picture doesn't quite show it). Load 1 and Load 2 are two separate gears connected to different loads (wheels etc) and initially not in contact with the bottom gears. On switching on the relays 1, 2 - the load bearing gears, move towards the output1 and output2 and mesh with them to drive Load 1, 2.</p>
","motor"
"1072","Is it possible to make Kite flying robot?","<p>This question was asked in <a href=""http://electronics.stackexchange.com/questions/61014/real-time-location-sensor-for-following-a-kite-in-the-air?noredirect=1#comment117178_61014"">electronics stackexchange</a>. I want to know if is it possible to make a robot that can fly kites. Is this idea practical? I was thinking that making a kite is just like making some flying quadcopter or helicopter. I just want to know is this idea really implementable?  Is there an example or similar work in reference to this?     </p>
","control quadcopter radio-control"
"1074","How to assemble brushless motors and propellers?","<p>I'm building a quadcopter and I've received my motors and propellers.</p>

<p>What's the right way to assemble those together?</p>

<p>I'm not confident with what I've done, as I'm not sure the propeller would stay in place on a clockwise rotating motor.</p>

<p>I mean, if the motor rotates clockwise, will the screw stay tightly in place, even with the prop's inertia pushing counter-clockwise?</p>

<p>Here's what I've done (of course i'll tighten the screw...) :</p>

<p><img src=""http://i.stack.imgur.com/mN36U.jpg"" alt=""Motor and propeller""></p>
","brushless-motor"
"1078","How do I measure the distance that a cord (string) has moved?","<p>For a pet project, I am trying to fly a kite using my computer.  I need to measure how far a cord extends from a device.  I also need to somehow read out the results on my computer. So I need to connect this to my pc, preferably using something standard like USB.</p>

<p>Since the budget is very small, it would be best if I could get it out of old home appliances or build it myself.</p>

<p>What technology do I need to make this measurement?</p>
","wheel usb encoding"
"1082","Stabilizing a Robot Arm at a Specified Height","<p>I have a 4-bar linkage arm (or similar design) for a telerobot used in the <a href=""http://vexrobotics.com"" rel=""nofollow"">VEX Robotics Competition</a>.  I want to be able to press buttons on my PS3-style controller to have the arm raise to certain angles.  I have a potentiometer to measure the 4-bar's angle.</p>

<p>The potentiometer measures the angle of one of the joints in the shoulder of the mechanism, which is similar to this:</p>

<p><img src=""http://i.stack.imgur.com/CwnAQ.jpg"" alt=""enter image description here""></p>

<p>What type of control should I use to stabilize the arm at these angles?</p>
","mobile-robot control arm"
"1083","Assigning Frames and Deriving Link Parameters","<p>The textbook I'm using doesn't have the answers to the practice questions, so I'm not sure how I'm doing. Are the following DH parameters correct given the frames I assigned?</p>

<p>The original question is as follows:
The arm with 3DOF shown below is like the one in example 3.3, except that joint 1's axis is not parallel to the other two. Instead, there is a twist of 90 degrees in magnitude between axes 1 and 2. Derive link parameters and the kinematic equations for $^bT_w$ (where b means base frame and w means wrist frame).</p>

<p><a href=""http://i.stack.imgur.com/tuo7v.jpg"" rel=""nofollow"">Link Parameters:</a></p>

<p>$$\begin{array}{ccc}
  &amp; const.        &amp; and      &amp;     &amp;               \\
  &amp; angle         &amp; distance &amp;     &amp;               \\
i &amp; \alpha_{i-1}  &amp; a_{i-1}  &amp; d_i &amp; \bf{\theta_i} \\
\\
1 &amp; 0             &amp; 0        &amp; 0   &amp; \theta_1      \\
2 &amp; -90           &amp; L_1      &amp; 0   &amp; \theta_2      \\
3 &amp; 0             &amp; L_2      &amp; 0   &amp; \theta_3      \\
\end{array}$$</p>

<p>Frame Assignments: <img src=""http://i.stack.imgur.com/I6XKZ.jpg"" alt=""Frame Assignments""></p>
","inverse-kinematics kinematics forward-kinematics"
"1086","Increasing the rotation range of a servo motor","<p>How do I increase the rotation range of a standard servo? Most servos have a rotation range of ~ 180 degrees. I would like to access the entire 360 degree range on the servo, partially because I would be attaching the servo's shaft to the robotic wheel and would like it to be able to make full rotations. Or is that not possible? </p>

<p>I would not like to however lose the 'encoder' part of the servo which allows me to clearly specify which angular position the wheel should stop at. </p>

<p>If I use gears in order to transform this system range, would it lead to loss of precision?</p>

<p>In addition, would such a transform allow the wheels to continuously rotate in one direction? From what I understand, this won't work.</p>

<p>Would a stepper motor with an external encoder or a dc motor with an external encoder work?</p>
","motor rcservo"
"1089","How to perform this reference system transformation?","<p>I have two quaternions that indicate the initial orientation of a four wheel robot, each one in relative to one reference systems. </p>

<p>The robot's orientation given by a quaternion q is not the same in the two reference systems: For one reference system the quaternion q1 might refer to the robot looking at positive x while the same quaternion components q1 in the second reference system might refer to the robot looking at the negative x.</p>

<p>I have two matrices which indicate the position of the robot in time in its correspondent reference system.</p>

<p>I want to find the correspondent points of the first matrix in to the second reference system. Each matrix is built with a different sensor, so the results will be similar but not exactly the same.</p>

<p>I think I should find the transformation from the first reference system to the second and then apply it for each point of the first matrix. How can I find this transformation? The translation part I think is clear, but the rotation not at all.</p>

<p><strong>Edit:</strong></p>

<p>@WildCrustacean</p>

<p>The solution proposed does not solve the problem, I think that the reason is because each system represents the robot in a different way. </p>

<p>In the initial one (A) the robot moving forward with no rotation would increase in the X axis. In the goal referential system (B) the robot moving forward with no rotation would increase in the Z axis. (See figure for more details) </p>

<pre><code>First system (A)
 ______
|\  T  \
| \_____\     z
|B |    | : y ^
 \ | R  |    \|
  \|____|     +--&gt; x


Second system (B)
 ______
|\  T  \
| \_____\     x
|B |    | :   ^
 \ | R  |     |
  \|____|     +--&gt; z
               \
                y

R: Right side
B: Back side
T: Top
</code></pre>

<p>I think I have to apply an extra rotation to change the initial quaternion that belongs to the first system to be in accordance with the second system before applying the transformation of your post.</p>

<p>One rotation of 180 degrees around x followed by one of 90 around y. Would rotate from A to B </p>

<p>This is how I tried to solve it:</p>

<pre><code># Quaternion to adjust reference system
first_quat = make_quaternion(unitary_x, pi) # Generates the quaternion that rotates pi around X 
second_quat = make_quaternion(unitary_y, pi/2.0) # Generates the quaternion that rotates pi/2 around Y 
composed_fs_q = first_quat*second_quat 

# Quaternion to rotate from one reference system to the other
quaternion_ini_A = quaternion_ini_A*composed_fs_q
A2B_quaternion = quaternion_ini_B*(quaternion_ini_A.inverse())
</code></pre>

<p>A2B_quaternion is the quaternion that I use for the rotation but still doesn't perform the right rotation. Any idea?</p>
","mobile-robot localization odometry"
"1091","Python libraries for image processing and feedback control on raspberry pi","<p>I'm building a motion detection and object recognition camera with feedback control for a hexy robot. Fortunately most of the servo control is handled by the analog servo controls and the high-level logic can be implemented in python on a raspberry pi. What's the right combination of python modules to implement:</p>

<ol>
<li>a daemon/service to trigger and execute image capture and processing</li>
<li>a daemon/service to regularly update the hexy with the latest motion plan and servo setpoints</li>
<li>the image processing for recognition and tracking of objects from the webcam</li>
</ol>

<p>I'm currently using python-daemon for the services and comparing the various pypi opencv libraries to see if any of them look promising. Anyone have experience with these on a raspberry pi or ARM processor in a robotics application? </p>

<ul>
<li>remotecv                  - remotecv is an OpenCV server for face recognition</li>
<li>ctypes-opencv             - ctypes-opencv - A Python wrapper for OpenCV using ctypes</li>
<li>pyopencv                  - PyOpenCV - Boost.Python and NumPy</li>
<li>opencv-cython             - An alternative OpenCV wrapper</li>
<li>CVtypes                   - Python OpenCV wrapper using ctypes</li>
<li>Tippy                     - another Toolbox for Image Processing, based on OpenCV</li>
</ul>

<p>These each depend on a deep list of low-level libraries and/or compilers like Boost->numpy->gfortran or cython->gcc or ctypes. I'm concerned about compatibility and performance of these lowlevel libraries on Raspbian and an ARM processor.</p>

<p>Anyone with a known working architecture for image processing and real-time control in python on an ARM processor will get their answer upvoted and/or accepted.</p>
","raspberry-pi real-time"
"1092","Numpy alternatives for linear algebra and kinematics in python?","<p>Are there any decent python numerical package libraries besides numpy for python? Numpy relies on gfortan which itself must be compiled correctly for your platform to avoid hidden/insidious numerical errors in numpy. </p>

<p>I need a matrix algebra package to do kinematics, path planing, and machine learning in python that isn't sensitive to  gfortran version and compiler options.</p>
","kinematics python"
"1097","At which stage should filtering be applied to the sensors data?","<p>Shall I filter (kalman/lowpass) after getting the raw values from a sensor or after converting the raw values to a usable data? Does it matter? If so, why? </p>

<p>Example:
Filter after getting raw values from IMU
or 
filter after converting raw values to a usable data eg. flight dynamics parameters.</p>
","kalman-filter imu"
"1100","Why are industrial machines called robots?","<p>The definition of a robot is as follow: ""A robotic paradigm can be described by the relationship between the three primitives of robotics: Sense, Plan, and Act.""</p>

<p>An example could be the famous ""Kuka Robots"". The Kuka robot is preprogrammed and does mainly one loop over and over again. Some of them could have measurement sensors but that is all. They do not think or plan nor do they make decisions. </p>

<p>An automatic door opener, used in a building is not a robot either but according to the robotic paradigm definition they are more a robot than a Kuka machine. They actually get some data from a sensor followed by planning and acting. </p>

<p>So why are Kuka machines called robots?</p>
","industrial-robot"
"1110","Robots minimum distance","<p>I am trying to implement a mechanism to make robots avoid being too close (Say in a distance less than <code>d</code>). I am not familiar with those systems and I have to implement a strategy to avoid robots being too close to each other. Could anyone recommend me some readings for such a problem or a set of keywords to search for? I don't know yet how to start.</p>
","algorithm movement"
"1113","Torque in kg/cm?","<p>I was looking up the motor parameters for some stepper motor where they listed the torque of the motor at different current/voltage but the torque they listed was in kg/cm.</p>

<p>How is kg/cm even a remotely acceptable unit for torque?</p>

<p>How do I calculate the torque in Nm from kg/cm?</p>

<p>Clarity note: Its not kgcm which represents [0.098 kilogram force = 1 Nm.]</p>

<p><a href=""http://www.nex-robotics.com/products/motors-and-accessories/high-torque-bipolar-stepper-motor.html"" rel=""nofollow"">Website</a> where this happens.</p>
","stepper-motor torque"
"1117","How to obtain stereo correspondences and what exactly is a disparity map?","<p>I am currently reading into the topic of stereo vision, using the book of Hartley&amp;Zimmerman alongside some papers, as I am trying to develop an algorithm capable of creating elevation maps from two images.</p>

<p>I am trying to come up with the basic steps for such an algorithm. This is what I think I have to do:</p>

<p>If I have two images I somehow have to find the fundamental matrix, F, in order to find the actual elevation values at all points from triangulation later on. If the cameras are calibrated this is straightforward if not it is slightly more complex (plenty of methods for this can be found in H&amp;Z).</p>

<p>It is necessary to know F in order to obtain the epipolar lines. These are lines that are used in order to find image point x in the first image back in the second image.</p>

<p>Now comes the part were it gets a bit confusing for me: Now I would start taking a image point x_i in the first picture and try to find the corresponding point x_i’ in the second picture, using some matching algorithm. Using triangulation it is now possible to compute the real world point X and from that it’s elevation. This process will be repeated for every pixel in the right image.</p>

<p>In the perfect world (no noise etc) triangulation will be done based on</p>

<pre><code>x1=P1X 
x2=P2X
</code></pre>

<p>In the real world it is necessary to find a best fit instead.</p>

<p>Doing this for all pixels will lead to the complete elevation map as desired, some pixels will however be impossible to match and therefore can't be triangulated.</p>

<p>What confuses me most is that I have the feeling that Hartley&amp;Zimmerman skip the entire discussion on how to obtain your point correspondences (matching?) and that the papers I read in addition to the book talk a lot about disparity maps which aren’t mentioned in H&amp;Z at all. However I think I understood correctly that the disparity is simply the difference x1_i- x2_i?</p>

<p>Is this approach correct, and if not where did I make mistakes?</p>
","computer-vision stereo-vision"
"1120","Has a robot ever taken a complete IQ test?","<p>And if so, what was the highest score so far?</p>

<p>Some news articles suggest only parts of tests were aced.</p>

<hr>

<p>Update since people censored this question and closed it. There was an AI that has taken an IQ test and scored similar to a 4 year old.</p>

<p><a href=""http://phys.org/news/2015-10-ai-machine-iq-score-young.html"" rel=""nofollow"">http://phys.org/news/2015-10-ai-machine-iq-score-young.html</a></p>

<blockquote>
  <p>The AI system which they used is ConceptNet, an open-source project run by the MIT Common Sense Computing Initiative.
  Results: It scored a WPPSI-III VIQ that is average for a four-year-old child, but below average for 5 to 7 year-olds</p>
</blockquote>

<h3>Abstract</h3>

<blockquote>
  <p>We administered the Verbal IQ (VIQ) part of the Wechsler Preschool and Primary Scale of Intelligence (WPPSI-III) to the ConceptNet 4 AI system. The test questions (e.g., ""Why do we shake hands?"") were translated into ConceptNet 4 inputs using a combination of the simple natural language processing tools that come with ConceptNet together with short Python programs that we wrote. The question answering used a version of ConceptNet based on spectral methods. The ConceptNet system scored a WPPSI-III VIQ that is average for a four-year-old child, but below average for 5 to 7 year-olds. Large variations among subtests indicate potential areas of improvement. In particular, results were strongest for the Vocabulary and Similarities subtests, intermediate for the Information subtest, and lowest for the Comprehension and Word Reasoning subtests. Comprehension is the subtest most strongly associated with common sense. The large variations among subtests and ordinary common sense strongly suggest that the WPPSI-III VIQ results do not show that ""ConceptNet has the verbal abilities a four-year-old."" Rather, children's IQ tests offer one objective metric for the evaluation and comparison of AI systems. Also, this work continues previous research on Psychometric AI. </p>
</blockquote>

<hr>

<p>Update. A robot has passed the Japanese college entrance test and has an 80% chance of being accepted. Since it scored more than the average, that would make the IQ > 100, especially since college applicants have an IQ greater than average, and especially since Japanese are smarter than average humans. <a href=""http://gizmodo.com/an-ai-program-in-japan-just-passed-a-college-entrance-e-1742758286"" rel=""nofollow"">http://gizmodo.com/an-ai-program-in-japan-just-passed-a-college-entrance-e-1742758286</a></p>

<blockquote>
  <p>The Wall Street Journal reports that the program, developed by Japan’s National Institute of Informatics, took a multi-subject college entrance exam and passed with an above-average score of 511 points out of a possible 950. (The national average is 416.) With scores like that, it has an 8 out of 10 chance of being admitted to 441 private institutions in Japan, and 33 national ones.</p>
</blockquote>
","artificial-intelligence"
"1128","Why is this electro motor going slower?","<p>From an old dust buster I've got this electro motor, the included battery pack and the charger:</p>

<p><img src=""http://i.stack.imgur.com/hVLhe.jpg"" alt=""enter image description here"">
<img src=""http://i.stack.imgur.com/0nFln.jpg"" alt=""enter image description here""></p>

<p>I ripped everything apart (the dust buster was broken) and the motor still works. After playing around with it for a while and letting it lying around for about two weeks it suddenly revs a lot slower. I supposed the battery pack was drained so I hooked up the battery pack to the charger and let it charge for a night. Unfortunately the motor still turns very slow.</p>

<p>Since I want to use this motor for my first home robotics project (making a kite fly with my computer), off I went to the local electronics store where they measured the charger to give 16V (even though it says 21V) and the battery pack to give about 5V. I then hooked up the motor directly to the charger, but unfortunately it doesn't even move an inch then.</p>

<p>So now I wonder:</p>

<ol>
<li>Why doesn't the motor spin at all when hooking it up to the charger? (Could that be because the 250mA is too low?)</li>
<li>Why doesn't the battery pack charge at all? (this bothers me the most!)</li>
</ol>

<p>All tips are welcome!</p>
","motor battery"
"1130","What is the name of this mechanical linkage?","<p><img src=""http://i.stack.imgur.com/Wl3uH.jpg"" alt=""enter image description here""></p>

<p>I am trying to find a joint like these for a robot I'm building. It is often called a swivel joint or a universal joint, but with a modified spider. I can't find one anywhere and would prefer not to make it. Searching for 'universal joint' returns the standard automotive type. Any help would be appreciated</p>
","joint"
"1132","How Should I Choose an Educational Robotics Kits for Beginner Programmers?","<p>I am a high school student, doing a research project in AI and Robotics. How should I choose a robotics kit (for example, will it be better to learn the basics by using a hexapod or robotic arm?)</p>

<p>I know C at good level.</p>
","arduino artificial-intelligence beginner"
"1143","How do I interface a TLC5947 with small motors?","<p>This is a follow-up to this question:  <a href=""http://robotics.stackexchange.com/questions/1064/prototyping-a-device-with-25-100-small-dc-3-0v-motors-is-arduino-a-good-fit"">Prototyping a device with 25-100 small DC 3.0V motors, is Arduino a good fit?</a></p>

<p>I've decided based on the answer that sending the control signals through multiple TLC5947 chips, then sending the PWM signal to the motors is the best way to go.  What I need to know is how to turn the PWM signals into something of the required power, since the TLC5947's won't be able to drive the motors by themselves.</p>

<p>I'm guessing an amplifier is what I'll need to make, but what's the best way to boost that many signals?</p>
","motor power pwm"
"1145","What would be the best way to handle food grains?","<p>I'm trying to handle food grains like rice, wheat in an automated way (to cook simple dishes). For this I have to transfer grain from a larger container to a weighing scale. I know I can use solenoid valves for liquids but all solid handling valves seem to be too big (gate valves etc) and for larger applications. Is there any better way to do this ? </p>
","automatic"
"1148","Dynamic programming algorithm aka Bellman equation in Robotics?","<p>The dynamic programming algorithm refers to the <a href=""http://en.wikipedia.org/wiki/Bellman_equation"" rel=""nofollow"">Bellman equation</a>. An open-loop control decides movement at the initial point while a closed-loop control decides control during the movement. Now most robotic application looks like closed-loop control: in every point, it checks how it is doing with respect to some reward function, this is my thinking. Now most participants in threads such as <a href=""http://robotics.stackexchange.com/questions/128/how-mature-is-real-time-programming-in-robotics"">How mature is real-time programming in robotics?</a> do not differentiate their scope, perhaps they haven't thought about it. Anyway, I am interested to know:</p>

<p><strong>How is dynamic programming used in robotics? Is there any research about DP usage in robotics?</strong></p>
","research dynamic-programming"
"1153","When should FPGAs be used in Robotics?","<p>FPGA has good points such as a lot of IO points but then again you need to think things on very low level with flip-flops and pioneer on areas where things are not yet mature -- for example see this question <a href=""http://electronics.stackexchange.com/questions/3107/looking-for-open-source-fpga-hardware-and-dev-tools"">here</a> about development-tools on FPGAs -- this is my understanding currently! Now FPGA has been used to create excellent dexterity in robotic hands like <a href=""http://www.youtube.com/watch?v=6rhH4YlObkM"">here</a>. Now some people market FPGA for fast prototyping and <em>""forward looking""</em> designs like <a href=""http://www.rtcmagazine.com/articles/view/101763"">here</a>, I don't fully understand them: if you don't need a lot of IO points for things such as sensors, why to choose FPGA for a robot? So</p>

<p><strong>When should FPGA be chosen for a project in robotics?</strong></p>
","design research logic-control"
"1158","Equation to limit rate of change of end-effector in X and Y coordinates","<p>Some vector math is involved here so prepare yourself.</p>

<p>I am developing a robotic arm that moves in two dimensions.  It is a rotary-rotary design which looks roughly like the picture in this post:
<a href=""http://robotics.stackexchange.com/questions/869/building-robotic-arm-joint"">Building Robotic arm joint</a></p>

<p>I am now trying to limit the speed of the end-effector.  I am using Simulink and believe that the best way to limit the speed is the limit the rate of change of the X and Y coordinates that I tell it to move to.  </p>

<p>Now, I also want the end-effector to be able to move in a straight line and believe that I can accomplish this by defining functions that calculate the maximum rate for movement in the X or Y direction based on the distance the arm is trying to travel.  The equasion I came up with is this :</p>

<pre><code>xRate = (abs(currentX - nextX) / max(abs(currentX - nextX), abs(currentY - nextY))
yRate = (abs(currentY - nextY) / max(abs(currentX - nextX), abs(currentY - nextY))
</code></pre>

<p>So basically, XRate: distance in x / max between distance in X and distance in Y.</p>

<hr>

<p>Now, for the actual problem.  Because this limits the speed in both X and Y, the end-effector can travel (for instance) 1 in./sec in both directions at the same time.  Meaning that it is travelling at OVER 1 in./sec overall.  If, however, it is only moving in ONE direction then it will only move at that 1 in./sec speed because there is no second component.  It boils down to the fact that the max speed the arm can move is 'sqrt(2)' and the minimum is '1'.</p>

<p><strong>My main question is</strong>:  Given that I need to calculate a max xRate and a max yRate, how can I limit the overall speed of the end-effector?</p>

<p>Secondarily, is there a way for me to implement a rate control that will limit the overall rate instead of limiting X and Y independantly using Simulink?</p>
","design"
"1160","Can I use the RaspberryPi to receive the GameCube remote's RF signals?","<p>I have an old gamecube that doesn't work and I want to gut it and fill it with Arduino boards and/or Raspberry Pi if necessary.  I want the project to eventually have some kind of AI aspect, but I'm also toying with the idea of using a <a href=""http://en.wikipedia.org/wiki/WaveBird_Wireless_Controller"" rel=""nofollow"">wireless GameCube remote and wavebird</a> to issue commands at the push of a button.  </p>

<p>I guess this would be mostly good for testing purposes, but I'm mostly curious if and how I would go about making my RaspberryPi understand Gamecube remote input.  Furthermore, would this kind of idea be feasible?</p>
","control arduino raspberry-pi"
"1163","How to know when a Li-Po battery is discharged?","<p>I'm building a quadcopter and I've seen that a Li-Po battery must not be entirely discharged, otherwise it could damage it.</p>

<p>How do you know when you have to stop your quadcopter or robot in order to prevent damages, since the voltage doesn't drop? Which part should control the battery charge? ESCs? BEC? Flight controller?</p>
","battery"
"1167","Can the rate of peristaltic pump's flow be accurate across changes in fluid viscosity?","<p>I'm building an arduino controlled pump system to be able to move fluids around. I need this to be fairly accurate, but extreme precision isn't required. Since there will be a variety of liquids moved through the pump, I've determined a peristaltic pump the best fit. But I don't think I fully understand them, and had a few questions..</p>

<ol>
<li><p>Since I'll need to purge the system... Can a peristaltic pump push air? Let's assume you have a 2m of tubing, and you pump a bunch of water through it. Can you remove the tube from the water reservoir so it is open to the air, and effectively purge the system of any remaining water?</p></li>
<li><p>Since I want to fairly accurately measure flow, could I simply count milliseconds instead of using a flowmeter? ... Will a peristaltic pump ALWAYS pump at a constant rate, regardless of the viscosity of the fluid? That is, will maple syrup come out at the same rate as water?</p></li>
<li><p>Shopping question, ignore I suppose ... Anyone know where I may find a fast/high flow peristaltic pump? I'm looking to be able to pump, at a minimum, .5oz/sec</p></li>
<li><p>Would be determinant upon #3 ... What sort of relay would I want for toggling this on/off with an arduino?</p></li>
</ol>
","arduino"
"1170","Where does Gazebo set the GAZEBO_MODEL_PATH environment variable?","<p>I'm starting out with Gazebo (1.5) at the moment and am following <a href=""http://bramridder.com/ai-blog.php/integrating-ros-with-gazebo"" rel=""nofollow"">a tutorial</a> off the internet. In order to get Gazebo to find the model, the author advocates manually exporting the <code>GAZEBO_MODEL_PATH</code> environment variable via </p>

<pre><code>export GAZEBO_MODEL_PATH=[...]/models:$GAZEBO_MODEL_PATH
</code></pre>

<p>But that will only work for the current terminal. So I wanted to change the environment variable permanently. </p>

<p>The <a href=""http://gazebosim.org/user_guide/started__components__env.html"" rel=""nofollow"">Gazebo User Guide</a> claims that <code>GAZEBO_MODEL_PATH</code>, along with all the other environment variables, is set by <code>/usr/share/gazebo-1.5/setup.sh</code> but my (virgin) Gazebo install doesn't list it:</p>

<pre><code>export GAZEBO_MASTER_URI=http://localhost:11345
export GAZEBO_MODEL_DATABASE_URI=http://gazebosim.org/models
export GAZEBO_RESOURCE_PATH=/usr/share/gazebo-1.5:/usr/share/gazebo_models
export GAZEBO_PLUGIN_PATH=/usr/lib/gazebo-1.5/plugins
export LD_LIBRARY_PATH=/usr/lib/gazebo-1.5/plugins:${LD_LIBRARY_PATH}
export OGRE_RESOURCE_PATH=/usr/lib/i386-linux-gnu/OGRE-1.7.4

# This line is needed while we're relying on ROS's urdfdom library
export LD_LIBRARY_PATH=/opt/ros/fuerte/lib:${LD_LIBRARY_PATH}
</code></pre>

<p>But when I start Gazebo, <code>GAZEBO_MODEL_PATH</code> is already set to <code>$HOME/.gazebo/models</code>, so it must be set somewhere. I guess I could probably simply add <code>GAZEBO_MODEL_PATH</code> to the <code>setup.sh</code> script, but since it is set somewhere, I'd still like to know where and whether it is better practice to set it in there.</p>
","gazebo"
"1174","PID tuning to make my balancing robot better","<p>See the video below of my balancing robot.<br>
<a href=""http://www.youtube.com/watch?v=Rjp8yoTVUcY"" rel=""nofollow"">Balancing robot</a></p>

<p>I was having trouble getting it to balance on hard surfaces but finally got it after playing with the PID gains a lot.  Previously it was balancing just fine on carpet.  </p>

<p>I set the PID gains by just picking a Kp, then increasing Ki until the robot oscillated very badly and tried to smash it's self into the ground,  then increasing Kd until it was finally stable. Here's the gains I'm using in the video.</p>

<pre><code>  Kp=20, Ki=4.5, Kd=45;
</code></pre>

<p>It will sit in one spot balancing without any problem.  You can see in the video that it can even stop from falling after I give it a pretty good kick.  The problem is that it stops from falling over but then greatly overshoots the other direction.  In the video you can see when I give it just a small tap it runs the other way for a while before finally becoming stable again.  </p>

<p>Any suggestions on what to try next?</p>
","pid"
"1175","Minimum speed controller refresh rate","<p>In a quadrotor we need to change each motor's speed depends on its position in space. More frequency will result more stability ( I mean if we can change motor's speed 400 times per second instead of 100 times per second we may stabilize our UAV quadrotor far better ).</p>

<p>Now my question targeting people who made a UAV quadrotor before or have any information about ESCs. I wanna know whats the minimum refresh rate for ESCs in a quadrotor to make it stable ? For example may an ESC with 50hz refresh rate enough for stabilizing quadrotor or not ? I'm asking this question because high speed ESCs are more expensive than lower speed ones.</p>

<p>I have <a href=""http://www.emaxmodel.com/views.asp?hw_id=21"" rel=""nofollow"">this one</a>. May it work ?</p>
","quadcopter esc"
"1178","How can I calculate processing speed of microcontroller","<p>I need a microcontroller that can process minimum 2mb data per second.
How do I determine what processors will be able to do this?</p>

<p>Also how can I calculate the processing speed in per second of any microcontroller?</p>

<p>I am very much scared with my college project and I need help.</p>
","microcontroller"
"1180","information filter instead of kalman filter approach","<p>I read many sources about kalman filter, yet no about the other approach to filtering, where canonical parametrization instead of moments parametrization is used. </p>

<p>What is the difference?</p>

<hr>

<p>Other questions:</p>

<ol>
<li><p>Using IF I can forget KF,but have to remember that prediction is more complicated <a href=""http://en.wikipedia.org/wiki/Kalman_filter#Information_filter"" rel=""nofollow"">link</a> </p></li>
<li><p>How can I imagine uncertainty matrix turning into an ellipse? (generally I see, area is uncertainty, but I mean boundaries) </p></li>
<li><p>Simple addition of information in IF was possible only under assumption that each sensor read a different object? (hence no association problem, which I posted <a href=""http://robotics.stackexchange.com/questions/1181/object-level-sensor-fusion-for-multiobject-tracking"">here</a></p></li>
</ol>
","kalman-filter algorithm sensor-fusion"
"1181","object level sensor fusion for multiobject tracking","<p>I want to fuse objects coming from several sensors, with different (sometimes overlapping!) fields of view. Having object lists, how can I determine whether some objects observed by different sensors are in fact the same object? Only then I can truly write an algorithm to predict future state of such an object. </p>

<p>From literature I read those 4 steps:</p>

<ol>
<li>Plot to track association (first update tracks estimates and then associate by ""acceptance gate"" or by statistical approach PDAF or JPDAF)</li>
<li>Track smoothing (lots of algorithms for generating new improved estimate, e.g.: EKF, UKF, PF)</li>
<li>Track initiation (create new tracks from unassociated plots)</li>
<li>Track maintenance (delete a track if was not associated for last M turns. also: predict those tracks that were associated, their new location based on previous heading and speed)</li>
</ol>

<p>So basically I am questioning point 1, acceptance gate. For a single sensor I can imagine it can be just a comparison of xy position of object and sensor measurement, velocity with heading eventually. My case is however, I have already ready object lists from each sensor in every cycle, there are some algorithms how to merge informations about an object collected by different sensors (great source is e.g. here: <a href=""http://www.mathworks.de/matlabcentral/fileexchange/37807-measurement-fusion-state-vector-fusion"" rel=""nofollow"">http://www.mathworks.de/matlabcentral/fileexchange/37807-measurement-fusion-state-vector-fusion</a>), but question is how to decide which objects should be fused, and which left as they were? Fields of view may overlap partly, not totally.</p>
","kalman-filter algorithm sensor-fusion"
"1182","Shedding light on ""cyber-physical systems""","<p>These days, one often hears of <a href=""http://en.wikipedia.org/wiki/Cyber-physical_system"" rel=""nofollow"">cyber-physical systems</a>. Reading on the subject, though, it is very unclear how those systems differ from distributed and/or embedded systems. Examples from Wikipedia itself only make them look more like traditional distributed systems. For example:</p>

<blockquote>
  <p>A real-world example of such a system is the Distributed Robot Garden at MIT in which a team of robots tend a garden of tomato plants. This system combines distributed sensing (each plant is equipped with a sensor node monitoring its status), navigation, manipulation and wireless networking.</p>
</blockquote>

<p>Obviously, <em>any</em> distributed system consists of sensing, actuations (which can easily include navigation) and networking.</p>

<p>My question is, how exactly does cyber-physical systems differ from traditional distributed systems? Is it just a fancy name, or is there something considerably different with it?</p>
","distributed-systems embedded-systems"
"1192","Fastest maze algorithm for robot","<p>I'm planning on programming a prebuilt robot to solve a maze as fast as possible.  The robot has forward obstacle sensors (no side sensors) and 3-axis accelerometer.  I'm planning on using the wall following algorithm.  Is this the fastest possible algorithm?  Also, since there are no side sensors, the robot needs to continuously turn to check if there is a wall on its side, so is there a clever way to use the accelerometer and sensors?</p>
","mobile-robot"
"1195","Where can I get A3 poster size of Mars Exploration Rover Spirit/Opportunity?","<p>I am looking for A3 size poster of Mars Exploration Rover Spirit/Opportunity for robotic education. </p>

<p>www.sunstartoys.com gives a little postcard-size of the MER along with its components on board when you buy the toy. But this is not large enough for classroom purpose.</p>

<p>Does anyone know where to buy A3 size poster of these MER for robotic education?</p>
","wheeled-robot"
"1198","Dynamic braille interface","<p>I'm a newbie in robotics, and I'm doing a project on dynamic Braille interface. Basically it's a 8*8 array of pins, which can be either totally up or down. How to use least motor as possible?</p>

<p>I'm thinking of using Arduino for easy interface with computer.</p>
","design"
"1200","What is the wire used for hand movement of robot called ? where can I find it online ?","<p>I am looking for a specific name of the wire used for the robotic arm movement control and where can I find some of this online. I want to control it using the micro controller so please suggest some good development kit.</p>
","microcontroller robotic-arm movement"
"1202","Problem with vibrations in air bearing","<p>We have an air bearing for a planar xy motion. Today it consists of four pockets according to picture. </p>

<p><img src=""http://i.stack.imgur.com/dET39.png"" alt=""Current design""></p>

<p>In the current design there are no sealings around the peripheries of the pockets and we suspect that is the reason we get vibrations. </p>

<p>In the current design we control the pressure, same for all for recesses. The flow is adjustable individually for each recess. In practice it is very hard to tune it.</p>

<p>For the non recess surfaces we have used <a href=""http://www.tss.trelleborg.com/remotemedia/media/globalformastercontent/downloadsautomaticlycreatedbyscript/catalogs/turcite_b_slydway_gb_en.pdf"" rel=""nofollow"">Slydway</a> as we need to be able to operate it without pressure occasionally.</p>

<p>To try to solve the problem we plan to develop a prototype where we can try out the effect of using sealings around the periphery of the pockets. The idea is something like this:
<img src=""http://i.stack.imgur.com/7yKuf.png"" alt=""Idea for new design""></p>

<p><strong>Questions</strong></p>

<ul>
<li>Is the idea with adding sealings good? (sanity check)</li>
<li><p>Suggestions for sealings? (I'm thinking a porous material like felt or cigarette filter) </p>

<p>Of course all suggestions are welcome.</p></li>
</ul>

<p><em>Edit</em></p>

<p>I'm going to try and add grooves around the recesses to evaquate the air that leaks. My thinking is that this will give us a more defined area under pressure.</p>
","linear-bearing"
"1205","Plastic shaft supports","<p>I would like to prevent a shaft from being pulled through it's bearings - that is, press a plastic ring around it on either side.  What are these rings called? They're not bearings or hubs.  And where can I find them?</p>
","mechanism"
"1207","Read Multiple Channels of RX-TX with arduino","<p>I have a 9 channel RF RX/TX and want to connect 3 motors to it.  I am able to connect channel 1 with motor 1 but unable to connect channel 2 with motor 2 simultaneously with arduino.</p>

<p>Here is the code I am currently using:</p>

<pre><code>int motor1Left = 7;// defines pin 5 as connected to the motor
int motor1Right= 9;// defines pin 6 as connected to the motor
int motor2Left = 22;// defines pin 7 as connected to the motor
int motor2Right = 26;// defines pin 8 as connected to the motor
int enable = 5;
int enable2 = 10;
int channel1 = 2; // defines the channels that are connected
int channel2 = 3;// to pins 9 and 10 of arduino respectively

int Channel1 ; // Used later to 
int Channel2 ; // store values

void  setup ()
{
  pinMode (motor1Left, OUTPUT);// initialises the motor pins
  pinMode (motor1Right, OUTPUT);
  pinMode (motor2Left, OUTPUT);
  pinMode (motor2Right, OUTPUT);// as outputs
  pinMode (channel1, INPUT);// initialises the channels
  pinMode (channel2, INPUT);// as inputs

  Serial.begin (9600); // Sets the baud rate to 9600 bps
}

void  loop ()
{
  Channel1 = (pulseIn (channel1, HIGH)); // Checks the value of channel1
  Serial.println (Channel1); //Prints the channels value on the serial monitor
  delay(1000);

  Channel2 = (pulseIn (channel2, HIGH)); // Checks the value of channel1
  Serial.println (Channel2); //Prints the channels value value on the serial monitor
  delay(1000);

  if (Channel1 &gt; 1470 &amp;&amp; Channel1 &lt; 1500) /*These are the values that I got from my transmitter, which you may customize according to your transmitter values */
  {
    digitalWrite (motor1Left, LOW); // Sets both the
    digitalWrite (motor1Right, LOW);// motors to low
    analogWrite(enable, 100);  
  }

  if (Channel1 &lt; 1460) // Checks if Channel1 is lesser than 1300
  {
    digitalWrite (motor1Left, HIGH);// Turns the left
    digitalWrite (motor1Right, LOW); // motor forward
    analogWrite(enable, 100);
  }

  if (Channel1 &gt; 1510) // Checks if Channel1 is greater than 1500
  {
    digitalWrite (motor1Left, LOW);// Turns the right

    digitalWrite (motor1Right, HIGH);// motor forward
    analogWrite(enable, 70);
  }

  if (Channel2 &gt; 1480 &amp;&amp; Channel1 &lt; 1500 )
  {
    digitalWrite (motor2Left, LOW);// Sets both the
    digitalWrite (motor2Right, LOW);// motors to low
    analogWrite (enable2, 100);
  }

  if (Channel2 &lt; 1300) // Checks if Channel2 is lesser than 1300
  {
    digitalWrite (motor2Left, LOW);// Turns the left
    digitalWrite (motor2Right, HIGH);// motor backward
    analogWrite (enable2, 100);
  }

  if (Channel2 &gt; 1500) // Checks if Channel2 is greater than 1500
  {
    digitalWrite (motor2Left, HIGH);// Turns the right
    digitalWrite (motor2Right, LOW);// motor backward
    analogWrite (enable2, 100);
  }
}
</code></pre>
","arduino"
"1209","How to stabilize a quadcopter","<p>Today was my quadcopter's first ""flight"". I'm running megapirate on a Crius AIOP v2 with a Turnigy Talon v2 frame.</p>

<p>I only touched the throttle stick on my remote, nothing else. When I felt the quadcopter was about to take off, I pushed the throttle just a little bit more, and the quadcopter oscillated 2 or 3 times and the just flipped over, landing on the propellers.</p>

<p>So, I broke 2 props, my frame feels a bit loose, I'll probably have to tighten the screws (I hope...). How can I tune the software so it will stabilize nicely after take off?</p>

<p>Edit :<br>
I don't know if it was true oscillation or just random air flows making it unstable. I made some more tests yesterday and it was quite OK (even if I crashed a few times). This time, it was really oscillating but it was quite windy outside and the quadcopter managed to stabilize after all. So i'll probably have to tune my PIDs and find a way to do it without crashing.</p>

<p>Edit 2 : After some PID tuning, I managed to stabilize my quadcopter pretty well but it's still oscillating just a little bit. I guess I'll have to slightly change the values to get a perfect stabilization.</p>
","quadcopter stability"
"1211","What kind of sensor do i need for knowing that something is placed at a position?","<p>I have a small device that's picking up small rocks from a pile and moving them to another place. Its a kind of crude way of trying to push the whole pile onto a bigger gear and hoping one of them are pushed to one of the spaces between gears and taken around and falls off on the other side of the spinning gear. Here i want to know if the machine successfully got a rock here, if not it should spin the gear until it turns up a single rock on the other side of it. If a rock is present at the spot, the gear should stop spinning until the rock is taken care of by the rest of the machine. </p>

<p>What kind of device can I use to sensor if I successfully succeeded in getting a rock on the other side of the gear? </p>

<p><img src=""http://i.stack.imgur.com/JEHag.png"" alt=""enter image description here""></p>

<p>This is just a part of a bigger system, to sum up, I need the sensor to signal when a rock is signalled out and separated from the rest so it can continue work on that single rock.</p>

<p>I am building this using an ardiuno to move the gear around, so the sensor need to be something that can be controlled by an arduino</p>
","motor sensors"
"1214","Space elevator: What is still needed, apart from the cable and propulsion?","<p>In order to build and operate a <a href=""http://en.wikipedia.org/wiki/Space_elevator"">space elevator</a> moving crafts and people into space, there are two big challenges that have not been solved yet:</p>

<ul>
<li>Finding a cable with enough <a href=""http://simple.wikipedia.org/wiki/Tensile_strength"">tensile strength</a>,</li>
<li>Moving stuff along the cable at a reasonnable speed.</li>
</ul>

<p>Apart from those two ones, what are the other technical challenges to solve, especially things that do not exist yet in robotics, and need to be invented?</p>
","mobile-robot research"
"1218","How do I adjust objects on a conveyor belt into the proper orientation?","<p>This is part two of my larger robot, it follows up what happens with the small rocks here: <a href=""http://robotics.stackexchange.com/questions/1211/what-kind-of-sensor-do-i-need-for-knowing-that-something-is-placed-at-a-position"">What kind of sensor do i need for knowing that something is placed at a position?</a></p>

<p>Now i am taking the rocks down a tube for placement. In the case they need to be altered so they always will stand up before they enter the tube. Obvioulsy a rectangular rock wont fit if it comes in sideways. The dimensions here are pretty small. The rocks are about 15 mm x 10 mm. The tube i use is actually a plastic drinking straw. And the material i use for the rest of the robot is Lego powered by step motors which draw the conveyor belts to move the rocks. The control is Arduino.</p>

<p><img src=""http://i.stack.imgur.com/J5nzR.png"" alt=""enter image description here""></p>

<p>(sorry for the lousy illustration, if you know a good paint program for mac like the one used to draw the picture in my other post, please tell me :-))</p>

<p>The rocks will always enter one at a time and have as much time they need to be adjusted to fit and enter the tube so the fall down. The question is, how to ensure all rocks are turned the right way when they get to the straw. Im not sure if using Lego when building the robot is off topic here, but a solution involving lego is preferable. And it has to be controlled by an Arduino. </p>

<p>General tips in how to split a complex task into subtasks robots can do is also good, is there any theory behind the most common sub tasks a job requires when designing multiple robots to do it?</p>
","arduino motor microcontroller motion"
"1219","Drone targeting","<p>Imagine a ""drone"" and a target point on a 2d plane. Assuming the target is stationary, there are eight parameters:</p>

<pre><code>P = my position
Q = target's position
V = my velocity
I = my moment of inertia
w = my angular velocity
s = my angular position
T = max thrust
U = max torque
</code></pre>

<p>The drone's job is to get to the target as fast as possible, obeying max torque and max thrust. There are only two ways to apply the torque, since this is only in a 2d plane. Thrust is restricted to only go in one direction relative to the orientation of the craft, and cannot be aimed without rotating the drone. Neglect any resistance, you can just pretend it is floating around in 2d outer space. Let's say the drone checks an equation at time interval <code>t</code> (maybe something like every .01 seconds), plugs in the parameters, and adjusts its torque and thrust accordingly.</p>

<ul>
<li>What should the equations for thrust and torque be?</li>
</ul>

<h3>What have we tried?</h3>

<p>We know that the time it takes for the drone to reach the target in the x-direction has to be the same for the same time in the y-direction. There is going to have to be some integral over time in each dimension to account for the changing thrust based on total thrust, and total thrust in each direction given the changing angular position. I have no idea how to tie the torque and thrust together in a practical way where a function can just be called to give what thrust and torque should be applied over the interval <code>t</code> unless there is some other technique.</p>
","design algorithm kinematics navigation"
"1221","Quadrotor control using ArduIMU","<p>We are using <a href=""http://code.google.com/p/ardu-imu/"" rel=""nofollow"">ArduIMU (V3)</a> as our Quadrotor's inertial measurement unit. (we have a separate board to control all motors, not with ArduIMU itself). </p>

<p>As mentioned <a href=""http://code.google.com/p/ardu-imu/wiki/Output#Output_Rate"" rel=""nofollow"">here</a> , the output rate of this module is only at about 8hz. </p>

<p>Isn't it super slow to control a quadrotor ? I'm asking because as mentioned in <a href=""http://robotics.stackexchange.com/a/246/1137"">this answer</a> a quadrotor needs at least 200hz of control frequency to easily stay in one spot, and our ESCs is configured to work with 450hz of refresh rate. Any working PID controller I saw before for Quadrotors used at least 200-400hz of control frequency.</p>

<p>I asked similar question before from Ahmad Byagowi (one of the developers of ArduIMU ) and he answered:</p>

<blockquote>
  <p>The arduimu calculates the dcm matrices and that makes it so slow. If
  you disable the dcm output, you can get up to 100 hz gyro, acc and so
  on.</p>
</blockquote>

<p>So, what will happen if I disable DCM from the firmware ? Is it really important ? We did a simulation before and our PID controller works pretty well without DCM.</p>
","arduino quadcopter imu pid"
"1227","Connecting More Than Six Analog Input Pins to arduino","<p>I'm in the planning stages for a project using the Arduino Uno to control 8 distance sensors, and have run into a little road block, the Uno only has six input pins. So I'm wondering, is there any way for this to work? If so, how?</p>
","arduino microcontroller input"
"1229","Limits of PWM, Timers and Interrupts?","<p>I have a robot with two wheels/motors and each has a quadrature encoder for odometry.  Using the <a href=""http://www.pololu.com/catalog/product/1218"" rel=""nofollow"">wheel/motor/encoder combo from Pololu</a>, I get 48 transition changes per rotation and my motors give me a max of 400RPM.  I've found it seems to miss some of the encoder state changes with the <a href=""https://github.com/pololu/libpololu-avr/tree/master/src/PololuWheelEncoders"" rel=""nofollow"">Pololu wheel encoder library</a>.</p>

<p>Would I run into issues or limitations on my Arduino Uno using interrupts to track the quadrature encoders while using PWM to drive my motors through an H-bridge chip?  </p>
","arduino pwm encoding interrupts"
"1232","How can I use the Arduino PID library to drive a robot in a straight line?","<p>I would like to create an Arduino based robot with 2 wheels, quadrature encoders on each wheel, a H-bridge driver chip (or motor controller) and a caster.  I want to use the PID library to ensure the speed is proportional to the distance to travel.  </p>

<p>At a conceptual level, (assuming the motors do not respond identically to PWM levels) how can I implement the PID control so that it travels in a straight line and at a speed proportional to the distance left to travel? </p>
","arduino pid driver encoding"
"1235","How does rocker bogie keep the body almost flat?","<p>How does rocker-bogie mechanism keep the body flat / keep the solar panel almost flat all the time? I know there is an differential system that connect both rocker bogie (left and right) together. But how does it actually work?</p>

<p><em><strong>Edited:</em></strong> Please provide relevant references.</p>
","wheeled-robot"
"1246","Using an Xbox controller to fly a Quadrocopter","<p>So I have a quadrocopter, it does come with a remote but I intend to run certain modifications to the copter, like installing a camera, a mechanical manipulator, and other random modifications. The remote that comes with the copter isn't flexible enough to help with such functions and plus it lacks any more buttons. </p>

<p>I was wondering if I could somehow program the quadrocopter to respond to my Xbox controller. I was planning on using my laptop's Bluetooth connection to talk to copter. The Xbox controller which is connected to the computer would be then used to control the quadrocopter. So my question is, how exactly do I program the controller? How do I go about making all of this possible? </p>

<p>I understand this question is really vague and that there are too many options out there, but I do need help figuring this out. </p>
","quadcopter"
"1249","Would the strength and speed of a robot skeleton be a danger to its wearer?","<p>Expanding upon the title, I am querying the use of robotic skeletons to augment human strength and speed. If such a robot had the capacity for example to bear weight 5 times heavier than the wearer and move its robotic limbs twice as fast as the wearer, is there not a danger because such powerful and sharp movements could break their bones and seriously injure them because it moves beyond their human capabilities?</p>

<p>The robots means of producing movement I would think is important here but unsure how so. The nature of passive or actively powered movement and when each mode is used will also determine performance of the exoskeleton. I am not well versed in this area so will appreciate any feedback.</p>
","mobile-robot"
"1253","What is the difference between Task-Level and Joint-Level Control Systems?","<p>While doing a literature review of mobile robots in general and mobile hexapods in particular I came across a control system defined as ""Task level open loop"" and ""Joint level closed loop"" system.</p>

<blockquote>
  <p>The present prototype robot has no external sensors by
  which its body state may be estimated. Thus, in our simulations and experiments, we have used joint space closed
  loop (“proprioceptive”) but task space open loop control
  strategies.</p>
</blockquote>

<p>The relevant paper is <a href=""http://ai.eecs.umich.edu/RHex/Papers/ijrr2001.pdf"" rel=""nofollow"">A simple and highly mobile hexapod</a></p>

<p>What is the meaning of the terms ""joint-level"" and ""task-level"" in the context of the Rhex hexapod?</p>
","mobile-robot control walking-robot hexapod"
"1257","How can I get Windows Kinect working on Angstrom on Beaglebone?","<p>I have tried following a number of guides on the internet but most of them fall down as libfreenect does not exist in opkg, which is the apt-get of Angstrom. Has anyone got it working and if so what is the method?</p>
","kinect"
"1259","How to measure and dispense a finite amount of powder or liquid","<p>I've been watching too much <a href=""http://science.discovery.com/tv-shows/how-its-made"" rel=""nofollow"">How It's Made</a>, and I've been wondering how they build devices that spray/inject/dispense a finite amount of liquid (to within some amount of error).  I wanted to try this for a hobby project. I'm working on that dispenses dry goods in the amount I specify.</p>

<p>Do I use some kind of special nozzle/valve which can open and close at high speeds? How can I dispense a known quantity from a reservoir of a fluid substance onto each individual unit passing along an assembly line, or an amount specified by the user into another container?</p>
","mechanism manufacturing"
"1263","What is the easiest way to install ROS on OSX Mountain Lion?","<p>The latest OSX documentation I found on the website is from 2011, and the latest build is from over a year ago. I'm a complete n00b to all things ROS and wanted to start playing with it. What is the easiest way?</p>

<p><strong>Edit:</strong> <a href=""http://www.ros.org/wiki/groovy/Installation/OSX/Homebrew/Source"" rel=""nofollow"">this version</a> of the installation instructions is more recent (April 2013), but it says that</p>

<blockquote>
  <p>OSX is not officially supported by ROS and the installation might fail for several reasons. This page does not (yet) contain instructions for most higher level ROS packages, only for the base system. This includes the middleware and command line tools but not much more.</p>
</blockquote>

<p>""Does not contain instructions"" also means it doesn't work? What do OSX users who work on ROS usually do? Run it on an Ubuntu VM? Install it just fine on their own on OSX, even though there aren't detailed instructions on the website?</p>
","ros"
"1266","What reward function results in optimal learning?","<p>Let's think of the following situations:</p>

<ul>
<li>You are teaching a robot to play ping pong</li>
<li>You are teaching a program to calculate square root</li>
<li>You are teaching math to a kid in school</li>
</ul>

<p>These situations (i.e. supervised learning), and many others have one thing (among others) in common: the learner gets a reward based on its performance.</p>

<p>My question is, what should the reward function look like? Is there a ""best"" answer, or does it depend on the situation? If it depends on the situation, how does one determine which reward function to pick?</p>

<p>For example, take the following three reward functions:</p>

<p><img src=""http://i.stack.imgur.com/UvrHs.png"" alt=""enter image description here""></p>

<ul>
<li>Function <code>A</code> says:
<ul>
<li>below a certain point, bad or worse are the same: you get nothing</li>
<li>there is a clear difference between almost good and perfect</li>
</ul></li>
<li>Function <code>B</code> says:
<ul>
<li>you get reward linearly proportional to your performance</li>
</ul></li>
<li>Function <code>C</code> says:
<ul>
<li>if your performance is bad, it's ok, you did your best: you still get some reward</li>
<li>there is not much difference between perfect and almost good</li>
</ul></li>
</ul>

<p>Intuitively, I'd think <code>A</code> would make the robot very focused and learn the exact pattern, but become stupid when dealing with similar patterns, while <code>C</code> would make it more adaptable to change at the cost of losing perfection.</p>

<p>One might also think of more complex functions, just to show but few:</p>

<p><img src=""http://i.stack.imgur.com/eCS4R.png"" alt=""enter image description here""></p>

<p>So, how does one know which function to pick? Is it known which behavior would emerge from (at least) the basic <code>A</code>, <code>B</code> and <code>C</code> functions?</p>

<hr>

<p>A side question is would this be fundamentally different for robots and human kids?</p>
","machine-learning"
"1270","How to detect when a stepper motor has stalled?","<p>How can I detect when a stepper motor has stalled?</p>

<p>A google search led me to some people who say that
when the stepper motor stalls, the current spikes up,
which is easily detectable with a Hall sensor.
(Or, I suppose, by any of the other current sensors mentioned at
<a href=""http://electronics.stackexchange.com/questions/17246/how-can-i-sense-the-motors-current"">""How can I sense the motor's current?""</a>
).</p>

<p>However, I measured the current through (one of the 4 wires of) my stepper motor,
and it's always within a few percent of 0.5 A, whether my stepper driver is holding one position, moving it normally (which in my application is very slowly), or the stepper driver thinks it is telling the stepper to move normally, but the motor has pegged out against the hard limit.
Measuring the current in the +12V power supply going to the stepper motor driver, also seemed to give a fairly constant current.
This may be because I turned down the current limit to that amount on my ""chopper"" stepper motor driver.</p>

<p>Am I missing some key detail in the ""measure the current"" approach?</p>

<p>A google search led me to some other people that measure the back-EMF (BEMF) in one coil of the stepper during the time the stepper driver is only driving the other coil.
But that only seems to distinguish between ""a motor moving quickly"" vs ""a motor stopped"", and doesn't seem to distinguish between my case of ""a motor moving slowly"" vs ""a motor stopped"".</p>

<p>Is there some way to apply the BEMF approach even in a system where I always drive the stepper slowly, and never spin it quickly?</p>

<p>I'm currently using <a href=""http://www.pololu.com/catalog/product/2132"">a stepper driver board</a> with the TI DRV8825 chip on it, and I hoped the ""fault"" pin would tell me when the stepper motor has stalled against my hard stop.
But it doesn't seem to be doing anything -- is it supposed to tell me about a stall, but I just have it wired up wrong?</p>

<p>Is there some other chip or drive technique that detects when the stepper has stalled out against the hard stop?</p>

<p>Is there some other technique for detecting a hard stall that I can ""add on"" to a system using an off-the-shelf stepper motor driver?</p>

<p>(Is there some other StackExchange site that is more appropriate for questions about motors and motor drivers?)</p>
","stepper-motor stepper-driver force-sensor"
"1274","How to connect an infrared remote control to PC or Arduino or Raspberry Pi?","<p>I bought my kid a <a href=""http://www.gigo.com.tw/english/_toy/detail.php?MID=100&amp;SID=101&amp;ID=974"" rel=""nofollow"">robotics kit</a> with several motors and an infrared remote control (you can steer the robot using IR remote control).</p>

<p>Now I want to take it to the next level and control the robots from a PC or a Raspberry Pi.</p>

<p>What is the simplest approach to do this?</p>

<p>I am thinking about 2 possible ways:</p>

<ol>
<li>Find out the protocol the existing remote control uses and then emulate the IR signals using Arduino (Arduino is sending the IR signals).</li>
<li>Find a piece of hardware, which presses the buttons on the remote control and control it via to Arduino (Arduino is sending signals to the button pushers, the remote control is sending the IR signals to the robot).</li>
</ol>
","mobile-robot arduino raspberry-pi"
"1276","How can a load be balanced between multiple AC electric drive motors?","<p>I have a three wheeled vehicle in a tricycle configuration attached to a fixed frame. Each wheel is powered by an AC electric motor. The AC motors are fed by motor controllers that take a speed demand. The single main wheel (which is also steerable) has a lower gear ratio than the rear wheels so it has a theoretical higher top speed. </p>

<p>When the vehicle drives in a straight line each of the motor controllers are given identical speed requests. Unfortunately feedback from the controller indicates that some motors are pushing while some are pulling. In particular we have a common scenario where one rear wheel is pushing while the front wheel is trying to slow down. The third wheel will often have almost no current. </p>

<p>What can be done to make all three motors work together and avoid situations where they fight?  Is there a way to change the request to the motor controller to encourage the drives to work together? Do we have to switch from a speed request setup to a current control setup? If so what is the appropriate way to control the motors then? </p>

<p>Let me know if I haven't included any important details and I will update my question.</p>
","control motor"
"1277","How can I convert RGB colors to CMYK for my airbrush robot?","<p>I am developing a robot which paints using an airbrush (3D painting). I intend to use several colors as a CMYK printer, but I do not know how to do the conversion of RGB colors in the computer to the dosage of colors in CMYK.</p>
","robotic-arm"
"1279","How can I create a robot like the EZ-B using a regular Arduino?","<p>I am interested in building a robot like the <strong><a href=""http://www.ez-robot.com/Shop/AccessoriesDetails.aspx?prevCat=100&amp;productNumber=40"" rel=""nofollow"">EZ-B</a></strong>, sold by <a href=""http://ez-robot.com"" rel=""nofollow"">ez-robot.com</a>. It comes with an SDK for <a href=""http://en.wikipedia.org/wiki/Microsoft_Visual_Studio"" rel=""nofollow"">Visual Studio</a> and has direct scripting in runtime through a USB, Bluetooth, Wi-Fi, <a href=""http://en.wikipedia.org/wiki/Internet_Relay_Chat"" rel=""nofollow"">IRC</a> or <a href=""http://en.wikipedia.org/wiki/HTTP_Secure"" rel=""nofollow"">HTTPS</a> connection.</p>

<p>If I get a regular <a href=""http://en.wikipedia.org/wiki/Arduino"" rel=""nofollow"">Arduino</a> board, will I be able to control it remotely in the same way? From what I've read, an Arduino needs to hold the instructions in its own memory, but I would rather have the brain in the computer, feeding signals back and forth to the microcontroller.</p>

<p>Also, is Arduino alone, a step down as the website niceley puts it?</p>
","arduino microcontroller research machine-learning artificial-intelligence"
"1286","Is there a benefit to using 2 IMU units on a UAV set at different sensitivities?","<p>I noticed that some IMU units are tuned to be sensitive to small changes, other to large changes and some that can be adjusted between different sensitivities. I am familiar with the use of a Kalman filter to normalize readings, but I was wondering if my UAV could benefit from a second IMU where the two are set at high and low sensitivities to get even more accurate and timely information.</p>
","kalman-filter quadcopter imu uav"
"1290","Issues upgrading Arduino code for Kinect controlled arm from 2 servos to 4","<p>I have arduino code for operating 2 servos, but we are using 4 servos and am having trouble getting the other 2 to talk. </p>

<p>The program so far as I can make out is that the angles for the servos that are calculated by the processing side are being sent out one after the other (shoulder, elbow, wrist, wrist2) then repeated. The arduino program gets this data and stores in into an array and then is written to the pin of the appropriate array segment. So 0 is shoulder, 1 is elbow, 2 is wrist and 3 is wirst2. </p>

<p>I can easily get 2 servos to run with no problem.  But when I try and add 1 or 2 more we get no response.  Can anyone please help me to get the other 2 servos to work?  My knowledge on this code is rather limited, so any help is appreciated.  </p>

<p>processing Data being sent to the arduino:</p>

<pre><code>byte out[] = new byte[4];
out[0] = byte(shoulderAngle);
out[1] = byte(elbowAngle);
out[2] = byte(wristAngle);
out[3] = byte(wrist2Angle);
port.write(out);
</code></pre>

<p>Arduino Code:</p>

<pre><code>#include &lt;Servo.h&gt;

//Declares the servos.
Servo shoulder;
Servo elbow;
Servo wrist;
Servo wrist2;

//Setup servo positions.
int nextServo = 0;
int servoAngles[] = {0, 0};

//Define pins for each servo.
void setup()
  {
  shoulder.attach(50);
  elbow.attach(51);
  wrist.attach(52);
  wrist2.attach(53);

  Serial.begin(9600);
}

void loop()
{
  if(Serial.available())
  {
    int servoAngle = Serial.read();  

    servoAngles[nextServo] = servoAngle;  
    nextServo++;  

    if(nextServo &gt; 3)
    {
      nextServo = 0;  
    }

    shoulder.write(servoAngles[0]);
    elbow.write(servoAngles[1]);
    wrist.write(servoAngles[2]);
    wrist2.write(servoAngles[3]);
  }
}
</code></pre>

<p>Sorry for the lengthy post but have been stuck for a while. </p>
","arduino kinect robotic-arm"
"1294","What are the mechanics of translational drift?","<p>Concerning robots which rotate at high speed by spinning the drive motors in opposite directions, while still being able to simultaneously move in a direction (translate):</p>

<p>As far as I know this originated with competitive fighting robots, where it is known as ""melty brain"" or ""tornado drive,"" according to wikipedia, and is based on alternately slowing down the motors on either side as they revolve around the centre of mass.</p>

<p>However, with the whole body spinning so fast how is the current ""heading"" of the robot established and maintained?</p>
","navigation movement"
"1299","Send Arduino sensor data to server with GPRS shield","<p>I'm trying to send Arduino sensor data to a server using a GPRS shield (<a href=""http://www.geeetech.com/wiki/index.php/Arduino_GPRS_Shield"" rel=""nofollow"">SIM900 shield from Geeetech</a>). I have this particular set up because the data will be updated to a website and the device will be roaming. I can't use <a href=""http://www.cosm.org"" rel=""nofollow"">http://www.cosm.org</a> because to the best of my knowledge that only updates every 15 minutes, I need to update about every 5-10 seconds.</p>

<p>In order to connect I tried the code below to form UDP connection but it does not get sent through to the receiving IP and port. I don't know why. No errors occur on the Arduino side, and the server side has been shown to work with an iPhone app that sends a UDP message.</p>

<pre><code>///connect
void connectUDP()
{
 mySerial.println(""AT+CSTT=\""APN\"""");
 delay(3000);
 ShowSerialData();
 mySerial.println(""AT+CIICR"");
 delay(3000);
 ShowSerialData();
 mySerial.println(""AT+CIFSR"");
 delay(3000);
 ShowSerialData();
 mySerial.println(""AT+CIPSTART=\""UDP\"",\""SERVER IP\"",\""SERVER PORT\"""");
 delay(3000);
 ShowSerialData();
 mySerial.println();

}


///send udp packet to server 
void sendUDP()
{
 for(int x = 0; x &lt; 30; x++){
   mySerial.println(""AT+CIPSEND""); 
   delay(100);
   ShowSerialData();
   mySerial.println(""\""hello world\"""");
   delay(100);
   ShowSerialData();
   mySerial.println((char)26);
   delay(1000);
   ShowSerialData();
 }
 mySerial.println();
 //ShowSerialData();
}
</code></pre>

<p>The server side is as follows (written in python):</p>

<pre><code>import SocketServer

PORTNO = 14

class handler(SocketServer.DatagramRequestHandler):
    def handle(self):
        newmsg = self.rfile.readline().rstrip()
    print (newmsg)
        self.wfile.write(self.server.oldmsg)
        self.server.oldmsg = newmsg

s = SocketServer.UDPServer(('',PORTNO), handler)
print ""Awaiting UDP messages on port %d"" % PORTNO
s.oldmsg = ""This is the starting message.""
s.serve_forever()
</code></pre>

<p>I can see a possible solution might be to change it to a TCP connection, but I don't know how to do that...</p>
","arduino sensors raspberry-pi programming-languages python"
"1301","What are the signs that a servo might be broken?","<p>I just got a kit and im not sure if its me or not but it appears one of the continuous servos might be broken. What happened first when I plugged it into the microcontroller, it made a humming sound when I sent it commands. The second continuous servo didnt work at all</p>

<p>I played around with different ports on the aurdino based board, and to no avail, just a <strong>hum</strong>.
Then I removed the humming servo altogether and just placed the second servo alone. the second continuous servo started to move in whatever direction I asked it to. </p>

<p>I plugged the first one in, only the second moved.</p>

<p>then I tried spinning them by hand, the second has much resistance, while the first one has dramatically less resistance, maybe 60% easier to spin by hand.</p>

<p>Is this something I can fix? Has anyone experienced these problems before?</p>

<p>Thanks in advance, you guys are great!</p>
","arduino microcontroller servos wheeled-robot"
"1302","Rotation ratio between left rocker and right rocker in rocker-bogie system","<p>Following, the <a href=""http://robotics.stackexchange.com/questions/1235/how-does-rocker-bogie-keep-the-body-almost-flat"">previous question</a>, I am trying to calculate how much one rocker would rotate when the other is being rotated. I attached my calculation here.</p>

<p>I am trying calculate the rotation of gear B that connects to right rocket. Given gear A rotates at 0.05 rad, what is the rotation of gear B in rad? Gear ratio A:D is 4:1, and D:B is 1:4.</p>

<p>At the end, I ended up with rotational gear A = gear B. This somewhat puzzles me. Is my calculation correct?</p>

<p><img src=""http://i.stack.imgur.com/HDtsx.jpg"" alt=""My calculation given rotation of gear A that connects to left rocker""></p>
","wheeled-robot"
"1305","Controlling Robots Through Serial","<p>I recently have been working on a little project. Unfortunately, I've ran into a bit of a road block with controlling servos using serial commands. The servos do appear to move when i put in any character into serial, but only a little. When i type in say, 90 characters of random gibberish, both servos connected to my arduino move several degrees. Here's my code:</p>

<pre><code>#include &lt;Servo.h&gt;
Servo ULF; // Upper left front servo
Servo LLF; // Lower left front servo
byte index = 0;
int commandnum=1;
int steps = 0; // position of LLF servo
int partnum = 0; // unused for now
String command = """"; // the command we're building
void setup()
{
  LLF.attach(0);
  ULF.attach(1);
  Serial.begin(9600);
}

void loop()
{
  while(Serial.available() &gt; 0) { // while there are more than zero bytes to read
      char in = Serial.read();
      if(in=='!') {
             //! is escape character
       commandnum++;
       partnum = 0;
       Serial.println(""New Command. Command #: ""+commandnum);
        break;
      }
      command+=in;
      if(in == ' ') {
        partnum++;
        //if we have a space, there's a new section to the command
      }
      if(command == ""LLF"") {
        Serial.read(); //skip a space

        Serial.println(""Lower Left Foot Selected."");
        int angle = Serial.parseInt(); // find the angle we want
        Serial.println(""ANGLE: ""+String(angle));

        for(int pos = 0; pos &lt; angle; pos++) // for loop through positions to reach goal
        {                                  
           LLF.write(pos); // write servo position
           delay(15);
        } 
        for(int pos = angle; pos &gt; 0; pos--) // for loop through positions to reach goal
        {                                  
           LLF.write(pos); // write servo position
           delay(15);
        } 
      }
   }
}
</code></pre>

<p>Any help would be much appreciated.</p>

<p>EDIT: Another note, nothing is printed in the serial monitor.</p>

<p>Also, these are micro towerpro rc servos.</p>
","arduino rcservo serial c++"
"1306","Why are the IRB 1410's servos running even when the joints are not moving?","<p>I was jogging the ABB IRB1410 and I noticed that the servo motors are humming even when the joints are not moving. The motor cuts off only when the guard switch in the flex pendant is released.</p>

<p><strong>What kind of mechanism which require the drive motors to keep running even when the joints are not moving ?</strong> I went through the manual but no luck. I suppose the holding torque is provided by some braking mechanism so I think I can rule it out.</p>
","industrial-robot servomotor"
"1314","Visualizing and debugging an EKF","<p>I am currently debugging and tuning an EKF (Extended Kalman Filter). The task is classical mobile robot pose tracking where landmarks are AR markers.</p>

<p>Sometimes I am surprised how some measurement affects the estimate. When I look at and calculate the numbers and matrices involved, I can work out how the update step got executed, what and why exactly happened, but this is very tedious. </p>

<p>So I wonder if anyone is using some technique, trick or clever visualization to get a better feel of what is happening in the EKF update step?</p>

<p><strong>UPDATE #1</strong> (will be more specific and show first approximation of what I have in mind)</p>

<p>What I am looking for, is some way to visualize one update step in a way that gives me a feel of how each component of the measurement affects each component of the state.</p>

<p>My very first idea is to plot the measurement and it's prediction together with some vectors taken from the K matrix. The vectors from K represent how the innovation vector (measurement - measurement prediction, not plotted) will affect each component of the state.</p>

<p>Currently I am working with an EKF where the state is 2D pose (x,y,angle) and the measurements are also 2D poses.</p>

<p><a href=""http://i.stack.imgur.com/9rLjB.png""><img src=""http://i.stack.imgur.com/9rLjB.png"" alt=""Plot of update step""></a></p>

<p>In the attached image(open it in new page/tab to see in full resolution), the (scaled) vector K(1,1:2) (MATLAB syntax to take a submatrix from 3x3 matrix) should give an idea how the first component of the EKF state will change with the current innovation vector, K(2,1:2) how the second component of EKF will change, etc. In this example, the innovation vector has a relatively large x component and it is aligned with vector K(2,1:2) - the second component of the state (y coordinate) will change the most.</p>

<p>One problem in this plot is that it does not give a feel of how the third component(angle) of the innovation vector affects the state. The first component of the state increases a bit, contrary to what K(1:1:2) indicates - the third component of the innovation causes this, but currently I can not visualize this.</p>

<p>First improvement would be to visualize how the third component of the innovation affects the state. Then it would be nice to add covariance data to get a feel how the K matrix is created.</p>

<p><strong>UPDATE #2</strong> Now the plot has vectors in state-space that show how each component of measurement changes the position. From this plot, I can see that the third component of the measurement changes the state most.</p>

<p><a href=""http://i.stack.imgur.com/PfIEN.png""><img src=""http://i.stack.imgur.com/PfIEN.png"" alt=""Added vectors corresponding to each component of measurement to state-space""></a></p>
","ekf visualization"
"1315","Non-Vision Based Target Tracking","<p>I am building 4-wheeled, knee-high robot with music and speakers on top that will follow a target person as the target moves around. I would like some help with the setup for tracking the target. The most obvious solutions are Ultrasound or Infrared sensors or some kind of vision tracking, but for this application, I don't want to use them. </p>

<p>Imagine that the robot is placed into a crowded area and asked to move towards a particular person in the area (for the sake of simplicity, assume the person is less than 5 meters away, but could be obscured by an object). Ideally, if someone walked between the target and the robot, the robot would not lose it's path (as would happen with vision-based sensing).<br>
Thanks!</p>
","sensors computer-vision navigation"
"1318","Robotics Location Following and Tracking?","<p>I am building a robot that will follow a target as the target moves around. I would like some help with the setup for tracking the target. The most obvious solutions are Ultrasound or Infrared sensors, but for this application, they won't work. Imagine that the robot is placed into a crowded area and asked to move towards a particular person in the area (for the sake of simplicity, assume the person is less than 5 meters away). Is there some kind of radar or radio solution to this, or anything?</p>
","mobile-robot sensors electronics"
"1320","Problem uploading to Roboduino AtMega328","<p>I bought a new Roboduino atmega 328 board. Basically Roboduino is a modded version of Arduino UNO made by robokits.co.in. The problem is </p>

<p><strong>On Windows Plaform:</strong>
When I tried to upload a simple Blink program that's listed in the examples of Arduino IDE 1.0.4, I got error that 
<strong>avrdude: stk500_getsync(): not in sync: resp=0x00</strong></p>

<p>I chose the correct COM port after verifying it with the Device manager. I installed the Prolific Drivers for the board. I selected the board as Arduino UNO in Arduino IDE.</p>

<p>The complete verbose for the upload is as follow:</p>

<pre><code>D:\Softwares\Installed Files\arduino-1.0.4\hardware/tools/avr/bin/avrdude -CD:\Softwares\Installed Files\arduino-1.0.4\hardware/tools/avr/etc/avrdude.conf -v -v -v -v -patmega328p -carduino -P\\.\COM10 -b115200 -D -Uflash:w:C:\Users\ANKITS~1\AppData\Local\Temp\build5865304215250534760.tmp\Blink.cpp.hex:i     
avrdude: Version 5.11, compiled on Sep  2 2011 at 19:38:36
         Copyright (c) 2000-2005 Brian Dean, http://www.bdmicro.com/
         Copyright (c) 2007-2009 Joerg Wunsch
         System wide configuration file is ""D:\Softwares\Installed Files\arduino-1.0.4\hardware/tools/avr/etc/avrdude.conf""
Using Port                    : \\.\COM10
Using Programmer              : arduino
Overriding Baud Rate          : 115200

avrdude: Send: 0 [30]   [20]   
avrdude: Send: 0 [30]   [20]    
avrdude: Send: 0 [30]   [20]    
avrdude: Recv:    
avrdude: stk500_getsync(): not in sync: resp=0x00

avrdude done.  Thank you.
</code></pre>

<p>When I plug in the board the power LED is on. The 13 pin LED blinks once. When the IDE shows uploading the 13 pin LED blinks 3-4 times and then the error appears on the screen. In between also sometimes it blinks randomly for 5-6 times. I also tried other example programs but the same follows.
I'm using 32 bit Windows 7 Ultimate and the baud rate is set to 9600.</p>

<p><strong>On Ubuntu 13.04:</strong>
I downloaded the IDE from Software Center. I was added to the dialouts group on  the first run. After connecting the board to my pc I ran two commands <code>lsusb</code> which returned following output:</p>

<pre><code>Bus 004 Device 003: ID 067b:2303 Prolific Technology, Inc. PL2303 Serial Port
Bus 001 Device 001: ID 1d6b:0002 Linux Foundation 2.0 root hub
Bus 002 Device 001: ID 1d6b:0001 Linux Foundation 1.1 root hub
Bus 003 Device 001: ID 1d6b:0001 Linux Foundation 1.1 root hub
Bus 004 Device 001: ID 1d6b:0001 Linux Foundation 1.1 root hub
Bus 005 Device 001: ID 1d6b:0001 Linux Foundation 1.1 root hub
</code></pre>

<p>and then <code>dmesg</code>. After this when I tried to upload the same program of blink, it gave me following error:<strong>avrdude: stk500_recv(): programmer is not responding</strong>. I'm using 64 bit ubuntu 13.04 and selected Arduino UNO as the board.</p>

<p>Thank you for reading this long. Please provide me suggestions for the problem.</p>
","arduino"
"1324","Additional Power to DC Motor via Second Power Source","<p>How can I provide more power to a DC motor that is in series behind a receiver circuit hacked out of a cheap RC car without burning up the receiver board? The board runs off two AAs at about 3V. I'm replacing the stock motor with a slightly larger one (12V, taken from a printer) and remounting it on a chassis for a homebrew robotics project... just messing around to learn more. I imagine I could go safely to 4.5V or even 6V with the receiver but I don't want to go much higher since half the stuff is epoxied and I can't really tell what's in there.</p>

<p>What I'd like to be able to do is add an additional two AA batteries behind the receiver to run the receiver system at 6V but add another two 3V 123A batteries to have the motor at 12V with the ability to run with the higher current draw due to the heavier load the motor will handle on its fancy new chassis... but without pulling that current through the receiver circuit.</p>

<p>My first thought is to simply connect my 123As negative to the motor and positive to a common ground... but I'm really not sure and I want to be careful to not damage the circuit or batteries. My next thought is to simply build a single power supply out of my 123As and use a current divider but I've only read about them and never actually tried so.</p>

<p>I've been doing some of those kiddie ""electronic playgrounds,"" a few books and probably cost Google an extra few bucks in energy costs and I'm still kinda at a loss.</p>
","motor power"
"1327","How do I interpret these specs for a motor and motor driver?","<p>I ran into confusion while reading about motors.</p>

<p>Consider a motor with these specs:</p>

<ul>
<li>Maximum motor voltage - 6VDC</li>
<li>No load current - 250mA max.</li>
<li>Stall current - around 1A</li>
</ul>

<p>I am considering using the Texas Instruments L293D, with these specs:</p>

<ul>
<li>Output Current - 600 mA Per Channel</li>
<li>Peak Output Current - 1.2 A Per Channel</li>
</ul>

<p>If I use the L293D to run 1 motor (back and forth), is this safe?  What would happen if my motor requires more than 600mA?  Does this simply mean I need different driver IC?</p>

<p>Also, the specs say that if I want to drive 2 motors then i'll need to compensate for the current. Is it current from my power supply or from the motor driver?</p>
","mobile-robot motor"
"1329","Battery pack discharged","<p>I used a <a href=""http://www.hobbyking.com/hobbyking/store/__8934__Turnigy_2200mAh_3S_25C_Lipo_Pack.html"" rel=""nofollow"">Turnigy 2200mAh 3S 25C Lipo</a> battery pack with <a href=""http://www.hobbyking.com/hobbyking/store/__7637__Turnigy_balancer_Charger_2S_3S.html"" rel=""nofollow"">Turnigy balancer &amp; Charger 2S-3S</a> for about a month. Yesterday I left the battery plugged into four ESCs of my quadrocopter. Today I've found the battery totally discharged. When I tried to charge it, the charger showed it as faulty. After replugging it to the charger it showed it as fully charged.</p>

<p>How can I charge it now?</p>

<p>P.S. I've got a multimeter, but I do not know what and how to measure... The battery pack has two plugs: one is connected to the charger and the other to the ESCs...</p>
","battery"
"1332","Multicopter odd or even","<p>I would like to ask which is better to design the multicopter with odd or even number of propellers? and why?</p>
","quadcopter"
"1334","Arduino Controlled RC Car. What now?","<p>I bought an RC car about a year ago. A few days later I integrated an arduino nano into the vehicle. The only thing the arduino does is to receive the RC signal and pass it on to the esc/servo. So, basically it just does a big amount of NOTHING :)</p>

<p>Right now the wiring looks like this:</p>

<blockquote>
  <p>[Remote] -> [rc receiver] -> [arduino] -> [servo/esc/lights]</p>
</blockquote>

<p>I added lights and I did some experiments with distance sensors and I will try to integrate car control via xbee + processing. This works via serial already.</p>

<p>What else could be possible with a setup like that? Here are some of my ideas:</p>

<ul>
<li>perhaps some sort of autonomic driving? The car is built for offroad and the suspension is not too bad but it is pretty fast (40 km/h) so a crash would be fatal.</li>
<li>FPV (first person view) driving? I could add another servo to move a small camera.</li>
<li>""swarm intelligence""? I have built two of those vehicles. Both feature the arduino nano, a zigbee and LED front lights.</li>
<li>steering correction? I could integrate a gyro sensor to check if the car is not driving straight when it should.</li>
<li>telemetry to another arduino? I could build some sort of arduino-zigbee-handheld that shows me some information for both cars like motor temperature, current speed, uptime, battery voltage, sensor values etc.</li>
</ul>

<p>Any ideas, anyone? Right now it is just driving like it normally would. I integrated an arduino into an RC toy that does an awesome amount of NOTHING. Makes me feel pretty stupid.</p>
","arduino sensors radio-control automatic"
"1336","Association of multiple measurements to multiple objects","<p>I have a matrix of M measurements and N objects. Each cell contains a cost of assignment a particular measurement to the object. I want to assign them optimally. As the condition, only one measurement can go to one object, and one measurement can go to only one object. I want to set some cost threshold, in effect there may be some measurement or object, which is not assigned at all.
How can I do it?
I was recently thinking of the auction algorithm, which however will never leave any unassigned measurement or object. If that is false, correct me please. Or help with some alternative solution. Thanks for your time!</p>
","artificial-intelligence sensor-fusion"
"1339","Permission to fly UAVs","<p>Okay, this might sound like a stupid question but, is there some sort of a permission in the US I might require to fly a quadcopter or a UAV for that matter? I couldn't find much help anywhere else.  </p>
","quadcopter"
"1342","Open source ""sci-fi""-like robot projects IRL","<p>I'm looking for a robot that is capable of moving around and has arms that can get objects in one place and drop in another. Something akin to what we see in most sci-fi movies, though much simpler. It may run on legs, wheels or tracks; it may have claws or hands. I'm looking for open-sourced design, schematics, specifications of the parts, coding - the whole package. It may be specific cases or projects/initiatives with a growing collection of robots.</p>

<p>As long as it can take out the trash, it's perfect. ;D</p>
","mobile-robot"
"1344","Aluminum vs Carbon Fiber","<p>So building a quadrocopter from scratch HAS a lot of decision making, and I need some input on the material choice. </p>

<p>I have short listed Aluminum and Carbon Fiber for the Arms and support of the Quadrocopter. 
I am a little short on cash to experiment with both of them. </p>

<p>So considering that I have enough money to buy either of those, and assuming that I have access to general tools like a Table Saw, Horizontal Band Saw, CNC Router and a Water jet. </p>

<p>What would be a better material to work with</p>

<p>EDIT:<br>
I will be deciding the specs around the frame so as to allow me some design liberty. So right now, my goal is to assemble a very durable, as-light-as possible frame, which can withstand a lot of experimentation on the electrical side.</p>
","design quadcopter"
"1346","Upgrading the motors on a SeaPerch ROV - more torque, or more RPMs?","<p>I am looking to upgrade the motors for SeaPerch underwater ROVs so we can carry heavier payloads and more equipment.  </p>

<p>My question is, should I look for motors which have a higher RPM and lower torque, or with lower RPM but higher torque to gain a substantial power increase? If the latter, what threshold of RPMs should I stay above to maintain speed?  </p>

<p>We are currently running <a href=""http://www.jameco.com/Jameco/Products/ProdDS/232022.pdf"" rel=""nofollow"">Jameco <code>PN 232022</code> motors</a> with ~1 1/2"" props (same setup as <a href=""http://robotics.stackexchange.com/a/313/1397"">here</a>). They are mainly run at max power as our ESC currently consists of a fuse and a toggle switch.  </p>
","motor underwater"
"1348","Single camera vision and mapping system","<p>Some time ago I saw a demo of a small 'toy tank' with a single camera mounted on it. This tank was able to drive around the floor and detect objects and then move/steer to avoid them.
The interesting part was that it used a single camera vision system and as far as I remember was taking advantage of the floor being flat. and then using the rate a feature was moving in the scene relative to the motors and directions of travel to evaluate and hence map the scene.</p>

<p>Can anyone send me pointers what to search for to get some more information on this, or some pointers to codebases that can do this.</p>

<p>The reason I ask is that this was a single camera system from a number of years ago (5+) and therefore (from what I remember) was a relatively low compute load.
I was intending to try this out on a Raspberry PI to build a car/tank that maps a room or set of rooms.</p>
","raspberry-pi computer-vision"
"1349","Working of Autonomous Lawn mower(ALM) in an unbounded area without a perimeter wire","<p>I have an Autonomous Lawn mower(ALM) which can mow a certain lawn area when that area is bounded by a perimeter wire. Even when that perimeter wire is removed, it has to mow the above mentioned area accurately without slipping into a neighboring area.</p>

<p>Constraints and problems:</p>

<ol>
<li>The ALM is an open loop system.</li>
<li>Differential GPS was tried, but it did not yield proper results.</li>
<li>Any iterative pattern of area coverage can be used provided the error in each iteration is not added cumulatively which can result in unpredictable error in the end.</li>
</ol>

<p>I do not expect full fledged solution. But I need a starting point to understand motion planning particularly for unbounded robotics to solve this problem. </p>

<p>I searched on internet to know about the knowledge sources about motion planning but could not get good results. Can anyone guide me to know about such sources preferably books and articles on internet which can help me to solve this problem?</p>

<p>EDIT:
Addition of information:</p>

<p><img src=""http://i.stack.imgur.com/a19Od.jpg"" alt=""enter image description here""> </p>

<p>The above picture shows the irregular lawn area which does not have any enclosures and perimeter
wire
1.The red mark shows the center point of lawn .</p>

<p>2.The grey area is the initial scaled down area which resembles in shape to the larger area .I could not draw the grey area which exactly resembles the larger green area .</p>

<p>3.The grey lines are the contours which from the tracks to be followed by the lawn mower</p>

<p>Idea description:</p>

<p>1.Using planimeter app for onetime , the shape and dimension of the lawn area (green area) can be known
Link:<a href=""https://play.google.com/store/apps/details?id=com.vistechprojects.planimeter&amp;hl=en"" rel=""nofollow"">https://play.google.com/store/apps/details?id=com.vistechprojects.planimeter&amp;hl=en</a></p>

<p>2.Center of polygon can be found by using the method in the following link
<a href=""http://en.wikipedia.org/wiki/Centroid#Centroid_of_polygon"" rel=""nofollow"">http://en.wikipedia.org/wiki/Centroid#Centroid_of_polygon</a></p>

<p>3.Calculation of area of grey shape in the above figure .</p>

<p>4 . Grey shape is the least possible area which can be grazed by the ALM . Grey shape is similar to the green area shape and it is formed when Green area is scaled down</p>

<p>To determine the scale down factor which is a numerical value ‘ n’ (n&lt;1) 
Where Grey area = n * Green area</p>

<p>Once the Grey area is known , the number of contours or tracks to be grazed by ALM have to be determined manually .</p>

<p>The width of contour is equal to the distance between the blades on the either end i.e. the width which can be grazed by ALM in a single stroke .</p>

<p>Green area = Grey area + area of track 1 + area of track 2 + area of track3 + . . . . . . + area of track n</p>

<p>5.Once the lawn mower is switched on ,it should reach the center of the lawn (red mark showed in the above figure)</p>

<p>6.Then, ALM should graze the least possible area or grey area .</p>

<p>7.After that ALM should Switch to contour circumscribing the grey area . It should continue circumscribing in each track till all the tracks are completed( decision has to be made by validating against the calculated and preset value ' No.of tracks' in ALM)   </p>

<p>In this way entire lawn can be mowed without the need of perimeter wire and also ALM would not mow the neighbor’s lawn </p>

<p>Challenges :</p>

<p>a. Enable ALM to reach the center point of the lawn</p>

<p>a. To make ALM mow the grey area accurately</p>

<p>b. To make the ALM switch from one track to track .</p>

<p>c. To bypass the obstacle in track and return to the same track .</p>

<p>When i mentioned this idea to my colleague ,he mentioned the about possible cumulative addition of error in each iteration resulting in an unpredictable error in the end .</p>

<p>I intend to minimize the error and fix the boundary as correct as possible. 
In fact this deviation should be predictable before it can be corrected .</p>
","motion-planning"
"1350","Building a Autonomous pesticide spraying system using swarm robotics based on odor (volatile Organic Compounds) detection","<p>Visible worms, pests, and diseased parts of plants emit a unique odor (Volatile Organic compounds with different concentrations). I understand that sensors which can quantitatively detect these compounds development are being developed. My idea is to build a swarm of robots which can spray pesticides by detecting VOCs on three targets present on plants across the fields.</p>

<ol>
<li>Target 1: Visible worms, pests, larvae. May these can be mechanically eliminated </li>
<li>Target 2: Invisible pathogens on certain areas of a plant </li>
<li>Target 3: Areas where pesticides have to be sprayed to prevent disease </li>
</ol>

<p>For these targets, pesticide has to be administered in the correct concentration 
This idea can optimize the use of pesticides and treat the plant properly 
Questions:</p>

<ol>
<li>Is swarm robotics still sci-fi or Did any one implement it?</li>
<li>Are the  any specific scenarios where implemented swarm robotic systems  are coming to help and establish the ease?   </li>
<li>Which is the implemented system or idea in conception that can help in formation of solution for the above problem? </li>
<li>How much time is required approximately to realize this idea?</li>
</ol>

<p>Hope this question does not sound sci-fi and is practical and the intention is to solve a definite problem 
How can i work by following some steps to further  make this idea concrete</p>
","sensors research"
"1356","Can I use Ziegler-Nichols's rules to find PID parameters for a non linear system","<p>I'm trying to use a PID to stabilize a system described from the following difference equation:</p>

<p>$$y_{k+1} = a y_k \sqrt{(1-y_k)}~~~ + b y_{k-1} ~+ c u_k$$</p>

<p>Can I use <a href=""http://en.wikipedia.org/wiki/Ziegler%E2%80%93Nichols_method"" rel=""nofollow"">Ziegler-Nichols's rules</a> to find PID parameters in this situation?</p>

<p>To be more precise. My system is an <a href=""http://en.wikipedia.org/wiki/Apache_HTTP_Server"" rel=""nofollow"">Apache Http Server</a>, in particular I'm trying to model how the CPU load can change in function of <a href=""http://httpd.apache.org/docs/current/mod/core.html#keepalivetimeout"" rel=""nofollow"">KeepAlive</a> parameter. When KeepAlive grows the cpu load should decrease.</p>

<p>So:</p>

<p>$$cpu_{k+1} = a \cdot cpu_k \sqrt{(1-cpu_k)}~~~ + b \cdot cpu_{k-1} ~+ c \cdot keepAlive_k$$</p>

<p>Obviously the Cpu load is a scalar $\in [0,1]$ , $keepAlive$ is just a time and the $a,b,c$ parameters are known to me through experimental data and multiple regression on them.</p>
","pid"
"1367","Prop Orientation on a Multirotor","<p>While looking up information for the right propellers for my quadcopter, I realized that they had different orientations i.e. Clockwise and Counterclockwise. On further research I found that all multi-rotors have different combinations of these orientations. So my question is WHY? How does it matter if the propeller is turning clockwise or anti-clockwise?   </p>
","quadcopter multi-rotor"
"1369","Instantaneous Center of Rotation for a differential Drive Robot","<p>I want to find the instantaneous center of rotation of a differential drive robot.</p>

<p>Assuming I know that the robot will travel with a particular linear and angular velocity $(v,w)$ I can use the equations (given at <a href=""http://rossum.sourceforge.net/papers/CalculationsForRobotics/CirclePath.htm"" rel=""nofollow"">A Path Following a Circular Arc To a Point at a Specified Range and Bearing</a>) which come out to be:</p>

<p>$$x_c = x_0 - |\frac{v}{w}| \cdot sin(\theta_0)$$
$$y_c = y_0 - |\frac{v}{w}| \cdot cos(\theta_0) $$</p>

<p>I'm using the webots simulator and I dumped gps points for the robot moving in a circle (constant v,w (1,1)) and instead of a single $x_c$ and $y_c$ I get a center point for every point. If I plot it out in matlab it does not look nice:</p>

<p><a href=""https://docs.google.com/file/d/0BzLnU1-OKHh7dmxvbGU1bDFKcFU/edit?usp=sharing"" rel=""nofollow""><img src=""http://i.stack.imgur.com/J1r3m.jpg"" alt=""circles.jpg""></a></p>

<p>The red points in the image are the perceived centers, they just seem to trace the curve itself. </p>

<p>Is there some detail I am missing? I'm really really confused as to what's happening. </p>

<p>I'm trying to figure out the center so I can check whether an obstacle is on this circle or not and whether collision will occur. </p>
","mobile-robot motion"
"1370","Prop orientation for tricopters","<p>This question stems from previous <a href=""http://robotics.stackexchange.com/questions/1367/prop-orientation-on-a-multirotor"">question</a>, where I asked why does the prop orientation matter so much for a multirotor. But on further research<sup>&dagger;</sup> I found that these reasons need not apply to a tri copter. and then again. Why? </p>

<p>Are these reasons general for all multi rotors with odd number of motors? or even rotors?</p>

<p>&dagger; <a href=""http://www.rcgroups.com/forums/showthread.php?t=1889111"" rel=""nofollow"">This forum</a> talks a lot about tricopters and prop orientations but nothing really answers the question. </p>
","multi-rotor"
"1378","How to tell when an ArduPilot has finished initialising its gyros (without referencing telemetry)?","<p>Using ArduPilot software (fixed wing, ArduPlane), I know that after I boot up I need to keep the system sit still while the gyros initialise.</p>

<p>When I have ground station in the field it's easy to know when it's safe to launch because the telemetry message tells me. But I don't always fly with a ground station. In these situations I currently just sit and wait for a while before arming, then again before launching. </p>

<p>Is there some reliable rule of thumb? information in the blinking of the arming switch or buzzing that I haven't worked out yet? This UAV has PX4 autopilot hardware (with both Px4FMU and PX4IOBoard), including with buzzer and illuminated arming switch. The LEDs on the board are obscured (but I could make light channels from them if required).</p>

<p>(Note: I'm asking this question here to test the theory that robotics stack exchange night be an appropriate forum for these sorts of questions, which has been suggested a couple of times in response to the Area51 drones proposal.)</p>
","uav"
"1383","Software for robot parts interaction modeling","<p>I'm robotic engineer, using OpenSCAD to model robotic components (gears, pulleys, parts, etc). But I need an application to model the physics and interaction of the components (for i.e. how will robot move if I rotate a given gear).</p>

<p>So, is there any software I can use for modelling interactions in Linux? Google SketchUp is good, but I can't use it in Linux.</p>
","software"
"1388","programming pan-tilt unit with C","<p>I am trying to write a C code for a pan-tilt unit model ptu-d46 using visual studio 2010 in Windows 7, but I can't find any tutorial or reference on how to do so. All the user's manual mentions is that there is a C programmer's interface (model ptu-cpi) available, but it doesn't say where to find it nor how to use. I looked for it on google but couldn't find anything.</p>

<p>There is a command reference manual along with the user's manual, but it only shows the different commands to control the tilt and does not explain how to make a C program that connects to the tilt controller and sends queries to it. </p>

<p>Does anyone please have an idea of where I should look or if there are any open source programs for that. I'm not trying to make a complicated program. I just need it to connect to the tilt controller (the computer is connected via USB cable to the host RS232 of the tilt controller) and makes it nod to say ""Yes"" and ""No"" !</p>
","c"
"1391","creating a robot from ez-b or regular arduino","<p>Does anyone have experience with this <strong>ez-b</strong>, it is sold by <em>ez-robot.com</em> and comes with an SDK for Visual Studio</p>

<p>It has direct scripting in runtime and through usb or bluetooth, wifi, irc, https</p>

<p>My question is, if I get a regular arduino board, will i be able to do the same?
from what ive read, arduino needs to hold the instructions on its own memory, but I rather have the brain in the computer, and feed signals back and forth to the microcontroller</p>

<p>Also, is <strong>arduino</strong> alone, a step down as the website niceley puts it</p>

<p>Thanks for your help in advance</p>
","arduino microcontroller"
"1392","Why does my LSM303 magnetometer reading not change in a while loop?","<p>I am using a LSM303 sensor to compute a heading and I want to turn my robot to a heading.</p>

<p>I have the simple code here:</p>

<pre><code>int mag;
mag = compass.heading((LSM303::vector){0,-1,0});;   //read the angle of the robot

Serial.println(mag);
while (mag != angle){   
    //while it isn't the desired angle turn and continue to update the robot angle      
    trex.write(0xE9);
    trex.write(90);
    trex.write(90);
    mag = compass.heading((LSM303::vector){0,-1,0});;   //read the angle of the robot


}
</code></pre>

<p>In a function called with a speed and angle (heading), the trex part tells the motor controller to turn on a point and the while loop should test for when the desired heading is reached. However, testing using a couple of instances of <code>Serial.println(mag);</code> I have determined that once inside the while loop, mag never changes which just means the robot turns indefinitely. </p>

<p>I have no idea why this would happen. Perhaps someone here does?</p>

<p>Thanks.</p>
","arduino"
"1393","What is the cheapest / easiest way of detecting a person?","<p>I'd like to know if anyone has had success detecting a warm-bodied mammal (ie. Human) using standard off the shelf, inexpensive sensors?  </p>

<p>Ideally, I'd like to use an inexpensive sensor or combination of sensors to detect a person within a room and localize that person. I would like the robot to enter a room, detect if a human(s) is/are present and then move to the detected human. The accuracy does not need to be 100%, as cost is more of a factor. I'd like the computational requirements of such a sensor to be such that it can run on an Arduino, although if it's impossible, I'd be willing to utilize something with more horespower, such as a Raspberry Pi or a BeagleBone Black. I have a few thoughts; however, none of them are ideal:</p>

<ol>
<li><strong>PIR Sensor</strong> - Can detect movement within a large field of vision (ie. usually 120 degrees or more).  Might be the closest thing to a ""human"" detector that I'm aware of; however, it requires movement and localizing/triangulating where a person is would be very difficult (impossible?) with such a large field of vision.</li>
<li><strong>Ultrasound</strong> - Can detect objects with good precision.  Has a much narrower field of view; however, is unable to differentiate between a static non-living object and a human.</li>
<li><strong>IR detectors</strong> - (ie. Sharp range sensors) Can again detect objects with great precision, very narrow field of view; however, it is again unable to differentiate objects.</li>
<li><strong>Webcam + OpenCV</strong> - Possibly use face detection to detect human(s) in a room.  This may be the best option; however, OpenCV is computationally expensive and would require much more than an arduino to run.  Even on a Raspberry Pi, it can be slow.</li>
<li><strong>Kinect</strong> - Using the feature detection capabilities of Kinect, it would be relatively easy to identify humans in an area; however, the Kinect is too expensive and I would not consider it a ""cheap"" solution.  </li>
</ol>

<p>Perhaps someone is aware of a inexpensive ""heat-detector"" tuned to body heat and/or has had success with some combination of (#1-4) above and would like to share their results?</p>
","sensors sensor-fusion"
"1394","qt ros tutorial issue","<p>I am working on a robotic application under R.O.S. groovy Galapagos.
I would like to make a tutorial about how create a template app with catkin_create_qt_pkg.
I'm unable to call the script <code>catkin_create_qt_pkg</code> from my catkin workspace.
I found it at the root : _/opt/ros/groovy/qt_ros/qt_create/script_
But even if I try to execute it as sudoer I got an error.</p>

<blockquote>
  <p>ImportError: No module named qt_create</p>
</blockquote>

<p>I'm unable to determine what I have to do to make it work.</p>

<p>Why?</p>
","c++ ros"
"1399","Wearable Accelerometor","<p>I've worked with wiimote accelerometer, but I think now I want to move past that. Mostly because I want to have a wider range of available gestures and I think that using only one accelerometer has too many limitations for what I want to do. I'm looking for something compatible with arduino or RPi. Does anyone have recommendations on how I should do this?</p>
","control sensors accelerometer"
"1400","Stream Real-time Video with Environmental Data Overlaid","<p>I want to embed environmental data collected from sensors into a live video stream from a camera. Has anyone done this or know how I would go about doing something like this? Is there a library available for the arduino or RPi?</p>
","sensors cameras"
"1408","Pushing Buttons Remotely over Ethernet","<p>So I want to program something that will simply push a button, but controllable over ethernet.  I'm new to robotics so I don't know where to start.  What's the best way to control an actuator over a network connection?</p>
","arduino actuator"
"1413","Is a Raspberry Pi processor powerful enough for a mobile chatbot?","<p>In general, is a Raspberry Pi processor powerful enough for a mobile chatbot? I want to make a small mobile robot that is like a chatbot. Is a Raspberry Pi processor powerful enough for any type of AI robotics?
As far as a mobile robot, I want to make a wheeled robot about one foot in every dimension. The chatbot abilities will be from ProgramPY-SH, a new chatbot program that uses Xaiml databases. The chatbot works by looking through a database for a match of the user's input (vocal or text-based). It then acts according to the instructions given by the XML-like database.</p>
","mobile-robot raspberry-pi artificial-intelligence"
"1414","PID Conundrums for Legged Robots","<p>I am currently working on a legged hexapod which moves around using a tripod gait. I have two sets of code to control the tripod. </p>

<p>Set 1: Time based control</p>

<p>In this code set, I set the tripod motor set to move at their rated rpm for a required amount of time before shifting to the other tripod motor set.</p>

<p>PID control would be based on counting the number of transitions using an optical speed encoder, Calculating the error based on difference between actual speed and required speed and then adjusting the error with fixed Kd and Ki values.</p>

<p>Set 2: Transitions based control</p>

<p>In this code set I count to the number of transitions required to complete one rotation of the leg(tripod motor set) before starting the other leg(tripod motor set).</p>

<p>PID control would be time based. Calculation of error would be the difference in time taken for individual motors of the motor set.</p>

<p>Query:
The set 2 shows promising results even without PID control, but the first set does not.Why so? The motors are basically set to move 1 rotation before the other set moves. </p>

<p>Would the speed differences between the motors cause it to destabilize?</p>

<p>How often do I update the PID loop?</p>

<p>My robot seems to drag a little bit. How do I solve this?</p>
","mobile-robot pid legged walking-robot hexapod"
"1416","Building robots with high reliability, durability, and battery life","<p>I'm involved in research on psychologically plausible models of reinforcement learning, and as such I thought it'd be nice to try and see how well some to the models out there perform in the real world (i.e. sensory-motor learning on a mobile robot). This is already been done in some robotics labs, such Sutton's <a href=""http://webdocs.cs.ualberta.ca/~sutton/papers/horde-aamas-11.pdf"" rel=""nofollow"">implementation of the Horde Architecture on the ""Critterbot""</a>. However, these implementations involve robots custom-build by robotics experts in order to deal with the trials and tribulations of learning on a long time-scale: </p>

<blockquote>
  <p>""The robot has been
  designed to withstand the rigors of reinforcement learning
  experiments; it can drive into walls for hours without damage or burning out its motors, it can dock autonomously
  with its charging station, and it can run continuously for
  twelve hours without recharging.""</p>
</blockquote>

<p>Unfortunately I'm no expert when it comes to designing robots, and don't have access to a high quality machine shop even if I did; I'm stuck with whatever I can buy off-the-self or assemble by hand. Are these constraints common enough for amateur robotics suppliers to cater to, or should I expect to have to start from scratch?</p>
","mobile-robot battery reinforcement-learning"
"1417","What is the best way to go about building a robot hand that can type on keyboard, move &click mouse, swipe touchscreens?","<p>How would you go about building a robot that can use a computer? Type on the keyboard, move &amp; click mouse? I am talking about physically manipulating the hardware inputs, and the robot would be able to see the screen. Not connected to anything. It's purely autonomous. My hope is that this will replace human QA testers.</p>
","wheeled-robot artificial-intelligence robotic-arm"
"1419","How can I improve the map in my Mobile Autonomous Robot using KINECT","<h1>A little background of my aim</h1>

<p>I am in the process of building a mobile autonomous robot which must navigate around an unknown area, must avoid obstacles and receive speech input to do various tasks. It also must recognize faces, objects etc. I am using a Kinect Sensor and wheel odometry data as its sensors. I chose C# as my primary language as the official drivers and sdk are readily available. I have completed the Vision and NLP module and am working on the Navigation part. </p>

<p>My robot currently uses the Arduino as a module for communication and a Intel i7 x64 bit processor on a laptop as a CPU.</p>

<p>This is the overview of the robot and its electronics: </p>

<p><a href=""http://i.stack.imgur.com/Oo3PD.jpg""><img src=""http://i.stack.imgur.com/Oo3PDm.jpg"" alt=""overview of the robot""></a>
<a href=""http://i.stack.imgur.com/sYjNL.jpg""><img src=""http://i.stack.imgur.com/sYjNLm.jpg"" alt=""electronics of the robot""></a></p>

<hr>

<h1>The Problem</h1>

<p>I implemented a simple SLAM algorithm which gets robot position from the encoders and the adds whatever it sees using the kinect (as a 2D slice of the 3D point cloud) to the map.</p>

<p>This is what the maps of my room currently look like:</p>

<p><a href=""http://i.stack.imgur.com/4NzwA.png""><img src=""http://i.stack.imgur.com/0jcKv.png"" alt=""what a map looks like of my room""></a> <a href=""http://i.stack.imgur.com/r81Bd.png""><img src=""http://i.stack.imgur.com/r81Bd.png"" alt=""Another map my room""></a></p>

<p>This is a rough representation of my actual room:</p>

<p><a href=""http://i.stack.imgur.com/tsbzE.jpg""><img src=""http://i.stack.imgur.com/AnKvn.jpg"" alt=""enter image description here""></a></p>

<p>As you can see, they are <strong>very</strong> different and so really bad maps.</p>

<ul>
<li>Is this expected from using just dead reckoning?</li>
<li>I am aware of particle filters that refine it and am ready to implement, but what are the ways in which I can improve this result? </li>
</ul>

<hr>

<h1>Update <br /></h1>

<p>I forgot to mention my current approach (which I earlier had to but forgot). My program roughly does this: (I am using a hashtable to store the dynamic map)
<br /></p>

<ul>
<li>Grab point cloud from Kinect</li>
<li>Wait for incoming serial odometry data</li>
<li>Synchronize using a time-stamp based method</li>
<li>Estimate robot pose (x,y,theta) using equations at <a href=""http://en.wikipedia.org/wiki/Dead_reckoning#Differential_steer_drive_dead_reckoning"">Wikipedia</a> and encoder data</li>
<li>Obtain a ""slice"" of the point cloud</li>
<li>My slice is basically an array of the X and Z parameters</li>
<li>Then plot these points based on the robot pose and the X and Z params</li>
<li>Repeat</li>
</ul>
","arduino slam kinect odometry"
"1427","Controlling a Quadrotor from a PC","<p>I need to control quadrotor from a PC, without using a joystick.</p>

<p>I have got a mini-beetle <a href=""http://www.banggood.com/Wholesale-WLtoys-V929-Beetle-4-Axis-Dexterous-Mini-UFO-BNF-p-46046.html"" rel=""nofollow"">quad V929 Beetle 4-Axis</a>  and also have this <a href=""http://www.ebay.in/itm/281061946755?ssPageName=STRK%3aMEWNX%3aIT&amp;_trksid=p3984.m1439.l2648"" rel=""nofollow"">NRF24L01+ Wireless Transceiver Module Chip</a> (2.4ghz transceiver)
Is it possible to write an arduino program to make them speak to each other?</p>

<p>I did some research and found that the quad v929 model uses flysky protocol and only works with A7105 NRF24L01 2.4ghz transmitter chip not the one which i mentioned above</p>

<p>Are there any other better ways of controlling the quad from PC or arduino board?</p>
","arduino quadcopter radio-control"
"1429","Pan and tilt bracket for a stepper motor","<p>Currently, I'm using a <a href=""https://www.sparkfun.com/products/10335"" rel=""nofollow"">pan tilt bracket</a> that works with the servos I have (spark fun product 9065), and are generic to all servos. However, servos aren't accurate enough. I'm looking at other options, but have limited knowledge about them. What would you recommend?</p>

<p>An important consideration is that I need my motor to tilt in two dimensions, to any random $x, y$ coordinate accurately. To do this, I think I'll need some sort of attachment to the motor shaft, like the pan tilt bracket.</p>

<p>What motor and bracket would you recommend I use to go to any random $(x,y)$ coordinate?</p>
","servos stepper-motor motion"
"1435","laser scanner distance","<p>I'm looking at laser scanners and I see a huge range of detection distances.  The furthest I've see are 30m if you exclude the very expensive ones that claim up to 150m.  My question is what is the reason for the huge difference in range/price.  I would think that with a laser it wouldn't be that difficult to detect something at distances greater than 30m.  What's the physical limitation that makes it so much more expensive to go above 30m for a laser scanner or is there one?</p>
","localization"
"1437","Most material efficient quadcopter frame","<p>I haven't made or flown any quadcopter, but I am fascinated by them.
But when looking at the frame of a lot of designs, after whachting <a href=""http://www.youtube.com/watch?v=dAyDi1aa40E"" rel=""nofollow"">this video</a> I wondered why a lot the frames are in an X shape. Since the most efficient shape would according to the video be something like this >-&lt;, where each corner is 120°.
I also did a quick search on the internet and found <a href=""http://deltaquadlog.blogspot.nl/2013/03/frame-design.html"" rel=""nofollow"">this blog</a> which stated the same (however he did not mention the exact angle) and said: ""Even though this is not entirely a new idea, it has not yet been widely accepted by the community.""</p>
","design quadcopter"
"1440","trapezoidal vs sinusoidal commutation","<p>How do you know if a commercial driver is working with trapezoidal or sinusoidal commutation? If you measure the 3-phase voltage applied to the PMSM by means of an oscilloscope, will you see a difference?</p>
","brushless-motor"
"1443","Learning about embedded systems","<p>The last robotics project I worked on involved autonomous outdoor navigation, using a microcontroller for lower-level control and a computer for image processing and decision making. I worked more with the higher-level software and another guy on the team did the electrical and embedded systems. I would like to be capable of doing everything, including stuff with embedded, but I'm not sure where to look for information. If I were to have done the project from scratch on my own, I'd need to know:</p>

<ul>
<li>What microcontroller to use</li>
<li>What motors are required </li>
<li>What motor controllers to get, and how to interface with the controllers</li>
<li>What encoders to use for motor feedback, and how to write drivers for them</li>
<li>What batteries to use and how to safely power everything </li>
</ul>

<p>If I were trying to learn about higher-level software, I'd probably take a few courses on Udacity. Are there any good resources out there like that for this kind of low-level stuff?</p>
","microcontroller power embedded-systems"
"1445","How to get data from ArduPilot through Serial Port","<p>I'm doing a project related to telemetry, and I want to make ArduPilot (programmed with ArduPlane 2.73) send through a serial port the sensors informations such as height, GPS position, etc.. I have tried to use ArduStation, but I could not change its firmware to do what I want. </p>

<p>The idea would be to read the Ardupilot's serial port using an Arduino Uno, and then saving it in a SD card in real-time. So ArduPilot needs to send data without any user input or something like that. I've already tried to manipulate ArduPlane source code to do that, but I couldn't either.</p>

<p>Has someone here did something like that before? I need some advice!</p>
","arduino ardupilot"
"1448","Maze Solving Algorithm For Mazes With Loops","<p>I am trying to implement a line following robot that can solve mazes similar to the Pololu robots you can watch on youtube. My problem is the maze that I am trying to solve is looped and therefore simple Left/Right hand rule can not solve the maze. I have done some research and think either Flood-Fill or Breadth-First-Search algorithm will be able to solve these looped mazes. Solving the maze is reaching a large black area where all the sensors will read black. When the robot is following the line some of the sensors will read white and the central ones black.</p>

<p>Is there any other algorithms that can solve looped mazes? I understand how the Flood-Fill algorithm works but am unsure how it could be implemented in this situation.</p>

<p>My robot has 3 sensors on the bottom. The one in the centre is expected to always read black (the black line it follows) and the sensors on the right and left are expected to read white (while following a straight line) and then black once a junction or turn is reached. My robot has no problem following the line, turning etc. I need to find an algorithm that can solve looped mazes (that is, find a path from an entrance to an exit).</p>
","mobile-robot motion-planning line-following routing"
"1449","How are units of noise measurement related to units of a sensor's data measurement?","<p>I'm trying to understand how noise is represented for accelerometers, gyroscopes, and magnetometers so that I can match the requirements of my project with the standard specs of these sensors. </p>

<p>I want the output of a 3 axis inertial sensor to be values in meters/sec/sec, gauss, and radians/sec for each axis, and noise to be represented by a range around the true value (so X as in +/- X m/s/s, gauss, and radians/sec) for accelerometers, magnetometers, and gyroscopes respectively. Switching out gauss for teslas, meters for feet, radians for degrees, etc. would all be fine.</p>

<p>After looking at a few datasheets, I'm surprised to find that... </p>

<ul>
<li>Accelerometer noise is measured in ""LSB rms"" and ""μg/√Hz""(<a href=""https://www.sparkfun.com/datasheets/Sensors/Accelerometer/ADXL345.pdf"" rel=""nofollow"">https://www.sparkfun.com/datasheets/Sensors/Accelerometer/ADXL345.pdf</a>, <a href=""http://dlnmh9ip6v2uc.cloudfront.net/datasheets/Sensors/Accelerometers/MMA8452Q.pdf"" rel=""nofollow"">http://dlnmh9ip6v2uc.cloudfront.net/datasheets/Sensors/Accelerometers/MMA8452Q.pdf</a>)</li>
<li>Gyroscope noise is measured in ""º/s-rms"" and ""º/s/√Hz"" (<a href=""https://www.sparkfun.com/datasheets/Sensors/Gyro/PS-ITG-3200-00-01.4.pdf"" rel=""nofollow"">https://www.sparkfun.com/datasheets/Sensors/Gyro/PS-ITG-3200-00-01.4.pdf</a>)</li>
<li>Magnetometer noise is measured in ""µT rms"" and ""Gauss/√Hz"" (<a href=""http://dlnmh9ip6v2uc.cloudfront.net/datasheets/Sensors/Magneto/MAG3110.pdf"" rel=""nofollow"">http://dlnmh9ip6v2uc.cloudfront.net/datasheets/Sensors/Magneto/MAG3110.pdf</a>, <a href=""http://www.vectornav.com/products/vn200-rug?id=54"" rel=""nofollow"">http://www.vectornav.com/products/vn200-rug?id=54</a>)</li>
</ul>

<p>What do these units mean, and how do they (or can they) translate into what I want?</p>
","imu accelerometer gyroscope noise magnetometer"
"1450","How would I go about interfacing the raspberry pi and arduino?","<p>I want to do a project where I control motors, and the arduino seems ideal for that. However, I'm going to need to do quite a bit of processing on the back end first. Connecting the arduino to a computer isn't an option, because it has to be quite mobile. Does anyone have any experience controlling arduino with raspberry pi?</p>
","arduino raspberry-pi"
"1454","Motor and Ethernet shields together","<p>I was wondering if it is possible to plug a motor shield on top of an Ethernet shield, even though the direction pins on the motor shield would be connected to the same pins as the spi bus.  I was thinking that it would work if, in the coding, I disabled both chip selects on the Ethernet shield before I used the motors.</p>
","arduino"
"1455","Help with adaptive fill algorithm for Water Color Painting Robot","<p><em><strong>TL;DR:</em></strong>
Can anyone point me to a good adaptive path fill algorithm?</p>

<p>Hey there, my name is James, and <a href=""http://sylviashow.com"" rel=""nofollow"">my daughter</a> built <a href=""http://watercolorbot.com"" rel=""nofollow"">an awesome painting robot</a> with her friends over at Evil Mad Scientist labs, even brought it to the white house, very fun stuff!</p>

<p>Anyways, I'm a web programmer by day, and though it might be fun to try and write some software to get the robot to do some cool stuff, and for some crazy reason I decided to do this using standard web technologies like SVG and JavaScript, and.. it actually works! [see the project at github.com/techninja/robopaint]</p>

<p><strong>But there's a problem:</strong> to fill in colors for given shapes, it requires some kind of path filling algorithm using a given size shape to cover every part of the shape internally, not to mention it has to take into account overlapping paths and occlusions.</p>

<p>I have successfully created ""<em>fake</em>"" fills, by following known paths like spirals and back and forth hatch lines over paths, while detecting occlusion using browser internal functions for detecting what object lies at a given x/y coordinate, but these fill functions fall incredibly short of doing anything other than simply following paths, and can be incredibly inefficient at filling certain paths (like filling borders, letters, or <strong>U</strong> shaped areas).</p>

<p><em><strong>The question:</em></strong> I need an adaptive path filling algorithm. I know they're currently being used in similar CNC setups for milling, and similar algorithms are used by the Roomba and 3D printers to figure out coverage in the most efficient way possible. The issue comes in that I don't think <strong><em>any</em></strong> have ever been done in JavaScript, using native SVG paths.</p>

<p>Anyone out there know where I should look? I'm not too afraid to attempt to port something over to JS, or possibly even use it as is for a native Node.JS module. All my work will be sure to go back to the community and become open source as well.</p>

<p>Thanks for the help!</p>
","algorithm motion-planning motion cnc"
"1457","Is this a good quadcopter build?","<p>3D CAD:
<img src=""http://i.stack.imgur.com/0heyS.jpg"" alt=""quadcopter image""></p>

<p><strong>After some editing:</strong></p>

<p>I'm building the frame myself from aluminum and polycarbonate.</p>

<p><strong>EDIT:</strong> all frame is polycarbonate except of the arms (and all screws &amp; nuts &amp; washers)), they are form aluminum covered in polycarbonate. the total length (from one motor to another) is exactly <em>60cm</em></p>

<p><strong>Approximate Mass: 1.736 kg</strong></p>

<h2><strong>What I thought to put in:</strong></h2>

<ul>
<li><strong>Battery</strong>: <a href=""http://www.hobbyking.com/hobbyking/store/__7634__ZIPPY_Flightmax_4000mAh_3S1P_20C.html"" rel=""nofollow"">ZIPPY Flightmax 4000mAh 3S1P 20C</a> (changed from this 1500mAh <a href=""http://www.hobbyking.com/hobbyking/store/uh_viewitem.asp?idproduct=6307&amp;aff=588847"" rel=""nofollow"">battery</a>)</li>
<li><strong>Motors</strong>: <a href=""http://www.hobbyking.com/hobbyking/store/uh_viewitem.asp?idproduct=18151&amp;aff=588847"" rel=""nofollow"">Turnigy Aerodrive SK3 - 2822-1090kv</a></li>
<li><strong>PDB</strong>: <a href=""http://www.hobbyking.com/hobbyking/store/uh_viewitem.asp?idproduct=23140&amp;aff=607489"" rel=""nofollow"">Hobby King Quadcopter Power Distribution Board</a></li>
<li><strong>ESC</strong>: <a href=""http://www.hobbyking.com/hobbyking/store/__6457__hobbyking_ss_series_18_20a_esc.html"" rel=""nofollow"">Hobbyking SS Series 18-20A ESC</a></li>
<li><strong>Core</strong>: <a href=""http://www.ebay.com/itm/Nano-V3-0-ATmega328P-Arduino-Compatible-/170962843198?pt=LH_DefaultDomain_0&amp;hash=item27ce2df63e"" rel=""nofollow"">Arduino Nano</a></li>
<li><strong>IMU</strong>: <a href=""http://cgi.ebay.com/ws/eBayISAPI.dll?ViewItem&amp;item=261231955458"" rel=""nofollow"">Invensense MPU-6050</a></li>
<li><strong>Propellers</strong>: <a href=""http://www.hobbyking.com/hobbyking/store/uh_viewitem.asp?idproduct=25822&amp;aff=607489"" rel=""nofollow"">10x4.5 SF Props 2pc Standa## Heading ##rd Rotation/2 pc RH Rotation</a> (changed from 8x4.5 ones)</li>
</ul>

<p>I'm gonna program it myself (or atleast try to).</p>

<p><strong>I WANT TO USE IT AS A UAV (self controlled)</strong></p>

<h2><strong>Questions:</strong></h2>

<ol>
<li><p>Is this a good setup?</p></li>
<li><p>It costs to much for me, any way to keep the frame and change the
parts to be cheaper and yet a good quadcopter?</p></li>
<li><p>Does the propeller fit the motor?</p></li>
<li><p>Any place to buy these for cheaper?</p></li>
<li><p>Is it a good frame?</p></li>
<li><p>Am I missing some parts?</p></li>
</ol>

<p>Thanks alot,
Dan</p>

<p><strong>EDIT:</strong></p>

<ul>
<li><p>I checked it using this <a href=""http://www.ecalc.ch/xcoptercalc.htm?ecalc&amp;lang=en"" rel=""nofollow"">site</a> and it gave no errors (guess its a good sign).</p></li>
<li><p>The camera on the image is just for the picture (no camera is intended there).</p></li>
</ul>
","arduino motor quadcopter"
"1463","Does more ""Power"" (W) means better motor?","<p>If a motor has more Power (W) from another it means that it is better?</p>
","motor quadcopter power"
"1466","Quadcopter application underwater","<p>Just out of curiosity, can the concept of a Quadcopter be applied to an ROV? Would it work the same way underwater as it would be in Air? If not what kind of modifications it would take to implement that idea, underwater?</p>
","quadcopter"
"1475","Pros/Cons of common robotics ""kits"" at the high school level","<p>I am interested in starting a robotics club at my high school next year, since we don't have one, and I want to know what the pros/cons are of common kits. I already have a Vex kit since my uncle is one of the regional suppliers for Indiana, but I'm not sure that Vex is the best choice. What are the pros/cons of the other kits out there?</p>

<p>Note: I looked at Lego Mindstorms, and have used them when I was learning robotics, but do not want them for this. Also, i own a Raspberry Pi.</p>
","raspberry-pi mindstorms beginner children"
"1483","Optimal-time trajectory in 2D and 3D with simple constraints","<p>I'm trying to find the optimal-time trajectory for an object (point mass) from point <code>p0</code> with initial velocity <code>v0</code> to point <code>p1</code> with velocity <code>v1</code>. I'm considering a simple environment without any obstacles to move around. The only constraint is a maximum acceleration in any direction of <code>a_max</code> and optionally a maximum speed of <code>s_max</code>.</p>

<p>This is easy to work out in 1D but I struggle with the solution 2D and 3D. I could apply the 1D solution separately to each dimension, but this would only limit acceleration and speed in a single dimension. The actual acceleration might be larger in multiple dimensions.</p>

<p>Are there any textbook, closed-form solutions to this problem? If not, how can I solve this in a discrete-event simulation?</p>
","control motion-planning"
"1489","automatic sorting with barcode identification inside a refrigerator","<p>I'm endeavoring to prototype a challenging sorting mechanism inside a fridge and would appreciate any constructive tips on how to get from the specs to a plausible design.</p>

<h3>Problem</h3>

<p>The aim of the game is to identify and sort food items in the limited space of a fridge
-   such that a user would push their unsorted shopping into a chamber at the top of the enclosure
-   and the machine inside would then try to identify the contents with help of bar-codes (first big problem)
-   and then sort and move the items according to their identities into different chambers below (second big problem). </p>

<h3>Solution?</h3>

<p>Are there any existing devices that already serve such functions (automatic bar-coding and sorting), the designs of which could perhaps inform the mechanics of the device I'm planning to construct?</p>

<ul>
<li>I'm thinking maybe manufacturing plants</li>
<li>or packing factories with conveyor belts etc may use systems that already solve such problems? </li>
<li>Or filtering mechanisms in candy dispensers,</li>
<li>mechanized lifting forks? </li>
<li>Textbook engineering mechanisms?</li>
</ul>
","arduino motor sensors cameras"
"1492","Single power source for electronics and actuators","<p>It seems popular to build robotic systems with separate supplies for the electronics and motors/servos, but what would be involved in a circuit that powers both from one source?</p>

<p>If it helps, here is my particular situation. I want to use a single 7.4V Li-Po pack to power an embedded system (2.5-6V operating voltage input, max 1A current draw) and up to 18 standard-sized servos (HS-645, 4.8-6V operating voltage). This would be a hexapod, so it is unlikely that all servos would be stalling at once with normal gait patterns.</p>
","electronics servos power legged"
"1494","How to build a liquid filling machine using piston?","<p>My team developed a filling machine for liquids which uses a piston to measure and deliver volumes 0.1 to 1 liters into bottles. It is built mostly with mechanical parts and we'd like to replace most of them with electronic ones.</p>

<p>How do I build a machine to pull liquid from a reservoir and fills a bottle using a piston, with electronic parts such as stepper motor, linear actuators and sensors?</p>

<p>I understand this is somewhat vague. Any aligned response is appreciated.</p>

<p>Update:</p>

<p>This machine should, at its max speed, fill a 1 litter bottle with water in 2 seconds (to deliver 30 bottles per minute). Higher viscosity liquids may take longer.</p>

<p>It should not spill so liquid needs some filling acceleration control.</p>

<p>You may assume two operation modes: with bubbles and without bubbles. The first is a plus.</p>

<p>I'd like to be able to change the volume electronically (via a LCD menu).</p>

<p>I thought of a single main valve that switches between the reservoir and the bottle. That should be controlled electronically too. I could use two valves too.</p>
","sensors stepper-motor mechanism"
"1496","On the ardupilot board, what is the difference between A10 / A11 ground pins and the PWM ground pins?","<p>I'm looking for an electrical explanation of the statement <a href=""http://copter.ardupilot.com/wiki/camera-mount/"" rel=""nofollow"">here</a>:</p>

<blockquote>
  <p>Warning: Do not connect power (red + &amp; black -) from the RC10 (A10) &amp;
  RC11 (A11) connectors to the servos, just use the signal lines. Power
  the servos via the PWM Outputs connectors. These solutions will avoid
  the scenario that can possibly happen when the Pan/Tilt servo draw too
  much current and cause the APM to brownout (reset)</p>
</blockquote>

<p>I examined the APM2.5 board drawing and schematic <a href=""http://stuff.storediydrones.com/APM_v252_RELEASE.zip"" rel=""nofollow"">here</a> and the grounding is a little unclear. On the schematic, they all just go to GND, but on the board drawing some of the ground pins appear unconnected to traces. I checked for continuity, and there is no continuity between the PWM grounds and the A10/A11 ground pins. By the way, my power setup is that I have J1 enabled and I am using an ESC to power the board.</p>

<p>Can anyone figure out, electrically, what is between these two sets of ground pins?</p>

<p>Ground pins appear unconnected to the traces:</p>

<p><a href=""http://i.stack.imgur.com/hLO2r.jpg"" rel=""nofollow""><img src=""http://i.stack.imgur.com/hLO2rm.jpg"" alt=""Ground pins appear unconnected to the traces""></a></p>

<p>PWM ground just connected to GND:</p>

<p><a href=""http://i.stack.imgur.com/Fyzmo.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/Fyzmom.png"" alt=""PWM ground just connected to GND""></a></p>

<p>analog output ground also connected to GND:</p>

<p><a href=""http://i.stack.imgur.com/arrQR.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/arrQRm.png"" alt=""analog output ground also connected to GND""></a></p>
","power ardupilot"
"1500","lithium thionyl chloride batteries to generate 20A @ 14.4V","<p>I am building a robot where power density is critical, and looking into using lithium thionyl chloride (SOCl2) batteries. I will be drawing around 20A constantly and need between 12 and 17V. The batteries I have found so far are AA-sized and designed to deliver 100mA, 3.6v, 2.4Ah, and weigh 19g each. I could picture a pack with 4 blocks of these batteries in series, where each block has 20 batteries in parallel. That would mean 48Ah, which is way more than the ~10Ah that I need, and 1.52kg, which is more than I would like the robot be carrying.</p>

<p>So, the question is, is there a way to achieve 10Ah at 20A and 14.4V (i.e. for 5 hours) using SOCl2, carrying much less weight than 1.52kg?</p>
","power"
"1503","Python3 Modules for motor movement","<p>Are there any Python3 modules used to program robotic movement by declaring a device or component and then providing the instructions? I am not looking for modules that test the components.</p>
","python"
"1514","Reducing motor speed without jamming up","<p>So I built three little bots so far. </p>

<p>One with a raspberry-pi, (6V motors), one with an arduino (12V motors), and another with an arduino but a hacked remote car (7ish, volt motors):<br>
<a href=""http://i.stack.imgur.com/AJ45l.jpg""><img src=""http://i.stack.imgur.com/AJ45ls.jpg"" alt=""enter image description here""></a>
<a href=""http://i.stack.imgur.com/f6NUt.jpg""><img src=""http://i.stack.imgur.com/f6NUts.jpg"" alt=""enter image description here""></a>
<a href=""http://i.stack.imgur.com/Rr5fH.jpg""><img src=""http://i.stack.imgur.com/Rr5fHs.jpg"" alt=""enter image description here""></a></p>

<p>The problem I have with all these is that the motors spin so fast the car just bumps into something in a few seconds. (I have a small house)</p>

<p>I tried to use PWM to control the speed, but at a little less than full throttle (255) they jam up and they can't take the weight I put on them.</p>

<p>Should I buy one of these chassis that slow the motor down and give it torque with a gearbox, or is there anything else I can do?</p>
","motor pwm"
"1515","Building a non rotating persistence of vision device","<p>I'm looking for a way to create a non-rotating persistence of vision device. I have all the electronics set up but I'm stumped with the mechanical design.
I tried these two designs:<img src=""http://i.stack.imgur.com/Xvb5l.png"" alt=""enter image description here""><br>
But these didn't work so well. Everything shakes violently and it doesn't go nearly as fast as I need (about 20 swipes per second)
Any ideas on how I can build this?</p>
","motor mechanism"
"1517","Light pattern is flashing intermittently using RVIZ/OpenNI with two Kinects","<p>I have two Kinects (each on its own USB card) whose cameras I'm watching in RVIZ through OpenNI, and the structured light pattern of one is flashing intermittently - it's only there for a short flash every two seconds. Obviously, the structured light depth calculations only work if the pattern is always projected when the camera is looking at it.</p>

<p>What causes this, and how do I fix it?</p>

<p>EDIT: I was also having this issue with a single Kinect, for which I discovered the issue, as detailed in my own answer. However, the problem persists when two Kinects are plugged in, with one Kinect flashing and the other functioning normally.</p>
","ros kinect openni"
"1518","How do visual obstructions impact the ability to localize using LIDAR?","<p>If a street is extremely crowded to an extent that the terrain is not visible from the point of view of the LIDAR (e.g. in google's self driving car), can it still manage to localize itself and continue to operate? I recall Sebastian Thrun saying that Google's car cannot navigate through snow filled roads since the onboard LIDAR cannot map the terrain beneath the snow (<a href=""http://www.economist.com/node/21560989"" rel=""nofollow"">e.g. here</a>).</p>

<p>[Edit : Based on the comments] Clarifying the context, here ""not visible"" means there is an obstacle between the LIDAR and the terrain</p>
","ugv lidar"
"1519","PX4 Communication","<p>Does anyone here know PX4 software? I'm using Eclipse to program it and I want to Open a new UART door and write on this, I'm doing the following commands...</p>

<pre><code>static int uart2=open(""/dev/ttyS1"", O_RDWR | O_NONBLOCK | O_NOCTTY);
write(uart2,""Hi"",3);
</code></pre>

<p>But this is not working! Anyone have any idea why?</p>
","microcontroller communication"
"1524","Line Follower optimization","<p>I'm working on building a line follower robot and want to optimize its performance. It was suggested that I use a PID algorithm. I read a lot about PID but am confused a bit regarding following:</p>

<p>I've calculated the error_value using $k_p * proportional + ...$
But regarding the change in the motor speed I'm confused as to what to use during comparison the difference (i.e. currentposition - setpoint) or the errorvalue. That is should I use </p>

<pre><code>if (difference &gt; 0)
{ //code for changing appropriate motor's speed using error_value }
</code></pre>

<p>or </p>

<pre><code>if (error_value &gt; 0)
{ //code for changing appropriate motor's speed using error_value }
</code></pre>

<p>Also is there any specified range for the values of the constants $k_p$, $k_i$ and $k_d$?
I'm using a <a href=""https://en.wikipedia.org/wiki/Differential_wheeled_robot"" rel=""nofollow"">differential wheeled robot</a> for my line follower.</p>

<p>Also I would be happy if someone suggests me any other advanced optimization algorithm for improving the line follower robot.</p>
","arduino pid line-following"
"1525","How to implement Bounded Angle Vision in Particle Filter?","<p>I have built a <a href=""http://sourceforge.net/projects/r-localization/"" rel=""nofollow"">Particles Filter simulator</a> and I wanted to add the following functionalities.</p>

<ol>
<li>Limited Range Vision (Robot can see up to 50 meters)</li>
<li>Limited Angle Vision (Robot can see within a certain angle w.r.t its current orientation.  <em>e.g.</em> If the current orientation is 30 degree then it can see in the range from 0 to 60 degree.)</li>
</ol>

<p>I have managed to add the Limited Range Vision functionality but unable to add Limited Angle Vision.</p>

<p><em>Method to Sense the landmarks distance within the range</em></p>



<pre><code>public double[] sense(boolean addNoise) {
    double[] z = new double[World.getLandmark().getLandmarks().size()];
    for (int i = 0; i &lt; z.length; i++) {
        Point lm = World.getLandmark().getLandmarks().get(i);
        double dx = x - lm.getX();
        double dy = y - lm.getY();
        double dist = Math.sqrt(Math.pow(dx, 2) + Math.pow(dy, 2));
        if (addNoise) {
            dist += Util.nextGaussian(0, sense_noise);
        }
        if (isBoundedVision()) {
            // TODO Limited angle vision
            // if robot can see within 60 degree angle w.r.t its orientation
            if (dist &lt;= laserRange) {
                z[i] = dist;
            }
        } else {
            z[i] = dist;
        }
    }
    return z;
}
</code></pre>

<p><em>Method to calculate the probability of this particle</em></p>

<pre><code>@Override
public double measurement_prob(double[] measurements) {
    double prob = 1.0;
    int c = 0;
    double[] myMeasurements = sense(false);
    for (int j = 0; j &lt; measurements.length; j++) {
        if (measurements[j] != 0) {
            prob *= Util.gaussian(myMeasurements[j], sense_noise, measurements[j]);
            c++;
        }
    }
    if (isBoundedVision()) {
        if (c &gt; 0) {
            // increase the probability if this particle can see more landmarks
            prob = Math.pow(prob, 1.0 / c);
        } else {
            prob = 0.0;
        }
    }
    return prob;
}
</code></pre>

<p>Coordinates are relative to the robot and for distance calculation I am using the Euclidean Distance method and my Robot gets localized correctly. </p>
","localization particle-filter visualization"
"1530","How do you dim 12 volt LEDs?","<p>I was considering using a Raspberry Pi controlling a USB Relay to dim 12V LED lights, but I'm having trouble finding a solution that isn't a simple ON/OFF.  What type of device would I need for dimming?</p>
","raspberry-pi"
"1538","How to convert vertical motion to horizontal","<p>I am interested in using this miniature motor (<a href=""http://www.newscaletech.com/technology/squiggle-motors.php"">Squiggle Micro Motor</a>) to create very tiny <strong>horizontal</strong> movements. However, due to very limited space, I can only place it <strong>vertically</strong> within my project.</p>

<p>Assuming this motor is placed as follows, how can one adapt it to simultaneous movement at a right angle? (Ideally with the X-axis movement matched to the Y-axis movement as well as possible.)
<img src=""http://i.stack.imgur.com/sdyln.png"" alt=""enter image description here""></p>
","motor motion movement driver linear-bearing"
"1544","Arduino C/C++ progamming tutorials","<p>I have no problem in reading circuit schemes, connecting wires, resistors etc. (basing on for example instructables.com), but I've only tried to learn Java (a while ago) and it's difficult to me to find out what's going on with all this C-code-stuff.
Are there any tutorials that are focused on the programming part?
thanks</p>
","arduino"
"1547","How can I simulate a changing environment with non-rigid objects?","<p>Many robot applications (actually - the most practical and appealing ones) include the robot's reaction to (and impact on) the evironment, e.g. due to stochastic nature of the environment (e.g. when robot should handle non-rigid objects like clothes) or due to the variability of environment (e.g. harvesting robots should be prepared to pick fruits of different sizes and shapes).</p>

<p>The question is - is there a robotics simulator that can simulate not only robot but also the environment as well? E.g. that can simulate the response of robots action on cloth folding or fruit picking and so on. I guess that such simulator is really non-trivial but maybe there is some ongoing project for it?</p>
","simulator industrial-robot"
"1552","Use of automated tricopters instead of quadcopters?","<p>Hy, I just found useful to post my idea here. </p>

<p>I've seen videos about automated quadcopters: <a href=""http://www.ted.com/talks/raffaello_d_andrea_the_astounding_athletic_power_of_quadcopters.html"" rel=""nofollow"">http://www.ted.com/talks/raffaello_d_andrea_the_astounding_athletic_power_of_quadcopters.html</a> and <a href=""http://www.ted.com/talks/vijay_kumar_robots_that_fly_and_cooperate.html"" rel=""nofollow"">http://www.ted.com/talks/vijay_kumar_robots_that_fly_and_cooperate.html</a>. </p>

<p>I surfed pages from the companies presenting this research and other information on the internet, but I haven't found why they use quadcopters specifically. 
I understand, how accelerating, rotating and rolling works in those quadcopters - it's simple, but they claim that quadcopters have minimum number of working parts to fulfill their needs, which I don't agree and I think that tricopters are better in this (duocopters can't rotate horizontally, but tricopters can by inclining and then powering the remaining left or right propeller). </p>

<p>I rode forums, calculated and draw drafts of both tri and quad and found, that tri is much more efficient just in everything than quad with same props and battery when taken in account that best properties has the smallest copter with the largest props so: 
3:4 moving parts (no vectored yaw in tri), 9.5:16 size, building Y instead of X construction take far less material 1.5:2.82, lesser maximum power input 3:4, better power efficiency makes longer flight time and tricopters have also improved agility over quadcopters. </p>

<p>The only disadvantage I see is a bit complicated horizontal rotating in tricopter without vectored yaw, which can be problem in man controlled machines but easily solved by simple algorithms in automated machines -> it's not a real disadvantage, just a small work to be done. I was thinking about doing that in my bachelor thesis, but for now I am looking for your opinions, thanks!</p>

<p>EDIT: Maybe the torque is the problem, because on tricopters you can can have all 3 props in 1 direction or 2 in 1 direction and 1 in the opposite and it's symmetrical in neither way, but I'm not sure if this is the main problem...</p>
","quadcopter multi-rotor"
"1554","Track a moving object","<p>If this has already been answered, by all means please point me to it.</p>

<p>I am in the process of building a quadcopter which I eventually plan to run autonomously by allowing it to track an object and take a video feed of it moving.</p>

<p>GPS is one of the options I've considered, basically:</p>

<ul>
<li>GPS antena on moving object (person, car, bike, surfer)</li>
<li>GPS antena on quadcopter</li>
<li>Radio to transmit coordinates from moving object to quad copter</li>
</ul>

<p>Some of the challenges I can foresee are</p>

<ul>
<li>Line of sight for camera. How does the camera know exactly where to point?</li>
<li>Angle, how can I pre-program the quad to always record, say... 10m to the right of the moving object, or even better, program a set of angles to record from whilst keeping up with the object</li>
<li>GPS accuracy, what happens if the GPS lock is weak?</li>
</ul>

<p>What are some of my other options? I saw this <a href=""http://www.youtube.com/watch?v=w2itwFJCgFQ"">TED Talk</a> where the quads are following a ball shaped sensor? I believe it uses Kinect cameras and lots of them which is not really an option for this challenge.</p>

<p>So I'm open to hearing some ideas before I start research and development of these features.</p>
","arduino quadcopter gps"
"1562","If you can create pure yaw motion with a quadcoptor then why won't this work with a tricoptor?","<p><a href=""http://robotics.stackexchange.com/a/549/37"">An answer</a> to the question <a href=""http://robotics.stackexchange.com/q/543/37"">Why do quadcopters have four propellers? (Besides the name)</a>  said:</p>

<blockquote>
  <p>You need 4 degrees of freedom to control yaw, pitch, roll and thrust.</p>
  
  <p>Four props is therefore the minimum number of actuators required. Tricoptors require a servo to tilt one or more rotors which is more mechanically complicated.</p>
</blockquote>

<p>In a comment, I asked:</p>

<blockquote>
  <p>How do you get pure yaw motion with a quadcoptor and if that's possible why won't this work with a tricoptor? I don't understand how can you get yaw motion with any system where all rotors are in a plane without first tilting and moving. I would have thought that the main difference between quadcopters and tricoptors would be the kinematic calculations would be more complex.</p>
</blockquote>

<p><a href=""http://robotics.stackexchange.com/a/1453/37"">Another answer</a> explained:</p>

<blockquote>
  <p>you get pure yaw in the following way:</p>
  
  <p>North and South motors rotating the same speed but collectively at a higher (or lower) speed than East and West Motors which are also at the same speed. </p>
</blockquote>

<p>This explains why it works with a quadcopter, but doesn't explain why it won't work with a tricopter.</p>

<p>Is it simply the fact that the asymmetry means that you can't imbalance the <a href=""http://en.wikipedia.org/wiki/Torque_effect"" rel=""nofollow"">torque effects</a> to provide yaw movement while still keeping the thrusts balanced to keep pitch and roll constant?</p>
","design quadcopter uav"
"1564","Roslaunch include file remotely","<p>I am trying to launch a file from remote computer but I could not success. Actually I can connect to remote computer but I think the problem is with including a file from remote computer. In other words, I am looking for a machine tag for include. Here is the my code:</p>

<pre><code>&lt;launch&gt;  

    &lt;group &gt;
      &lt;machine name=""marvin-1"" address=""tek-marvin-1"" user=""blabla"" password=""blabla"" env-loader=""/home/blabla/.rosLaunchScript.sh""/&gt;  

      &lt;include file=""$(find openni_launch_marvin)/launch/kinect_left.launch""/&gt;
    &lt;/group&gt;     

&lt;/launch&gt;
</code></pre>
","ros"
"1567","H-bridges and stall current","<p>For a DC motor with a stall current of 950 mA, what should the H-bridge's current rating be?  What will happen if we use our H-bridge L293D whose max. output current is 600 mA?</p>
","motor electronics h-bridge"
"1570","Accelerometer bias removal","<p>I found <a href=""http://www.kionix.com/sites/default/files/AN012%20Accelerometer%20Errors.pdf"" rel=""nofollow"">a good explanation</a> on how to remove accelerometer bias (when on flat table only one axis should show values, the other two should be 0). I've calculated S and B factors (page 3):</p>

<blockquote>
  <p>Record $B_x^{0g}$, $B_y^{0g}$, $B_z^{0g}$, $S_{xx}$, $S_{yy}$, and $S_{zz}$ in EEPROM or flash memory
  and use these values in all subsequent calculations of acceleration to
  get the corrected outputs.</p>
</blockquote>

<p>I don't know how to incorporate these into the final calculation of accelerations. I guess the bias should be substracted from my sensor reading. What about sensitivities (S)?</p>
","accelerometer calibration errors"
"1571","How to raise/drop a spider","<p>For Halloween, I'd like to build a spider that drops from the ceiling when it detects motion, and then rewinds itself back up to scare the next kids. I already have a lightweight foamish spider from Hobby Lobby that I'd like to use, but I need help adding the smarts to it to scare the kids.  </p>

<p>Ideally it'd be able to detect how tall/far away the kid is to drop to a custom height each time, but I'd settle for a standard dropping height if that's too much. I even had an idea of having a motion sensor and having it shoot Silly String webs in front of people. </p>

<p>I have a very technical background, but I'm a total n00b when it comes to robotics, so ideas as to what components and considerations I'd need would be greatly appreciated!</p>
","motor rcservo"
"1579","Accelerometer calibration - how to get cross-axis sensitivities","<p>I've already <a href=""http://robotics.stackexchange.com/questions/1570/accelerometer-bias-removal"">asked a related question (accelerometer bias removal)</a> here on robotics and got a bit better results on corrected accelerometer output. To get even better results I found the <a href=""http://www.vectornav.com/support/library?id=86"" rel=""nofollow"">calibration equations (7th &amp; 8th paragraph)</a> from Vectornav which are just a bit enhanced than the solution in the linked question:</p>

<p><img src=""http://i.stack.imgur.com/ZlbWs.png"" alt=""enter image description here""> </p>

<p>However, six more variables are needed:</p>

<blockquote>
  <p>Sensitivity of sensor X-axis to Y-axis inputs ($M_{xy}$)</p>
  
  <p>Sensitivity of sensor X-axis to Z-axis inputs ($M_{xz}$)</p>
  
  <p>Sensitivity of sensor Y-axis to X-axis inputs ($M_{yx}$)</p>
  
  <p>Sensitivity of sensor Y-axis to Z-axis inputs ($M_{yz}$)</p>
  
  <p>Sensitivity of sensor Z-axis to X-axis inputs ($M_{zx}$)</p>
  
  <p>Sensitivity of sensor Z-axis to Y-axis inputs ($M_{zy}$)</p>
</blockquote>

<p>Below it is also stated:</p>

<blockquote>
  <p>IEEE-STD-1293-1998 [...] provides a detailed test procedure for
  determining each of these calibration parameters</p>
</blockquote>

<p>However, after searching through the <a href=""https://docs.google.com/file/d/0B8zqB-WeBKRBTUN4OU0weEV0eVE/edit"" rel=""nofollow"">1293-1998 standard</a> (especially page 201 in Google Docs) I didn't find any clue on how to calculate the $M$ values. Also, $B_{d}$ and $V_x$ values from Vectornav equations is not explained anywhere. Can someone point me further?</p>
","sensors accelerometer calibration errors"
"1581","Choosing motor and battery for a robot","<p>I have a project which requires a robot to move around a room with a flat surface (concrete floor).</p>

<p>The robot must carry a laptop. I estimated that the total weight would be 6-7kg (including motors, battery, laptop, motor controller board and other mics items). I would like it to move at about the same speed as a Roomba moves. The robot will have two motors and a castor.</p>

<p>I have tried doing the calculation to determine the type of motor to use, but I'm very confused.</p>

<p>Can someone advise me on the type of motor and type of battery (Lipo/SLA) to use?</p>
","motor wheeled-robot battery"
"1582","Interference between 900 MHz video transmitter and 2.4 GHz control radio","<p>I'm starting to attempt to fly FPV on my quadrotor. I am using a FrSky D8R-II 2.4 GHz frequency hopping diversity receiver (two antennas) for control and recently added a <a href=""http://www.readymaderc.com/store/index.php?main_page=product_info&amp;cPath=11_30_31&amp;products_id=348"" rel=""nofollow"">no-name 910 MHz 1.5 watt analog video transmitter</a> for FPV flying:</p>

<p><img src=""http://i.stack.imgur.com/UFrHc.jpg"" alt=""enter image description here""></p>

<p>When the video transmitter is powered up, my control range drops from about 1.5km to under 50m. I'm surprised that the 910 MHz video channel affects my 2.4 GHz control channel this way.</p>

<p>Is this sort of interference expected? Is it because my transmitter is low quality? What changes are recommended&nbsp;— should I switch to a UHF control radio? Or a different frequency (eg 5.8 GHz?) for the video radio? Or just try moving it a few more inches (they are already about 5in apart)?</p>
","multi-rotor"
"1584","How to deal with sonar crosstalk","<p>Our robot has a circular array of 12 sonar sensors that looks like this:</p>

<p><img src=""http://i.stack.imgur.com/Q5dBX.jpg"" alt=""enter image description here""></p>

<p>The <a href=""http://www.hobbyking.com/hobbyking/store/__31136__ultrasonic_module_hc_sr04_arduino.html"">sonar sensors themselves</a> are pretty good. We use a low-pass filter to deal with noise, and the readings seem pretty accurate. However, when the robot comes across a flat surface like a wall, something weird happens. The sonars don't show readings that would indicate a wall, instead, it appears like a curved surface. </p>

<p>The plot below was made when the robot was facing a wall. See the curve in the blue lines, as compared to the straight red line. The red line was produced by using a camera to detect the wall, where the blue lines show filtered sonar readings. </p>

<p><img src=""http://i.stack.imgur.com/B4KWm.png"" alt=""enter image description here""></p>

<p>We believe this error is due to crosstalk, where one sonar sensor's pulse bounces off the wall at an angle and is received by another sensor. This is a systematic error, so we can't really deal with it like we would with noise. Are there any solutions out there to correct for it?</p>
","sonar sensor-error"
"1589","existence probability of an object in fusion","<p>I want to compute an existence probability of an object in a sensor fusion on the high level (having from each sensor list of objects already filtered with e.g. Kalman Filter).</p>

<p>There are these formulae:</p>

<p>$$LR(G)_{old} = \frac{p(Ex_{out})_{old}}{1 - p(Ex_{out})_{old}}$$
$$\alpha = \frac{p(Ex_{in})_{old}}{p(Ex_{in})_{old}*(1 - p(Ex_{in})_{new})}$$
$$LR(G)_{new} = LR(G)_{old} * \alpha$$
$$p(Ex_{out})_{new} = \frac{LR(G)_{new}}{1 + LR(G)_{new}}$$</p>

<p>Where $p(Ex)$ is the probability of existence and $LR$ is the Likelihood Ratio.  </p>

<p>The idea is that $p(Ex)_{in}$ is some probability existence of local object, which was fused into the global $p(Ex_{out})$, and its probability influences that global one.  $old$ would mean values from previous cycle. </p>

<p>How do you condition that computation to avoid situations of dividing by zero, obtaining <code>NaN</code>, or <code>Inf</code>? Also, if $p(Ex_{in})_{new}$ is almost 1, then $\alpha$ will be huge, increasing output, and increasing it enormously in each later cycle, so that the object will live forever. How to prevent it?</p>
","sensors kalman-filter sensor-fusion"
"1590","What is the name for the transfer function, GH","<p>What is the name for the transfer function, GH, in a simple feedback control system like</p>

<p>$$y=\frac{G}{1+GH}u$$</p>

<p>What do you call G? What about (G/(1+GH))?</p>

<p>I am confused by the fact that that  ""open-loop transfer function"" and ""Loop transfer function"" are used to mean GH by different people. Which is academically correct or widely accepted?</p>

<p>There are several terms: ""closed-loop transfer function""
""open-loop transfer function""
""overall transfer function""
""Loop transfer function""
""loop gain""
""loop ratio""</p>

<p>Thanks</p>
","control"
"1595","measuring gears for servo motors","<p>I'm quite new to mechanical engineering and not familiar with gears and motors. A few days ago, I bought a second-hand GWS servo motor for my project, and it didn't include gears.</p>

<p>Can someone help me understand the correct measurement of my motor so I can buy the correct <strong>gear</strong> to fit on my servo motor. Specifically, the measurement of the <strong>gear hole</strong>:</p>

<p><a href=""http://oi43.tinypic.com/2ekktw7.jpg"" rel=""nofollow""><img src=""http://i.stack.imgur.com/fK4KE.jpg"" alt=""GWS S35 Continuous rotation servo""></a></p>
","rcservo"
"1598","Making a robotic arm that can draw a circle","<p>I am software engineering student and don't know much about hardware.</p>

<p>I recently have started a project in my AI course. The project is <strong>playing a 3x3 <a href=""http://en.wikipedia.org/wiki/Tic-tac-toe"" rel=""nofollow"">tic-tac-toe</a> game between computer and a human</strong>. </p>

<p>In a tic-tac-toe board, suppose you play first and you put a cross mark in a certain place.(in <a href=""https://dl.dropboxusercontent.com/u/80047546/tic.png"" rel=""nofollow"">tic-tac-toe board</a> your position is (3,1)).
 Now my computer will take a picture with webcam and analyse this picture with help of Opencv(It is a open source c++ library for image processing. for details opencv.org). After finding the position of your cross mark it will find the optimal position with the help of my algorithm at which it will put a circle. For output it will just speak out the position.</p>

<p>Now, I want to make a robotic hand that can draw this circle as output. Would anybody please help me to find the hardware and other materials that needed to make that robotic hand? It will be very helpful if anybody suggest me some tutorials.</p>

<p>I Googled and found many many suggestions, but I am confused about what to choose. </p>
","robotic-arm"
"1602","How to design a differential steering mechanism?","<p>I want to give my robot a differential mechanism for the system of turning and steering. Considering the case of turning a right-angled corner, the robot will achieve this by following a gradual circular arc through the intersection while maintaining a steady speed. To accomplish this end, we increase the speed of the outer wheel while slowing that of the inner. But supposing i want the turn to be within a definite radius, how do i calculate what ratio the 2 speeds have to be in? Can someone give me an insight into this? </p>

<p>What Ive done is this, although I have my doubts.</p>

<p>If the speed of the right wheel is $V_r$ and the speed of the left wheel is $V_l$, then the ratio of their speeds while turning will be equal to the ratio of the circumferences of their corresponding quadrants.</p>

<p><strong>Therefore</strong>
$$V_r :V_l =\frac{r+A}{r}$$</p>

<p>Is this right? I have a sinister feeling Im missing something out..</p>

<p><img src=""http://i.stack.imgur.com/FjWpW.jpg"" alt=""enter image description here""></p>
","kinematics"
"1603","How to implement the Wavefront algorithm","<p>I am thinking of creating a robot that can navigate using a map. It is controlled from a PC. An 8-bit controller performs low level tasks and the PC is doing the image processing. I plan to implement it in a single room where the robot is placed and the robot and environment are tracked by a camera from a height or from the ceiling of the room. First, the robot needs to be mapped, like this <a href=""http://www.societyofrobots.com/programming_wavefront.shtml"" rel=""nofollow"">http://www.societyofrobots.com/programming_wavefront.shtml</a></p>

<p>To do:</p>

<ul>
<li>Track the robot from some height  using camera Following the
wavefont algorithim to locate robot and obstacles.</li>
</ul>

<p><strong>Procedure:</strong>(just my idea)</p>

<p>The camera will give image of the robot surrounded by obstacles in the random places.
 using some opencv technique draw some grind over the image. </p>

<ul>
<li><p>Locating the  grid which contain  robot(by having some colored
symbol over the robot) and locating the grids containing the 
obstacle.</p></li>
<li><p>Now the grids with obstacle is thought as wall and the remaining is
the free space for the robot to navigate.</p></li>
<li><p>robot is going to get  the goal place which should be reached is
given from the pc(may be like point the place to reach in the image
by mouse click).</p></li>
</ul>

<p><strong>Unknowns</strong> :</p>

<ul>
<li>Mapping the room and locating the robot</li>
</ul>

<p>How  to do that? The robot should know where it is in the map or the image. We cannot believe only the camera is enough to locate the robot. So I thought of adding <strong><em>triangulation</em></strong> mapping like placing two IRs in the room and a receiver in the robot.</p>

<p>The doubt I have in this is how an IR receiver can know from which direction it is receiving the IR signal (from left or right ). I think it knows only that it receives IR not the direction. Then how is the triangulation going to happen if I don't know the angle and direction?</p>

<ul>
<li>coming to the image processing, how can I implement the Wavefront
   algorithm(that is capture the live vedio and draw grids over it to
   find robot and the obstacles)?</li>
</ul>

<p>I have HC-05 Bluetooth module, Arduino, Bluetooth dongle, chassis with dc motors and driver, and a dc supply.</p>
","algorithm mapping"
"1607","Do servos stop at their limits automatically?","<p>I'm moving from controlling a robot arm with basic time based motors controlled using a raspberry pi and python to a more advanced robotic arm that has servos (e.g <a href=""http://dlnmh9ip6v2uc.cloudfront.net/datasheets/Robotics/Hitec-HS-425BB-Servo-Specsheet.pdf"" rel=""nofollow"">HS-425BB</a>). With the time based motors I must constantly keep track of the arms position (guess work) and make sure it doesn't over turn.</p>

<p>Do servos automatically stop if you give them a position that is outside of their boundaries rather than grinding the gears?</p>
","rcservo"
"1610","How to make a motion sensor circuit, that can communicate with my LAN?","<p>I'm a newbie to Electronics/ Robotics, but I love to do it as a hobby. </p>

<p>So I want to build a circuit (a really small in size would be much better) with a motion sensor that can communicate data (Basically when it sense a motion send a signal) to my computer over Wifi. Is this something possible? 
If so how do I do it, may be a schematics diagram, or someway to start the project would be a grate help.</p>

<p>Thank you!!  </p>
","sensors wifi communication circuit"
"1613","Motor driver selection","<p>I am building a 2 wheel robot to carry 7KG of load. I used this <a href=""http://www.robotshop.com/dc-motor-selection.html?lang=en-us"" rel=""nofollow"">link</a> to get the torque required for each motor which is <strong>11Kg-cm</strong>. So I choose 2 of these <a href=""http://www.robot-r-us.com/motor-brushed/planetary-gearhead-motor-12v-13.71-with-encoder.html"" rel=""nofollow"">motor</a> which has <strong>36Kg-cm</strong>(2x more) torque. I noticed that the stall current is 14A. </p>

<p>Can I use Adafruit Motor Shield for Arduino which is rated 3A peak current capability?</p>

<p>If not what current rating driver should look into?</p>

<p>Thanks!!</p>
","motor driver current"
"1614","ArduIMU noisy output in Quadrotor","<p>We are using <a href=""http://code.google.com/p/ardu-imu/"" rel=""nofollow"">ArduIMU (V3)</a> as our Quadrotor's inertial measurement unit. (we have a separate board to control all motors, not with ArduIMU itself). </p>

<p>Now we have a problem with ArduIMU's sensors output. When we put our quadrotor steady on the ground with motors on, instead of getting 0 degree in roll and pitch we have a noisy output something like the image below( -6 to 6 degree error ):</p>

<p><img src=""http://i.stack.imgur.com/cd62D.png"" alt=""enter image description here""></p>

<p>delta_t = 0.2s</p>

<p>We are sure that this isn't a mechanical problem, because we checked the mechanical joints and everything.</p>

<p>I should mention that with motors off everything is going well. Also we checked that if we vibrate the device slowly on yaw axis or any other axis, it still shows the noisy output.</p>

<p>We are using DCM filter inside ArduIMU, also we tested with Kalman filter but no difference.</p>

<p>We also tested FRI low-pass filter, results is good but there is about 3 seconds delay in the output.</p>

<p>We also checked that if we separate the ArduImu's power from our circuit, it still no difference.</p>

<p>What's the problem with ArduIMU and how we can get rid off this noisy output ?</p>

<p><strong>Update:</strong> 
We think that the problem with our PID controller is because of these noises ... Is this a true assumption ? We can't tune our PID parameters ( using Ziegler–Nichols method ) when we have noisy data. We tested Ziegler–Nichols method when we have low rate noises and we successfully tuned our PID but when noise appears we are unable to tune PIDs. Is there anyway for us for tuning our PID in such situation ? Is this problem is because of the noises or the PID itself can get rid of them ?</p>
","arduino quadcopter noise ardupilot"
"1615","Why can Humans single out audio in a crowd? What would it take for a robot to do the same?","<p>I was at a Robotics conference earlier today and one of the speakers mentioned robots not being able to function as well in a crowd because they can't single out audio like a person can.  </p>

<p>Why can people single out audio so well?  And what would it take for a robot to do the same?</p>

<p>I'm aware of Active Noise Reduction (ANR) like on Bose Aviation headset, but that is not what I'm talking about.  I am thinking about the ability to take everything in but process only what you feel is important. </p>
","artificial-intelligence"
"1618","Beaglebone Black power draw","<p>What is the minimum amount of power that a beaglebone needs to start up? This would be with no   peripherals attached besides host usb. The getting started guide claims that it can run off of a computer's usb power, but makes no mention of how many amps are actually needed. I saw a mention of older kernels limiting current draw to .5 amps when working off of usb, although that was all I could find. </p>

<p>Could one start a BeagleBone Black off of .3 amps? If not, how many?</p>
","microcontroller"
"1622","How do I motorize the elbow socket and other joints in a powered exo-skeleton?","<p>How would you motorize the joints in an Iron Man suit? You need something fairly shallow, I would think, probably dual servos sitting on either side of an elbow or knee joint or either side of your hips, but how do you get motorized action there without dramatically adding to the thickness of the joint? </p>

<p>Bicycle-style chain drives wouldn't work, I would think, since the length of the chain would need to vary depending on what position you're in for at least a lot of joints.</p>

<p>How would you motorize the joints?</p>
","design mechanism"
"1627","Why do quadcopters use brushless motors","<p>I've been thinking about starting a quadcopter project, maybe building it from scratch. One of the main barriers-to-entry for me is the motors: it seems like most quadcopters use brushless motors. I have some experience with DC motors and using PWM signals to regulate speed, but no experience with brushless motors. As I understand it, brushless motors are more expensive than the typical DC motor I would use on a land robot, and they also require electronic speed controllers (ESCs), which seem to make them (from my perspective) even more expensive and more complicated to use.</p>

<p>So, my question: what is it about brushless motors that make them useful in a quadcopter? Is it more torque, less weight, something to do with efficiency? And would it be significantly harder (or even possible) to achieve lift using DC motors instead? </p>
","quadcopter brushless-motor"
"1630","Controlling a conveyor belt with a time based motor","<p>I have some crude time based motors taken from a robot arm that we upgraded to proper servos. I want to be able to power a conveyor belt with one of them and I was wondering how I would go about the following setup:</p>

<p>A ball drops through a hole onto the conveyer belt hitting a lever switch on its way through. This switch triggers the motor to start. When the ball gets to the top of the belt and falls off it hits another lever switch that turns the motor off.</p>

<p>I could handle this logic by hooking it up to my raspberry pi and using python to start and stop the motor depending on which GPIO pin received input (top or bottom lever). Or I could use a single lever and set a constant time interval to stop the motor. I would prefer to use both to handle any change in scale/construction.</p>

<p>I was wondering however if this could be done with the breadboard alone, using logic gates or similar?</p>
","motor raspberry-pi"
"1633","How can I get data from my kinect?","<p>Whenever I try using <code>openni_launch</code>, it works normally, however, when I try viewing an image using the kinect's rgb or depth camera, or even recording a simple <code>bagfile</code> with data from the kinect, I am unable to see any picture and <code>rosbag</code> does not record any data, and after a few seconds of running <code>image_view</code> or <code>rosbag record</code>, I got this error:</p>

<pre><code>terminate called after throwing an instance of 'openni_wrapper::OpenNIException'
  what():  virtual void openni_wrapper::OpenNIDevice::startImageStream() @ /tmp/buildd/ros-groovy-openni-camera-1.8.8-0precise-20130418-2203/src/openni_device.cpp @ 224 : starting image stream failed. Reason: Xiron OS got an event timeout!
[camera_nodelet_manager-2] process has died [pid 3788, exit code -6, cmd /opt/ros/groovy/lib/nodelet/nodelet manager __name:=camera_nodelet_manager __log:=/home/rosbotics/.ros/log/16b63744-e043-11e2-ac16-080027486aa8/camera_nodelet_manager-2.log].
log file: /home/rosbotics/.ros/log/16b63744-e043-11e2-ac16-080027486aa8/camera_nodelet_manager-2*.log
</code></pre>

<p>After searching around and trying various fixes, I figured it might be a problem with openni and started using freenect, however I encountered the same problems, I could not record any data using bagfiles or see any images from the kinect (using <code>rviz</code> or <code>image_view</code>)</p>

<p>Then someone asked me to use something completely unrelated, <code>freenect-glview</code>, however that too gave me a black screen.</p>

<p><code>lsusb</code> shows that all 3 parts of the kinect are connected and I've been able to control the kinect's motor through ubuntu so I know that there is at least a connection established between both.</p>

<p>Additional Info:</p>

<ul>
<li><p>I run ROS on Ubuntu using VirtualBox V.4.2.14 and Windows 7 with USB 2 ports</p></li>
<li><p>I am using ubuntu 12.04 and ROS-Groovy (all up to date)</p></li>
<li><p>I've had the exact same errors on my Mac OSX Lion</p></li>
<li><p>When I try using Rviz with the kinect, VirtualBox crashes all together</p></li>
</ul>

<p>I would appreciate anyone's help on the matter.</p>
","ros kinect"
"1634","How to control velocity ratio when turning angle is θ?","<p>Im designing a differential steering mechanism for my robot. Supposing my robot is going in a straight line and I want it to change it direction by a certain angle( $θ$ in the diagram). What should the velocity ratio be of the 2 wheels so that it gradually turns and starts moving along a line that is $θ$ degrees to the initial line of movement?
If there's any ambiguity in the question please take a look at my earlier question which is similar. <a href=""http://robotics.stackexchange.com/questions/1602/how-to-design-a-differential-steering-mechanism"">How to design a differential steering mechanism?</a>
<img src=""http://i.stack.imgur.com/a0vKQ.jpg"" alt=""enter image description here""></p>
","kinematics"
"1637","Paper work before I do make my own Line follower Robot","<p>I am a graduate student trying to make my own Line follower Robot for my minor assessment, I've all <strong>hardware</strong> parts and all <strong>data-sheets</strong> with me, I've attended a workshops of Robotics and studied a lot on <strong>Line follower robot</strong>. I have a good knowledge of C Programming and Embedded systems, but the problem is I've a very <strong>limited amount</strong> of time(2 days).</p>

<p>Please help me to suggest a good paper work about my Project - Line follower robot, where should I start from ? I am getting myself confused should I start from <strong>Programming</strong> or should I first do circuit <strong>simulations</strong> as I know It is not a better approach to use directly hardware. </p>

<p>Please suggest me a fine <strong>Paper work</strong> or some links/videos so that I can make my Robotics projects fast. Any help would be really appreciated, Thanks.</p>
","mobile-robot"
"1638","WHY are there no operating torque specifications on steppers?","<p>I have been looking online for a while, and I cannot seem to find any steppers without any torque ratings. (<strong>Operating torque, not holding torque.</strong>) I even looked on hobby sites that usually have all the ratings, including Adafruit and Sparkfun. I only have found one ever that said the operating torque, however, it didn't seem that reputable and it didn't have holding torque, so it might be likely that it's a mistake. I might contact them and ask.</p>

<p><strong>Am I missing something? Can I calculate what it will run at with certain factors? (How long in between each step, etc.)</strong></p>

<p>The reason that I say that is I found a tutorial saying how much <em>torque</em> (didn't specify which kind, but I kinda assume it isn't holding) you need for a CNC machine (what I'm building).</p>

<hr>

<p><strong>Equation (From <a href=""http://buildyourcnc.com/torquemotion.aspx"">this</a> site):</strong></p>

<pre><code>Torque = ((weight)(inches/revolution))/2(pi)
</code></pre>

<p>Also on the page:</p>

<blockquote>
  <p>By the way, we are talking about torque during a continual turning motion, not at a holding position.</p>
</blockquote>

<p>That seems like operating torque, but what makes it the most confusing is they sell steppers and they only list the holding.</p>

<hr>

<p><strong>What am I missing?</strong></p>
","stepper-motor cnc"
"1642","How to speed up robotic arm?","<p>I need  my robotic arm to ring a desk bell. <a href=""http://www.maplin.co.uk/robotic-arm-kit-with-usb-pc-interface-266257"" rel=""nofollow"">I one on the maplins site  usb robotic arm.</a></p>

<p>It does seem very slow.  What can I hack on it to boost the downwards and upwards speed. I need it hit the bell tip/platform quickly once or twice.</p>

<p><img src=""http://i.stack.imgur.com/9tJZC.jpg"" alt=""enter image description here""></p>

<p><em>This is purely a LOL project for work. Ever time we get an order we want the arm to ring the bell. :)</em></p>

<p><img src=""http://i.stack.imgur.com/X3imO.png"" alt=""enter image description here""></p>

<p>-EDIT</p>

<p>This is the gearbox assembly - And it much much to slow - What can i change in here to speed up one gearbox by at least 4 times?</p>

<p>The grabber gearbox is different though. The gear marker P7 is white and seems to move the grabbers at a faster speed.</p>

<p><img src=""http://i.stack.imgur.com/4pUS5.jpg"" alt=""enter image description here""></p>
","robotic-arm"
"1644","What is rotor torque?","<p>On my stepper's <a href=""http://download.lulzbot.com/AO-100/hardware/electronics/spec_sheets/SY42STH47-1504A_stepperMotors.pdf"" rel=""nofollow"">datasheet</a>, it has the category ""rotor torque"" (labeled in N-CM). <strong>What does that mean? Is this the torque it has can supply when turning? (Hopefully)</strong></p>
","torque stepper-motor"
"1649","How can I manipulate real-time sonar data from my Arducopter in Arduino?","<p>I have a APM 3DR Quad with a 3DR radio telemetry kit.  I would like to send real-time sonar data to my laptop (running Windows 7) in order to manipulate it in an additional Arduino Sketch.  </p>

<p>The sonar sensor is connected to an Analog In channel on my Arduino. That data is processed for altitude calculations, and I would like to send this altitude data to some sort of ground station on my computer through the use of a telemetry kit (2 3DR Radios: 1 on the quadcopter and 1 on my computer).</p>

<p>I am not quite sure how to go about this task.  Is there a way that I can modify the source code (GCS.h or GCS_Mavlink.pde) in conjunction with Mission Planner Mav 1.0 ground station to do this?  Or would I need to write a python module to accomplish this?  </p>
","quadcopter python sonar"
"1653","Calculate position of differential drive robot","<p><strong>How do you calculate or update the position of a differential drive robot with incremental sensors?</strong></p>

<p>There is one incremental sensor attatched to each of the two differential wheels. Both sensors determine the distance $\Delta left$ resp. $\Delta right$ their wheel has rolled during a known time $\Delta t$.</p>

<p>First, let's assume the center between both wheels marks the position of the robot. In this case, one could calculate the position as:</p>

<p>$$
x = \frac{x_{left}+x_{right}}{2} \\
y = \frac{y_{left}+y_{right}}{2}
$$</p>

<p>""Deriving"" those equations under the assumption that both wheels rolled in a straight line (which should be approximately correct for small distances) I get:</p>

<p>$$
\frac{\Delta x}{\Delta t} = \frac{1}{2}\left( \frac{\Delta left}{\Delta t} + \frac{\Delta right}{\Delta t}\right)cos(\theta) \\
\frac{\Delta y}{\Delta t} = \frac{1}{2}\left( \frac{\Delta left}{\Delta t} + \frac{\Delta right}{\Delta t}\right)sin(\theta)
$$</p>

<p>Where $\theta$ is the angle of orientation of the robot. For the change of this angle I found the equation</p>

<p>$$
\frac{\Delta \theta}{\Delta t} = \frac{1}{w} \left( \frac{\Delta left}{\Delta t} - \frac{\Delta right}{\Delta t}\right)
$$</p>

<p>Where $w$ is the distance between both wheels.</p>

<p>Because $\Delta x$ and $\Delta y$ depend on $\theta$, I wonder whether I should first calculate the new $\theta$ by adding $\Delta \theta$ or if I should rather use the ""old"" $\theta$ ? <strong>Is there any reason to use one over the other?</strong></p>

<p>Then, let's now assume the center between both wheels does <em>not</em> mark the position of the robot. Instead I want to use a point which marks the geometric center of the robot's bounding box. Then $x$ and $y$ change to:</p>

<p>$$
x = \frac{x_{left}+x_{right}}{2} + l\, cos(\theta)\\
y = \frac{y_{left}+y_{right}}{2} + l\, sin(\theta)
$$</p>

<p>""Deriving"" the first gives:</p>

<p>$$
\frac{\Delta x}{\Delta t} = \frac{1}{2}\left( \frac{\Delta left}{\Delta t} + \frac{\Delta right}{\Delta t}\right)cos(\theta) - l\,sin(\theta)\,\frac{\Delta \theta}{\Delta t}
$$</p>

<p>Now there is a dependance on $\Delta \theta$. <strong>Is this a reason to use the ""new"" $\theta$ ?</strong></p>

<p><strong>Is there any better method to do simulatenous update of position and orientation?</strong> May be using complex numbers (same approach as with quaternions in 3D?) or homogeneous coordinates?</p>
","mobile-robot kinematics motion two-wheeled forward-kinematics"
"1654","What is the difference between a Robot and a Machine?","<p>What is the difference between a Robot and a Machine? At what point does a machine begin to be called a robot?</p>

<p>Is it at a certain level of complexity? Is it when it has software etc?.</p>

<p>For instance: A desktop printer has mechanics, electronics and firmware but it is not considered a robot (or is it). A Roomba has the same stuff but we call it a robot. So what is the difference.</p>

<p>I have always believed that a robot is a robot when it takes input from it's environment and uses it to make decisions on how to affect it's environment; i.e. a robot has a feedback loop.</p>
","industrial-robot"
"1658","Implementing Slip Compensation into a Half-Size Micromouse","<p>I would like to know if there are any other solutions to implement slip compensation into a Half-Size Micromouse other than the conventional method. I have spoken to a few Japanese competitors, and they told me that the only solution they have to such a problem is creating a table of predetermined values and using these values to increase or decrease the before turn/after turn distances. The values used are determined by the Mouse's intelligence. Due to the fact that this method has too many limitations, I would like to hear more suggestions from people who are familiar with this matter.</p>
","micromouse"
"1660","How to protect the milk in a homemade vending machine?","<p>I am working on a homemade vending machine project that serves milk and cookies, using arduino and some basic servos and stuff.</p>

<p>The problem is: I really have no clue on how to protect the milk to last long, or how to even know if the milk is still ok to drink.. All I really know is that air is bad for the milk (and the cookies), so here is what I came up with:</p>

<p><img src=""http://i.stack.imgur.com/mGUMy.png"" alt=""enter image description here""></p>

<p>Two solenoids that activates at the same time, to allow air in, and milk out. All of this should be inside a ""slightly"" colder place.</p>

<p>I'm sure this design might sound stupid to some of you, but this is where I need your help please, do you think this design can work ? (Would that solenoid on top make any difference to protect milk?) How to improve it to make the milk last as long as possible ?</p>

<p>I'v heard about the big guys making machines that keep milk fresh for weeks even months, while i'm probably sure my milk won't stand a couple of hours..</p>

<p>Any idea or any information, link, or clue would be greatly appreciated. Thank you.</p>
","arduino"
"1666","classify if two adjacent surfaces belong to same object","<p>I have a box (cuboid) lying on floor or table. So there are 6 surfaces of the box and 1 surface of the floor. If I take each pair of surface such that the surfaces are ""adjacent"" to each other, I get two kind of pairings:</p>

<p>1) two surfaces of the box: the surface normals of the surfaces diverge from each other.</p>

<p>2) 1 surface of the box + surface of the floor : the surface normals converge and intersect at an angle of 90 degrees. ( 8o to 100 degrees, if we want to add some tolerance).</p>

<p>I want to distinguish these two cases by representing through a function? What function can distinguish between these two situations?</p>

<p>In both cases, the the normalized dot product of the surface normals is 0, since the angle b/w them is 90 degrees. So this is not the right solution...</p>
","kinect computer-vision machine-learning"
"1670","Moldable rubber for ""feet""","<p>I’m trying to inject some kind of rubber around an aluminum strut to form “feet” for a robot. I’ve already milled the mold, but I’m having trouble finding an inexpensive and readily available rubber compound that will cure without exposure to air. Ideally it should cure to about the consistency of a silicone O-ring. I’ve tried silicone gasket-maker (the automotive stuff), however a week later it hasn’t cured in the mold, as there is no exposure to the air.  Is there anything out there with a similar consistency to silicone, but doesn’t require air to cure?  Or is there a way to get what I’m currently using to set up without waiting a millennium?  There aren’t any real mechanical requirements, I’m just trying to clean up the look of the robot and prevent its legs from scratching my table. </p>
","cnc"
"1676","funny behaviour or what - dc motor control","<p>I'm trying to control the speed of this <a href=""http://www.pololu.com/catalog/product/1117"" rel=""nofollow"">motor</a>, with this <a href=""http://www.pololu.com/catalog/product/2130"" rel=""nofollow"">motor driver</a> and pic16f690. </p>

<p>pwm in my program</p>

<pre><code>5 kHz frequency (0.2 ms period)
</code></pre>

<p>Starting with 50% duty cycle the motor doesn't run.
But moving from say, 80% to 50% (i.e. program my PIC with 80% duty cycle, and then re-program it with 50%), the motor will run at 50% (of course at a lower speed). I consider this funny. Anyone to explain this?</p>

<p>my motor powered by 5V.</p>
","motor"
"1678","Smooth servo movement for a crawling robot","<p>I made a small crawler robot a little while ago that had two legs with two degrees of freedom each, so 4 RC servos total. While I was programming the movement of the legs I noticed that they moved rather stiffly. It makes sense that the RC servo's internal controller would have a very quick response to position commands, but I wanted my crawler to move in a way that seems a little more smooth and life-like.</p>

<p>My solution was create a cubic function of time that describes the path of the servos, and then set their position in small time increments, resulting in more smooth motion. Essentially what I did was solve for the $a_i$ coefficients in a cubic equation using the time interval, starting and ending position of the servo, and starting and ending rates the servo should move (which is just the derivative of the position):</p>

<p>Solve for $a_0$, $a_1$, $a_2$, and $a_3$:</p>

<p>$$ position(t) = a_0 + a_1t + a_2t^2 + a_3t^3 $$
$$ rate(t) = position'(t) = a_1 + 2a_2t + 3a_3t^2 $$</p>

<p>Given: $position(0)$, $position(t_f)$, $rate(0)$, $rate(t_f)$</p>

<p>I set the rate of the servo between a pair of movements to be zero if the movements were in opposite directions, and positive or negative if the movements were both in the positive or negative direction, respectively. </p>

<p>This worked pretty well, but this solution is limited in a few ways. For one, it's difficult to decide what exactly the rates between movements that go in the same direction should be. I used the average of the slopes ahead and behind of a particular position between movements, but it isn't clear to me that is optimal. Second of all, cubic curves could take the servo to a position outside of the range of the positions at the beginning and end of a movement, which may be undesirable. For example, at some point during the time interval, the curve could cause the servo to go beyond the second position, or below the first position. Thirdly, curve generation here does not consider the maximum rate that the servo can turn, so a curve may have the servo move at a speed that is unrealistic. With that, a minor concern is that the maximum turning rate depends on the response of servo's internal controller, and may change depending on the size of the position interval. </p>

<p>Neglecting that last concern, these issues may be solved by increasing the degree of the polynomial and adding constraints to solve for the coefficients, but I'm now starting to wonder...</p>

<p>Is there a better way than this to make servo movement smooth and seem more life-like?</p>
","servos kinematics"
"1680","Management of asynchronous commands","<p>I am working on a robotics project with C++ (drawing signs on board), on CRS CataLyst5 arm.</p>

<p>I have faced a problem:</p>

<p>I have many methods move in different directions, goToLocalizations, etc, but the problem is that when I run many of them in main without Sleep() function between each function they does not run properly. I think that the first one needs time (the time of robot movement) but when I put Sleep(10000) between them (I guessed that 10 seconds are enough for the movement) all is ok. This is very ineffective and slow solution. Would you like to give me some solutions to avoid the use of Sleep ?</p>
","activerobot"
"1682","Does a vehicle with defferential gear still move straight?","<p>I am in the concept phase of a driving robot. The two wheels on the front axle will be powered, while the rear will be dragged along. The rear is also responsible for steering but this has noting to do with my question.</p>

<p>Since the robot is required to make relatively sharp turns at high speed. Therefore I have two options to compensate the different speeds on both sides. On the one hand, a differential gear in the front axle could be used. It would be powered by one motor then. On the other hand, I could simply use two motors directly powering each a front wheel. This way I could simulate the differentiation in software.</p>

<p>I'd like to go for the first approach, using the hardware differential. But I have the one concern with it. Would a robot vehicle with differential gear still move straight, without explicit steering applied?</p>

<p>My imagination is that, with those wheels not being solidly connected, the robot would move in random curves which I'd have to compensate with a lot of steering then. I know that for real cars, differential gears are standard and do their work, but now I am talking about a small robot measuring about 6 inches.</p>
","motor wheeled-robot motion wheel"
"1684","Wind force impact on torque mechanical arm","<p>I've got an arm attached to a shaft. The arms dimensions are 40x5 inches the arm weights about 10 lbs.</p>

<p>If I have a wind acting on the side of the arm, how would I translate the wind force into torque on the shaft?</p>

<p>To give some more information, I'm rotating the arm using a stepper motor, and I would like to know how to size the motor depending environmental conditions.</p>

<p>What should my formula look like in order to arrive a required oz-in of torque given my requirements being:</p>

<ul>
<li>I need to be able to accelerate the arm from 0 to 12 rpm in 1.5 seconds</li>
<li>The wind speed can be as high as 30 mph</li>
</ul>

<p>Using the formula <strong>P = .00256 x 30^2</strong> i find the wind pressure per square foot being <strong>2.304</strong></p>

<p>Using the formula <strong>F = A x P x Cd</strong> for calculating force, I get <strong>1.389 x 2.304 x 2 = 6.4</strong> </p>

<p>So I know that the wind force on my arm is <strong>6.4 lbs</strong>. But now how do I translate this to torque on my arm? </p>

<p><img src=""http://i.stack.imgur.com/awzfj.png"" alt=""mechanical arm""></p>

<p>Source: <a href=""http://k7nv.com/notebook/topics/windload.html"">http://k7nv.com/notebook/topics/windload.html</a></p>
","force torque"
"1687","Can GPS modules work inside plastic enclosures?","<p>I'm going to be embarking on an autonomous robot project and I was going to be using GPS to navigate to waypoints (I'm aware of the margin of error when it comes to localization with GPD but I live in a lovely area with many open fields). </p>

<p>I was going to use Adafruit's Ultimate GPS Breakout board with my RaspberryPi, and I was wondering how I should protect or mount the GPS to protect it from the elements. Do all GPS units need to be face up and unobstructed (ex. wood or plastic) in order to work? If so, how can I still protect a GPS unit from the outdoors?</p>
","gps protection coverage"
"1693","Do 6 motors require 6 individually-assigned batteries?","<p>For the Dagu Wild Thumper 6 Wheeled platform, or any multiple motor system, do I really need 1 battery for each motor? Or should I just buy 2 for either side of the platform. In addition, for larger motors like the ones on this platform, how do I deal with the power generated from a coasting motor?</p>

<p>I want to jump into the deep end with robotics, as I already hold all the programming skills, and I realize a platform of this magnitude may be a difficult endeavor.</p>

<p>Recommended motor voltage is 2 – 7.5 Volts, so should one use two 22 Volt batteries for the left and right side, or six 7.5 volt batteries?</p>
","motor battery"
"1695","Help with PID ""units"" in a quadcopter control system","<p>I'm in the process of writing my own simple quadcopter controller for experimental use, and I'm having trouble getting my head around how to convert from the degrees which my PID controller demands to an appropriate 1k-2k range for PWM output. For example, take the roll axis on a '+' configured 'copter (pseudo-code):</p>

<pre><code>setpoint = scaleToRange(receiver.rollValue, -30, 30); //scale the command (1000-2000) to between -30 and 30 degrees, as that's the maximum roll permitted.
demandedRoll = rollPID.calculate(setpoint, imu.currentRoll, PID_params);

/// the part I'm having trouble with

motorLeft_command = receiver.throttle - rollPWM;
motorRight_command = receiver.throttle + rollPWM;
</code></pre>

<p>How do I take the roll demanded by my PID controller and convert it to a value useful to the motors, that is to say, where does <code>rollPWM</code> come from? My first instinct is to use a simple linear relationship, i.e.:</p>

<pre><code>rollPWM = scaleToRange(demandedRoll, MinValue=receiver.throttle/2, MaxValue=2000-receiver.throttle);
//don't let it go beyond 50% of throttle on low end, and the ESC's max on the high end. 
</code></pre>

<p>However this seems far too simplistic to work.</p>

<p>Or should I be doing more calculations before everything goes through PID control? Any help would be great. </p>
","pid motion multi-rotor"
"1697","MPU-6050 + Arduino nano - Logic converter or not?","<p>I bought this MPU-6050: <a href=""http://www.ebay.com/itm/MPU-6050-6DOF-3-Axis-Gyroscope-Accelerometer-Module-for-Arduino-DIY-/261231955458"" rel=""nofollow"">link</a></p>

<p>According to the manufacture site, the sensor logic level is 3.3V (though the <em>ebay</em> page says <code>Power supply :3-5v</code>)</p>

<p>Should I use a 4 channel Bi-Directional Logic Level Converter (<a href=""http://www.dash.co.il/image/cache/data/AD-757/ID757_LRG-500x500.jpg"" rel=""nofollow"">like this one</a>) for the <code>SDA, SCL, INT</code> channels? or can I connect it directly to my arduino nano?</p>

<p>I saw some places that says I should use it with a logic level converter and some who say it's ok without it. (I guess it depends on the sensor board, so please take a look, link above)</p>

<p><strong>Current Setup:</strong></p>

<pre><code>SDA &lt;-&gt; LLC &lt;-&gt; A4
SCL &lt;-&gt; LLC &lt;-&gt; A5
INT &lt;-&gt; LLC &lt;-&gt; D2
VCC &lt;- LLC &lt;- 5V (arduino)
GND &lt;- LLC &lt;- GND (arduino)
</code></pre>

<p><em>I still don't have the parts so I can't test it, and I'm probably going to use <a href=""http://www.i2cdevlib.com/devices/mpu6050"" rel=""nofollow"">Jeff Rowberg library</a></em> to communicate with the sensor (I<sup>2</sup>C)</p>
","arduino sensors quadcopter logic-control"
"1707","Build an WiFi IP camera with webcam","<p>I have a USB webcam and a WiFi module which it can convert Serial data to WiFi and vice versa.</p>

<p>The question is can I simply convert the data coming from the webcam to serial with a USB to Serial IC (like FT232R ) and then hand it over to my WiFi Module?</p>

<p><strong>Update:</strong></p>

<p>The WiFi module DataSheet is <a href=""http://www.uplooder.net/cgi-bin/dl.cgi?key=1bbc1bb2c5731ba520a0a206664ba57b"" rel=""nofollow"">here</a></p>
","cameras wifi usb"
"1711","Approach to using PID to get a differential robot driving straight","<p>Consider a differential drive robot that has two motorized wheels with an encoder attached to each for feedback. Supposed there is a function for each DC motor that takes a float from -1 to 1 and sets the PWM signals to provide a proportional amount of power to that motor. Unfortunately, not all motors are created equal, so sending each motor the same PWM signal makes the robot veer left or right. I'm trying to think about how to drive the robot straight using the encoders attached to each motor as input to a PID loop.</p>

<p>Here's how I would do it: I would take the difference between the left and right encoders, bound the error between some range, normalize it to be from [-1, 1], and then map it to the motor powers 0 to 1. So if I and D were zero, and we get an error of 1 (so the left motor has turned much more than the right motor), then left motor would be set to 0, and the right motor set to 1 (causing a hard left). </p>

<p>Are there any issues with this? What is a better approach?</p>
","pid differential-drive"
"1712","What is the difference between RC motors for cars and helicopters?","<p>I am working on a robot with focus on speed. At the moment I am looking for a suitable motor but it world help if I understood the difference between the various options.</p>

<p>To provide some background, I have not worked with RC model components before, but I think this is the only place for me to find the components needed for my robot, such as the motor.</p>

<p>I have already figured out how much power the motor needs to accelerate my robot as desired, taking energy conversion efficiency and tractional resistance into account. It's about 170 watts, depending on the final weight.</p>

<p>To limit my search further, I need to decide on either using a RC car motor or a RC helicopter motor now, but I don't understand the difference between these options.</p>

<p>Focussing on brushless motors (if that matters), what are the differences between RC car and RC helicopter motors which might need to be taken into account when choosing between them?</p>
","motor brushless-motor"
"1717","How to determine the parameter of a Complementary Filter?","<p>I know that the Complementary Filter has the functions of both LPF and HPF. But I think my understanding on the principal behind it is still unclear.</p>

<p>I am quite new on digital signal processing, and maybe some very fundamental explanations will help a lot.</p>

<p>Say I have a Complementary Filter as follows:</p>

<p>$$y =a\cdot y+(1-a)\cdot x$$</p>

<p>Then my parameter $a$ may be calculated by $$a=\frac{\text{time constant}}{\text{time constant}+\text{sample period}}$$
where the $\text{sample period}$ is simply the reciprocal of the $\text{sampling frequency}$.</p>

<p>The $\text{time constant}$ seems to be at my own choice.</p>

<p><strong>My Questions:</strong></p>

<ol>
<li>What is the theory behind this calculation?</li>
<li>How do we choose the $\text{time constant}$ properly?</li>
</ol>

<p><sub><strong>Note:</strong> I also <a href=""http://stackoverflow.com/q/18095785"">posted this question on Stack Overflow</a>, as the answers there are likely to be slightly different in emphasis.</sub></p>
","gyroscope magnetometer"
"1724","Off-the-shelf micro fluid dispenser","<p>Need a way to dispense micro liter amounts of water (lets say 1-10ul). Only thing I've found is piezoelectric dispensers and they are >$100. Any suggestions? </p>

<p>I can build, but preferably would be an off-the-shelf component.</p>
","electronics"
"1728","What is an effective distribution of grayscale sensors on robot","<p>I'm working on a robotics project, and I am using grayscale sensors to automatically follow a black line: turning 90 degrees, going round in a circle, and passing through gaps in the lines etc.  I was wondering what is an effective way to detect the colours and move it through the lines, with five or six grayscale sensors.  </p>

<p>Thank you very much.</p>
","mobile-robot sensors automatic line-following"
"1729","How do SLAM algorithms handle a changing environment?","<p>I'm doing some groundwork for a project, and I have a question about the current state of SLAM techniques.</p>

<p>When a SLAM-equipped device detects an object, that object's position is stored. If you look at the point cloud the device is generating, you'll see points for this object, and models generated from it will include geometry here.</p>

<p>If an object is placed in a previously-empty space, it is detected, and points are added. Subsequent models will feature geometry describing this new object.</p>

<p>How does the device react if that object is removed? As far as I've seen, SLAM systems will tend to leave the points in place, resulting in ""ghost"" geometry. There are algorithms that will disregard lone points caused by transient contacts, but objects that remained long enough to build up a solid model will remain in the device's memory. Are there any systems that are capable of detecting that previously-occupied space is now empty?</p>
","slam"
"1730","How to correctly compute direct kinematics for a delta robot?","<p>I'm trying to put together a simple simulation for a delta robot and I'd like to use forward kinematics (direct kinematics) to compute the end effector's position in space by passing 3 angles.</p>

<p>I've started with the <a href=""http://forums.trossenrobotics.com/tutorials/introduction-129/delta-robot-kinematics-3276/"" rel=""nofollow"">Trossen Robotics Forum Delta Robot Tutorial</a> and I can understand most of the math, but not all. I'm lost at the last part in forward kinematics, when trying to compute the point where the 3 sphere's intersect. I've looked at spherical coordinates in general but couldn't work out the two angles used to find to rotate towards (to E(x,y,z)).
I see they're solving the equation of a sphere, but that's where I get lost. </p>

<p>Can someone please 'dumb it down' for me ?</p>

<p>Also, I've used the example code to do a quick visualization using <a href=""http://processing.org/"" rel=""nofollow"">Processing</a>,
but the last part seems wrong. The lower leg changes length and it shouldn't:</p>

<pre><code>//Rhino measurements in cm
final float e = 21;//end effector side
final float f = 60.33;//base side
final float rf = 67.5;//upper leg length - radius of upper sphere
final float re = 95;//lower leg length - redius of lower sphere (with offset will join in E(x,y,z))

final float sqrt3 = sqrt(3.0);
final float sin120 = sqrt3/2.0;   
final float cos120 = -0.5;        
final float tan60 = sqrt3;
final float sin30 = 0.5;
final float tan30 = 1/sqrt3;
final float a120 = TWO_PI/3;
final float a60 = TWO_PI/6;

//bounds
final float minX = -200;
final float maxX = 200;
final float minY = -200;
final float maxY = 200;
final float minZ = -200;
final float maxZ = -10;
final float maxT = 54;
final float minT = -21;

float xp = 0;
float yp = 0;
float zp =-45;
float t1 = 0;//theta
float t2 = 0;
float t3 = 0;

float prevX;
float prevY;
float prevZ;
float prevT1;
float prevT2;
float prevT3;

boolean validPosition;
//cheap arcball
PVector offset,cameraRotation = new PVector(),cameraTargetRotation = new PVector();

void setup() {
  size(900,600,P3D);
}

void draw() {
  background(192);
  pushMatrix();
  translate(width * .5,height * .5,300);
  //rotateY(map(mouseX,0,width,-PI,PI));

  if (mousePressed &amp;&amp; (mouseX &gt; 300)){
    cameraTargetRotation.x += -float(mouseY-pmouseY);
    cameraTargetRotation.y +=  float(mouseX-pmouseX);
  }
  rotateX(radians(cameraRotation.x -= (cameraRotation.x - cameraTargetRotation.x) * .35));
  rotateY(radians(cameraRotation.y -= (cameraRotation.y - cameraTargetRotation.y) * .35));

  stroke(0);
  et(f,color(255));
  drawPoint(new PVector(),2,color(255,0,255));
  float[] t = new float[]{t1,t2,t3};
  for(int i = 0 ; i &lt; 3; i++){
    float a = HALF_PI+(radians(120)*i);
    float r1 = f / 1.25 * tan(radians(30));
    float r2 = e / 1.25 * tan(radians(30));
    PVector F = new PVector(cos(a) * r1,sin(a) * r1,0);
    PVector E = new PVector(cos(a) * r2,sin(a) * r2,0);
    E.add(xp,yp,zp);
    //J = F * rxMat
    PMatrix3D m = new PMatrix3D();
    m.translate(F.x,F.y,F.z);
    m.rotateZ(a);
    m.rotateY(radians(t[i]));
    m.translate(rf,0,0);

    PVector J = new PVector();
    m.mult(new PVector(),J);
    line(F.x,F.y,F.z,J.x,J.y,J.z);
    line(E.x,E.y,E.z,J.x,J.y,J.z);
    drawPoint(F,2,color(255,0,0));
    drawPoint(J,2,color(255,255,0));
    drawPoint(E,2,color(0,255,0));
    //println(dist(F.x,F.y,F.z,J.x,J.y,J.z)+""\t""+rf);
    println(dist(E.x,E.y,E.z,J.x,J.y,J.z)+""\t""+re);//length should not change
  }
  pushMatrix();
    translate(xp,yp,zp);
    drawPoint(new PVector(),2,color(0,255,255));
    et(e,color(255));
    popMatrix();
  popMatrix(); 
}
void drawPoint(PVector p,float s,color c){
  pushMatrix();
    translate(p.x,p.y,p.z);
    fill(c);
    box(s);
  popMatrix();
}
void et(float r,color c){//draw equilateral triangle, r is radius ( median), c is colour
  pushMatrix();
  rotateZ(-HALF_PI);
  fill(c);
  beginShape();
  for(int i = 0 ; i &lt; 3; i++)
    vertex(cos(a120*i) * r,sin(a120*i) * r,0);
  endShape(CLOSE);
  popMatrix();
}
void keyPressed(){
  float amt = 3;
  if(key == 'q') t1 -= amt;
  if(key == 'Q') t1 += amt;
  if(key == 'w') t2 -= amt;
  if(key == 'W') t2 += amt;
  if(key == 'e') t3 -= amt;
  if(key == 'E') t3 += amt;
  t1 = constrain(t1,minT,maxT);
  t2 = constrain(t2,minT,maxT);
  t3 = constrain(t3,minT,maxT);
  dk();
}

void ik() {
  if (xp &lt; minX) { xp = minX; }
  if (xp &gt; maxX) { xp = maxX; }
  if (yp &lt; minX) { yp = minX; }
  if (yp &gt; maxX) { yp = maxX; }
  if (zp &lt; minZ) { zp = minZ; }
  if (zp &gt; maxZ) { zp = maxZ; }

  validPosition = true;
  //set the first angle
  float theta1 = rotateYZ(xp, yp, zp);
  if (theta1 != 999) {
    float theta2 = rotateYZ(xp*cos120 + yp*sin120, yp*cos120-xp*sin120, zp);  // rotate coords to +120 deg
    if (theta2 != 999) {
      float theta3 = rotateYZ(xp*cos120 - yp*sin120, yp*cos120+xp*sin120, zp);  // rotate coords to -120 deg
      if (theta3 != 999) {
        //we succeeded - point exists
        if (theta1 &lt;= maxT &amp;&amp; theta2 &lt;= maxT &amp;&amp; theta3 &lt;= maxT &amp;&amp; theta1 &gt;= minT &amp;&amp; theta2 &gt;= minT &amp;&amp; theta3 &gt;= minT ) { //bounds check
          t1 = theta1;
          t2 = theta2;
          t3 = theta3;
        } else {
          validPosition = false;
        }

      } else {
        validPosition = false;
      }
    } else {
      validPosition = false;
    }
  } else {
    validPosition = false;
  }

  //uh oh, we failed, revert to our last known good positions
  if ( !validPosition ) {
    xp = prevX;
    yp = prevY;
    zp = prevZ;
  }

}

void dk() {
  validPosition = true;

  float t = (f-e)*tan30/2;
  float dtr = PI/(float)180.0;

  float theta1 = dtr*t1;
  float theta2 = dtr*t2;
  float theta3 = dtr*t3;

  float y1 = -(t + rf*cos(theta1));
  float z1 = -rf*sin(theta1);

  float y2 = (t + rf*cos(theta2))*sin30;
  float x2 = y2*tan60;
  float z2 = -rf*sin(theta2);

  float y3 = (t + rf*cos(theta3))*sin30;
  float x3 = -y3*tan60;
  float z3 = -rf*sin(theta3);

  float dnm = (y2-y1)*x3-(y3-y1)*x2;

  float w1 = y1*y1 + z1*z1;
  float w2 = x2*x2 + y2*y2 + z2*z2;
  float w3 = x3*x3 + y3*y3 + z3*z3;

  // x = (a1*z + b1)/dnm
  float a1 = (z2-z1)*(y3-y1)-(z3-z1)*(y2-y1);
  float b1 = -((w2-w1)*(y3-y1)-(w3-w1)*(y2-y1))/2.0;

  // y = (a2*z + b2)/dnm;
  float a2 = -(z2-z1)*x3+(z3-z1)*x2;
  float b2 = ((w2-w1)*x3 - (w3-w1)*x2)/2.0;

  // a*z^2 + b*z + c = 0
  float a = a1*a1 + a2*a2 + dnm*dnm;
  float b = 2*(a1*b1 + a2*(b2-y1*dnm) - z1*dnm*dnm);
  float c = (b2-y1*dnm)*(b2-y1*dnm) + b1*b1 + dnm*dnm*(z1*z1 - re*re);

  // discriminant
  float d = b*b - (float)4.0*a*c;
  if (d &lt; 0) { validPosition = false; }

  zp = -(float)0.5*(b+sqrt(d))/a;
  xp = (a1*zp + b1)/dnm;
  yp = (a2*zp + b2)/dnm;

  if (xp &gt;= minX &amp;&amp; xp &lt;= maxX&amp;&amp; yp &gt;= minX &amp;&amp; yp &lt;= maxX &amp;&amp; zp &gt;= minZ &amp; zp &lt;= maxZ) {  //bounds check
  } else {
    validPosition = false;
  }

  if ( !validPosition ) {    
    xp = prevX;
    yp = prevY;
    zp = prevZ;
    t1 = prevT1;
    t2 = prevT2;
    t3 = prevT3;  
  }

}

void  storePrev() {
  prevX = xp;
  prevY = yp;
  prevZ = zp;
  prevT1 = t1;
  prevT2 = t2;
  prevT3 = t3;
}

float rotateYZ(float x0, float y0, float z0) {
  float y1 = -0.5 * 0.57735 * f; // f/2 * tg 30
  y0 -= 0.5 * 0.57735    * e;    // shift center to edge
  // z = a + b*y
  float a = (x0*x0 + y0*y0 + z0*z0 +rf*rf - re*re - y1*y1)/(2*z0);
  float b = (y1-y0)/z0;
  // discriminant
  float d = -(a+b*y1)*(a+b*y1)+rf*(b*b*rf+rf); 
  if (d &lt; 0) return 999; // non-existing point
  float yj = (y1 - a*b - sqrt(d))/(b*b + 1); // choosing outer point
  float zj = a + b*yj;
  return 180.0*atan(-zj/(y1 - yj))/PI + ((yj&gt;y1)?180.0:0.0);
} 
</code></pre>
","kinematics forward-kinematics"
"1737","H-Bridge using atmega8 microcontroller","<p>I want to use my atmega8 uC as a <a href=""/questions/tagged/h-bridge"" class=""post-tag"" title=""show questions tagged 'h-bridge'"" rel=""tag"">h-bridge</a>.</p>

<p>Can anybody give me the source code using C, so that the microcontroller acts as an H-Bridge.</p>
","microcontroller c h-bridge avr"
"1743","AT command in SIM900A GSM/GPRS module to find out originating address of an SMS","<p>I am using SIM900A for some purpose and want to know the number of the sender from where a message comes. I am unable to find the specific AT command related to receiving message which give me number from where latest message comes.</p>

<p>I had used AT+CNMI (it corresponds to notification regarding latest received message), but am unable to find sender number.</p>

<p>I had seen AT+CMGL=&lt;stat&gt;[,&lt;mode&gt;] will give you a string which will have oa i.e. originating address and once that is stored in a string I can easily parse it out, but when I had data format of that string. Need help or any suggestion if somebody can help me out with any other possible solution.</p>
","arduino microcontroller"
"1745","What modelling tools are available to design a robot","<p>I am planning to build a robot.</p>

<p>1) What free or low cost robot modelling tools exist.</p>
","design"
"1750","Design Calculations & Mathematical Modeling of Tricopters","<p>I have been studying about building a tricopter.  But I couldn't find the design calculations or mathematical modeling of the tricopter any where over the internet. </p>

<p>What are the mathematical relationships or equations of motion and forces in tricopter?  How do I calculate the requirements of the structural design and the energy requirements of the motors?</p>
","design"
"1753","Assigning Serial number and GUID to a microcontroller","<p>This might be a out of league question and may seems to be very odd.I am using multiple Arduino UNO boards over network and want to assign a GUID and Serial number to each board so that when ever they send any data to a central server, server is able to find which Device it is if we have assign name for each device.</p>

<ul>
<li>first way to do this is to assign GUID and serial number of device before each message that is send to central server manually while programming and then burn that hex to arduino.</li>
</ul>

<p>Now is there any way out that we can burn a program that always give a output as a string (GUID+Serial number of device) like we program EEPROM for this and then burn our main code in Arduino which pick the GUID+Serial ID combo from EEPROM and write it before every message which arduino is pushing to central server.</p>

<p>Or my another way of asking is can we program EEPROM with a different program and our Arduino separately like 2 files running in parallel or is it not possible?</p>

<p>Is there any other way of doing this?</p>
","arduino microcontroller communication"
"1755","Any globally unique signature in Ardupilot hardware, or Arduino in general?","<p>I have several <a href=""http://store.3drobotics.com/products/apm-2-5-kit"">APM 2.5 boards</a> and need to identify them based on some globally unique hardware signature that does not change with programming.</p>

<p>Arduinos and atmel AVR chips in general <a href=""http://forum.arduino.cc/index.php?topic=45104.0"">do not have</a> (also <a href=""http://forum.arduino.cc/index.php?topic=45060.0"">this thread</a>) an accessible serial number.</p>

<p>However, it seems that the Ardupilot has so many integrated sensors and other ICs that one of them must have something unique I can use ( <a href=""http://stuff.storediydrones.com/APM_v252_RELEASE.zip"">see schematic</a> )!</p>

<p>I will be checking datasheets for MPU-6000, HMC5883L-TR and MS5611, but in the meantime, if someone has already figured this one out, please answer.</p>
","arduino ardupilot"
"1757","Laser / photosensor pair or similar","<p>I'm looking for a laser / photosensor pair (or product of similar function) for detecting when a beam is interrupted (no more than 3ft apart, probably more like 1ft).</p>

<p>I'd like these to run off of 5V, since I'm using an Arduino. My main requirement, however, is that these parts have nice housings, ideally with some mounting screw holes or something along those lines. This is going into a project where sturdiness and durability are important.</p>

<p>I don't know how to search for parts like what I am looking for. Could you please point me either to some good product sources, give me some better keywords for searching, or link me directly to potentially useful products?</p>
","arduino sensors"
"1758","Polling or Timer interrupt?","<p>We hope to build a simple line follower robot and we got a problem when we were discussing about PIC programming.</p>

<p>We planed to write a endless loop, check the sensor panel reading and do the relevant stuff for that reading.</p>

<p>But one of our friends told us to use a timer interrupt to generate interrupts in definite time periods and in each interrupt check the sensor panel reading and do the relevant stuff for that reading.</p>

<p>But we can't figure out which is best: the endless loop in main method OR timer interrupt method.</p>

<p>What is the best way, and why?</p>
","sensors microcontroller interrupts"
"1765","Place for GPS antenna on autonomous vehicle","<p>I used to think that the higher GPS antenna position the better until I read the following on <a href=""http://gpsd.berlios.de/faq.html#accuracy"" rel=""nofollow"">GPSd FAQ</a>:</p>

<blockquote>
  <p>One <strong>common error is to place the GPS or antenna as high as possible</strong>.
  This will increase multipath effects due to signal bounce from the
  ground or water, which can cause the GPS to mistake its position and
  the time signal. The <strong>correct location for a boat GPS antenna</strong> is on the
  gunwale rail or pushpit rail, <strong>close to the water</strong> and as far from the
  mast as possible (to reduce signal bounce from the mast). If you're
  outside or in a fixed location, put the GPS antenna as far from
  buildings as possible, and on the ground.</p>
  
  <p>If you're <strong>in a car</strong>, <strong>don't
  put the GPS antenna on the roof, put it on the towbar</strong> or some similar
  location. If you're driving in a heavily built up area, you're going
  to get signal bounce off buildings and reduced accuracy. That's just
  how the physics works. Note, however, that as your velocity goes up it
  becomes easier for the convergence filters in your GPS to spot and
  discard delayed signal, so multipath effects are proportionally less
  important in fast-moving vehicles.</p>
</blockquote>

<p>Does anyone has experience placing GPS antenna on a towbar of the car as suggested? Does it give reasonable effect?</p>

<p>My concern is that placing antenna there will not reduce an error that much, but will expose the device (antenna) to possible mechanical damage.</p>

<p>So, are there any better positions apart from roof and towbar?</p>

<p>Thanks</p>
","gps ugv"
"1766","How to model unpredictable noise in Kalman Filter?","<p><strong>Background:</strong></p>

<p>I am implementing a simple Kalman Filter that estimates the heading direction of a robot. The robot is equipped with a compass and a gyroscope.</p>

<p><strong>My Understanding:</strong></p>

<p>I am thinking about representing my state as a 2D vector $(x, \dot{x})$, where $x$ is the current heading direction and  $\dot{x}$ is the rotation rate reported by the gyroscope.</p>

<p><strong>Questions:</strong></p>

<ol>
<li>If my understanding is correct, there will be no control term, $u$ in my filter. Is it true? What if I take the state as a 1D vector $(x)$? Then does my $\dot{x}$becomes the control term $u$? Will these two methods yield different results?</li>
<li>As we know, the main noise source comes from the compass when the compass is in a distorted magnetic field. Here, I suppose the Gaussian noise is less significant. But the magnetic distortion is totally unpredictable. How do we model it in the Kalman Filter?</li>
<li>In Kalman Filter, is the assumption that ""all the noises are white"" necessary? Say, if my noise distribution is actually a Laplacian distribution, can I still use a Kalman Filter? Or I have to switch to another filter, like Extended Kalman Filter?</li>
</ol>
","localization kalman-filter gyroscope compass"
"1767","What to do when the control input of the Kalman filter is unknown?","<p>I am implementing a simple Kalman Filter that estimates the heading direction of a robot. The robot is equipped with a compass and a gyroscope.</p>

<p>Say at time $t-dt$, the compass reports a reading $\theta_{t-dt}$, and the gyroscope reports a reading $\omega_{t-dt}$. Then I assume from time $t-dt$ to $t$, the rotation rate can be regarded as a constant. Thus, my current heading direction is $$\theta_{t}=\theta_{t-dt}+\omega_{t-dt}\cdot dt$$
As can be seen, the $\theta$ can be easily time-updated.</p>

<p>But what about my $\omega$? The robot is not at my control. So its rotation rate at next moment is <strong>unpredictable</strong>.</p>

<p><strong>How should I do the time update in this case?</strong></p>
","kalman-filter gyroscope compass"
"1774","In SLAM, how does a laser range finder produce pseudo-segments from dynamic objects?","<p>In this <a href=""http://www.mdpi.com/1424-8220/12/1/429"" rel=""nofollow"">paper</a>, the author says that during SLAM process, pseudo segments that appear from any momentary pause of dynamic objects in laser data would make the map unsatisfied.</p>

<p>How is this caused?  </p>

<p>If the dynamic object moved, won't laser data update and eliminate the segment of dynamic objects?</p>
","sensors slam"
"1775","How to control the position of a pneumatic piston?","<p>How can I control the position of a <strong>pneumatic</strong> piston?</p>

<p>The only way I know about is using a magnetic reed switch (magnetic sensor) with a matching piston and use some type of control algorithm, like PID for instance, to keep the piston where the sensor is.</p>

<p>The problem with that is that it gives you only limited control of the position, it just adds another ""state"" (open, closed, sensor position) and not full control. for example I want it to be 43% once and 70% the other time, but without using a sensor for each position because I would like all the ""options"" to be available (I mean that the percentages aren't pre-defined)</p>

<p>This is an example of the pistons I use:</p>

<p><img src=""http://i.stack.imgur.com/Lm29K.jpg"" alt=""piston""></p>

<p>This is a good example of what I want: <a href=""http://www.youtube.com/watch?v=A8LZ15uiuXU"" rel=""nofollow"">http://www.youtube.com/watch?v=A8LZ15uiuXU</a></p>
","sensors control pid movement"
"1776","running UWSim commands in ROS","<p>Where can I find a good documentation about the UWSim in ROS. Actually having the source files is not enough and it is actually hard to follow all the functions. for example, how can I use these command correctly :</p>

<p>&amp; rosrun UWSim gotoAbsolutePosition   0 0 0 0 0 0</p>

<p>I know that there is a node 'gotoAbsolutePosition' in the Package 'UWSim' and I knwo the variables, but I cannot set the two topics properly.</p>
","ros"
"1777","Tracking with accelerometer and gyro versus multiple accelerometers","<p>I'm building quadcopter and most of the control systems use one accelerometer and gyro. I've read few papers and usually accelerometer is used as reference to the ground because gyro slowly drifts away in time. But if quadcopter does some crazy maneuvering when force direction from accelerometer does not have to point to the ground than accelerometer data is useless. As well there is problem with centripetal force if the accelerometer is not directly in the centor of mass.</p>

<p>I was thinking about using multiple accelerometers. To fully determine position and motion of quadcopter one would need three accelerometers(If I have done the math right). This would kind of solve the problem with centripetal force</p>

<p>So I would like to know if anyone tried to use multiple accelerometers for better orientation estimation. </p>
","quadcopter imu accelerometer gyroscope"
"1782","Actuator design. plausible?","<p>So i got this idea waay back when i was in highschool as a kind of electromagnetic analogue to a biological muscle. it is basically a long stack of thin electromagnets connected in parallel. 
<img src=""https://lh5.googleusercontent.com/-DUdbOfkO9bI/UhtIXyefiII/AAAAAAAAB0c/vmkKU3K6rF0/w2350-h1762-no/IMG_20130826_152132.jpg"" alt=""(the picture)"">. 
when current is applied gaps between electromagnets shrink thus providing contraction of the whole chain. </p>

<p>I am pretty sure it can work. It can't offer great contraction range (up to 50% i would guess) but it has potential to provide good speed and be compact so that multiple chains can be combined to form stong and fast linear actuators. The thing is, i never heard of this type of actuator being used. so what is the catch? is there a better alternative? is there a design flaw? too much heat generated making them unpractical?</p>
","actuator"
"1787","How do I send text to a Torobot USB device?","<p>I'm trying to get the ""Torobot"" USB servo controller to work with Angstrom Linux on a Beagle Board XM.</p>

<p>The servo controller registers as a USB device. The device just takes simple text commands, but there is no TTY associated with it. So I'm not sure how to send commands to it.</p>

<p>Can I just send data like this (assuming that 002/005 is the device):</p>

<pre><code>$ cat file.txt &gt;&gt; /dev/bus/usb/002/005
</code></pre>

<p>Or do I need to associate it with the generic USB device? If so, how do I do that?</p>
","control microcontroller rcservo usb embedded-systems"
"1790","Motor Controller with micro USB interface","<p>I am a Robotics enthusiast and planning to make a small and simple four wheel car whose motors are supposed to be controlled by an Android device housed inside the car by means of the micro USB port of the device. The car has to move forward or backward only as directed by the signals from the Android device.</p>

<p>So my assumption is that there should be a circuit board which accepts the signals from the microUSB/USB of the Android device and controls the power to the electric DC motor. Also the power for the motor will be supplied from a battery pack inside the car.</p>

<p>Could anyone suggest me a cheap motor driver circuit which supports micro USB or USB? And where can I get the parts for this online? I did a lot research but very confused with the technical terms which I am not familiar with.</p>
","mobile-robot motor wheeled-robot"
"1791","How to control a brushless motor?","<p>I consider using a brushless outrunner motor, designed for helicopters, in my driving roboter. How can I control such a brushless motor with my micro controller? Of course I'll have a separate power source.</p>

<p>The roboter should be able to move forwards and backwards, so I need to control the motor in a way to determine direction of rotation, too. I think this isn't related to the question, but I need to ensure high acceleration.</p>

<p>Specially, I am talking about <a href=""http://www.modellhobby.de/Motoren/E-Motoren/DYMOND-Brushless-Antriebe/Aussenlaeufer/DYMOND-MASTER-HQ-2838.htm?shop=k_staufenb&amp;SessionId=&amp;a=article&amp;ProdNr=03121706&amp;t=11&amp;c=3258&amp;p=3258"" rel=""nofollow"">this motor</a> which is listed in a German shop.</p>

<p><img src=""http://i.stack.imgur.com/Q3ih6.jpg"" alt=""DYMOND MASTER HQ motor""></p>
","motor control microcontroller power brushless-motor"
"1795","Localizing a swarm of robots","<p>I have a 300cm x 300cm room with a 25cm high ceiling (yes twenty five centimeters). It contains 50 small wheeled robots (about 20cm x 20cm). A central computer will orchestrate the movements of the robots, using a wireless network to send position commands to them. The robots will perform their own closed loop position control to save wireless bandwidth. The robots have 32-bit ARM microcontrollers. They have wheel position sensors, and the ability to do accurate wheel control. </p>

<p>Problem: The robots can't actually do this yet because they have no way to measure their position in the room.</p>

<p>Question: How can the robots be given the ability measure their position and orientation to an accuracy of better than ±5mm? I am looking for an accurate and robust solution, which is not affected by occlusions, and doesn't require a high power PC on each robot. Whatever sensors are necessary for localisation can easily be added to the robots. The microcontrollers can easily be changed for more powerful ones if the localisation solution demands it.</p>
","localization wireless swarm"
"1797","How do I Calibrate Analog Thumb stick?","<p><em>Outline:</em></p>

<p>I'm trying to work with an Arduino and Analog thumb stick to get values for a simple differential drive robot I'm working on. The <a href=""http://www.plexishop.it/en/modulo-joystick-keyes-sjoys.html"" rel=""nofollow"">Keyes_Sjoys Arduino Joystick Module</a> I have in my possession is giving me some strange numbers. </p>

<p><em>Following axises Data I have:</em></p>

<ul>
<li>X-axis range of 0 to a shaky 470-520 with a center value of 40.</li>
<li>Y-axis range of a solid 4 to solid 1023 with a center value of 605.</li>
</ul>

<p><em>Problem</em></p>

<p>I haven't used analog sensors before but it seems pretty obvious that my X-axis ranges should feel somewhat similar to the Y-axis but they don't. In addition, the X-axis hits zero way way before even coming close to the edge for its operating range.</p>

<p>Is my sensor broken (it's new), or is there some way I can recalibrate the potentiometer?</p>

<p><sub>Note, I also asked this over on <a href=""http://electronics.stackexchange.com/q/80870"">Electrical Engineering Stack Exchange</a>.</sub></p>
","microcontroller"
"1806","Gears modeling in Google SketchUp and SketchyPhisics","<p>I'm trying to make differential in Google SketchUp using this tutorial <a href=""http://support.ponoko.com/entries/21249896-Gears-and-Joints-with-SketchUp-Sketchy-Physics"">http://support.ponoko.com/entries/21249896-Gears-and-Joints-with-SketchUp-Sketchy-Physics</a> for gears modeling.
But I have problem: gears don't collide with any objects (and other gears). What's wrong? How to fix this?
How to make a bevel gear placed at 90 degrees relative to each other and conical cylindrical gears joints?</p>

<p>P.S. Is there something like SketchUp and SketchyPhisics in Linux?</p>
","design mechanism 3d-printing"
"1808","Tiny high torque actuator/sensor design","<p>I need to assemble a small (about 8cm x 5cm x 5cm maximum), actuator with as much torque as I can get at that size. It will be driving a small reel and pulley (reel is ~1.25cm^3, 5mm center diameter), and needs to respond to load (eg. stop if the load increases beyond a certain threshold). Power to the actuator will be provided via a common bus line, so the space limit isn't further limited by the size of the battery.</p>

<p>My thought is to use a worm drive for this (for torque) and watch for change in current/voltage (for load), but I'm not sure if that is mechanically sound. It seems like the mechanical advantage provided by the worm would make it hard to detect a change in load.</p>

<p><strong>Plan B</strong></p>

<p>I could add another sensor that would gauge the force being exerted. I'd prefer to avoid adding too many points of failure to the system, but if I did what sort of sensor would I use?</p>

<p>How should I approach this?</p>
","sensors control actuator"
"1811","Localising a robot swarm non-optically","<p>This question is further to <a href=""http://robotics.stackexchange.com/questions/1795/localizing-a-swarm-of-robots"">Localizing a swarm of robots</a>.</p>

<p>In summary: I want to create a swarm of robots which can each measure their own position in a 3x2m room with a 25cm high ceiling, to an accuracy of ±5mm. </p>

<p>There were some good answers, but most of them were optical methods. I would be interested to hear some non-optical localisation ideas, so I will impose the following further constraints:</p>

<ul>
<li>Localisation method cannot use optical means, either visible or invisible.</li>
<li>Nothing can be added to the floor or ceiling.</li>
<li>There's no appreciable gap between the top of the robots and the ceiling.</li>
<li>There are no walls, and equipment can be added around the perimeter of the room.</li>
</ul>

<p>I look forward to hearing some creative ideas.</p>
","wireless swarm"
"1813","Software for designing mechanical systems/robotic parts","<p>Which software can be used to prototype/design robot parts (mechanical parts, body, gears, etc)></p>

<p>I have some crazy idea I would like to try (quadripedal walking robot, animal-like), but I'd like to design the mechanism and test (to some degree) the mechanism in some kind of simulator before I start wasting money on parts/materials. What tool could I use for that? </p>

<p>I'm only interested in mechanical design (chassis + servo/motor placement + cogs/gears), not in electronic design. I'm not interesting in robot <em>control</em> software, because I'll be probably able to slap something like arduino onto it and program behavior I want (experienced programmer)</p>

<p><strong>Details (what I'd like to see)</strong>:</p>

<ol>
<li>Should work in 3d. I.e. finished system should be viewable in 3d.</li>
<li>I should be able to cut materials like plywood/metal, drill holes, place gears on it, etc.</li>
<li>It would be nice if it had some kind of part catalog so to place a gear/cog I wouldn't need to design it from scratch.</li>
<li>It would be nice I could test if parts can actually move. I don't need full-blown simulation, just to see if gears can turn or if they'll get stuck.</li>
<li>Not interested in electronic circuitry, just need mechanical parts, but should be able to place servos.</li>
<li>It would be nice if it could produce blueprints.</li>
<li>cheap/inexpensive, if possible.</li>
</ol>

<p>Basically, I should be able to construct robot mechanism in it (by placing/connecting parts like gears,cogs, motors, springs), or some kind of clock, and test (to some degree) if it actually works.</p>

<p>I know that I could use blender3d for that, but it wasn't exactly designed for this purpose.</p>

<p>I also heard that solidworks could be used for designing mechanical parts, but it is too expensive, especially for one-time-project. </p>

<p>Any recommendations?</p>
","software"
"1815","Roller Screw drive - axial movement instead of friction","<p>I need an equation or a some hints to solve the following problem.</p>

<p>Imagine a <a href=""http://en.wikipedia.org/wiki/Roller_screw"" rel=""nofollow"">roller screw</a> drive. I apply a torque of <code>T</code> to translative move my load mass <code>M</code>. I assume my screw has an efficiency of 90%. Now an additional axial force affects my mass in the opposite moving direction. Is this force completely transformed into torque (of course considering the efficiency) or is it possible, that my whole roller screw is moving, because it is not fixed? I just found papers/books/articles for movable slides/loads, but fixed shafts. But in my case motor and shaft are part of an osciallation system.</p>

<p>I'm not a mechanical engineer, so I'm sorry if the answer may is trivial.</p>

<p>I made a little sketch now <img src=""http://i.stack.imgur.com/70UmY.jpg"" alt=""enter image description here""></p>

<p>The process force <code>Fp</code> is pushing my mass, most of the force is transformed into a load torque <code>Tp</code> which acts against my drive torque <code>TD</code>. Some of the energy is lost by friction. The question is, if there is also a partial force <code>Tp?</code> which is affecting the bearing and therefore exciting my chassis.</p>
","movement torque differential-drive"
"1826","Why do 1000 rpm and 10 rpm DC motors cost the same?","<p>Today I was going to buy a motor online, and saw that 10 rpm and 1000 rpm DC motors cost the same. How is it possible to change the rpm without requiring any additional parts cost?</p>
","motor"
"1831","quadcopter parameters calculations for simulink model","<p>I want to make a mathematical model of quadcopter in simulink. I have studied quadcopter, although I am new and not build any flying robot before. I studied so far that I have to use four brushless DC motors PID speed control, two motors will rotate clock wise and two anti clock wise. I want to make very simple mathematical model. </p>

<p>The input of the model will be the xyz locations on 3d space, copter will always fly from 0,0,0 path. </p>

<p>So far I decided that I will increment the coordinates step by step for example if I want the next location of the to be x=10, y=10, z=10; then I will increment in these locations and input to a flight control block. </p>

<p>My question is how can I decide the speed of motors according to x,y,z next location and how to convert that speed into Yaw Pitch and Roll and finnally convert the Yaw, Pitch and Roll into X,Y,Z coordinates. </p>

<p>I need the convertion formulas that can be easily implemented into simulink. 
Please provide help thanks </p>
","quadcopter simulator"
"1835","Effective motor type for robotic arm?","<p>I am trying to build an arm that will have about 5 by 5 by maybe 7 or so centimeters of room for a rotary motor capable of lifting it. The joint will basically allow the arm to rotate in a single degree of freedom in a circle that is perpendicular to the ground.The rest of the arm will probably be around 64 centimeters long and weigh around a minimum of 9 kilograms before it lifts anything. </p>

<p>What kind of motor type would give it the best chance of lifting the arm quickly<sup>&dagger;</sup> and reasonably accurately<sup>&ddagger;</sup>? </p>

<p><sup>&dagger; Raising from straight down to out 90 degrees in around 1 to .5 seconds maximum.</sup><br>
<sup>&ddagger; At least a centimeter at the end of the arm which means probably at the very least 300 positions for the motor.</sup></p>
","motor robotic-arm"
"1837","Choosing a platform to start","<p>I'd like to start with robotics, but unfortunately I know very little about HW engineering. Moreover I used to use such languages as Python, C# and Java, and do not have much experience in C. Still I want very much to be able to program a robot, and I have very big interest in Computer Vision and AI. Are there any platforms/kits that you can buy, and with little time spent you already can program them, preferably in high-order languages?</p>

<p>I'd prefer something wheeled (something flying would also be nice, but it may be too hard to be the case for a first robot), with a camera and some additional sensors. Would be also nice to have there something, that could help to avoid obstacles, like laser distance sensor or ultra-sonic sensor. Ideally I would like to build a robot that can navigate in the room without the help of operator. I'd like to look at SLAM some time in future, but for now I just need something to get familiar with the robotics.</p>

<p>Also it should probably be not very expensive, at least not before I will be very sure that I am ready to go deeper into robotics. Something for 300-500$ would be awesome.</p>

<p>Can somebody suggest kits/platforms/tutorials/any other info?</p>
","beginner"
"1838","How Yaw, Pitch and Roll effect the flight of Quadcopter?","<p>I am doing simulation of Quadcopter in simulink. I want to know how Yaw, Pitch and Roll effect the flight of Quadcopter? and How these are different from a single rotor helicopter? 
Mainly how to change the RPM to change the x,y,z coordinates of the Quadcopter? 
Is there any Differential Equation that can convert the rpm to yaw pitch and roll? Please help.</p>
","quadcopter"
"1839","What frame of reference is used during Visual Servoing?","<p>I'm new to the whole visual servoing area.</p>

<p>I'm now reading the tutorial <a href=""http://www.irisa.fr/lagadic/pdf/2006_ieee_ram_chaumette.pdf"" rel=""nofollow"">Visual Servo Control 
Part I: Basic Approaches""</a> and I don't understand something fundamental - what information is available to the robot?</p>

<ul>
<li>Is the 3D location of the tracked features in the current frame known?</li>
<li>Is it known for the desired frame?</li>
<li>Is it known for both?</li>
</ul>

<p>If it's known for both - then what would be the best thing to do? 
Would it be to compute the current and desired 3D location and orientation of the robot, and plan an optimal path accordingly, essentially knowing everything in advance?</p>

<p>Also, in what sense could a control law (i.e translation + rotation path) be optimal for a visual servo?</p>
","localization research visual-servoing"
"1844","Stabilizing a quadcopter with optical flow","<p>My quad copter can balance itself in the air using data collected from mpu6050. With the sonar sensor, it can hover at a specific height, but it  moves on the horizontal plane in a random direction.  If i put an object below it, it will ascend to keep the distance between the sonar senor and the object.</p>

<p>Now i want to make it have the ability to hover stably. Is it possible to add a downward-facing camera to calculate the speed of optical flow in order to keep it hovering on the same point in the horizontal plane?  Could I use a forward-facing camera to stabilize its vertical speed?</p>
","sensors quadcopter cameras visual-servoing"
"1853","Digital servo shaking","<p>i need your advice if someone experienced something similar. I try using digital servo but when i tried to connect it to board by this tutorial <a href=""https://blogs.oracle.com/hinkmond/entry/connect_robot_servo_to_rpi3"" rel=""nofollow"">https://blogs.oracle.com/hinkmond/entry/connect_robot_servo_to_rpi3</a><br>
servo motor only shakes for first ten cycles but after that turns normally. I have no idea why is that but in every article i read that controlling digital servo is same as analog with no need to program them after unboxing.
Thanks for any idea</p>
","raspberry-pi servos"
"1856","Need suggestions for object recognition","<p>I am tasked with creating a system that will recognize fish pulled out of a lake. The system should be able to identify the type of species of fish. Typically, I turn to Arduino for projects like this. However, based on what I've read about image processing, it's sounding like Arduino doesn't have the processing power. Does anyone have any suggestions for development boards and cameras that can easily interface with the board?</p>

<p>I've look at this option, <a href=""http://www.finboard.org/videos/introducing-finboard?utm_campaign=Embedded%20Processing%20and%20DSP%20Newsletter%2013-Q3%20NA&amp;utm_medium=email&amp;utm_source=Eloqua"" rel=""nofollow"">http://www.finboard.org/videos/introducing-finboard?utm_campaign=Embedded%20Processing%20and%20DSP%20Newsletter%2013-Q3%20NA&amp;utm_medium=email&amp;utm_source=Eloqua</a>. It seems like it would be a nice all in one type of thing. Has anyone used anything like this?</p>

<p>Thanks!</p>
","arduino microcontroller raspberry-pi cameras"
"1857","Transducer for underwater applications (passive sonar)","<p>I'm kicking around the idea of building a small passive sonar for an autonomous submarine. I've looked through the net for parts and finding a good transducer for converting the sound underwater into an electrical impulse. After looking at parts I got into the piezoelectric materials used for doing this such as barium titanate or Lead zirconate titanate. From what I've read on the web, some of these materials are toxic. </p>

<p>My question is, are there piezoelectric materials that one could to build a sensor from scratch that does not possess the toxic qualities? Something that could preferably thrown in a pool w/ my kids and not give them or me any defects.</p>
","sensors"
"1858","Why do 3-axis accelerometers seemingly have a left-handed coordinate system?","<p>Careful inspection of page 35 (figure 58) of the <a href=""http://www.analog.com/media/en/technical-documentation/data-sheets/ADXL345.pdf"" rel=""nofollow"">ADXL345 datasheet</a> shows that under gravitational loading only, the chip uses a left-handed coordinate system.  My own experiments with this chip confirm this.  </p>

<p>I typically only use the chip to indicate the gravity vector.  So when using this chip, I simply negate the values to get a right handed coordinate system.  But this doesn't seem right.  I assume there is a logical and mathematical explanation for the left-handed coordinate system but i can't figure out what it might be. </p>

<p><img src=""http://i.stack.imgur.com/bItD6.png"" alt=""Image from ADXL345 Datasheet""></p>
","sensors imu accelerometer calibration"
"1861","question about car-like robot localization based on dead-reckoning","<p>I have a question about car-like robot localization using only dead-reckoning. Given: </p>

<ul>
<li>robot position (at current time step) in the form $\begin{bmatrix}x &amp; y &amp; \theta\end{bmatrix}$ (theta is the heading)</li>
<li>steering angle </li>
<li>distance traveled between two time steps</li>
</ul>

<p>How can I estimate the position of the robot at the next time step?</p>
","mobile-robot localization"
"1863","May I have some suggestion on inexpensive robotic arm?","<p>I want to build some simple application. I need a 5 or 6 degrees of freedom robotic arm. The arm must have feedback, so that I can control it preciously. The arm must be able to handle at least 5 lbs. And the arm would be able to work 10 hours a day.</p>

<p>My budget is USD$300 . Any suggestion?</p>
","robotic-arm"
"1864","Razor IMU Arduino interfacing","<p>I was looking into the  Razor IMU from Sparkfun, and realized that the only example code on sparkfun's website was for it was meant for hooking it up to a computer (the AHRS head tracker used a serial to usb chip). I looked on google and saw nothing but stories about how it did not work. </p>

<p>Is there any good way to hook up the  Razor IMU to an arduino uno (or any arduino without hardware support for more than one serial port), and if so does example code exist?</p>
","arduino imu"
"1874","Where I can learn algorithms or and find examples of code for controlling a rover?","<p>I am a programmer by profession and new to Robotics. I have studied ECE, so know electronics, but not very familiar with mechanical aspects of robotics. I am working on a learning project with Dagu Rover 5 platform.</p>

<p>I am trying to control the 4 DC motors with PWM and want to use the optical encoders for feedback. I am looking for some algorithms, example code in C to effectively control the rover. I know how to control the GPIO, PWM and interrupts from the processor. I am more interested in learning the algorithm that controls the motors based on this. For now, i am working on a manual robot, controlled with up/down/left/right keys. In future, I would like to add sensors, camera etc and work on autonomous aspects. Any pointers would be helpful.</p>

<p>For reference, I am working on the Raspberry Pi platform to control the rover.</p>
","mobile-robot algorithm pwm c"
"1876","Depth map with Raspberry Pi","<p>Is it possible to get two images from the Raspberry Pi camera mounted on a remote controlled bot and have them sent to a computer through Wi-Fi and process the images in the computer to generate a depth map?</p>

<p>All this is to be done in a very short time so that the robot can be helped with its locomotion without making it completely autonomous.</p>
","raspberry-pi"
"1877","Controlling bot using video and image processing","<p>I am going to start a project on controlling robots using hand gestures.
Though I have used MATLAB for this purpose earlier, I realized it is extremely slow for any practical real-time system.
I am currently planning to use OpenCV for this purpose.
I want suggestions on, if OpenCV is the best alternative, are there any other alternatives and if I use OpenCV, which language should I go for, C, C++ or Python?</p>
","real-time"
"1881","Should you learn assembly language for robotics?","<p>I ask this since assembly language is really close to the micro-controller hardware, and what micro-controller would you reccomend.</p>

<p>The bot has to search for object that I show it before and then I 'lose' the object. Note, I do not know anything about micro-controllers.</p>
","microcontroller software programming-languages"
"1884","Blender a good robotic simulator for quadcopters / swarm simulations?","<p>I'm interested in simulating Quadcopter control and Swarm co-ordinations. Was wondering if Blender or specifically <a href=""http://www.openrobots.org/wiki/morse/"">MORSE</a> was going to be good enough? According to the <a href=""http://www.openrobots.org/morse/doc/latest/what_is_morse.html#but-even-morse-has-its-limitations"">limitations of MORSE</a>, it states:</p>

<blockquote>
  <ul>
  <li><p>MORSE was never meant to be a physically accurate simulator: while we rely on a state-of-the-art physics engine (Bullet), do not expect to accurately simulate robot arm dynamics or fine grasping. Other projects are doing that much better (like OpenGrasp for grasping).</p></li>
  <li><p>While on-going efforts try to tackle this issue, we do not consider MORSE to have a good enough temporal accuracy and time synchronization capabilities for application like hybrid simulation (where some robots are simulated while others are physically operated).</p></li>
  </ul>
</blockquote>

<p>Was wondering if anyone has experience in using blender for these types of applications.</p>
","simulator"
"1895","Arduino for simple Data-Glove. Should I go with Mega or Due?","<p>First, I'm a beginner in MCU/Robotic world (been working with ATMega+CVavr, but that's all). so please bear with me.</p>

<p>I'm making a prototype data glove (Like <a href=""http://keyglove.com"" rel=""nofollow"">KeyGlove</a>, but much more simpler), it consist of:</p>

<ol>
<li>IMU sensors (MPU 9150 9DOF, all reading is fused with built in DMP) ->
Reads hand position and orientations</li>
<li>Minimum of 2 flex sensors -> Reads Figer flexion</li>
<li>MCU (well, Arduino to be specific)</li>
</ol>

<p>The sensors are plugged in to the Arduino and the reading will be filtered (e.g Low pass, Kalmann) in the Arduino before being transferred over serial to PC. The PC will then translates the data into virtual gripper to move an object (VR)  </p>

<p>Initially I planned to use UNO +  Pansenti’s MPU9150 Library in my code, then I realised the flash memory size left would be so tiny (i.e MPU9150 lib code size is ~29k, Uno has 32k).
My project is still in very early stage, so a lot things are expected to be changed and added, with so little flash memory left. I can only do so much.  </p>

<p>I immediately looking for Mega as replacement (256k flash) but I realised there is also newer Due with faster processor. </p>

<p>They cost effectively the same as for now.
.</p>

<p>My main concern here is the robustness and compatibility when:</p>

<ul>
<li><p>Designing  the HW interface to Arduino (making circuits, addding
shield)</p></li>
<li><p>Code development (available library)</p></li>
<li><p>Streaming and filtering the sensor readings <strong>(would 32 bit MCus helps, or it's overkill?)</strong></p></li>
</ul>

<p><em>I know this question might sound as too localized, but I believe a lot of projects that utilize multiple sensor reading + filtering similarly will also benefits from this discussion</em>.</p>

<p>I'll revise the question if it's needed. The main question is probably
<strong>Would 32 bit MCUs perform <em>significantly</em> better in multiple sensor reading and signal filtering compared to 8 bit MCUs?</strong></p>

<p>Or in my case.. should I go with Mega or Due?</p>
","arduino imu"
"1897","Mechanism for changing gears on a bicycle","<p>I'm looking to make an automatically shifting bicycle for my senior design project (along with some additional features TBD). However, I come from an electrical/software background; not a mechanical one. So I need to figure out a decent way to shift gears. I was thinking of leaving the gear system in place as is and using some sort of motor (servo or stepper motor with worm gears) to pull and release the wire cable as needed. However, I have some concerns with this; namely the amount of torque needed to pull the wire and finding something with enough holding torque. Perhaps my best option is to use the trigger shifters on as well and perhaps use a solenoid. My other concern (namely with the worm gear) is that it'll be too slow. </p>

<p>So I would like to pick your brains here for a moment. Thanks</p>
","motor automatic"
"1899","Making a brushless servo using Hall sensors","<p>I'd like to assemble a prototype of brushless servo system using a RC brushless motor (heavily geared down), a sensored Electronic Speed Controller for RC motors, and a microcontroller to do the PID control.</p>

<p><img src=""http://i.stack.imgur.com/anmOB.png"" alt=""Brushless motor and sensored ESC""></p>

<p>I'd add three Hall sensors around the motor, and feed those signals into the ESC and the microcontroller. The MCU will run a PID controller, and output an RC servo compatible PWM signal to the ESC.</p>

<p>Question: Is this likely to work, or will I find that the ESC is trying to be clever? I have one RC car which only switches into reverse if you double pulse the reverse signal.</p>

<p>(Note, the reason I'm trying to get this working using an off-the-shelf ESC, rather than designing my own proper one is that development time is <em>much</em> more expensive than parts cost at the moment).</p>
","servos brushless-motor esc"
"1901","Does Monte Carlo Localization need a predefined map?","<p>So I'm doing some reading on Monte Carlo Localization, and it sounds like the approach is based on using a predefined map, but I just need to make sure (because I haven't read anywhere that it absolutely needs a predefined map). I just want to make 100% sure that my understanding is correct:</p>

<p>Does it absolutely need a predefined map?</p>

<p>[maybe I need to add the below stuff as another question, but here goes nothing]
And what other localization approaches are there that don't need a predefined map? So far I've only read about SLAM (which sounds to me like a general approach instead of a specific implementation).</p>

<p>Thanks in advance!</p>
","localization slam mapping"
"1903","Is finished plywood a comparable prototyping substitute for polyoxymethylene?","<p>I'm working on a robot with a team, and we're building our robot out of acetal <a href=""http://en.wikipedia.org/wiki/Polyoxymethylene"" rel=""nofollow"">polyoxymethylene (POM)</a> (specifically, delrin) plastic. However, we'd like to prototype the robot before we build it out of POM, as POM is somewhat expensive.</p>

<p>There are several critical areas that plywood would be used in place of POM:</p>

<ul>
<li>Over sliding surfaces</li>
<li>Around gearboxes </li>
<li>Under weight stress</li>
</ul>

<p>We'd like to take into account the friction coefficients, smoothness, and rigidity of the materials in deciding whether plywood is a valid prototype substitute. The material will be 1/4"" thick.</p>

<p>What differentiates plywood from acetal POM with respect to the relevant points?</p>
","design"
"1904","Strafing of Mecanum wheels","<p>I am part of my college robotics team. We are participating in Robocon 2014 and are thinking about using mecanum wheels. We have done our research but one thing id like to clarify is: Does the number of rollers in the mecanum wheel effect its strafing? if yes then how?</p>
","wheeled-robot"
"1909","Arduino isp bootloader burning","<p>In earlier versions of the Arduino IDE there was a option to burn a the bootloader using an Arduino as the programmer. As of current there is only a burn bootloader option. Was the burn using a Arduino as a isp integrated into the still existing one, or did it disappear?</p>
","arduino"
"1910","Use computer to throw a small switch","<p>Would like a product that enables me to use my computer to throw an small DC ON / OFF switch. Seems like a stupidly simple thing to do, but for the life of me I can't seem to find such a device when I search online.</p>

<p>Is there a device floating around out there that I can order? Or is there some kind of term I should be searching for?</p>

<p>Thanks so much!</p>
","control"
"1911","How are the optical encoders used in platforms like Rover 5?","<p>I just got my rover 5 chassis with 4 motors and 4 <a href=""http://en.wikipedia.org/wiki/Rotary_encoder#Incremental_rotary_encoder"" rel=""nofollow"">quadrature encoders</a> and I am trying to utilize the optical encoders. I know the encoders generate pulse signals which can be used to measure speed and direction of the motor.</p>

<p>I want to know how 4 separate optical encoders add value for the controller of rover 5 like platform. The controller normally uses PWM to control the speed of the motor. If two motors are running at same speed then the encoder output will be same. So, why should the controller monitor all 4 encoders?</p>
","mobile-robot motor control"
"1918","Installing MORSE simulator on Ubuntu 12.04","<p>I've been having trouble installing MORSE.
I am trying to install it on Ubuntu 12.04 and on a VirtualBox with Ubuntu 13.04 (I don't need it on a VirtualBox, I'm just trying to make <em>something</em> work). 
On Ubuntu 12.04 I get the following errors at the cmake stage:</p>

<pre><code>$ cmake -DCMAKE_INSTALL_PREFIX=/home/oferb/opt/morse_build ..
-- will install python files in /home/oferb/opt/morse_build/lib/python3/dist-packages
CMake Error: The following variables are used in this project, but they are set to NOTFOUND.
Please set them or make sure they are set and tested correctly in the CMake files:
PYTHON_INCLUDE_DIR (ADVANCED)
   used as include directory in directory /home/oferb/mnt/svr_home/opt/morse
   used as include directory in directory /home/oferb/mnt/svr_home/opt/morse/src
   used as include directory in directory /home/oferb/mnt/svr_home/opt/morse/src/morse
   used as include directory in directory /home/oferb/mnt/svr_home/opt/morse/src/morse/builder
   used as include directory in directory /home/oferb/mnt/svr_home/opt/morse/src/morse/modifiers
   used as include directory in directory /home/oferb/mnt/svr_home/opt/morse/src/morse/sensors
   used as include directory in directory /home/oferb/mnt/svr_home/opt/morse/src/morse/multinode
   used as include directory in directory /home/oferb/mnt/svr_home/opt/morse/src/morse/middleware
   used as include directory in directory /home/oferb/mnt/svr_home/opt/morse/bindings
   used as include directory in directory /home/oferb/mnt/svr_home/opt/morse/testing
   used as include directory in directory /home/oferb/mnt/svr_home/opt/morse/testing/base
   used as include directory in directory /home/oferb/mnt/svr_home/opt/morse/testing/failures
   used as include directory in directory /home/oferb/mnt/svr_home/opt/morse/testing/robots/human
   used as include directory in directory /home/oferb/mnt/svr_home/opt/morse/testing/robots/segway
   used as include directory in directory /home/oferb/mnt/svr_home/opt/morse/testing/robots/pr2
   used as include directory in directory /home/oferb/mnt/svr_home/opt/morse/testing/robots/pionner3dx
</code></pre>

<p>On a fresh VMBox with Ubuntu 13.04, after 'morse check' succeeds, I try ""morse create mysim"" and get:</p>

<pre><code>adminuser@adminuser-VirtualBox:~$ morse create mysim
usage: morse [-h] [-b BASE] [--name NAME] [-c] [--reverse-color] [-g GEOM]
         [-v]
         {check,edit,run} [scene] ...
morse: error: argument mode: invalid choice: 'create' (choose from 'check', 'edit', 'run')
</code></pre>

<p>Any suggestions?</p>

<p>UPDATE:</p>

<p>I've managed to install MORSE on Ubuntu 12.04. Make sure your Blender was compiled with the same version of python (i.e python 3.2.2) that MORSE was compiled in. I used this Blender: 
<a href=""http://download.blender.org/release/Blender2.62"" rel=""nofollow"">http://download.blender.org/release/Blender2.62</a></p>
","simulator"
"1919","How can we manage stepper motor cables?","<p>I need to actuate 3 or 4 Cnc-like Nema 23 (~1N.m torque) stepper motors, I would like some cable solution to connect easily the motor to the motor driver. </p>

<p>I have not yet bought anything, I have searched various robotic stores and ebay, but did not yet found a triple (motor, cables, driver) which would be ""plug and play"". </p>

<p>As stepper motors usually have 4 to 6 cables, and there are multiple motors, manual soldering everything would be too time consuming, error prone and messy.</p>

<p>Is there a standard way to deal with cables for stepper motors ?</p>
","stepper-motor wiring"
"1924","Any ideas for a robot?","<p>I am in the FLL (First Lego League), and while we are waiting for the competitions, we want to work on a robot. Anyone have any ideas?</p>
","sensors"
"1925","Performing the proper coordinate system transformation","<p>I could use some guidance regarding a coordinate system transform problem. My situation is this: my system begins at some unknown location, which I initialize the location (x y) and orientation (roll, pitch, and yaw) all to zero. I establish a frame of reference at this point, which I call the ""local"" coordinate frame. It is fixed in the world and does not move. At system startup, the body frame is perfectly aligned with the local frame, where body +x points forward, +y to the right and +z down. The body frame is fixed to my system, and travels with the system as it moves.</p>

<p>I have an estimation routine that provides me with the x and y position, as well as the roll, pitch, and yaw of the system. Yaw is rotation about the z axis of the local frame. Pitch and roll are with respect to the body frame (I.e.,if the robot pitches up, I always get a positive value. If it rolls right, I get a positive value.)</p>

<p>How can I take the known roll and pitch values and transform them to be with respect to the local (fixed) frame?</p>
","mobile-robot kinematics"
"1927","DC motor direct loading","<p>I found many tutorials and online calculators for the selection of dc motor to drive wheel, I understood how the torque affect the driving of wheel.</p>

<p>But what happen when I change the orientation of motor and load? What is the main criteria for a dc motor to work when I want to rotate a plate which is vertically mounted on motor's shaft, when the motor is placed vertically also (as shown in the picture)?
<img src=""http://i.stack.imgur.com/aSlKL.png"" alt=""enter image description here""></p>

<p>I am not an engineering student so please provide me an answer as simple as possible.</p>
","motor torque"
"1931","Is a thrift store a good place to get a servo motor?","<p>I'm trying to learn about a very basic motor, the servo motor.  Can these be found a thrift stores like Goodwill in old toys?  Are these ""robotic"" quality?  What toys or other kinds of things would I scavenge?  All I want to do is get a motor.  After that I want an Arduino and make it ""work.""  Nothing complex.</p>
","servomotor"
"1934","Field oriented control of brushless motors","<p>If I was controlling a normal brushed motor as a servo, I would measure the motor's position, and adjust the PWM signal to control the voltage. This way I could achieve a precise velocity/position profile if my control was good enough.</p>

<pre><code>pwm_duty = CalcPID(motor_position - target_position);
</code></pre>

<p>When doing Field Oriented Control (FOC) of a brushless motor, there are two parameters I can control: The voltage angle, and the voltage magnitude. There are three things I can measure, the current angle and magnitude, and the rotor position.</p>

<p>I want to achieve a precise velocity/position profile including good control down to zero speed and reverse.</p>

<p>Question: how can I calculate the correct voltage field angle (or phase lead) and magnitude?  Do I need two PID algorithms?</p>

<pre><code>phase_lead  = CalcPID_1( ... );
voltage_mag = CalcPID_2( ... );
</code></pre>

<p>Assume I can take any reasonable measurements of the motor state, including rotor position and winding current.</p>
","servos pid brushless-motor"
"1935","How to open a sliding window?","<p>I live in an apartment that has sliding windows in it. The apartment is naturally warm because we live above a mechanical room, such that we either opened the windows or ran the air conditioning through the winter. I want to create a device than can open and close the windows in the apartment depending on temperature. The software and electronics are already figured out, I just need to figure out how to move the windows.</p>

<p><img src=""http://i.stack.imgur.com/ffas3.png"" alt=""enter image description here""></p>

<p>This is a sample of the window. It takes about 4 lbs of force to pull it open, and they only open 6 inches(since I'm 16 stories high).</p>

<p>Ultimately, I want to make this cheap enough that I could replicate it on 6 windows.</p>

<p>My first thought was a linear actuator, but most of the ones I have seen are designed for moving 100+lbs and cost several hundred dollars. Pneumatic actuators are cheaper, but I'd have to run a network of air lines and solenoids, and would have a compressor that would randomly kick in. A double winch system would be very complicated to set up and prone to failure. Lastly, I was thinking of a cheap metal gear servo(dealextreme has 15kg/cm servos for under $15.00), but it would be somewhat difficult to use a series of turnbuckles and arms to translate into 6 inches of linear movement.</p>

<p>Any help would be appreciated.</p>
","design actuator"
"1944","Arc welder for 3d printing","<p>Has anybody experimented with <a href=""http://en.wikipedia.org/wiki/Gas_metal_arc_welding"" rel=""nofollow"">GMAW</a> for additive manufacturing? The thing is, welding wire is so much cheaper than ABS or PLA filaments and, well, it is steel you are printing in, not some flimsy plastic! I imagine the arc deposition printhead would be constructed similarly to one used in plastic filament printers, except there is no need for the heating element (so, even simpler). Welding often requires fast Z speed (to finely control the arc) so i think Delta (<a href=""http://www.wired.com/design/2013/02/deltamaker-3d-printer/"" rel=""nofollow"">DeltaMaker</a>) chassis would work best. GMAW calls for some sort of inert gas to insulate heated metal from oxygen. It would make sense to seal off most of the interior of the printer and fill it with heavier than air inert gas during printing. </p>

<p>I would highly appreciate any pointers on existing 3d printer designs employing this deposition method as well as flaws in design i outlined here.</p>
","3d-printing"
"1946","What is the most appropriate SLAM algorithm for quadrotors with RGB-D camera?","<p>I have been researching on SLAM. I came across EKF SLAM which uses odometry to measure the robot's initial position in the map and as well as landmarks which helps the robot's position to be more accurate. Based on the SLAM for dummies, it has a problem of loop closure. In another journal, it was compared to fastSLAM and EKF has a big-O function of $O(K^2)$ where $K$ is the number of landmarks while fastSLAM has $O(M\log(K))$.</p>

<p>It was also said that the most promising SLAM algorithm from the journal <a href=""http://www.vision.caltech.edu/mariomu/research/papers/vSLAM-krs.pdf"" rel=""nofollow"">""The vSLAM Algorithm for Navigation in Natural Environments""</a>
 is FastSLAM 
However, the vSLAM used by an experiment done by the University of Pennsylvania is the occupancy grid SLAM.</p>

<p>I want to ask what would be the most approriate SLAM algorithm for vSLAM given an unmanned aerial vehicle like the quadrotor and RGB-D camera + IMU? Also are there any algorithm that can be extended to support co-operation?</p>
","localization slam quadcopter mapping"
"1947","I want my stepper motor to switch speed while traveling (not acceleration wise)","<p>I have this project I'm working on where I'll need the speed of the stepper motor to change set speed at a certain distance, I just can't figure out a way to do it. I'm using arduino and a stepper motor, this is the current code.</p>

<pre><code>#include &lt;AccelStepper.h&gt;

AccelStepper stepper1(AccelStepper::FULL4WIRE, 0, 1, 2, 3);

void setup()
{  
    stepper1.setMaxSpeed(200.0);
    stepper1.setAcceleration(400.0);
    stepper1.moveTo(5000);

}

void loop()
{
    // Change direction at the limits
    if (stepper1.distanceToGo() == 0)
    stepper1.moveTo(-stepper1.currentPosition());
    stepper1.run();
</code></pre>

<p>What I want it to do basically is to first <code>moveTo(2500)</code>  at the current speed 200 then after 2500 I want it to increase speed to 400. After it has moved 5000 it turns and moves back to position but that's implemented already.</p>
","arduino control stepper-motor"
"1949","In EKF-SLAM, why do we even need odometry when there is a more reliable sensor?Also, are all SLAM algorithms feature-based?","<p>In the book of SLAM for dummies, why do we even need the odometry when the robot would use the data retrieved from the laser scanner which is more accurate than odometry? Why not just rerly on the laser scanner and do away from the odometry? Is there any contribution by the odometry that the laser scanner does not have? Also, are all SLAM algorithms feature-based?</p>
","localization slam mapping"
"1954","Is it possible to use HC-SR04 ultrasonic range sensor to indicate thickness of a material","<p>The HC-SR04 is directly connected to an Arduino board with the receiver end(echo) connected to analog pin 2 and the transmitter (trigger) connected to digital pin 4.</p>

<p>I am wondering if I can use the sensor to sense the change in saturation from when object block its path. The receiver and transmitter will be positioned like this </p>

<p><img src=""http://i.stack.imgur.com/phIBB.jpg"" alt=""enter image description here""></p>

<p>The line in the middle is supposed to be a paper. I'll be using it to see the difference between one paper and two paper when they travel trough the two. </p>

<p>Now I'm not sure if this is possible but the way I see it working is kind of similar to an IR LED Arduino program connected to an LED, where when one paper passes trough the light gets a little bit weaker and with two it takes a heavier hit.</p>

<p>Is this possible?</p>
","arduino sensors"
"1955","Choosing path planning and obstacle avoidance algorithm for 2D space","<p>I am working on a 2D space where my robot needs to follow a trajectory while avoiding some obstacles.</p>

<p>I've read recently about methods for path planning as ""Vector Field Histogram"" and the ""Dynamic window approach"".</p>

<p>Is it worth to use these kind of algorithms for a 2D space or should I go with something as Potential Fields or Rapidly-Exploring Random Trees?</p>
","mobile-robot motion-planning"
"1959","Will wiring a unipolar stepper to a bipolar stepper driver decrease the holding torque?","<p>I have read that you can wire a unipolar stepper to a bipolar driver, which I have, by ignoring the two extra wires. One concern I have is whether connecting a unipolar stepper to a bipolar driver will cause it to lose torque (holding or operating)? </p>

<p>Will it be the same? Increase? </p>

<p>I've read that bipolars are more bang for your buck energy-wise, and since you can ""transform"" a unipolar stepper to a bipolar good enough that the driver will still work right, I would think that it might run more efficiently. Is this true?</p>
","torque stepper-driver stepper-motor"
"1965","What is telemetry used for?","<p>I'm pretty new to the world of UAS after a ten year holiday from RC flying.</p>

<p>I'm looking at Ardupilot and am wondering what purpose telemetry serves? Is it just to get in flight data back to a ground station or can it also be used to program the system in flight? Are there other capabilities that I am missing?</p>
","uav ardupilot"
"1966","Public training data for vehicle detectors in computer vision?","<p>This question is to anyone familiar with object (specifically vehicle) detection research.</p>

<p>I'm new to computer vision and am confused about training object detection classifiers. Specifically, the objective is vehicle detection. I've been reading through vehicle detection literature for weeks now, but I'm still a bit confused.</p>

<p>What I'm confused about is evaluation. For the evaluation of a system, the research community usually has a benchmarked dataset which can be used for testing data. But the performance of a system also depends very much on the data that was used to train it, no? </p>

<p>So aren't there any <em>training datasets</em> out there, too? That would make for far more uniform method comparisons. I seem to keep finding papers using benchmarked datasets for evaluation, but making no mention of where they got their training data from.</p>
","computer-vision"
"1974","Autonomous car steering using IR sensors","<p>I want to steer a RC car in a straight line.The car has 4 sharp IR sensors on each corner of the car to help it steer the corridor.The corridor is irregular and looks something similar to the picture below.</p>

<p>The car needs to be stay exactly at the middle(shown by lighter line) and take help of the IR sensors to correct its path.</p>

<p>The car has a servo on the front wheel to steer and another that controls the speed.</p>

<p>I tried running it using a algorithm where it summed the values on each side of the car and took the difference.THe difference was then fed to a pid control the output of which went to steer the car.The greater the value from the pid (on either sides), the greater the value of the steering angle till it reaches the middle.</p>

<p>It works for the part where the walls are at similar distance from the center and even then it oscillates a lot around the center and fails miserably around the bumps in the corridor.</p>

<p>I need to make changes to the algorithm and need some help in steering me  in the right direction.</p>

<p>The IR sensors are too finicky and is there a way to filter out the noise and make the readings more stable?</p>

<p>Any help regarding the changes that needs to be implemented is much appreciated.</p>

<p>Currently the car only uses 4 IR sensors to guide.I can also use 2 ultrasonic sensors.</p>

<p><img src=""http://i.stack.imgur.com/sXdUT.png"" alt=""enter image description here""></p>
","mobile-robot sensors"
"1976","Usage of Multibeam 2D Imaging Sonar for AUVs, testing them in the pool environment","<p>I belong to an AUV team at my university. We are planning to have a Multibeam 2D Imaging Sonar (the Blueview P900) for our AUV to detect obstacles underwater.</p>

<p>I have the following questions to ask on the feasibility of testing/implementing such sonars on AUVs.</p>

<ol>
<li><p>As we know that these multibeam sonars have multiple reflections arriving at different times from various surfaces while testing in a pool, is there any recommended way to filter these noises in the image obtained from the sonar pings?</p></li>
<li><p>Are such sonars in use/test by any other team/organization anywhere else who do pool testing other than a ocean/reservoir testing where multiple reflections are almost zero except the reflections from the obstacle(s)?</p></li>
<li><p>Also i would like to know the recommended image processing algorithms that can be implemented/used to detect obstacles from the sonar images.</p></li>
</ol>
","sonar"
"1978","Accelerometers error (BMA020 and BMA180)","<p>Recently I am working with two accelerometers: BMA020 and BMA180. I will try to explain my problem using BMA020 as example because it is less accurate therefore problem is more visible. When I hold my Acc in neutral position I get correct average result: -1G. Now I turn my Acc upside down but this time my average result is +0,91G. The same problem occurs for other axis. For BMA180 problem is less visible (-1G in normal position and +0.98G upside down). Do you know why accelerometer behaves like this ?</p>
","accelerometer"
"1979","How to retrieve parameters from mavlink .tlog using pymavlink?","<p>I've been able to use <a href=""https://github.com/mavlink/pymavlink/blob/master/mavutil.py"" rel=""nofollow"">pymavlink.mavutil</a> to read telemetry from a .tlog created by <a href=""https://github.com/diydrones/MissionPlanner"" rel=""nofollow"">MissionPlanner</a>.</p>

<p>To do this, I create a <code>mavlogfile</code> like this:</p>

<p><code>mlog = mavutil.mavlink_connection('mylogfile.tlog')</code></p>

<p>Now I want to read the flight parameters (settings) from the .tlog . The method mavlogfile.param_fetch_all() appears to be designed only to work with a live telemetry link rather than a log. It sends a parameter request command, which obviously has no result when you are linked to a log rather than an actual aircraft.</p>

<p>I know the parameters are in the .tlog... how do I get them out?</p>
","python ardupilot"
"1981","1 cm accuracy radio rangefinder?","<p>I need to track a point in space. The point is less than 2&nbsp;m away, it has to be passive, no batteries, and no charging.</p>

<p>I don't always have a line of sight. I need to pinpoint it to about a centimeter. I need to sample it at a frequency of 10&nbsp;Hz or more.</p>

<p>Can it be done at all? Does such a solution exist?</p>
","sensors"
"1983","Can you have a career in robotics if you hate mechanics?","<p>I'm a first year electronics engineering student. I love almost all the aspects of robotics - the electronics, algorithms, control theory etc. I can't stand the mechanical aspect of robotics though.</p>

<p>Can I have a fulfilling career in Robotics if I hate mechanics but love all other parts of robotics? I'm ready to learn mechanics if I absolutely have to, but would strongly prefer not to learn anymore than the absolute basics.</p>

<p>Thanks.</p>
","software electronics mechanism"
"1985","SLAM without landmarks?","<p>First, is it possible to build map without landmarks for a robot in 2D? Let's say we have an aisle surrounded  by two walls. The robot moves in this environment. Now is it feasible to build such a SLAM problem? Or landmarks must be available to do so?</p>
","mobile-robot slam"
"1991","Are time-of-flight cameras like the swissranger affected by outdoor fog?","<p>I'm looking to build an outdoor robot and I need to know if <a href=""http://en.wikipedia.org/wiki/Time-of-flight_camera"" rel=""nofollow"">time-of-flight cameras</a> like the <a href=""http://www.mesa-imaging.ch/swissranger4500.php"" rel=""nofollow"">SwissRanger™ SR4500</a> work in fog, does anybody have some experiences on that?</p>
","mobile-robot cameras"
"1992","Jacobian of the observation model?","<p>The state vector is 
$$ \textbf{X} = \begin{bmatrix} x \\ y \\ v_{x} \\ v_{y} \end{bmatrix}$$</p>

<p>transition function is 
$$
\textbf{X}_{k} = f(\textbf{X}_{k-1}, \Delta t) =
\begin{cases} 
x_{k-1} + v_{xk} \Delta t \\
y_{k-1} + v_{yk} \Delta t 
 \end{cases} 
$$</p>

<p>$z_{b} = atan2(y, x)$ and
$z_{r} = \sqrt{ x^{2} + y^{2}}$</p>

<p>the Jacobian of the observation model:
$$
\frac{\partial h}{\partial x} =
 \begin{bmatrix} \frac{-y}{x^{2}+y^{2}} &amp; \frac{1}{x(1+(\frac{y}{x})^{2})} &amp; 0 &amp; 0 \\
                 \frac{x}{\sqrt{ x^{2} + y^{2}}} &amp; \frac{y}{\sqrt{ x^{2} + y^{2}}} &amp; 0 &amp; 0 
 \end{bmatrix}
$$</p>

<p>My question is how  the Jacobian of the observation model has been obtained? and why it is 2X4?</p>

<p>the model from <a href=""http://www.mrpt.org/Kalman_Filters"" rel=""nofollow"">Kalman filter</a>.</p>
","kalman-filter"
"1995","Pre-built PID motor controller","<p>I lead a university robotics team that needs PID controllers for four drive motors and two additional motors that are used in a secondary system. I would strongly prefer to buy pre-built PID controllers that provide just about any reasonable interface for setting PID constants, motor velocity and direction, as the controllers are not remotely central to the difficult, interesting problems we're trying to solve. To my astonishment, the Internet doesn't seem to be saturated with such controllers (talk about reinventing the wheel - hundreds of tutorials but almost no pre-built solutions! Did Willow Garage build their own PID controller for the PR2?!). </p>

<p>Does anyone have recommendations/experience, preferably pointers to such controllers? </p>

<p>I've Googled around quite a bit, and so far <a href=""https://github.com/Exadler/DualMotorControlCape"" rel=""nofollow"">this</a> is the best option I've found. It's a cape for a BeagleBone Black (which is the board we're using). The problem is that the Python library is not finished - it resets PID constants at every call, it doesn't support changing the direction of the motor, and it seems to only support setting motor power, not velocity, which gives me the impression that it's not actually using the PID controller at all.</p>

<p>Additional details:</p>

<ul>
<li>The stall current of our <a href=""http://www.pololu.com/catalog/product/2271"" rel=""nofollow"">drive motors</a> is 6A. They are brushless DC motors with quadrature encoders. The secondary motors are much smaller, and we're building our own encoders for them.</li>
<li>Our code base is in Python, and we're running on a BeagleBone Black using the latest Debian image from Robert Nelson (that guy's awesome!). Our batteries provide 14.8V, and we already have 3.3V and 5V rails.</li>
<li>Our robot is fairly small, about 1x1x2 feet, and weighs about 9 pounds. This info is meant to give perspective with regard to scale.</li>
<li>$350 or so is the comfortable top range of what we could spend to get all 6 motors PID-controlled.</li>
</ul>

<p>Any help would be greatly appreciated!</p>
","motor pid brushless-motor"
"1997","Is there a way to determine which degrees of freedom are lost in a robot at a singularity position by looking at the jacobian?","<p>For a 6DoF robot with all revolute joints the Jacobian is given by:
$$
\mathbf{J} = 
\begin{bmatrix}
\hat{z_0} \times (\vec{o_6}-\vec{o_0}) &amp; \ldots &amp; \hat{z_5} \times (\vec{o_6}-\vec{o_5})\\
\hat{z_0} &amp; \ldots &amp; \hat{z_5}
\end{bmatrix}
$$
where $z_i$ is the unit z axis of joint $i+1$(using DH params), $o_i$ is the origin of the coordinate frame connected to joint $i+1$, and $o_6$ is the origin of the end effector.  The jacobian matrix is the relationship between the Cartesian velocity vector and the joint velocity vector:
$$
\dot{\mathbf{X}}=
\begin{bmatrix}
\dot{x}\\
\dot{y}\\
\dot{z}\\
\dot{r_x}\\
\dot{r_y}\\
\dot{r_z}
\end{bmatrix}
=
\mathbf{J}
\begin{bmatrix}
\dot{\theta_1}\\
\dot{\theta_2}\\
\dot{\theta_3}\\
\dot{\theta_4}\\
\dot{\theta_5}\\
\dot{\theta_6}\\
\end{bmatrix}
=
\mathbf{J}\dot{\mathbf{\Theta}}
$$</p>

<p>Here is a singularity position of a Staubli TX90XL 6DoF robot:</p>

<p><img src=""http://i.imgur.com/Ikjas0J.png"" alt=""robot with joint 4 and joint 6 aligned pointed down""></p>

<p>$$
\mathbf{J} = 
\begin{bmatrix}
          -50     &amp;    -425     &amp;    -750      &amp;      0     &amp;    -100      &amp;      0\\
       612.92     &amp;       0     &amp;       0      &amp;      0     &amp;       0      &amp;      0\\
            0     &amp; -562.92     &amp;       0      &amp;      0     &amp;       0      &amp;      0\\
            0     &amp;       0     &amp;       0      &amp;      0     &amp;       0      &amp;      0\\
            0     &amp;       1     &amp;       1      &amp;      0     &amp;       1      &amp;      0\\
            1     &amp;       0     &amp;       0      &amp;     -1     &amp;       0      &amp;     -1
\end{bmatrix}
$$</p>

<p>You can easily see that the 4th row corresponding to $\dot{r_x}$ is all zeros, which is exactly the lost degree of freedom in this position.</p>

<p>However, other cases are not so straightforward.</p>

<p><img src=""http://i.imgur.com/cAUoVq3.png"" alt=""robot with joint 4 and joint 6 aligned pointed at an angle"">
$$
\mathbf{J} = 
\begin{bmatrix}
          -50   &amp;   -324.52  &amp;    -649.52   &amp;         0   &amp;   -86.603   &amp;         0\\
       987.92   &amp;         0  &amp;          0   &amp;         0   &amp;         0   &amp;         0\\
            0   &amp;   -937.92  &amp;       -375   &amp;         0   &amp;       -50   &amp;         0\\
            0   &amp;         0  &amp;          0   &amp;       0.5   &amp;         0   &amp;       0.5\\
            0   &amp;         1  &amp;          1   &amp;         0   &amp;         1   &amp;         0\\
            1   &amp;         0  &amp;          0   &amp;    -0.866   &amp;         0   &amp;    -0.866
\end{bmatrix}
$$</p>

<p>Here you can clearly see that joint 4 and joint 6 are aligned because the 4th and 6th columns are the same.  But it's not clear which Cartesian degree of freedom is lost (it should be a rotation about the end effector's x axis in red).</p>

<p>Even less straightforward are singularities at workspace limits.</p>

<p><img src=""http://i.imgur.com/ykxmMUH.png"" alt=""robot at workspace limit with no aligned joint axes""></p>

<p>$$
\mathbf{J} = 
\begin{bmatrix}
          -50     &amp;     650   &amp;       325  &amp;          0    &amp;        0     &amp;       0\\
       1275.8     &amp;       0   &amp;         0  &amp;         50    &amp;        0     &amp;       0\\
            0     &amp; -1225.8   &amp;   -662.92  &amp;          0    &amp;     -100     &amp;       0\\
            0     &amp;       0   &amp;         0  &amp;    0.86603    &amp;        0     &amp;       1\\
            0     &amp;       1   &amp;         1  &amp;          0    &amp;        1     &amp;       0\\
            1     &amp;       0   &amp;         0  &amp;        0.5    &amp;        0     &amp;       0
\end{bmatrix}
$$</p>

<p>In this case, the robot is able to rotate $\dot{-r_y}$ but not $\dot{+r_y}$.  There are no rows full of zeros, or equal columns, or any clear linearly dependent columns/rows.  </p>

<p>Is there a way to determine which degrees of freedom are lost by looking at the jacobian?</p>
","robotic-arm inverse-kinematics industrial-robot"
"2000","Maintaining positive-definite property for covariance in an unscented Kalman filter update","<p>I have an unscented Kalman filter (UKF) that tracks the state of a robot. The state vector has 12 variables. Each time I carry out a prediction step, my transfer function (naturally) acts on the entire state. However, my sensors provide measurements of different parts of the robot's state, so I may get roll, pitch, yaw and their respective velocities in one measurement, and then linear velocity in another.</p>

<p>My approach to handling this so far has been to simply create sub-matrices for the covariance, carry out my standard UKF update equations, and then stick the resulting values back into the full covariance matrix. However, after a few updates, the UKF yells at me for trying to pass a matrix that isn't positive-definite into a Cholesky Decomposition function. Clearly the covariance is losing its positive-definite properties, and I'm guessing it has to do with my attempts to update subsets of the full covariance matrix. </p>

<p>As an example taken from an actual log file, the following matrix (after the UKF prediction step) is positive-definite:</p>

<pre><code>   1.1969            0            0            0            0            0      0.11567            0            0            0            0            0
        0       1.9682            0            0            0            0            0      0.98395            0            0            0            0
        0            0       1.9682            0            0            0            0            0      0.98395            0            0            0
        0            0            0       1.9682            0            0            0            0            0      0.98395            0            0
        0            0            0            0       1.9682            0            0            0            0            0      0.98395            0
        0            0            0            0            0       1.9682            0            0            0            0            0      0.98395
  0.11567            0            0            0            0            0      0.01468            0            0            0            0            0
        0      0.98395            0            0            0            0            0            1            0            0            0            0
        0            0      0.98395            0            0            0            0            0            1            0            0            0
        0            0            0      0.98395            0            0            0            0            0            1            0            0
        0            0            0            0      0.98395            0            0            0            0            0            1            0
        0            0            0            0            0      0.98395            0            0            0            0            0            1
</code></pre>

<p>However, after processing the correction for one variable (in this case, linear X velocity), the matrix becomes:</p>

<pre><code>   1.1969            0            0            0            0            0      0.11567            0            0            0            0            0
        0       1.9682            0            0            0            0            0      0.98395            0            0            0            0
        0            0       1.9682            0            0            0            0            0      0.98395            0            0            0
        0            0            0       1.9682            0            0            0            0            0      0.98395            0            0
        0            0            0            0       1.9682            0            0            0            0            0      0.98395            0
        0            0            0            0            0       1.9682            0            0            0            0            0      0.98395
  0.11567            0            0            0            0            0         0.01            0            0            0            0            0
        0      0.98395            0            0            0            0            0            1            0            0            0            0
        0            0      0.98395            0            0            0            0            0            1            0            0            0
        0            0            0      0.98395            0            0            0            0            0            1            0            0
        0            0            0            0      0.98395            0            0            0            0            0            1            0
        0            0            0            0            0      0.98395            0            0            0            0            0            1
</code></pre>

<p>The difference between the two matrices above is </p>

<pre><code>        0            0            0            0            0            0            0            0            0            0            0            0
        0            0            0            0            0            0            0            0            0            0            0            0
        0            0            0            0            0            0            0            0            0            0            0            0
        0            0            0            0            0            0            0            0            0            0            0            0
        0            0            0            0            0            0            0            0            0            0            0            0
        0            0            0            0            0            0            0            0            0            0            0            0
        0            0            0            0            0            0     -0.00468            0            0            0            0            0
        0            0            0            0            0            0            0            0            0            0            0            0
        0            0            0            0            0            0            0            0            0            0            0            0
        0            0            0            0            0            0            0            0            0            0            0            0
        0            0            0            0            0            0            0            0            0            0            0            0
        0            0            0            0            0            0            0            0            0            0            0            0
</code></pre>

<p>As you can see, the only difference between the two is the value in the location of the variance of linear X velocity, which is the measurement I just processed. This difference is enough to ""break"" my covariance matrix.</p>

<p>I have two questions:</p>

<ol>
<li><p>Updating a subset of the filter doesn't appear to be the right way to go about things. Is there a better solution?</p></li>
<li><p>Alternatively, am I missing a step that would keep my covariance matrix as positive-definite?</p></li>
</ol>

<p>Thanks!</p>

<p>EDIT:</p>

<p>It looks like I'm not properly placing the values back into the original covariance matrix. Simply copying the values back isn't sufficient. I need to track the correlation coefficients for the covariance matrix, and make sure that when I update a variance value, I update all the values in its row/column to maintain the correlation coefficient value. I have to do some more testing to verify that this is my issue, but some initial analysis in Matlab suggests that it is. If I'm correct, I'll answer my own question.</p>

<p>EDIT 2:</p>

<p>Given the response below and after trying it, I can see that my original edit idea won't fly. However, I have one more question:</p>

<p>As this is a UKF, I don't actually have Jacobian matrices. I think I see how I would make it work within the UKF update equations, but even in an EKF - and I ask because I have one of those as well - my state-to-measurement function $h$ is going to end up being the identity matrix, as I am directly measuring my state variables. In the case, I take it my ""Jacobian"" would just be an $m \times n$ matrix with ones in the $(i, i)$ location, where $i$ is the index of the measured values in the measurement vector?</p>
","kalman-filter"
"2009","EKF partial state update question","<p>This is a follow up to </p>

<p><a href=""http://robotics.stackexchange.com/questions/2000/maintaining-positive-definite-property-for-covariance-in-an-unscented-kalman-fil/2004#2004"">Maintaining positive-definite property for covariance in an unscented Kalman filter update</a></p>

<p>...but it's deserving of its own question, I think.</p>

<p>I am processing measurements in my EKF for a subset of the variables in my state. My state vector is of cardinality 12. I am directly measuring my state variables, which means my state-to-measurement function $h$ is the identity. I am trying to update the first two variables in my state vector, which are the x and y position of my robot. My Kalman update matrices currently look like this:</p>

<p>State $x$ (just test values):
$$
\left(\begin{array}{ccc}
0.4018 &amp; 0.0760
\end{array} \right) 
$$</p>

<p>Covariance matrix $P$ (pulled from log file): 
$$
\left(\begin{array}{ccc}
0.1015 &amp; -0.0137 &amp; -0.2900 &amp; 0 &amp; 0 &amp; 0 &amp; 0.0195 &amp; 0.0233 &amp; 0.1004 &amp; 0 &amp; 0 &amp; 0 \\
-0.0137 &amp; 0.5825 &amp; -0.0107 &amp; 0 &amp; 0 &amp; 0 &amp; 0.0002 &amp; -0.7626 &amp; -0.0165 &amp; 0 &amp; 0 &amp; 0 \\
-0.2900 &amp; -0.0107 &amp; 9.6257 &amp; 0 &amp; 0 &amp; 0 &amp; 0.0015 &amp; 0.0778 &amp; -2.9359 &amp; 0 &amp; 0 &amp; 0 \\
0 &amp; 0 &amp; 0 &amp; 0.0100 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 \\
0 &amp; 0 &amp; 0 &amp; 0 &amp; 0.0100 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 \\
0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0.0100 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 \\
0.0195 &amp; 0.0002 &amp; 0.0015 &amp; 0 &amp; 0 &amp; 0 &amp; 0.0100 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 \\
0.0233 &amp; -0.7626 &amp; 0.0778 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 1.0000 &amp; 0 &amp; 0 &amp; 0 &amp; 0 \\
0.1004 &amp; -0.0165 &amp; -2.9359 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 1.0000 &amp; 0 &amp; 0 &amp; 0 \\
0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0.0100 &amp; 0 &amp; 0 \\
0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0.0100 &amp; 0 \\
0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0.0100 \\
\end{array} \right) 
$$</p>

<p>Measurement $z$ (just test values):
$$
\left(\begin{array}{ccc}
2 &amp; 2
\end{array} \right) 
$$</p>

<p>""Jacobean"" $J$:
$$
\left(\begin{array}{ccc}
1 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 \\
0 &amp; 1 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 \\
\end{array} \right) $$</p>

<p>Measurement covariance $R$ (just test values):
$$
\left(\begin{array}{ccc}
5 &amp; 0 \\
0 &amp; 5 \\
\end{array} \right) $$</p>

<p>Kalman gain $K = PJ^T(JPJ^T + R)^{-1}$:</p>

<p>$$
\left(\begin{array}{ccc}
0.0199 &amp; -0.0024 \\
-0.0024 &amp; 0.1043 \\
-0.0569 &amp; -0.0021 \\
0 &amp; 0 \\
0 &amp; 0 \\
0 &amp; 0 \\
0.0038 &amp; 0.0000 \\
0.0042 &amp; -0.1366 \\
0.0197 &amp; -0.0029 \\
0 &amp; 0 \\
0 &amp; 0 \\
0 &amp; 0 \\
\end{array} \right) $$</p>

<p>$K$ is 12x2, meaning that my innovation - and therefore both measurement and current state - would need to be 2x1 in order to have a 12x1 result to add to the current full state:</p>

<p>$x' = x + K(z - h(x_s))$</p>

<p>where $x_s$ is a vector containing only the parts of the full state vector that I am measuring. </p>

<p>Here's my question: $K(z - h(x_s))$ yields</p>

<p>$$
\left(\begin{array}{ccc}
    0.0272 \\
    0.1969 \\
   -0.0948 \\
         0 \\
         0 \\
         0 \\
    0.0062 \\
   -0.2561 \\
    0.0258 \\
         0 \\
         0 \\
         0 \\
\end{array} \right) $$</p>

<p>Does it make sense that this vector, which I will add to the current state, has non-zero values in positions other that 1 and 2 (the x and y positions of my robot)? The other non-zero locations correspond to the robot's z location, and the x, y, and z velocities. It seems strange to me that a measurement of x and y should yield changes to other variables in the state vector. Am I incorrect in this assumption?</p>

<p>Incidentally, the covariance update works very well with the Jacobean in this form, and maintains its positive-definite property.</p>
","kalman-filter"
"2011","How to calculate the right and left speed for a tank-like rover?","<p>I am trying to control the <a href=""https://www.sparkfun.com/products/10336"" rel=""nofollow"">Rover 5 robot</a> using an Android app with a touch-based joystick control in the app UI. I want to calculate the speed of the left and right motors in the rover when joystick is moved.</p>

<p>From the joystick, I get two values, pan and tilt. I convert them into the <a href=""http://en.wikipedia.org/wiki/Polar_coordinate_system"" rel=""nofollow"">polar coordinate system</a> with <code>r</code> and <code>theta</code>. Where r ranges from 0 to 100 and theta from 0 to 360. I want to derive an equation which can convert the <strong><code>(r, theta)</code></strong> to <strong><code>(left_speed, right_speed)</code></strong> for rover. The speed values also are in the [0;100] range.</p>

<p>Now, here is what I have figured out till now. For any value of <code>r</code>, <br></p>

<p>If <code>theta = 0</code> then <code>left_speed = r, right_speed = -r</code> (turning right on spot) <br>
If <code>theta = 90</code> then <code>left_speed = r, right_speed = r</code> (moving forward at speed r) <br>
If <code>theta = 180</code> then <code>left_speed = -r, right_speed = r</code> (turning left on spot) <br>
If <code>theta = 270</code> then <code>left_speed = -r, right_speed = -r</code> (moving backwards at speed r) <br></p>

<p>For other values, I want it moving and turning simultaneously. For example,</p>

<p>If <code>theta = 45</code> then <code>left_speed = alpha*r, right_speed = beta*r</code> (moving forward while turning right) <br></p>

<p>So, basically for any <code>(r, theta)</code>, I can set speeds as,</p>

<p><code>(left_speed, right_speed) = (alpha*r, beta*r)</code></p>

<p>I need to formulate an equation where I can generalize all these cases by finding <code>alpha</code> and <code>beta</code> based on <code>theta</code>.</p>

<p>How can I do this? Is there is any existing work I can refer to?</p>
","control kinematics movement"
"2018","Arduino Uno R3 or Roboduino ATMega168 or Arduino Mega 2560 R3 which board is better for small robots","<p>I am new in robotics. May be this question looks like too naive but i want to know which is a better board among Arduino Uno R3 or Roboduino ATMega168 or Arduino Mega 2560 R3 for my purpose mention below - </p>

<ol>
<li>A simple robot with wheels, can move around.</li>
<li>Can have IR sensors and camera.</li>
<li>Is powerful enough to deal with computer vision computations.</li>
</ol>

<p>Arduino Mega 2560 R3 looks more powerful than the other too, I just want to know if my purpose can be solved with other two boards too?</p>

<p>Thanks</p>
","arduino"
"2021","Telemetry with Ardupilot 2.6","<p>I'm using the telemetry kit from <a href=""http://store.3drobotics.com/products/3dr-radio"" rel=""nofollow"">3DR robotics</a> (433MHz) to interface with Ardupilot Mega 2.6, controlling a quadcopter. The <a href=""http://planner.ardupilot.com/"" rel=""nofollow"">Mission Planner</a> (v1.2.84) by Michael Oborne works well with the telemetry kit, transmitting flight data (IMU, compass, GPS etc.) from the quadcopter to the GCS and displaying them in their GUI.</p>

<p>However, I would like to see the same data in the hyperterminal (windows system). The radio receiver on the GCS connects to my PC through a USB drive. I have tried calling the remote radio station using all possible Baud rates, starting from 110 to 921600 (including 57600). I've set the <strong>data bits to 8</strong> and <strong>stop bits to 1</strong>. <strong>'None'</strong> for <strong>parity</strong> and <strong>flow control</strong>.</p>

<p>However, all that I ever get on my terminal is either gibberish or no data at all. I also tried burning this <a href=""http://vps.oborne.me/3drradioconfig.zip"" rel=""nofollow"">software</a> to the radio receiver and tried using AT commands on the radio. </p>

<p>It connects OK with '+++', but keeps returning error for AT1, ATT etc.</p>

<p>Please give me an idea about how to get flight data at the hyperterminal.</p>

<p>Thanks.</p>
","quadcopter ardupilot"
"2022","How will the currently evaluated computer technology influence robotics and embedded systems in the forseeable future?","<p>This is my first question on this site, might be a little subjective :)</p>

<p>There is an ongoing process of many cool cyclonic changes of technology in the electronics and software industry.</p>

<p><strong>Concurrency and Parallelism</strong></p>

<p>What will be the implications of GPGPU, Multi-core  and ""programming in the large"" model in the specific case of embedded software, and how will it influence the styles and conventions in the community?</p>

<p>Single board multicore hardware like soon to be released Parallela can be an example?</p>

<p><strong>Programming language research</strong></p>

<p>The results have been excellent. Functional Programming has supposedly started reaching the masses. It was late night previous weekend when I briefly skimmed through an example of functional reactive programming to solve real time problems. AI people are also suggesting that we should be programming our robots in Declarative Domain Specific languages soon. It would be nice to know the implications on the robotics community. </p>

<p>There has been a tremendous growth of frameworks like ROS and Urbi too!! That should be the region to look upon.. </p>

<p>Most of the robotics, embedded and high performance AI codebase directly depends on C/C++ , and though languages like Rust and D are bubbling up, wouldn't it take massive amount of time to adopt the new languages, if ever adaptation begins? </p>

<p><strong>AI</strong></p>

<p>Correct me, but it seems like a lot of time has passed and there are not many major production results from the AI community. I've heard about cognitive architectures of old like ACT-R and 4CAPS. They seem to be in hibernation mode! </p>

<p>There seems to be a lot of work lately on otherwise intelligent systems (solved problems) like Computer vision and Data mining, but these problems cater to supercomputing and industrial crowd more. Could there be any possible shift towards low powered systems soon?</p>

<p>Thanks</p>
","software artificial-intelligence programming-languages beginner embedded-systems"
"2024","Roboticize an old netbook?","<p>I have an old beat-up netbook that is currently collecting dust. I've also only taken stuff apart, without having to worry about putting it back together, so please bear with my possibly stupid questions. </p>

<p>a) I imagine it's possible to wire this baby up to servos, breadboards, and all that good stuff. Am I correct? </p>

<p>b) I'd like to start with some simple Raspberry Pi-like projects (think automating my irrigation system, feeding the dog from work, etc). Obviously barring the energy expenditure, wouldn't a netbook be more apt than a Raspberry Pi for handling this type of thing?</p>

<p>c) I have basic Python experience, but I wouldn't mind picking up more as I go. Would that be sufficient?</p>

<p>Cheers!</p>
","raspberry-pi python"
"2027","ROS AMCL does not need odometry data?","<p>I'm reading <a href=""http://wiki.ros.org/amcl"" rel=""nofollow"">amcl</a> document on ROS Wiki. In its subscribed topics there is not odometry topic, why? It works only with laser?</p>

<p><strong>Subscribed Topics:</strong> (From ROS Wiki)</p>

<p>scan (sensor_msgs/LaserScan)</p>

<p>tf (tf/tfMessage)</p>

<p>initialpose (geometry_msgs/PoseWithCovarianceStamped)</p>

<p>map (nav_msgs/OccupancyGrid)</p>

<p>And my next question is how can I use <code>amcl</code> in <code>Gazebo</code> simulator for turtlebot? Any tutorial available?</p>
","slam ros navigation"
"2028","Is it possible to run multiple loops at the same time? (Arduino)","<p>I've got a code where I have a motor running back and forth and buttons connected to a scanner, when I press the buttons it causes the motor to stop  and over rides it. I would like them to run parallel to each other so the codes don't interrupt each other.</p>

<p>Here is my code</p>

<pre><code>#include &lt;AccelStepper.h&gt;

// Define some steppers and the pins they will use

AccelStepper stepper1(AccelStepper::FULL2WIRE, 2, 3);

const int buttonPin = 4;
const int button2Pin = 14;
const int pulseopto1 = 9;
const int startScan = 11;

int buttonState = 0;
long previousMillis = 0;
long interval = 5;

void setup()
{
    pinMode(buttonPin, INPUT);
    pinMode(button2Pin, INPUT);
    pinMode(pulseopto1, OUTPUT);
    pinMode(startScan, OUTPUT); 
    stepper1.setMaxSpeed(40000.0);
    stepper1.setAcceleration(100000.0);
    stepper1.moveTo(25000);
}

void loop()
{
    buttonState = digitalRead(buttonPin);

    if (buttonState == LOW)
    {
        digitalWrite(startScan, HIGH);
    }
    else (buttonState == HIGH);
    {
        digitalWrite(startScan, LOW);
    }

    {
        buttonState = digitalRead(button2Pin);

        if (buttonState == LOW)
        {
            // turn LED on:
            digitalWrite(pulseopto1, HIGH);
            delay(5);
            digitalWrite(pulseopto1, LOW);
            delay(5);
        }
        else
        {
            // turn LED off:
            digitalWrite(pulseopto1, LOW);
        }
    }

    // Change direction at the limits
    if (stepper1.distanceToGo() == 0)
    {
        stepper1.moveTo(-stepper1.currentPosition());
    }

    stepper1.run();
}
</code></pre>
","arduino stepper-motor c"
"2031","Calculate covariance matrix from x,y,z data","<p>In <strong>ROS</strong> I've recorded a bag file from a custom robot (in real world) that does not provide covariance matrix and I want to use <code>/odom</code> to feed an <strong>EKF</strong>, but covariance matrix is 0. How can I calculate it?</p>

<p><strong>Note</strong>:
Covariance matrix is needed by EKF to estimate position.</p>

<p>It's a sample of <code>/odom</code>:</p>

<pre><code>pose: 
  pose: 
    position: 
      x: 0.082
      y: 0.507
      z: 0.0
    orientation: 
      x: 0.0
      y: 0.0
      z: -0.789272088731
      w: 0.614043622188
  covariance: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
twist: 
  twist: 
    linear: 
      x: 0.104
      y: 0.0
      z: 0.0
    angular: 
      x: 0.0
      y: 0.0
      z: 0.0663225115758
  covariance: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
</code></pre>
","localization ros odometry"
"2032","Rocker bogie suspension stability","<p>From the designs usually shown of rocker bogie systems, the whole weight of platform seems to be supported by only one rod, be it differential bar or gear. Isn't this a bit unstable system because if we have arms of rover at one end we will have a high torque about that rod?</p>

<p>Is my understanding such rocker bogie systems correct? If so, are there any solutions to this problem which don't sacrificing the functionality of rover?</p>

<p>To clarify, I want to know how rovers like curiosity are designed so as to balance such a heavy platform with a differential bar mechanism. I am trying to make a small rocker bogie myself and I want to avoid this anticipated problem.</p>
","mobile-robot design wheeled-robot"
"2034","Cheapest 3D printer for gears?","<p>It would be incredibly useful if I could print my own gearing solutions, <em>even if I have to print the gears one at a time.</em>  However, I do not know how the market's cheapest printers will accommodate this task.</p>

<p>--The gears need be 2-3 inches in diameter, and they will bear only a very light load (much less than 1 foot pound), so the material need not be strong or machinable. </p>

<p>--The tolerances need only be sufficient for the teeth to mate robustly, preventing any hang.  <em>Unfortunately, I do not have a sense of what tolerances will allow gears to mate properly.</em></p>

<p>--(If the machine is precise enough to print a hole to statically mate with a shaft of a specified dimensions due to friction, excellent.  If not, I can probably improvise a tiny shaft hole with adhesive.)</p>

<p>--Because this may be used in close proximity to pavement, a melting temperature in excess of 100F is desirable but not required.  </p>

<p>--Because any given element will interact kinetically only with other elements that have also been 3D printed (except a metallic shaft), compatibility with external resources is not required.  </p>

<p>I would be grateful to anyone who could shed some light on this issue!</p>
","wheeled-robot"
"2036","Making high CAN baud rates work","<p>I'm dealing with a board that no matter what I do I can't seem to make CAN work over 125&nbsp;kbit/s. I'll give some detail about the board on the bottom, but I'm going to keep this question generic.</p>

<p>First of all, regarding hardware. From what I've gathered, there isn't any need for a <a href=""https://en.wikipedia.org/wiki/Pull-up_resistor"" rel=""nofollow"">pull-up resistor</a> on the TX of CAN. Is that correct? It may perhaps be chip-specific, but wherever I see, it seems that the TX/RX lines are directly connected to the transceiver.</p>

<p>Second, regarding bit-timing: Using different calculators, for example, <a href=""http://www.kvaser.com/en/support/bit-timing-calculator.html"" rel=""nofollow"">Kvaser</a> or <a href=""http://www.intrepidcs.com/support/mbtime.htm"" rel=""nofollow"">the one from Microchip</a>, I can see the following configuration (for 64&nbsp;kHz input clock):</p>

<pre><code>             SYNC     PROP     PHASE1      PHASE2       BRP (prescaler)

125  kbit/s   1        1        3           3            32
250  kbit/s   1        1        3           3            16
500  kbit/s   1        1        3           3             8
1000 kbit/s   1        1        3           3             4
</code></pre>

<p>I've seen this from more than one source. Furthermore, the numbers fit to the formula in the datasheet of the microcontroller.</p>

<p>However, only the configuration for 125&nbsp;kbit/s works for me. I'm using <a href=""http://www.esd-electronics-usa.com/esd-electronics-usa/canreal.htm"" rel=""nofollow"">CANreal</a> to monitor the messages.</p>

<p>I've tried different configurations for the CAN, for example with 16 time quanta instead of 8 as well as changing my microcontroller's clock to 16&nbsp;MHz and using again different values. Regardless of all that, speeds higher than 125&nbsp;kbit/s result in only errors and warnings in CANreal (which are taken from the CAN driver). Note that the same CAN board, driver and software works with 1&nbsp;Mbit/s with some other hardware I have.</p>

<p>This all is made harder since, as soon as I put a probe from my oscillator on the TX line, it becomes a continuous 0-1 alteration like the following:</p>

<pre><code>      __------     __------     __------     __------     __------
     /       |    /       |    /       |    /       |    /       |
   /         |  /         |  /         |  /         |  /         |
  /          | /          | /          | /          | /          |
  |          | |          | |          | |          | |          |
  |          |_|          |_|          |_|          |_|          |
</code></pre>

<p>Which is not something I would be outputting by software. In fact, as soon as I remove the probe, the messages start arriving (again, only at 125&nbsp;Mbit/s). So basically, I don't seem to be able to have any oscillator debugging available.</p>

<p>Back to my ""first of all, regarding hardware"", the shape of the signal suggests a pull-up resistor may be necessary, but I haven't seen the need for that in any datasheet I found. Furthermore, my microcontroller configures the pin when used as CAN, so I don't have control over making it push-pull (since it looks like it's <a href=""http://en.wikipedia.org/wiki/Open_collector#MOSFET"" rel=""nofollow"">open-drain</a>). Not to mention the microcontroller doesn't even have a configuration to make the pin push-pull.</p>

<p>Is there any hidden parameter somewhere that should also be set? Is a pull-up resistor necessary after all? Why would the oscillator probe cause such a behavior?</p>

<hr>

<p>Details from the board:</p>

<ul>
<li><a href=""http://en.wikipedia.org/wiki/Microcontroller"" rel=""nofollow"">MCU</a>: P18F45K80. CAN is connected to its default RB2 and RB3.</li>
<li>CAN transceiver: ISO1050  </li>
<li>Compiler: <a href=""https://en.wikipedia.org/wiki/Mikroelektronika#Products"" rel=""nofollow"">mikroC</a></li>
</ul>
","microcontroller can"
"2039","quadrotor experimental identification","<p>i want to model the quadrotor using experimental method ""i have not built it yet"" what i want to do is: turn only one motor at a specific speed ,then plot the x,y,z and the angles then identify the transfert functions from the plots,x/w1,y/w1,... and so on, but i don't know if it's possible or what graphs i will have,so if you know about the subject or maybe tried something like that, and feel free to add anything you think might be helpfull</p>
","control quadcopter brushless-motor uav"
"2042","Choosing correct power supply for Stepper Motors","<p>I am building a machine and need 2 Stepper Motor for that.
The motors are driven using by a 3.3v arm device.</p>

<p>I have made the following selections regarding the stepper motor, stepper motor driver and the power supply.</p>

<p><a href=""http://www.circuitspecialists.com/12-volt-3.5-amp-power-supply.html"" rel=""nofollow"">Power Supply 12 Volt Power Supply - 3.5 Amp Single Output</a></p>

<p><a href=""http://www.pololu.com/product/1200"" rel=""nofollow"">Stepper Motors Stepper Motor: Unipolar/Bipolar, 200 Steps/Rev, 42×48mm, 4V, 1.2 A/Phase</a></p>

<p><a href=""http://www.pololu.com/product/2133"" rel=""nofollow"">Stepper Motor Driver DRV8825 Stepper Motor Driver Carrier, High Current</a></p>

<p>I tried my best to research the compatibility and came up with these.</p>

<p>Is this a good selection considering the fact that the Power Supply will be driving 2 of these motors.</p>

<p>I will be running the motors at 1/16 step for high resolution.As far as the speed is concerned,it's going to be pretty slow but they will be running continuously for hours in end.Basically what I am trying to do here is make a V-Plotter.As far I can tell, there will be loads of start stop motion in the motors though.</p>
","stepper-motor power stepper-driver"
"2045","Questions about Irobot Create","<p>I'm interested in getting a Create for a project I'll be working on, and wanted some information about it from somebody that already has one:</p>

<ol>
<li><p>How much weight can it safely carry? I talked with Irobot's tech support and they told me the maximum is 5lb, but searching on the internet it seems like this limit is actually not as strict as it appears to be. I'm asking because I'd need to put a 3kg laptop on top of it, which would mean ~3.5-4kg if you also consider the kinect and eventual supports for both. I guess I could use a netbook and send the data I need to another computer, but I wanted to avoid the additional overhead of the wireless link.</p></li>
<li><p>For how long does it run using AA batteries? I'm inclined on not getting the battery pack, since I'd be using the robot in europe, so I'd also need a transformer if I went with the battery pack option.</p></li>
</ol>

<p>Thanks!</p>
","mobile-robot irobot-create"
"2048","Encoder based speed control for Rover 5","<p>I am trying to get precise control over the speed of rover 5 based robot. It has four PWM controlled motors and 4 Optical Quadrature Encoders. I am using <a href=""https://www.sparkfun.com/products/11593"" rel=""nofollow"">4-channel motor controller</a> with <a href=""https://www.sparkfun.com/products/10336"" rel=""nofollow"">rover 5 chassis</a>. I am using arduino Nano for control. I am able to read encoder INT output and change PWM based on pulse width to control speed. But, as a result, I am getting heavy oscillations in the control output. That makes, the robot to move in steps, as PWM is changing constantly. I need an algorithm which can minimize this ringing and have a smooth moving robot. Here is my arduino code snippet.</p>

<pre><code>void setup() {
    Serial.begin(9600);
    init_motors();
    init_encoders();        
    req_speed[0] = 20;
    req_speed[1] = 20;
    req_speed[2] = 20;
    req_speed[3] = 20;
}

void loop() {
  update_encoders();
  update_motors();
}

void update_motors() {
  int i, err;
  unsigned long req_width;
  if(micros() - mtime &gt; 2999) {
    mtime = micros();

    for(i=0; i&lt;4; i++) {
      digitalWrite(pins_dir[i], req_speed[i]&gt;0);
      if(mtime - change_time[i] &gt; 50000ul &amp;&amp; req_speed[i] != 0) {
        cur_pwm[i] += 5;
      } 
      if(req_speed[i] &gt; 0)
        cur_err[i] = req_speed[i]*10  - cur_speed[i];
      else
        cur_err[i] = (-req_speed[i]*10)  - cur_speed[i];
      if(cur_err[i] &gt; 0 &amp;&amp; cur_pwm[i] &lt; 255) {
        cur_pwm[i]++;
      } else if(cur_err[i] &lt; 0 &amp;&amp; cur_pwm[i] &gt; 0) {
        cur_pwm[i]--;
      }
      analogWrite(pins_pwm[i], cur_pwm[i]);
    }
  }
}

void update_encoders() {
  int i;
  unsigned long w;
  enc_new = PINC &amp; B00001111;
  unsigned long etime = micros();
  for (i=0; i&lt;4; i++) {
    if((enc_old &amp; (1 &lt;&lt; i)) &lt; (enc_new &amp; (1 &lt;&lt; i)))
    {
      w = (unsigned long)(((etime - change_time[i])));
      pulse_width[i] = (w + pulse_width_h1[i] + pulse_width_h2[i])/3;
      pulse_width_h2[i] = pulse_width_h1[i];
      pulse_width_h1[i] = pulse_width[i];
      change_time[i]=etime;
      pulse_count[i]++;
      cur_speed[i] = (3200000ul / pulse_width[i]);
    }
  }
  enc_old=enc_new;
}
</code></pre>

<p>Here req_speed is between -100 to 100, where sign indicates direction. Please consider all undefined variables as globals. I experimentally measured that, when motor is running at full speed, the pulse width is around 3200us.</p>

<p>Encoders' INT outputs (XOR of A and B) are connected to A0 thru A3. Motor PWM is connected to D3, D5, D6, D9. Please let me suggest any improvements to this code and advice me about what am I missing here.</p>
","arduino motor pwm"
"2053","Odd L293D behavior: Pin 16 seems to act as enable","<p>I have a chip that is labeled L293D with a small 'ST' logo, which does not behave like I believe a L239D should:</p>

<p>I have the chip on a breadboard with pins 4,5,12 and 13 connected to the ground rail. The positive rail gets 6V from a battery pack.
A motor is connected to pins 3 and 6.
Pin 2 is connected to the positive rail.</p>

<p>Now, when I connect pin 1 (enable 1) to the positive rail, the motor spins, which is expected.</p>

<p>The weird thing is that if I connect pin 16 instead of pin 1 to positive, the motor spins, as well. </p>

<p>Also, with the motor connected to 11 and 14, and 15 connected to positive, the motor spins if I connect pin 1 or pin 16 to positive, but not if I connect pin 9 (which should be the enable pin for that side).</p>

<p>Does any of that make sense? Am I missing something here?</p>

<p>Thanks!</p>
","motor h-bridge"
"2056","Bluetooth module HC-05 giving ERROR :(0)","<p>I am working right now with Arduino UNO and HC-05 bluetooth module.I followed the instruction given on <a href=""http://www.instructables.com/id/Modify-The-HC-05-Bluetooth-Module-Defaults-Using-A/?ALLSTEPS"" rel=""nofollow"">this link for wiring</a>. So there are 2 mode of working with this HC-05 module</p>

<ol>
<li>Simple serial communication</li>
<li>Working in AT command mode so as to change the parameters of HC-05 module</li>
</ol>

<p>As long as I work in simple serial communication mode, everything works fine but when I tried to change the parameters of module, it didn't work out. For working in At command mode, PIN NO 34 of HC-05 module needs to be high which I had taken care of. Lately I found that in mu module they had knowingly not connected the Berg strip to PIN 34, so I connected the PIN directly, even though I am not able to change the parameters of module and when I write any command on COM port of arduino IDE, I get this response</p>

<pre><code>Enter AT commands:
ERROR:(0)
ÿÿÿÿÿÿÿÿÿÿÿÿÿÿÿÿÿÿÿÿÿÿÿÿÿÿÿÿÿÿÿÿÿÿÿÿÿÿÿõÿýì¢^
ERROR:(0)
ÿÿÿÿÿÿÿÿÿÿÿÿÿÿÿÿÿÿÿÿÿÿÿÿÿÿÿÿÿÿÿÿÿÿÿÿÿÿÿõÿýì¢^
ERROR:(0)
ÿÿÿÿÿÿÿÿÿÿÿÿÿÿÿÿÿÿÿÿÿÿÿÿÿÿÿÿÿÿÿÿÿÿÿÿÿÿÿõÿýì¢^
ERROR:(0)
ÿÿÿÿÿÿÿÿÿÿÿÿÿÿÿÿÿÿÿÿÿÿÿÿÿÿÿÿÿÿÿÿÿÿÿÿÿÿÿõÿýì¢^
ERROR:(0)
ÿÿÿÿÿÿÿÿÿÿÿÿÿÿÿÿÿÿÿÿÿÿÿÿÿÿÿÿÿÿÿÿÿÿÿÿÿÿÿõÿýì¢^
ERROR:(0)
ÿÿÿÿÿÿÿÿÿÿÿÿÿÿÿÿÿÿÿÿÿÿÿÿÿÿÿÿÿÿÿÿÿÿÿÿÿÿÿõÿýì¢^
</code></pre>

<p>I think that garbage is due to my code</p>

<p>Here is my code:</p>

<pre><code>#include &lt;SoftwareSerial.h&gt;

SoftwareSerial BTSerial(10, 11); // RX | TX

void setup()
{
  pinMode(9, OUTPUT);  
  digitalWrite(9, HIGH);
  Serial.begin(9600);
  Serial.println(""Enter AT commands:"");
  BTSerial.begin(38400);  
}

void loop()
{
  uint8_t x;
  char CommandFromSerial[50]="" "";
  char ResponseFromBluetooth[50]= "" "";

  if ((Serial.available())){
    if(Serial.available()&gt;0){
      for(x=0;x&lt;50;x++)
        CommandFromSerial[x]=Serial.read();
      BTSerial.println(CommandFromSerial);
    }
  }

  if ((BTSerial.available())){
    if(BTSerial.available()&gt;0)
      for(x=0;x&lt;50;x++)
        ResponseFromBluetooth[x]=BTSerial.read();
    Serial.println(ResponseFromBluetooth);
  }
}
</code></pre>

<p>I am not able to figure out what I am doing wrong. I used this command on COM port 
<code>AT\r\n</code> and many other commands but every time I get the same response.</p>

<p>Did I mess up with my bluetooth module unknowingly?</p>
","arduino c communication serial"
"2062","Guidance for compensating internal forces on closed loop chain","<p>I'm working on a legged robot and generating joint torques. Basically the robot seems to be statically stable to some extend. The robot goes instable if the center of pressure moves to the border of the feet. I'm looking for some method to move away the center of pressure  from the feet edges after having calculated my joint torques. In Sentis thesis ( <a href=""http://ai.stanford.edu/~lsentis/files/Thesis-Sentis-2007.pdf"" rel=""nofollow"">http://ai.stanford.edu/~lsentis/files/Thesis-Sentis-2007.pdf</a> ) , it is mentioned that he somehow manages to cancel out the internal forces to keep the feet flat against the supporting surfaces. </p>

<p>Does anyone has got experience in dealing with internal forces? As far as I understood the literature one can modify the nullspace of the calculated torques to achieve that the COP remains in the geometrical center of the considered foot. I'm looking for methods apart from the virtual linkage model as it did not seem to work for me or someone with whom I could discuss the virtual linkage model described in ( <a href=""http://ai.stanford.edu/~lsentis/files/tro-2010.pdf"" rel=""nofollow"">http://ai.stanford.edu/~lsentis/files/tro-2010.pdf</a> ) as I might not have it understood it correctly.</p>
","stability legged"
"2063","Unable to read pushbutton press properly in Arduino","<p>I am trying to use a push button in order to know and print number of time a push button is pressed using a counter.But everytime i press the button , counter get incremented to sometime 3 and sometime 5 and some time counter does start >100 and continue.</p>

<p>I had preferred the <a href=""http://arduino.cc/en/Tutorial/Button"" rel=""nofollow"">this link</a> for wiring PUSH button with arduino.</p>

<p>and here is my code</p>

<pre><code>const int buttonPin = 2;     // the number of the pushbutton pin
const int ledPin =  13;      // the number of the LED pin

// variables will change:
int buttonState = 0;         // variable for reading the pushbutton status
int count = 0;
void setup() {
  // initialize the LED pin as an output:
  pinMode(ledPin, OUTPUT);      
  // initialize the pushbutton pin as an input:
  pinMode(buttonPin, INPUT);   
  pinMode(buttonPin, INPUT); 
  Serial.begin(9600);
    buttonState = digitalRead(buttonPin);
  digitalWrite(ledPin,LOW);  
}

void loop(){
    // read the state of the pushbutton value:
  buttonState = digitalRead(buttonPin);

  // check if the pushbutton is pressed.
  // if it is, the buttonState is HIGH:
  if (buttonState == HIGH) {

    // turn LED on:    
    digitalWrite(ledPin, HIGH); 
    count = count + 1;  
    Serial.print(count);
  } 
  else {
    // turn LED off:
    digitalWrite(ledPin, LOW); 
  }
}
</code></pre>

<p>I dont know why count is coming absurdly and unevenly.</p>
","arduino c serial"
"2068","My Raspberry Pi is losing power in a surge","<p>I have an <a href=""http://en.wikipedia.org/wiki/Radio-controlled_model"" rel=""nofollow"">RC</a> car. The battery provides power to the <a href=""https://en.wikipedia.org/wiki/electronic_speed_control"" rel=""nofollow"">ESC</a> and then the ESC provides 6&nbsp;V back out to the receiver. Instead of the receiver I have a Raspberry Pi, which uses the 6&nbsp;V, steps it down to 5&nbsp;V and provides power to the Raspberry Pi.</p>

<h3>The problem</h3>

<p>Every time we go full power*, there is a lack of voltage and the Raspberry Pi seems to hard reset.</p>

<p><em>* By full power we mean direct to 100% and not ranging from 0-100</em></p>

<p>I am not an expert in electrical circuits, but some of the suggestions are to use a capacitor to provide the missing 5&nbsp;V in the interim. How do I prevent the Raspberry Pi from dying in the event of full power?</p>
","raspberry-pi electronics esc"
"2074","Omron G5V-2 relay NO pins not working","<p>I could swear that it was working for a while. I got back to my desk, tried it again, and it's no longer working. Could I have fried the NO pins on both sides? This is a DPDT relay. Everything works normally on the NC pins. I have never applied more than 5V. I do hear the relay click when I apply 5V to the coil. But when I measure voltage on the NO pins, I get 0V. Has anyone else seen this? I have two of these relays and I can't seem to get voltage on the NO pins with either relay. I should clarify that I'm expecting the same 5V power source to power both the coil and the common pins. If the NC pins work then I don't see why the NO pins shouldn't. In both cases the 5V is shared between the coil and any load attached to the NC/NO pins. I did try driving the entire circuit off a 9V power supply, but that did not change the results (and that does contradict my earlier statement that I've never applied more than 5V to this relay). My circuit is based on Charles Platt's ""Make: Electronics"", p. 59.</p>

<p>Here's a pic of the schematic I am following, except that I am using a 5V relay and a 5V power supply (USB port) and I am using piezo buzzers without resistors instead of LEDs.</p>

<p><img src=""http://i.stack.imgur.com/nYbnL.jpg"" alt=""enter image description here""></p>
","electronics"
"2075","Particle filter implementation in ROS","<p>I'm looking for particle filter implementation in ROS to use in mobile robot localization, but it seems the only available package is <a href=""http://wiki.ros.org/amcl"" rel=""nofollow"">amcl</a> (Adaptive Monte Carlo), I'm not sure is it possible to use it as particle filter or not, and if it's feasible, how?</p>

<p><strong>Note</strong>: The robot (wheeled robot) provides odometry data and another data source is <code>Kinect</code>, that provides visual odometry data using <a href=""http://wiki.ros.org/fovis"" rel=""nofollow"">fovis</a>.</p>
","mobile-robot localization ros particle-filter"
"2077","Recommendations for system to repeatedly force contact between head and desk","<p>I frequently bang my head on my desk after performing a task poorly. I would like to eliminate the unnecessary middle step of actually performing a task poorly. As such, I would like to design a system to hold my head and repeatedly strike it against my desk. Alternatively, a system that holds the desk and repeatedly strikes it against my head would be acceptable. Requirements are at least 2 strikes per second maximum with ~50cm travel.</p>

<p>Can anybody make any recommendations for a system to base this device off of? Denso products, while small and affordable, do not have the required load capacity (some users may have a rather large head, and involuntary resistance is to be expected -- at least near the start of the cycle). I am thinking of something more industrial, perhaps:</p>

<p><img src=""http://i.stack.imgur.com/irpHH.jpg"" alt=""Fanuc R-2000iA""></p>
","robotic-arm"
"2081","Soft LED Protection Material","<p>I am looking for some material to build a soft clear protective covering for RGB LEDs. The material needs to be close to transparent to allow light to shine through, be soft and compliant but sturdy enough to withstand someone standing on it. The ultimate goal is to have a floor of these LEDs that someone can jump in barefoot and change led colors. </p>

<p>I have tried Gel Candle Wax and Silicone but neither worked very well. I am looking for other material ideas and this was the most relevant of the StackExchanges that I could find. </p>
","arduino"
"2083","Arduino Uno getting a type of ""HANGED"" while runing samll code of switc debounce and Serial print","<p>I am using Arduino UNO to read a push button every time it is pressed.Earlier i was simply reading the Digital IO pin to read the count and then i faced the condition of switch debounce regarding which i had asked a <a href=""http://robotics.stackexchange.com/questions/2063/unable-to-read-pushbutton-press-properly-in-arduino"">question here</a> and get to know that i must use Interrupt instead of reading a digital IO pin but even after using interrupt, i was facing the problem of Switch Debouncing.</p>

<p>So i used <a href=""http://www.arduino.cc/en/Tutorial/Debounce"" rel=""nofollow"">this link</a> and code given on this link</p>

<pre><code>const int buttonPin = 2;    // the number of the pushbutton pin
const int ledPin = 13;      // the number of the LED pin

// Variables will change:
int ledState = HIGH;         // the current state of the output pin
int buttonState;             // the current reading from the input pin
int lastButtonState = LOW;   // the previous reading from the input pin

// the following variables are long's because the time, measured in miliseconds,
// will quickly become a bigger number than can be stored in an int.
long lastDebounceTime = 0;  // the last time the output pin was toggled
long debounceDelay = 50;    // the debounce time; increase if the output flickers

void setup() {
  pinMode(buttonPin, INPUT);
  pinMode(ledPin, OUTPUT);

  // set initial LED state
  digitalWrite(ledPin, ledState);
}

void loop() {
  // read the state of the switch into a local variable:
  int reading = digitalRead(buttonPin);

  // check to see if you just pressed the button
  // (i.e. the input went from LOW to HIGH),  and you've waited
  // long enough since the last press to ignore any noise:  

  // If the switch changed, due to noise or pressing:
  if (reading != lastButtonState) {
    // reset the debouncing timer
    lastDebounceTime = millis();
  }

  if ((millis() - lastDebounceTime) &gt; debounceDelay) {
    // whatever the reading is at, it's been there for longer
    // than the debounce delay, so take it as the actual current state:

    // if the button state has changed:
    if (reading != buttonState) {
      buttonState = reading;

      // only toggle the LED if the new button state is HIGH
      if (buttonState == HIGH) {
        ledState = !ledState;
      }
    }
  }

  // set the LED:
  digitalWrite(ledPin, ledState);

  // save the reading.  Next time through the loop,
  // it'll be the lastButtonState:
  lastButtonState = reading;
}
</code></pre>

<p>and change </p>

<pre><code> long debounceDelay = 50;
</code></pre>

<p>to 10(means read any thing ina time gap of 10 mili second) as the code says.Now what is happening, code is running on the board and after some time my board get hang and LED stop toggling on any press of push button and then i had to manually reset my board.I also want to add upon a thing that i am also using a serial port in between when LED toggles or switch is pressed.</p>

<p>I am totally confused why this is happening.There can beone possibility that this is happening because i reduced time gap between two consecutive events to 10 from 50 miliseconds and that might be making AVR get hanged and thus require a manual reset.</p>
","arduino c serial communication"
"2084","Fake localization using bag file in ROS","<p>I have a bag file that contains couple of topics needed for localization, odometry data, kinect data and <code>/tf</code>. What I want is watching robot's movement path in <code>rviz</code> after initializing robot position (even I don't know how to initial it). Any help?</p>

<p>All topics:</p>

<pre><code>/scan
/tf
/clock
/map
/odom
</code></pre>
","localization ros"
"2087","Which Kinect to a movement base?","<p>I am making a mobile base for a robot with wheels. I want to use a Kinect like a movement sensor (to avoid obstacles, recognition of people, etc...) but I read that there is 2 models, the 360 and the Developer.</p>

<p>Which Kinect works well for my job? And another thins, its there another thing that can I use like a movement sensor? To see diferent posibilities,</p>
","mobile-robot wheeled-robot kinect"
"2088","Powering down servos completely in RobotC+Tetrix","<p>For a certain robotic application (actually for the FTC challenge this year) our team is performing an operation where a servo-driven arm could potentially be forced into an unknown position. We are using NXT+Tetrix.</p>

<p>Since this could damage a powered servo working against this forced position (servo holding arm weight on fixed base is now trying to move heavy base relative to fixed arm), we are thinking about somehow de-powering our servos (or servo controller), in order to get the servos to ""relax"" and accept the mechanically-forced position.</p>

<p>Originally, we were thinking of having our RobotC code determine the physical position of a given servo and set its desired position to there every loop, limiting how much a servo would try to fight the movement, but to our dismay, <code>ServoValue[fooServo]</code> actually gives us the setpoint, and not the physical location (due to the servo being unable to provide this information).</p>

<p>We also considered setting <code>ServoChangeRate[fooServo]</code> to 1(the minimal value) but this only changes the rate of the target location changing relative to the previous target.</p>

<p>So, we're concluding that the only way to really do this is to fully depower the servos. Is this possible on NXT/Tetrix with RobotC?</p>

<p>A few notes:</p>

<ul>
<li><p>I realized as well that one could suggest to rig an encoder(associated with a Tetrix motor that does not need an encoder) onto the rotating area. That actually would not work for mechanical constraints.</p></li>
<li><p>I looked into setting <code>PWM enable</code> as shown <a href=""http://stuyfissionfusiondevelopment.googlecode.com/files/HiTechnic%20Servo%20Controller%20Brief%20v1.2.pdf"" rel=""nofollow"">here</a> but am not sure how to send the i2c commands needed. If someone could clue me in to how these commands would be sent in terms of c code, that would be very helpful.</p></li>
</ul>
","power rcservo nxt robotc"
"2089","Compiling Code for EY-80","<p>I recently purchased at EY-80 from electrodragon: <a href=""http://www.electrodragon.com/?product=all-in-one-9-axis-motion-sensor-gyroscope-accelerometer-magnetometer-barometer"" rel=""nofollow"">EY-80 All in one 9-Axis Motion Sensor (Gyro + Acceler + Magneto + Baro)</a></p>

<p>I am having a hard time compiling the <a href=""https://github.com/Edragon/Arduino_sketch/wiki"" rel=""nofollow"">example code</a> on my arduino:</p>

<p><img src=""http://i.stack.imgur.com/C7xKr.png"" alt=""enter image description here""></p>

<p>This is what is happening.  So far, I am only copy and pasting the code.  Any help? (I am somewhat new to programming, so don't fully understand all of the code)</p>
","arduino sensors gyroscope"
"2090","Wifi to pass through aluminium","<p>I am about to make an rc car which uses a wifi connection. The body for the car would be made from aluminium and the wifi receiver will be placed inside this aluminium casing. </p>

<p>How do I make sure that this will work?</p>

<p>Would I be forced to change my material or can I just make an extension for the receiver and make sure it is out of the casing? </p>

<p>If so , would that really help me?</p>
","wifi"
"2091","How to use Arduino for ESC control?","<p>I am using an Arduino Uno to control an ESC for my (in progress) quadrocopter.  I am currently using the servo library to control the ESC, which works great.</p>

<p>Except..</p>

<p>A count of 100 is max speed, meaning I only have 10 speeds between 90 (stopped) and 100 (motor at full power) to correctly run my quadrocopter, I would like to have many more speed options.  Any ideas?  I'm having a hard time using a PWM signal, I might not be doing it right though.</p>

<p>My current code is <a href=""https://github.com/toozinger/Quad/blob/master/run_motor_as_servo.ino"">here</a>:</p>

<pre><code>#include &lt;Servo.h&gt;

Servo myservo; // create servo object to control a servo
                // a maximum of eight servo objects can be created

int pos = 0; // variable to store the servo position

void setup()
{
  myservo.attach(8); // attaches the servo on pin 8 to the servo object
}

void loop()
{

 int maxspeed=100;
 int minspeed=0;
 int delaytime=5;
 int count;

 for(count=0; count &lt;1; count+=1) {
  for(pos = minspeed; pos &lt; maxspeed; pos += 1) // goes from 0 degrees to 180 degrees
  { // in steps of 1 degree
    myservo.write(pos); // tell servo to go to position in variable 'pos'
    delay(delaytime); // waits 15ms for the servo to reach the position
  }
  for(pos = maxspeed; pos&gt;=minspeed; pos-=1) // goes from 180 degrees to 0 degrees
  {
    myservo.write(pos); // tell servo to go to position in variable 'pos'
    delay(delaytime); // waits 15ms for the servo to reach the position
  }
  if(count&gt;1){
    break;
  }
 }

 myservo.write(92);
 delay(100000);
 myservo.write(90);
 delay(10000000);
}
</code></pre>
","arduino control quadcopter esc servomotor"
"2096","Using Armatures in Morse Robotic Simulator","<p>I'm trying to add my own robot in <a href=""https://www.openrobots.org/wiki/morse"" rel=""nofollow"">Morse</a> 1.1 (using Ubuntu 12.04). I am struggling to add an armature actuator and armature pose sensor to an existing robot. Can someone please explain how this can be done (preferably with some sample code and using the socket interface).</p>

<p>Thanks. </p>
","mobile-robot sensors robotic-arm simulator python"
"2098","Are there any GPS sensors that provide data at 1Hz or faster?","<p>I searched for GPS devices that provide 1 sec updates to server, but I have not found any.</p>

<p>I found this </p>

<blockquote>
  <p>T=30s.Module has sent a monitoring data packet. After 12 seconds the
  server sends an acknowledgement. In 18 seconds later (T = 30s) module
  sends the next monitoring data packet</p>
</blockquote>

<p>Are there any products that take less than this time?</p>

<p>Why do gps devices take this much time to send data?</p>
","sensors gps"
"2101","Do I need an accurate flight model for a UAV?","<p>As I understand it, a Kalman filter uses a mathematical model of the robot to predict the robot's state at t+1. It then combines that prediction with information from sensors to get a better sense of the state.</p>

<p>If the robot is an aeroplane, how accurate/realistic does the model need to be? Can I get away with simple position and velocity, or do I therefore need an accurate flight model with computational fluid dynamics?</p>
","kalman-filter uav"
"2104","How to calculate probability of particle survival for particle filter?","<p>I'm trying to figure out a way that I can calculate the probability that a particle will survive the re-sampling step in the particle filter algorithm.</p>

<p>For the simple case of multinomial re-sampling, I can assume that we can model the process like a Binomial distribution if we only care about one sample/particle.</p>

<p>So if the particle has a weight of w that is also the probability that it will get selected in a step of the re-sampling. So we use 1 - P(k, p, n) where P is the Binomial distribution, k is 0 (we did not select the particle in all our tries), p is equal to w and n is equal to M, the number of particles.</p>

<p>What is the case though in the systematic re-sampling, where the probability of a particle being selected is proportional but not equal to its weight?</p>
","particle-filter"
"2105","360 degree ultrasonic beacon sensor","<p>Basically, I want to detect an ultrasonic beacon in a radius around the robot.
The beacon would have a separate ultrasonic emitter while the robot would have the spinning receiver.</p>

<p>Are there any existing ultrasonic sensors that would meet this use case or am I stuck hacking one together myself?</p>

<p>Is ultrasonic even the best choice? I was hoping that the beacon would be kept in a pocket, so I figured optical sensors were out.</p>

<p>Edit: The beacon and robot will both be mobile so fixed base stations are not an option.</p>
","sensors"
"2106","What happened to Butler's car?","<p>Having read the <a href=""http://books.google.com/books?id=jd8DAAAAMBAJ&amp;lpg=PA128&amp;dq=automatic%20car&amp;pg=PA128#v=onepage&amp;q&amp;f=false"" rel=""nofollow"">article</a> ""This Car Has Electric Brains"" in Popular Mechanics, August 1958 I have some questions.</p>

<p>How practical were his methods? Was his work acquired by a car manufacturer or some other company? Were his methods developed further?</p>

<p>How did his corner navigation work? I don't think he needed to know the distances of road segments; I think he could have used sonar or radar to detect a corner but if cars were entering the corner before him, he could misinterpret those cars as a wall and the absence of a corner. Additionally I think he'd need two sonar/radar systems on both sides of the cars which aren't mentioned; all that's mentioned is a set of relays.</p>

<p>What is the compensator that is mentioned (it's said to function as a gyroscope)? I cannot find any information on this device (that I'm sure is relevant).</p>
","design navigation"
"2109","POMDPs in robotics","<p>POMDPs are used when we cannot observe all the states.
However, I cannot figure out when these POMDPs can be useful in robotics. What is a good example of the use of POMDPs? (I have read one paper where they used them, but I didn't find it obvious why pomdps should be used) What would be good projects ideas based on POMDPs?</p>
","algorithm artificial-intelligence"
"2110","RC Transmitter Quadcopter with Arduino","<p>I have a WL v262 quadcopter and I want to control it using Arduino instead of the joysticks on the transmitter. I opened up the transmitter and saw that each joystick has 2 potentiometers on the PCB and that the voltage for each pot goes from 0-3.3V. I used arduino's PWM and a low pass filter and connected the output of the filtered output to the potentiometer's analog pin which is connected to the PCB (I cannot desolder and take out the pots from the PCB) but even with this $V_{out}$ going onto the analog pin, my transmitter's display gave ???? </p>

<p>Now I am really confused and frustrated because I don't know how else to control this transmitter other than attaching stepper motors to the joysticks and manually controlling the transmitter but this is really my last resort. Can someone help me with this? I have spent hours and hours trial and error but I am getting nowhere. </p>

<p>Here is the PCB of the transmitter:</p>

<p><img src=""http://i.stack.imgur.com/Kd2Rw.jpg"" alt=""enter image description here""></p>

<p><img src=""http://i.stack.imgur.com/vxRRg.jpg"" alt=""enter image description here""></p>
","arduino sensors radio-control wireless"
"2111","Have bloodstream nanobots been approved in any countries?","<p>A Google search on ""bloodstream nanobots"" yields thousands of results and just on the first page, many results of blog posts that date back to 2009. It is nearly 4 years later. </p>

<p>I've had no luck in finding any information on actual APPROVAL of these bots.</p>

<p>Are there any countries at all who have approved this? People seem to have talked about it like crazy 4 years ago, yet, we're still not seeing anything.</p>
","microcontroller"
"2114","sending and receiving parameters to ardupilot","<p>I am interested in getting an arducopter with an ardupilot(APM). I read through the documentation and from what I understand, ardupilot is the low level hardware and firmware that directly controls the motors of the arducoptor. </p>

<p>I would like to know if there is a higher level programmatic interface to the ardupilot? The mission planner provides a user interface to control the ardupilot. But is there a programmatic interface to control it?</p>

<p>In other words, would it be possible for a user written 'linux process' to receive and send sensory data to and from the ardupilot respectively?</p>
","quadcopter ardupilot"
"2117","Defining frames for 5DOF robotics arm","<p>For examples if i have this robotic arm:
<a href=""http://www.youtube.com/watch?v=bKafht51Juw"" rel=""nofollow"">Example</a>, for the base rotation (5th DOF in the clip at 0:58), we know that the Z axis for that joint will be the same as the Z axis for the base frame{0}, but I don't know about Y and Z axises of the base rotation respects to the base frame, should they be the same or not ?</p>

<p>And one more thing, for defining the frame of the base rotation (at 0:58 in the clip), the vertical arm pitch (at 0.47 in the clip) and the horizontal arm pitch (at 0:46 in the clip), it's pretty easy, but I don't know how to continue for defining the frame of wrist roll (at o.12 in the clip) and wrist pitch (0.23 in the clip) since the angle between the Z axis of wrist roll and the wrist pitch is now 90o.
<br> Thank you very much.</p>
","localization kinematics robotic-arm"
"2119","Deburring Robot (Plastic Box)","<p>For a university course I have been asked to design a rough ""specification"" for a system that will deburr a plastic box that appears in a workspace. Due to irregularities in the boxes edges I cannot use simple position control and must use force control.</p>

<p>I have so far decided on;</p>

<p>Using an IR sensor to detect the box has appeared in the workspace.</p>

<p>Use an Epson 2 axis robot to move around the work piece</p>

<p>Use an ATI 6 axis force sensor to maintain a constant force against the edge of the box as the deburrer/robot moves around it.</p>

<p>Is there a simple means of detecting the end of each side of the box ?
A 0N force value would indicate reaching the edge of a box but it could also mean a breakage in the box which was also specified. How can I distinguish between the two ?</p>

<p>Also does my work so far sound sensible ?</p>

<p>Thanks for any help</p>
","control"
"2121","Resampling attitude states (quaternions, rotation matrix) in a Particle Filter","<p>Suppose I have a particle filter which contains an attitude state (we'll use a unit quaternion from the body to the earth frame for this discussion) $\mathbf{q}_b^e$.</p>

<p>What methods should or should not be used for resampling? Many resampling schemes (e.g. <a href=""http://www.stat.columbia.edu/~liam/teaching/neurostat-spr12/papers/EM/resampling.pdf"">this paper</a>) seem to require the variance to be calculated at some stage, which is not trivial for $SO\{3\}$. Or, the variance is required when performing roughening.</p>

<p>Are there any good papers on resampling attitude states?  Especially those that re-sample complete poses (e.g. position and attitude)?</p>
","particle-filter pose"
"2124","Image retrieval through a multibeam imaging sonar","<p>I would like to know if anyone here has used the Blueview SDK (Linux) for retrieval of images from the pings obtained by a multibeam sonar (P450, P900, etc.) ?
If so, I'd like to know why would anyone get a <em>null head</em> when I trying to retrieve the head (eventually for the pings to be converted to an image) using the <code>BVT_GetHead()</code> method. My snippet for retrieving the image from a <code>.son</code> file (some_son_data.son) is given below:</p>

<pre><code>int main() {
    BVTSonar son = BVTSonar_Create();
    BVTSonar_Open(son, ""FILE"", ""some_son_data.son"");

    if (NULL != son) cout &lt;&lt; ""son not null"" &lt;&lt; endl;
    BVTHead head = NULL ;
    BVTSonar_GetHead(son, 0, &amp;head);
    return 0;
}
</code></pre>
","auv sonar"
"2127","where can i learn electronics from intro to advance digital","<p>I'd like some well put video series of like 30 videos. Or anything but it needs to thorough and in easy English...less mundane. So far all resources i have found either go upto resistors code or of projects that tell you do this and this and this and tada you got this.</p>

<p>Is there really no online resource for people to learn electronics. I want further master analog and do move on to digital cause it's better to spend 0.40 cents.... than
 spend $95 on components and get the whole thing on tiny chip.</p>

<p>Please bare with me like six months i have been searching for legit source, material that is meant to teach you. I like pictures and colors. </p>
","electronics"
"2130","Transform Image Using Roll-Pitch-Yaw Angles (image rectification)","<p><strong>UPDATE:</strong> <em><a href=""http://stackoverflow.com/questions/20445147/transform-image-using-roll-pitch-yaw-angles-image-rectification/20469709#20469709"">This exact problem</a> has been solved in StackOverflow. Please read this post there for further explanation and a working solution. Thanks!</em></p>

<p>I am working on an application where I need to rectify an image taken from a mobile camera platform. The platform measures roll, pitch and yaw angles, and I want to make it look like the image is taken from directly above, by some sort of transform from this information. </p>

<p>In other words, I want a perfect square lying flat on the ground, photographed from afar with some camera orientation, to be transformed, so that the square is perfectly symmetrical afterwards. </p>

<p>I have been trying to do this through OpenCV(C++) and Matlab, but I seem to be missing something fundamental about how this is done.</p>

<p>In Matlab, I have tried the following:</p>

<pre><code>%% Transform perspective
img = imread('my_favourite_image.jpg');
R = R_z(yaw_angle)*R_y(pitch_angle)*R_x(roll_angle);
tform = projective2d(R);   
outputImage = imwarp(img,tform);
figure(1), imshow(outputImage);
</code></pre>

<p>Where R_z/y/x are the standard rotational matrices (implemented with degrees).</p>

<p>For some yaw-rotation, it all works just fine:</p>

<pre><code>R = R_z(10)*R_y(0)*R_x(0);
</code></pre>

<p>Which gives the result:</p>

<p><img src=""http://i.stack.imgur.com/pyiNq.jpg"" alt=""Image rotated 10 degrees about the Z-image axis""></p>

<p>If I try to rotate the image by the same amount about the X- or Y- axes, I get results like this:</p>

<pre><code>R = R_z(10)*R_y(0)*R_x(10);
</code></pre>

<p><img src=""http://i.stack.imgur.com/cbZoE.jpg"" alt=""Image rotated 10 degrees about the X-image axis""></p>

<p>However, if I rotate by 10 degrees, divided by some huge number, it starts to look OK. But then again, this is a result that has no research value what so ever:</p>

<pre><code>R = R_z(10)*R_y(0)*R_x(10/1000);
</code></pre>

<p><img src=""http://i.stack.imgur.com/kssGS.jpg"" alt=""Image rotated 10/1000 degrees about the X-image axis""></p>

<p>Can someone please help me understand why rotating about the X- or Y-axes makes the transformation go wild? Is there any way of solving this without dividing by some random number and other magic tricks? Is this maybe something that can be solved using Euler parameters of some sort? Any help will be highly appreciated!</p>
","computer-vision cameras"
"2131","Line following robot with EV3 Colour Sensor","<p>I am trying to build an advanced coloured lines following robot with the ability to differentiate between many different coloured lines and follow them. I am looking for the right sensor that will help my robot achieve its objective.</p>

<p>As I was researching I came across the <em>EV3 Colour Sensor</em> which can detect up to 7 colours.</p>

<p>Is this sensor suitable for my project?</p>

<p>What other sensors can I use and how?</p>

<p>Thank You</p>
","mobile-robot sensors line-following"
"2132","What dependencies do I need for USB programing in python with pyUSB?","<p>I am trying to get the <code>usb.find</code> command to work properly in a python script I'm writing on Angstrom for the Beagleboard.</p>

<p>Here is my code:</p>

<pre><code>#!/usr/bin/env python

import usb.core 
import usb.util 
import usb.backend.libusb01 as libusb


PYUSB_DEBUG_LEVEL = 'debug'
# find our device
# Bus 002 Device 006: ID 1208:0815
#  idVendor           0x1208
#  idProduct          0x0815
# dev = usb.core.find(idVendor=0xfffe, idProduct=0x0001)
# iManufacturer           1 TOROBOT.com

dev = usb.core.find(idVendor=0x1208, idProduct=0x0815,
backend=libusb.get_backend() )
</code></pre>

<p>I don't know what's missing, but here is what I do know.
When I don't specify the backend, no backend is found.  When I do specify the backend <code>usb.backend.libusb01</code> I get the following error: </p>

<pre><code>root@beagleboard:~/servo# ./pyServo.py
Traceback (most recent call last):
  File ""./pyServo.py"", line 17, in &lt;module&gt;
    dev = usb.core.find(idVendor=0x1208, idProduct=0x0815, backend=libusb.get_backend() )
  File ""/usr/lib/python2.6/site-packages/usb/core.py"", line 854, in find
    return _interop._next(device_iter(k, v))
  File ""/usr/lib/python2.6/site-packages/usb/_interop.py"", line 60, in _next
    return next(iter)
  File ""/usr/lib/python2.6/site-packages/usb/core.py"", line 821, in device_iter
    for dev in backend.enumerate_devices():
  File ""/usr/lib/python2.6/site-packages/usb/backend/libusb01.py"", line 390, in enumerate_devices
    _check(_lib.usb_find_busses())
  File ""/usr/lib/python2.6/ctypes/__init__.py"", line 366, in __getattr__
    func = self.__getitem__(name)
  File ""/usr/lib/python2.6/ctypes/__init__.py"", line 371, in __getitem__
    func = self._FuncPtr((name_or_ordinal, self))
AttributeError: python: undefined symbol: usb_find_busses
</code></pre>

<p>What am I missing so that this will work properly?</p>

<p>Thank you.</p>
","rcservo python usb"
"2133","Turning a differential drive robot to a specific angle","<p>Given a robot with 2 wheels with radius r on one axle with length D, I want to set the wheel speed so that it turns to an angle phi as fast as possible. The timestep t is 64 milliseconds.</p>

<p>I thought the wheel speed could be set to v = ((desired_heading-actual_heading) * circumference_wheel_trajectory)/(2*pi * t * wheel_radius). This will converge to a somewhat right angle, eventually, but its very slow and becomes slower as I approach the angle I want to be at.</p>

<p>Is there an alternative/better way to do this?</p>
","kinematics"
"2135","Algebraic and geometric in inverse kinematic","<p>I'm just wondering that is there any case that when algebraic way can't solve the problem while the geometric can ? Cause I'm working on a 2DOF robotics arm <a href=""http://www.quanser.com/Products/2dof_serial_flexible_joint"" rel=""nofollow"">This one</a>, I know the length of L1 and L2, location that I want for the end effector, then I tried calculating the angles by using algebraic but it gave me cos(alpha) > 1, but when I tried solving with geometric, I can find the solution, so is it because I use a wrong way in algebraic ? 
<br>Thank you very much.</p>
","localization kinematics robotic-arm inverse-kinematics"
"2138","Move ATRV robot to specific distance using ROS","<p>Is there a node or package that can send commands to <code>/cmd_vel</code> to move <strong>ATRV-Jr</strong> like 2 meters forward or turn it 90 degree to right/left? I don't want to tell the robot to move with specified speed. For example when I use this command <code>rostopic pub /cmd_vel geometry_msgs/Twist '[1.0,0.0,0.0]' '[0.0,0.0,0.0]'</code> the robot starts moving forward until I send another command or send <code>break</code> command.</p>
","mobile-robot ros navigation"
"2145","Another SDK like OpenNI","<p>I am taking information for my project and I need to see libraries ans SDKs. Searching in the web I found that OpenNI has a lot of functions and when I try to found another SDK, I dont find any other. I am working with a Kinect and a XTION so I need an SDK who works in both. </p>

<p>Is there any other SDK o set of libraries that works well in both?</p>

<p>Thanks!</p>
","kinect openni"
"2146","APM Accelerometer Calibration","<p>I am trying to manually calibrate the on-board accelerometer of an APM 2.6 controller.</p>

<p>I am using the following code (I found this somewhere, don't remember where) with Arduino 1.0.5 (in Windows environment) to fetch the accelerometer and gyro data:</p>

<pre><code>  #include &lt;SPI.h&gt;
  #include &lt;math.h&gt;

  #define ToD(x) (x/131)
  #define ToG(x) (x*9.80665/16384)

  #define xAxis 0
  #define yAxis 1
  #define zAxis 2

  #define Aoffset 0.8

  int time=0;
  int time_old=0;

  const int ChipSelPin1 = 53;

  float angle=0;
  float angleX=0;
  float angleY=0;
  float angleZ=0;



  void setup() {
     Serial.begin(9600);  

     pinMode(40, OUTPUT);
     digitalWrite(40, HIGH);


     SPI.begin();  
     SPI.setClockDivider(SPI_CLOCK_DIV16); 

     SPI.setBitOrder(MSBFIRST); 
     SPI.setDataMode(SPI_MODE0);


      pinMode(ChipSelPin1, OUTPUT);

     ConfigureMPU6000();  // configure chip
    }

void loop()

{
 Serial.print(""Acc X "");


  Serial.print(AcceX(ChipSelPin1));
  Serial.print(""   "");
  Serial.print(""Acc Y "");
  Serial.print(AcceY(ChipSelPin1));
  Serial.print(""   "");
  Serial.print(""Acc Z "");  
  Serial.print(AcceZ(ChipSelPin1));  
  Serial.print("" Gyro X "");  
  Serial.print(GyroX(ChipSelPin1)); 
  Serial.print("" Gyro Y "");  
  Serial.print(GyroY(ChipSelPin1)); 
  Serial.print("" Gyro Z "");  
  Serial.print(GyroZ(ChipSelPin1)); 
  Serial.println();
}


void SPIwrite(byte reg, byte data, int ChipSelPin) {
  uint8_t dump;
  digitalWrite(ChipSelPin,LOW);
  dump=SPI.transfer(reg);
  dump=SPI.transfer(data);
  digitalWrite(ChipSelPin,HIGH);
}


uint8_t SPIread(byte reg,int ChipSelPin) {
  uint8_t dump;
  uint8_t return_value;
  uint8_t addr=reg|0x80;
  digitalWrite(ChipSelPin,LOW);
  dump=SPI.transfer(addr);
  return_value=SPI.transfer(0x00);
  digitalWrite(ChipSelPin,HIGH);
  return(return_value);
}


int AcceX(int ChipSelPin) {
  uint8_t AcceX_H=SPIread(0x3B,ChipSelPin);
  uint8_t AcceX_L=SPIread(0x3C,ChipSelPin);
  int16_t AcceX=AcceX_H&lt;&lt;8|AcceX_L;
  return(AcceX);
}


int AcceY(int ChipSelPin) {
  uint8_t AcceY_H=SPIread(0x3D,ChipSelPin);
  uint8_t AcceY_L=SPIread(0x3E,ChipSelPin);
  int16_t AcceY=AcceY_H&lt;&lt;8|AcceY_L;
  return(AcceY);
}


int AcceZ(int ChipSelPin) {
  uint8_t AcceZ_H=SPIread(0x3F,ChipSelPin);
  uint8_t AcceZ_L=SPIread(0x40,ChipSelPin);
  int16_t AcceZ=AcceZ_H&lt;&lt;8|AcceZ_L;
  return(AcceZ);
}


int GyroX(int ChipSelPin) {
  uint8_t GyroX_H=SPIread(0x43,ChipSelPin);
  uint8_t GyroX_L=SPIread(0x44,ChipSelPin);
  int16_t GyroX=GyroX_H&lt;&lt;8|GyroX_L;
  return(GyroX);
}


int GyroY(int ChipSelPin) {
  uint8_t GyroY_H=SPIread(0x45,ChipSelPin);
  uint8_t GyroY_L=SPIread(0x46,ChipSelPin);
  int16_t GyroY=GyroY_H&lt;&lt;8|GyroY_L;
  return(GyroY);
}


int GyroZ(int ChipSelPin) {
  uint8_t GyroZ_H=SPIread(0x47,ChipSelPin);
  uint8_t GyroZ_L=SPIread(0x48,ChipSelPin);
  int16_t GyroZ=GyroZ_H&lt;&lt;8|GyroZ_L;
  return(GyroZ);
}


//--- Function to obtain angles based on accelerometer readings ---//
float AcceDeg(int ChipSelPin,int AxisSelect) {
  float Ax=ToG(AcceX(ChipSelPin));
  float Ay=ToG(AcceY(ChipSelPin));
  float Az=ToG(AcceZ(ChipSelPin));
  float ADegX=((atan(Ax/(sqrt((Ay*Ay)+(Az*Az)))))/PI)*180;
  float ADegY=((atan(Ay/(sqrt((Ax*Ax)+(Az*Az)))))/PI)*180;
  float ADegZ=((atan((sqrt((Ax*Ax)+(Ay*Ay)))/Az))/PI)*180;
  switch (AxisSelect)
  {
    case 0:
    return ADegX;
    break;
    case 1:
    return ADegY;
    break;
    case 2:
    return ADegZ;
    break;
  }
}


//--- Function to obtain angles based on gyroscope readings ---//
float GyroDeg(int ChipSelPin, int AxisSelect) {
  time_old=time;
  time=millis();
  float dt=time-time_old;
  if (dt&gt;=1000)
  {
    dt=0;
  }
  float Gx=ToD(GyroX(ChipSelPin));
  if (Gx&gt;0 &amp;&amp; Gx&lt;1.4)
  {
    Gx=0;
  }
  float Gy=ToD(GyroY(ChipSelPin));
  float Gz=ToD(GyroZ(ChipSelPin));
  angleX+=Gx*(dt/1000);
  angleY+=Gy*(dt/1000);
  angleZ+=Gz*(dt/1000);
  switch (AxisSelect)
  {
    case 0:
    return angleX;
    break;
    case 1:
    return angleY;
    break;
    case 2:
    return angleZ;
    break;
  }
}


void ConfigureMPU6000()
{
  // DEVICE_RESET @ PWR_MGMT_1, reset device
  SPIwrite(0x6B,0x80,ChipSelPin1);
  delay(150);

  // TEMP_DIS @ PWR_MGMT_1, wake device and select GyroZ clock
  SPIwrite(0x6B,0x03,ChipSelPin1);
  delay(150);

  // I2C_IF_DIS @ USER_CTRL, disable I2C interface
  SPIwrite(0x6A,0x10,ChipSelPin1);
  delay(150);

  // SMPRT_DIV @ SMPRT_DIV, sample rate at 1000Hz
  SPIwrite(0x19,0x00,ChipSelPin1);
  delay(150);

  // DLPF_CFG @ CONFIG, digital low pass filter at 42Hz
  SPIwrite(0x1A,0x03,ChipSelPin1);
  delay(150);

  // FS_SEL @ GYRO_CONFIG, gyro scale at 250dps
  SPIwrite(0x1B,0x00,ChipSelPin1);
  delay(150);

  // AFS_SEL @ ACCEL_CONFIG, accel scale at 2g (1g=8192)
  SPIwrite(0x1C,0x00,ChipSelPin1);
  delay(150);
}
</code></pre>

<p>My objective use to calibrate the accelerometers (and gyro), so that I can use them without having to depend on Mission Planner.</p>

<p>I'm reading values like:</p>

<pre><code>Acc X 288   Acc Y -640   Acc Z 16884 Gyro X -322 Gyro Y 26 Gyro Z 74
Acc X 292   Acc Y -622   Acc Z 16854 Gyro X -320 Gyro Y 24 Gyro Z 79
Acc X 280   Acc Y -626   Acc Z 16830 Gyro X -328 Gyro Y 23 Gyro Z 71
Acc X 258   Acc Y -652   Acc Z 16882 Gyro X -314 Gyro Y 22 Gyro Z 78
Acc X 236   Acc Y -608   Acc Z 16866 Gyro X -321 Gyro Y 17 Gyro Z 77
Acc X 238   Acc Y -642   Acc Z 16900 Gyro X -312 Gyro Y 26 Gyro Z 74
Acc X 226   Acc Y -608   Acc Z 16850 Gyro X -321 Gyro Y 26 Gyro Z 68
Acc X 242   Acc Y -608   Acc Z 16874 Gyro X -325 Gyro Y 27 Gyro Z 69
Acc X 236   Acc Y -576   Acc Z 16836 Gyro X -319 Gyro Y 19 Gyro Z 78
Acc X 232   Acc Y -546   Acc Z 16856 Gyro X -321 Gyro Y 24 Gyro Z 68
Acc X 220   Acc Y -624   Acc Z 16840 Gyro X -316 Gyro Y 30 Gyro Z 77
Acc X 252   Acc Y -594   Acc Z 16874 Gyro X -320 Gyro Y 19 Gyro Z 59
Acc X 276   Acc Y -622   Acc Z 16934 Gyro X -317 Gyro Y 34 Gyro Z 69
Acc X 180   Acc Y -564   Acc Z 16836 Gyro X -320 Gyro Y 28 Gyro Z 68
Acc X 250   Acc Y -596   Acc Z 16854 Gyro X -329 Gyro Y 33 Gyro Z 70
Acc X 220   Acc Y -666   Acc Z 16888 Gyro X -316 Gyro Y 19 Gyro Z 71
Acc X 278   Acc Y -596   Acc Z 16872 Gyro X -307 Gyro Y 26 Gyro Z 78
Acc X 270   Acc Y -642   Acc Z 16898 Gyro X -327 Gyro Y 28 Gyro Z 72
Acc X 260   Acc Y -606   Acc Z 16804 Gyro X -308 Gyro Y 31 Gyro Z 64
Acc X 242   Acc Y -650   Acc Z 16906 Gyro X -313 Gyro Y 31 Gyro Z 78
Acc X 278   Acc Y -628   Acc Z 16898 Gyro X -309 Gyro Y 22 Gyro Z 67
Acc X 250   Acc Y -608   Acc Z 16854 Gyro X -310 Gyro Y 23 Gyro Z 75
Acc X 216   Acc Y -634   Acc Z 16814 Gyro X -307 Gyro Y 27 Gyro Z 83
Acc X 228   Acc Y -604   Acc Z 16904 Gyro X -326 Gyro Y 17 Gyro Z 75
Acc X 270   Acc Y -634   Acc Z 16898 Gyro X -320 Gyro Y 31 Gyro Z 77
</code></pre>

<p>From what I understand, SPIread(...,...) returns an analog voltage value from the data pins of the sensor, which happens to be proportional to the acceleration values. Right?</p>

<p><strong>My question is:</strong> How do I go about calibrating the accelerometer?</p>

<p><strong>What I've tried till date:</strong> I've tried the ""place horizontal... place nose down... left side, right side"" technique used by mission planner. </p>

<p>Basically, when placed on horizontal position, the sensor is experiencing +1g on it's Z axis and 0g in X and Y axis. Left/right side provides ±1g on Y axis and nose down/up provides ±1g on X axis. </p>

<p>Now for every orientation, I've passed the raw sensor data through a LPF and then computed the mean, median and SD of this sensor data over 100 iterations. I store this mean, median and SD value in the EEPROM for each axis (one for +1g and one for 0g).</p>

<p>Now, when I use the sensor, I load the stats from the EEPROM, match the mean/median and standard deviation with the current reading of 4/5 iterations. </p>

<p>Here I'm working under the assumption that the values between 0g and +1g (and anything above 1g) can be interpolated/extrapolated from the data using a linear plot. </p>

<ul>
<li>Is this the correct approach for calibration?</li>
<li>Can you suggest a better way?</li>
<li>I noticed that the maxima/minima for each axis is different. Is this
the expected outcome or is there something wrong in the code?</li>
<li>What do I do with the gyro? How to calibrate for angular
acceleration?</li>
</ul>
","arduino accelerometer ardupilot"
"2147","How can I interface my CMOS camera module to an Arduino?","<p>I am totally new to the camera interface and usage in an Embedded project, and would like to use a CMOS vision sensor <a href=""https://www.sparkfun.com/products/8667"" rel=""nofollow"">like this</a>.This project further will be used to power a small robot with on-board video processing power using processors like ARM 9.</p>

<p>I do have a limitation that until now I have worked only on 8-bit micro-controllers like the atmega 8, 16, 32 and on the Arduino platform. I think that for better processing we can use Arduino Due.</p>

<p>With the data sheet for the CMOS camera above, we can build its breakout board. But what next? I haven't I found any useful resources while searching. All I need to do is to capture a small video and store it in a SD card.</p>

<p>I have seen <a href=""http://www.cmucam.org/projects/cmucam1/wiki/Wiki"" rel=""nofollow"">these links</a> but they haven't proved to be very useful as they don't provide me the required form factor. I am looking to interface this module to a customized board.</p>

<p>So what so I need to understand about what commands they accept for their proper functioning like starting to take video and posting them out on a output pin.</p>

<p>If we get a video on a output pin, to which pin should I take that output to on my controller, i.e. on UART or I2C or SPI?</p>
","arduino microcontroller computer-vision cameras c"
"2148","Continuous or Discrete","<p>I am new to robotics and control and I have been thinking about how to deal with problems in real life. I have passed a course in control, but I do not have any idea about control for discrete/digital systems.<br>
There are a lot of robots and in general dynamic systems which are controlled by microcontrollers or computers with some software, i.e. simulink. Usually there are sensors which send feedback to the microcontroller or the computer and the controller sends a signal w.r.t the input signal from sensors. I was wondering how we decide if the system is discrete or continuous? How one can decide if he should use discrete or continuous blocks in simulink to control a dynamic system. Does it really matter which one we use?<br>
After all computers are digital and I think it is easier to work with digital signals and also  do we really have continuous signal? I have not passed any signals course, so my questions might be really easy. I did not find any other place for my question.</p>
","control microcontroller"
"2149","Extended Kalman Filter with Laser Scan + Known Map","<p>I am currently working on a project for school where I need to implement an extended Kalman Filter for a point robot with a laser scanner. The Robot can rotate with 0 degree turn radius and drive forward. All motions are piecewise linear (drive,rotate,drive).</p>

<p>The simulator we are using does not support acceleration, all motion is instantaneous. </p>

<p>We also have a known map (png image) that we need to localize in. We can ray trace in the image in order to simulate laser scans. </p>

<p>My partner and I are little confused as to the motion and sensor models we'll need to use. </p>

<p>So far we are modelling the state as a vector $(x,y,\theta)$.</p>

<p>We are using the update equations as follows</p>

<pre><code>void kalman::predict(const nav_msgs::Odometry msg){
    this-&gt;X[0] += linear * dt * cos( X[2] ); //x
    this-&gt;X[1] += linear * dt * sin( X[2] ); //y
    this-&gt;X[2] += angular * dt; //theta

    this-&gt;F(0,2) = -linear * dt * sin( X[2] ); //t+1 ?
    this-&gt;F(1,2) =  linear * dt * cos( X[2] ); //t+1 ?

    P = F * P * F.t() + Q;

    this-&gt;linear = msg.twist.twist.linear.x;
    this-&gt;angular = msg.twist.twist.angular.z;
    return;
}
</code></pre>

<p>We thought we had everything working until we noticed that we forgot to initialize <code>P</code> and that it was zero, meaning that there was no correction happening. Apparently our propagation was very accurate as we haven't yet introduced noise into the system.</p>

<p>For the motion model we are using the following matrix for F:</p>

<p>$F = \begin{bmatrix}1 &amp; 0 &amp; -v*\Delta t*sin(\theta)  
\\ 0 &amp; 1 &amp; v*\Delta t*cos(\theta)   
\\ 0 &amp; 0 &amp; 1 
\end{bmatrix}$</p>

<p>As its the Jacobian of our update formulas. Is this correct?</p>

<p>For the sensor model we are approximating the Jacobian (H) by taking finite differences of the robots $x$, $y$ and $\theta$ positions and ray tracing in the map. We talked to the TA who said that this would work but I'm still unsure it will. Our prof is away so we can't ask him unfortunately. We are using 3 laser measurements per correction step so H is a 3x3. </p>

<p>The other issue where having how to initialize P. We tried 1,10,100 and they all place the robot outside the map at (-90,-70) when the map is only 50x50.</p>

<p>The code for our project can be found here: <a href=""https://github.com/en4bz/kalman/blob/master/src/kalman.cpp"">https://github.com/en4bz/kalman/blob/master/src/kalman.cpp</a></p>

<p>Any advice is greatly appreciated.</p>

<p>EDIT:</p>

<p>At this point I've gotten the filter to stabilize with basic movement noise but no actual movement. As soon as the robot starts to move the filter diverges quite quickly and exits the map. </p>
","mobile-robot ros kalman-filter ekf"
"2151","What micro controller should I use?","<p>I am planning on building a robot with wheels (later legs, if possible), that can move around the room and analyze certain things, using a couple sensors.
In the later steps more functions such a grabbing are the things I want to add.</p>

<p>Could you recommend me a micro controller?</p>

<p>My concern about Arduino is that there aren't enough slots, Raspberry Pi seems like it constantly needs a screen for the user.</p>

<p>I am a complete amateur when it comes to robotics. However, I am quite familiar with the computer languages Java and Python. Since I wrote a fun app for Android for myself I would love the robot to be compatible with Android, too.</p>
","arduino raspberry-pi beginner"
"2156","How to calibrate an industrial Robot?","<p>I am testing an industrial robot (<a href=""http://www.abb.us/product/seitp327/aebd514c0e25af94c125714c0044106a.aspx"" rel=""nofollow"">ABB IRB 1410</a>) using three simple Micron Dial gauges to get x,y,z values at particular point by varying Speed, Load and distance from home position. 
My questions are,
Whether these three parameters influencing the repeatability or only the accuracy?
Using dial gauges, without any relation to the Base frame, is it possible to measure accuracy?
Is any other cost effective method to measure the repeatability and accuracy like above method?</p>
","industrial-robot calibration"
"2158","Making a tiny robot by using a remote brain","<p>I'd like to build a robot as small as possible and with as few ""delicate"" parts as possible (the bots will be bashing into each other).</p>

<p>I was wondering if it was possible to use a small chip that could receive bluetooth/IR/wifi commands to move the motors, and in turn, send back feedback based on sensors such as an accelerometer (to detect impact).</p>

<p>I can probably achieve something like this with the <a href=""http://www.piborg.org/picypack"">PiCy</a> </p>

<p><a href=""http://i.stack.imgur.com/vjVzf.jpg""><img src=""http://i.stack.imgur.com/vjVzfm.jpg"" alt=""picy""></a></p>

<p>however this is slightly bigger than I'd like (due to the size of the Pi) and I'm not sure how long the Pi would last taking continuous impacts.</p>

<p>I'd therefore like to try to offset the brain (the Pi) to the side of the arena and just use a small chip to receive move commands, and send back data from the accelerometer.</p>

<p>Do you have any recommendations for such a chip? Wifi would be my choice but if it impacts the size I could try BT</p>

<p>Edit: After further research it seems an Arduino nano with a WiFi RedBack shield might do the job along with something like this for the motors: <a href=""http://www.gravitech.us/2mwfecoadfor.html"">http://www.gravitech.us/2mwfecoadfor.html</a></p>
","raspberry-pi rcservo accelerometer"
"2159","computer aided RC airplane combat","<p><a href=""http://www.youtube.com/watch?v=4Vh_R1NlmX0"" rel=""nofollow"">http://www.youtube.com/watch?v=4Vh_R1NlmX0</a> is from 2011 SWARM and shows RC aircraft or combat wings trying to hit each other in the air. </p>

<p>Scoring a hit is pretty rare, and I'd like to increase a pilot's chances by using a computer targeting system.  It would be an offline system that gets data from sensors on the airplane.</p>

<p>What sensor(s) would work for this application?</p>
","mobile-robot sensors"
"2165","SPP Bluetooth profile compatibility with phone","<p>I'm building a project that uses a cell phone to control a microcontroller via Bluetooth.</p>

<p>I've decided to use the HC-05 Bluetooth module.
<br>HC-05 Manual: <a href=""http://www.exp-tech.de/service/datasheet/HC-Serial-Bluetooth-Products.pdf"" rel=""nofollow"">http://www.exp-tech.de/service/datasheet/HC-Serial-Bluetooth-Products.pdf</a></p>

<p>And the phone I'm using is the Nokia C3-00 (series 40).
<br><a href=""http://developer.nokia.com/Devices/Device_specifications/C3-00/"" rel=""nofollow"">http://developer.nokia.com/Devices/Device_specifications/C3-00/</a></p>

<p>The HC-05 module uses the SPP Bluetooth profile while my phone only supports DUN, FTP, GAP, GOEP, HFP, HSP, OPP, PAN, SAP, SDAP profiles. But to my knowledge the phone API utilizes RFCOMM.</p>

<p>Question is, can I use this Bluetooth module with my phone?</p>

<p>Thanks in advance and my apologies if my question is too trivial as I'm quite new to Bluetooth.</p>

<p>-Shaun </p>
","microcontroller"
"2167","Quadcopter instability with simple takeoff in autonomous mode","<p>I'm trying to get a quad rotor to fly. The on board controller is an Ardupilot Mega 2.6, being programmed by Arduino 1.0.5.</p>

<p>I'm trying to fly it in simple autonomous mode, no Radio controller involved. I've done a thorough static weight balancing of the assembly (somewhat like this: <a href=""http://www.youtube.com/watch?v=3nEvTeB2nX4"" rel=""nofollow"">http://www.youtube.com/watch?v=3nEvTeB2nX4</a>) and the propellers are balanced correctly.</p>

<p>I'm trying to get the quadcopter to lift using this code:</p>

<pre><code>#include &lt;Servo.h&gt;


int maxspeed = 155;
int minspeed = 0;

Servo motor1;
Servo motor2;
Servo motor3;
Servo motor4;

int val = 0;
int throttleCurveInitialGradient = 1;

void setup()
{


val = minspeed;

motor1.attach(7);
motor2.attach(8);
motor3.attach(11);
motor4.attach(12);


}


void loop()
{
setAllMotors(val);
delay(200);
val&gt;maxspeed?true:val+=throttleCurveInitialGradient;
}

void setAllMotors(int val)
  {
    motor1.write(val);
    motor2.write(val);
    motor3.write(val);
    motor4.write(val);
  }
</code></pre>

<p>But the issue is, as soon as the quadcopter takes off, it tilts heavily in one direction and topples over. </p>

<p>It looks like one of the motor/propeller is not generating enough thrust for that arm to take-off. I've even tried offsetting the weight balance against the direction that fails to lift, but it doesn't work (and I snapped a few propellers in the process);</p>

<ul>
<li>Is there something wrong with the way the ESCs are being fired using
the Servo library?</li>
<li>If everything else fails, am I to assume there is something wrong
with the motors?</li>
<li>Do I need to implement a PID controller for self-balancing the roll
and pitch just to get this quadrotor to take off?</li>
</ul>

<p><strong>Edit 1:</strong>    Thanks for all the replies.</p>

<p>I got the PID in place. Actually, it is still a PD controller with the integral gain set to zero. </p>

<p>Here's how I'm writing the angles to the servo:</p>

<pre><code>motor1.write((int)(val + (kP * pError1) +(kI * iError1) +(kD * dError1)));  //front left
motor2.write((int)(val + (kP * pError2) +(kI * iError2) +(kD * dError2)));  //rear right
motor3.write((int)(val + (kP * pError3) +(kI * iError3) +(kD * dError3)));  //front right
motor4.write((int)(val + (kP * pError4) +(kI * iError4) +(kD * dError4)));  //rear left 
</code></pre>

<p>kI is zero, so I'll ignore that.</p>

<p>With the value of kP set somewhere between 0.00051 to 0.00070, I'm getting an oscillation of steady amplitude around a supposed mean value. But the problem is, the amplitude of oscillation is way too high. It is somewhere around +/- 160 degrees, which looks crazy even on a tightly constrained test rig. </p>

<hr>

<p>[  <strong>Edit 2:</strong> <em>How I calculate the term 'pError'</em> - Simple linear thresholding. </p>

<p>I've a precomputed data of the average readings (mean and SD) coming out of the gyro when the IMU is steady. Based on the gyro reading, I classify any motion of the setup as left, right, forward or backward. </p>

<p>For each of these motion, I increase the pError term for two of the motors, i.e, for right tilt, I add pError terms to motors 2 &amp; 3, for left tilt, I add pError term to motors 1 &amp; 4 etc. (check the comment lines in the code snippet given above). </p>

<p>The magnitude of error I assign to the pError term is = <em>abs(current gyro reading) - abs(mean steady-state gyro reading)</em>. This value is always positive, therefore the side that is dipping downwards will always have a positive increment in RPM.  ]</p>

<hr>

<p>As I crank up the derivative gain to around 0.0010 to 0.0015, the oscillation dampens rapidly and the drone comes to a relatively stable attitude hold, but not on the horizontal plane. The oscillation dies down (considerably, but not completely) only to give me a stable quadrotor tilted at 90 - 100 degrees with horizontal. </p>

<p>I'm using only the gyros for calculating the error. The gyros were self calibrated, hence I do expect a fair amount of noise and inaccuracy associated with the error values. </p>

<ul>
<li>Do you think that is the primary reason for the high amplitude
oscillation?</li>
</ul>

<p>One other probable reason might be the low update frequency of the errors. I'm updating the errors 6 times a second. </p>

<ul>
<li>Could that be a probable reason it is taking longer to stabilise the
error?</li>
</ul>

<p>And, for the steady state error after the wild oscillations dampen, is it necessary to fine tune the integral gain to get rid of that?</p>

<p>Please help.</p>

<hr>

<p><strong>Edit 3:</strong>  I cranked up the frequency of operation to 150+ Hz and what I get now is a very controlled oscillation (within +/- 10 degrees). </p>

<p>I'm yet to tune the derivative gain, following which I plan to recompute the errors for the integral gain using a combination of gyro and accelerometer data. </p>

<hr>

<p><strong>Edit 4:</strong>  I've tuned the P and D gain, resulting in +/- 5 degrees oscillation(approx). I can't get it to any lower than this, no matter how much I try.</p>

<p>There are two challenges about which I'm deeply concerned:</p>

<p>After 5 to 8 seconds of flight, the quadcopter is leaning into one side, albeit slowly. </p>

<p>A) Can this drift be controlled by tuning the integral gain?</p>

<p>B) Can the drift be controlled by using accelerometer + gyro fused data?</p>

<p>C) Given that my drone still shows +/- 5 degrees oscillation, can I consider this the optimal set point for the proportional and derivative gains? Or do I need to search more? (In which case, I'm really at my wits end here!) </p>
","arduino quadcopter pid ardupilot"
"2173","Visualizing kinect data on rviz","<p>I am a beginner of ROS, Kinect and Ubuntu. What I want is to visualize Kinect's data on rviz environment then run object recognition on it.</p>

<p>I've tried a few tutorials but had no luck. All I got was an empty rviz world.</p>

<p>Since I am a beginner I would appreciate any step-by-step instructions (preferably for hydro or groovy).</p>

<p>I would also like to note that I've managed to get visual from Kinect so the device is working fine.</p>
","ros kinect"
"2180","Lawn mower robot (type of cutter)","<p>If not all, but major types of lawn mower robots are rotary mowers.
I presume<sup>1</sup> that reel mower is more efficient, and is said to leave a better lawn health and cut. So, why industry go to the other option?</p>

<hr>

<p><sup>1 - I'm assuming the efficiency, as electrical rotary mowers have at least 900W universal-motors or induction motors, and a manual reel mower is capable nearly the same cutting speed.</sup></p>
","motor mechanism"
"2182","What should I use for speech recognition?","<p>I was wondering, my team and me are working on a robot communication-oriented and we wanted to add speech recognition on it.</p>

<p>What technology should I use ?</p>
","software"
"2186","Understanding Arduino bootloader","<p>That is what I came to understand while reading here and there about flashing a new bootloader/understanding what a bootloader is etc etc</p>

<p>The bootloader is supposed to be the first thing that runs when I power up my Arduino Duemilanove (or micro controllers in general). It does some setup then runs my app. It also listens to the usb cable so that if I upload some code it erases the old one and run the new one. There are 2 sections in the memory, one for the bootloader (S1) and one for the app (S2). Code on S1 can write to S2 but not to S1 (or strongly discouraged I don't remember).</p>

<p>There are things that I don't understand though :</p>

<ol>
<li><p>If I upload some new code <em>while my app is running</em>, the upload works. What happened ? I thought that the bootloader gave hand to my app</p></li>
<li><p>How can we flash a new bootloader ? If the bootloader is the thing that runs on section 1 (S1) and can only write to S2 and if the bootloader is the only thing that listens to new code uploads, ...</p></li>
</ol>

<p>Can you help me correct my thoughts and answer my questions ? Many thanks !</p>
","arduino microcontroller"
"2189","What autopilot to purchase APM 2.6 or PixHawk?","<p>I'm a newbie in UAV stuff, your advice would be very helpful, i want to start mapping using fixed wing UAV, but my main choice was APM 2.6, but after some researches, i found that APM 2.6 won't be actively maintained in the future because the future releases will be PixHawk.</p>

<p>i wonder if i should choose APM 2.6 for its stability, on the other side i don't see the benefits of Pixhawk apart having long time support. or being a newbie i should start with something experimental like APM 2.5.2 (cheap chinese version for APM).</p>

<p>Thanks in advance</p>
","uav"
"2192","How can I build a 10cm-200cm IR range sensor?","<p>Everybody here is probably aware of the Sharp distance sensors (GP2Y0 series, e.g. GP2Y0A02YK0F). They use a diode to emit infrared light and measure the angle of the reflected light with a PSD (i.e. they do triangulation). They seem to be the only producers of this technology.</p>

<p>I am only aware of a few similar but incomparable devices (sensors of ambient light and distance or proximity like Si114x). Which other comparable products are out there?</p>

<p>Another way to ask this question: ""What are the different ways to build a 10cm - 200cm range low-cost IR range sensor, and what is an example of each of those ways?""</p>
","sensors manufacturing"
"2195","Re-Calibration of an articulated industrial robot","<p>We are planning to recalibrate ABB IRB 1410 robot and conduct series of accuracy &amp; repeatability tests using FaroArm.</p>

<p>My questions are</p>

<p>i) Is there any physical identification marker on the robot which can be used to identify the location of base co-ordinate frame?</p>

<p>ii) If locating the base frame is not possible, can accuracy be measured from fixed arbitrary point in space?</p>
","industrial-robot calibration"
"2196","Robots without microcontrolers (beam robots). Are they technologically limited?","<p>BEAM robotics seem to be a good approach to teach learners about electronics in robotics. But can these robots be like regular programmed ""cognitive"" robots? Can these robots, with just analog circuits, take us to the level of robotic assistants, worker robots and other kinds of self sufficient autonomous robots?</p>

<p>I specifically want to know that, when creating mission critical robots -></p>

<p>1) What are the areas in robotics which are practically impossible without a real time software system?</p>

<p>2) What areas of the field can be done without programming? If yes, are these areas feasible without an onboard software system?</p>

<p>3) Could an intelligent space rover, work without a cpu in the future?</p>
","control software electronics artificial-intelligence embedded-systems"
"2197","How can i make a boat","<p>I'm having an event for a boat race.Simple boat has to be made.All i have is 5 days.The restriction is 24V motor not more than 1000 rpm.What best material and shape will you suggest to make a boat.I know basic circuits.We have to make a boat with a wired circuit.That circuiting i can do but what can be an ideal shape for boat with maximum speed it can achieve? </p>
","activerobot"
"2205","Low speed control of bldc motors","<p>I'm having a problem with controlling my BLDC motor when starting up and when running in low rpm. I have a custom board to measure rotation of the motor using an optical sensor and send servo pwm commands to an <a href=""http://www.hobbyking.com/hobbyking/store/__14630__Turnigy_TrackStar_18A_1_18th_Scale_Brushless_Car_ESC_.html"" rel=""nofollow"">esc</a>.
The problem is, that I can't start the motor smoothly. When I slowly increase the control signal, it starts stuttering and then jumps directly up to about 1500rpm.</p>

<p>Is there a way to improve this situation without using a sensored motor/esc combo?</p>
","motor control brushless-motor"
"2209","What are the different types of electric motors?","<p>I am beginning to learn about the hardware aspect of robotics, and in order for a lot of this new information to be useful to me (whether on this site or elsewhere) I will need a basic understanding of the terminology.</p>

<p>One thing that comes up repeatedly is different electric motors: servo, DC motor, brushless motor, step motor, gear motor... etc</p>

<p>Is there a comprehensive list? Or at least a list of the most common ones, and their descriptions / differences?</p>
","motor"
"2215","How do robotics startups work?","<p>In software engineering startups, you generally go to a room with a computer or bring your own laptop, and write code. I'm interested in how robotics startups work: Is there a separate location for designing the robots? Take for example, <a href=""http://anki.com/"" rel=""nofollow"">Anki</a>. Do they have separate research labs for designing robots? How does a robot get from a single design to being manufactured?</p>

<p>I couldn't find a better place on SE to post this (the startups business section is defunct): Please link me to another SE site if there is a better place to ask this question.</p>
","manufacturing"
"2216","what method should I use for putting rubber bands on wheels?","<p>I have a BOE bot PBasic2 stamp based robot that came with rubberband ""tires"" for the wheel. however, they are very tight and I can't figure out how to get them onto the plastic hubs.
the furthest I've gotton was mostly covering the outside, but when trying to make it less crooked it came off again.
is there some trick to getting those pesky tires to stay on?</p>
","wheeled-robot"
"2217","Access denied during PIC Programming in Windows XP","<p>I'm programming a <strong>PIC16F77</strong> with <strong>ProPic 2</strong> which communicates via serial port. As I don't have this port in my PC, I used serial to USB adapter.
I'm using <strong>ICProg</strong> in <strong>Windows 8</strong>.</p>

<p>I've proggrammed it before but it was in Windows XP using the driver who specifies in <a href=""http://www.ic-prog.com/index1.htm"" rel=""nofollow"">http://www.ic-prog.com/index1.htm</a> and worked perfectly.
But in this OS the only difference is the adapter, the program gives some errors while loading the driver:</p>

<blockquote>
  <p>""Error occured (Access is denied) while loading the driver!""</p>
  
  <p>""Privileged instruction""</p>
</blockquote>
","microcontroller"
"2218","What is the purpose of electronic braking in motors?","<p>I have a <a href=""http://letsmakerobots.com/files/userpics/u1533/Descriptive_photo_600_0.jpg"" rel=""nofollow"">Micro Magician v2 micro controller</a>. It has a A3906 Dual FET “H” bridge motor driver built in.</p>

<p>In the manual it states ""Electronic braking is possible by driving both inputs high."" </p>

<p>My first question is, what is the purpose of these brakes? If I set the left/right motor speed to 0, the robot stops immediately anyway. What advantage is there to using these brakes, or am I taking the word ""brake"" too literally?</p>

<p>My second question is, the driver has ""motor stall flags that are normally held high by pullup resistors and will go low when a motor draws more than the 910mA current limit. Connect these to spare digital inputs so your program will know if your robot gets stuck."" But when my robot hits a wall, the wheels just keep on spinning (slipping if you will), I take it these stall flags can be used on a rough surface where the wheels have more friction?</p>
","motor h-bridge"
"2222","I am an entrepreneur and I want to start building robots for businesses, where do I start?","<p>Over the last couple of years I've had good success with my technology startups and now looking to enter into robotics. I was interested in robotics and automation ever since I was a kid (yes, that sounds nerdy). So my question is: Where to get started, what to build? and how to sell? And lastly, how difficult it is to sell in this industry?</p>
","industrial-robot"
"2224","Udoo board + Kinect sensor?","<p>I am wondering if it would be possible to get Kinect to work with Udoo board (Quad). I have found that there is now support for <a href=""http://wiki.ros.org/hydro/Installation/UDOO"" rel=""nofollow"">ROS + Udoo</a>. Also saw a question asked about <a href=""http://answers.ros.org/question/113131/can-use-freenect-library-for-asus-xtion/"" rel=""nofollow"">Xtion + Udoo</a> which shows some more interest. It would really be great if it could be possible for Kinect+Udoo. Was hoping to implement perhaps a miniature version of TurtleBot. I wish someone could give some insights on this matter. Thanks.</p>
","ros kinect arm embedded-systems"
"2227","Quadcopter configuration","<p>I'm building a quadcopter. It will be controlled by a Beaglebone black with several Sensors and a cam.</p>

<p>I new to the quadcopter stuff, therefore it would be nice if someone could have a look at my setup before I buy the parts.</p>

<ul>
<li>Frame: <a href=""http://www.hobbyking.com/hobbyking/store/__29600__Hobbyking_X650F_Glass_Fiber_Quadcopter_Frame_550mm.html"" rel=""nofollow"">X650F - 550mm</a></li>
<li>Battery: <a href=""http://www.hobbyking.com/hobbyking/store/uh_viewItem.asp?idProduct=20676"" rel=""nofollow"">Turnigy nano-tech 5000mah 4S 25~50C Lipo Pack</a></li>
<li>Motor: <a href=""http://www.hobbyking.com/hobbyking/store/__25080__NTM_Prop_Drive_28_30S_800KV_300W_Brushless_Motor_short_shaft_version_.html"" rel=""nofollow"">NTM Prop Drive 28-30S 800KV / 300W Brushless Motor</a></li>
<li>ESC: <a href=""http://www.goodluckbuy.com/hobbywing-brushless-esc-4in1-25a-x-4-skywalker-quattro-throttle-hub-for-quadcopter-.html"" rel=""nofollow"">Skywalker 4x 25A Brushless</a></li>
</ul>

<p>This sums up to ~ 2kg. Giving me still some room for about 700g payload.</p>

<p>What do you think? Did I miss something important? Better ideas for some parts?</p>
","quadcopter"
"2228","Zero crossing events with brushless DC motors","<p>I would like to ask a question about zero crossing event in a trapezoidal commutation on a brush-less DC motor. Here is a waveform that shows that the zero crossing event occurs every 180 electrical degrees in a sinusoidal commutation:<br> <img src=""http://i.stack.imgur.com/lcZrH.png"" alt=""sinusoidal""></p>

<p>But what about trapezoidal commutation. Here is the waveform that I found about the trapezoidal commutation:<br> <img src=""http://i.stack.imgur.com/jIez1.png"" alt=""trapezoidal""></p>

<p>So as you see, the zero crossing occurs 30 electrical degrees after the previous commutation and 30 electrical degrees before the next commutation.
In a motor with one pole pair, we would have 30 electrical degrees = 30 mechanical degrees, so we would have this waveform: <img src=""http://i.stack.imgur.com/T5BCZ.png"" alt=""zero crossing"">
You see that the zero crossing in phase A occurs when the magnet faces the phase C, or in other words, after 30 electrical degrees from the last commutation.
My question in why does the zero crossing happen at that moment, why not after 60 electrical degrees, or 15 electrical degrees?
Is it related to some law's of induction? What are those law and how do this law's appear in this motor?
Can someone explain to me this with some pics?</p>
","brushless-motor"
"2234","Zero crossing events in trapezoidal commutation","<p>I would like to ask a question about zero crossing event in a trapezoidal commutation on a brush-less DC motor. Here is the waveform that shows that the zero crossing event occurs every 180 electrical degrees in a sinusoidal commutation:<br> <img src=""http://i.stack.imgur.com/lcZrH.png"" alt=""sinusoidal""></p>

<p>But what about trapezoidal commutation. Here is the waveform that I found about the trapezoidal commutation: <br><img src=""http://i.stack.imgur.com/jIez1.png"" alt=""trapezoidal""></p>

<p>So as you see, the zero crossing occurs 30 electrical degrees after the previous commutation and 30 electrical degrees before the next commutation.
In a motor with one pole pair, we would have 30 electrical degrees = 30 mechanical degrees, so we would have this waveform: <img src=""http://i.stack.imgur.com/T5BCZ.png"" alt=""zero crossing"">
You see that the zero crossing in phase A occurs when the magnet faces the phase C, or in other words, after 30 electrical degrees from the last commutation.
My question in why does the zero crossing happen at that moment, why not after 60 electrical degrees, or 15 electrical degrees?
Is it related to some law's of induction? What are those law and how do this law's appear in this motor?
Can someone explain to me this with some pics?</p>
","brushless-motor"
"2236","ComputerCraft (Minecraft mod) navigation: Collision avoidance and path planning/finding in 2D/3D space","<p>I'm programming Lua for controlling computers and robots in-game in the Minecraft mod <a href=""http://computercraft.info"" rel=""nofollow"">ComputerCraft</a>.</p>

<p>ComputerCraft has these robots called Turtles, that are able to move around in the <em>grid based</em>(?) world of Minecraft. They are also equipped with sensors making them able to detect blocks (obstacles) adjacent to them. Turtles execute Lua programs written by a player.</p>

<p>As a hobby project I would like to program a <code>goto(x, y, z)</code> function for my Turtles. Some Turtles actually have equipment to remove obstacles, but I would like to make them avoid obstacles and thus prevent the destruction of the in-game environment.</p>

<p>I have no prior experience in robotics, but I have a B.Sc. in Computer Science and am now a lead web developer.</p>

<p>I did some research and found some basic strategies, namely <em>grid based</em> and <em>quadtree based</em>. As I have no experience in this area, these strategies might be old school.</p>

<p>Note that Turtles are able to move in three dimensions (even hover in any height). I could share the obstacles as well as obstacle free coordinates in a common database as they are discovered if that would help me out, as most obstacles are stationary once they are placed.</p>

<p>What are my best options in this matter? Are there any <em>easy fixes</em>? Where do I look for additional resources?</p>

<p>Thank you very much in advance! :-)</p>

<p><strong>EDIT</strong>: Thank you for your feedback!</p>

<p>I started reading the book <em>Artificial Intelligence: A Modern Approach, 3rd Edition</em> to get up to speed on basic theory as suggested by <strong>Ian</strong>. Pointers to other educational resources are appreciated.</p>

<p>Also, I started developing a basic navigation algorithm for moving in unexplored areas, similar to what <strong>Cube</strong> suggested.</p>

<p>The priority for me is as few moves as possible, as it costs time and fuel cells for each additional move (approx. 0.8 seconds and 1 fuel cell per move in either direction). I plan on using the Euclidean heuristics function in a Greedy Best-First Search for computing a path that is expected to be quite optimal in reducing the number of moves to reach the goal, if enough data is available from the shared database from previous exploration.</p>

<p>Each time an obstacle is reached, I plan to use the following very basic algorithm, exploiting the fact that Turtles are able to move vertically:</p>

<pre><code>1. Calculate direct horizontal path to the goal.
2. Turn to the direction of the next step of the path.
3. If an obstacle is detected in front of the Turtle go to 5. If this is the 4th time that an obstacle is detected in front of the Turtle after moving up, go to 6.
4. Move forward, go to 2.
5. If no obstacle is detected above the Turtle, move up and go to 3, else go to 7.
6. Backtrack to the coordinates the Turtle was in before moving upwards.
7. Turn left, go to 3.
</code></pre>

<p>When using this algorithm, records are kept of the explored coordinates and uploaded to a shared database. However, there are some cases, that I did not consider:</p>

<pre><code>- When should it move down?
- What if the goal is not reachable from a coordinate directly above it?
- If no horizontal move in any direction is possible, how long should it backtrack?
- How to detect unreachable goals (obstacles can then be removed if requested)
</code></pre>

<p>Maybe if enough exploration data of the area is available, a Jump Point Search is performed to calculate an optimal path. However this assumes a 2D map. How can I take the 3rd dimension into account?</p>

<p>Also, what would be a good data structure to store the exploration data?</p>
","mobile-robot navigation"
"2244","How to know the desired orientation of a quadcopter?","<p>I am trying to simulate a quadcopter model on Simulink. I want to implement a PID controller for each of X,Y,Z and phi,theta, psi angles. PID gets the error, as input, which is to be minimized.
For the X,Y and Z, the desired values are entered by the user and the actual values are calculated from the accelerometer data, hence, the error is the desired set value - actual value.</p>

<p>For phi,theta and psi, the actual values may be obtained from the gyroscope and accelerometer (sensor fusion) but I don't actually know how to calculate the desired values for each one of them since the user is usually interested in giving the position values X,Y and Z as desired not the angle values! The absence of the desired values prevents me form calculating the angular error which is needed for the PID controller.</p>
","design pid quadcopter"
"2245","Kalman Filter and the state noise vector?","<p>I'm reading Probabilistic Robotics by Thrun. In the Kalman filter section, they state that 
$$
x_{t} =A_{t}x_{t-1} + B_{t}u_{t} + \epsilon_{t}
$$</p>

<p>where $\epsilon_{t}$ is the state noise vector.  And in
$$
z_{t} = C_{t}x_{t} + \delta_{t}
$$
where $\delta_{t}$ is the measurement noise. Now, I want to simulate a system in Matlab. Everything to me is straightforward except the state noise vector $\epsilon_{t}$. Unfortunately, majority of authors don't care much about the technical details. My question is what is the state noise vector? and what are the sources of it? I need to know because I want my simulation to be rather sensible. About the measurement noise, it is evident and given in the specifications sheet that is the sensor has uncertainty ${\pm} e$.</p>
","kalman-filter noise"
"2250","Finding inverse kinematics algorithm for a specific manipulator","<p>I need to find a way to solve invrese kinematics for Comau SMART-3 robot. Could you give me a few hints where to start looking? I have no idea about robotics and I couldn't find an algorithm for this specific robot.</p>
","inverse-kinematics"
"2251","Difference between Rao-Blackwellized particle filters and regular ones","<p>From what I've read so far, it seems that a <em>Rao-Blackwellized</em> particle filter is just a normal particle filter used after marginalizing a variable from:</p>

<p>$$p(r_t,s_t | y^t)$$</p>

<p>I'm not really sure about that conclusion, so I would like to know the precise differences between these two types of filters. Thanks in advance.</p>
","slam particle-filter"
"2256","Flipping an old manual switch (physical one)","<p>I Have an old <a href=""http://www.hifido.co.jp/photo/10/520/52010/b.jpg"" rel=""nofollow"">audio amplifier</a> that has those switches to turn it on. 
I'm looking for the simplest motor/robotic arm (or any other relevant component) to control this switch - eventually via Raspberry Pi .</p>

<p>Are there any options ?</p>
","motor raspberry-pi robotic-arm"
"2261","How to gyroscopically measure number of rotations on one axis when there is concurrent random motion on another axis","<p>Can a gyroscopic sensor (comparable to the type that are typically used in smartphones) that is embedded in this black object 
<br><img src=""http://i.stack.imgur.com/4KvPC.jpg"" alt=""some object""> <br>
that is rotating around the X axis measure the number of rotations around the X axis if the object may or may not also be rotating at the same time in random ways (number of partial or full rotations, speeds, and directions) around the Z axis?</p>

<p>If so, is the Z axis rotation irrelevant, or is there special mathematics involved in filtering out the affects of the Z rotation on the measurement of X axis rotation?  Or does another measurement such as acceleration or magnetism need to be used to solve the problem?</p>

<p>Is there any impact in using a 2-axis vs. a 3-axis gyroscopic sensor for this measurement scenario?</p>
","sensors gyroscope"
"2262","How to choose WiFi signal strength detecting sensors","<p>We want to create robot that will localize itself by the signals of wifi routers.
Which sensors should we buy to detect strength of 3 WiFi signal?
Which of following is necessary for us?
<a href=""http://www.dfrobot.com/index.php?route=product/category&amp;path=45_80"" rel=""nofollow"">http://www.dfrobot.com/index.php?route=product/category&amp;path=45_80</a>
or can be any other more suitable variants?
We are using arduino as a platform.</p>
","arduino sensors localization wifi"
"2263","Quadcopter Position Measurement (Accelerometer, GPS or Both)?","<p>I previously thought that an accelerometer on a quadcopter is used to find the position by integrating the data got from it. After I read a lot and watched <a href=""http://www.youtube.com/watch?v=C7JQ7Rpwn2k"" rel=""nofollow"">this Youtube video</a> (specifically at time 23:20) about Sensor Fusion on Android Devices, I seem to get its use a little correct. I realized that it's hard to filter out the considerable noise, generated from error integration,  to get useful information about the position. I also realized that it is used along with the gyroscope and magnetometer to for fused information about orientation not linear translation.</p>

<p>For outdoor flight, I thought of the GPS data to get the relative position, but is it so accurate in away that enables position measurement (with good precision)? How do commercial quadcopters measure positions (X,Y and Z)? Is it that GPS data are fused with the accelerometer data?</p>
","quadcopter accelerometer navigation sensor-fusion"
"2264","Are Artificial Intelligence and Robotics Different?","<p>I need help in differentiating between AI and Robotics. Are AI and Robotics two different fields or is robotics a subject in AI?</p>

<p>I want to pursue a career in AI and Robotics. So I need your valuable suggestion. I searched the web and also some universities that I want to apply and I cannot find any such thing that I am searching for.</p>
","artificial-intelligence"
"2273","Unwanted Arduino reconnect: Servo + Arduino + Python (Raspberry Pi)","<p>I am having difficulty sustaining a connection between my Raspberry Pi (Model B running <a href=""http://en.wikipedia.org/wiki/Raspbian"" rel=""nofollow"">Raspbian</a>) and my Arduino (<a href=""http://arduino.cc/en/Main/ArduinoBoardUno"" rel=""nofollow"">Uno</a>) while sending signals from the Raspberry Pi to a <a href=""http://www.pololu.com/product/2149"" rel=""nofollow"" title=""title here"">continuously rotating servo</a> (PowerHD AR- 3606HB Robot Servo) via Python. I'm not sure if there is a more efficent way of sending servo instructions via Python to the Arduino to rotate the servo. I'm attempting to communicate signals from the Raspberry Pi to the Arduino via USB using what I believe is considered a ""digital Serial connection"". My current connection:</p>

<p><code>Wireless Xbox 360 Controller
-&gt;
Wireless Xbox 360 Controller Receiver
-&gt;
Raspberry Pi
-&gt;
Externally Powered USB Hub
-&gt;
Arduino
-&gt;
Servo</code></p>

<p><img src=""http://i.stack.imgur.com/u038Q.png"" alt=""Enter image description here""></p>

<pre><code>Servo connection to Arduino:
   Signal (Orange) - pin 9
   Power (Red) - +5 V
   Ground (Black) - GND
</code></pre>

<p>On the Raspberry Pi I have installed the following (although not all needed for addressing this problem):</p>

<ul>
<li><a href=""https://github.com/Grumbel/xboxdrv"" rel=""nofollow"">xboxdrv</a></li>
<li><a href=""http://pyserial.sourceforge.net/"" rel=""nofollow"">pyserial</a></li>
<li><a href=""https://github.com/thearn/Python-Arduino-Command-API"" rel=""nofollow"">Python-Arduino-Command-API</a></li>
<li><a href=""http://www.pygame.org/news.html"" rel=""nofollow"">PyGame</a></li>
<li><a href=""https://github.com/zephod/lego-pi"" rel=""nofollow"">lego-pi</a></li>
<li><a href=""http://arduino.cc/"" rel=""nofollow"">Arduino</a></li>
</ul>

<p>The sketch I've uploaded to the Arduino Uno is the <a href=""https://github.com/thearn/Python-Arduino-Command-API/blob/master/sketches/prototype/prototype.ino"" rel=""nofollow"">corresponding sketch</a> provided with the Python-Arduino-Command-API. *Again, I'm not positive that this is the best method means of driving my servo from Python to Arduino (to the servo).</p>

<p>From the Raspberry Pi, I can see the Arduino is initially correctly connected via USB:</p>

<pre><code>pi@raspberrypi ~/Python-Arduino-Command-API $ dir /dev/ttyA*
/dev/ttyACM0  /dev/ttyAMA0
</code></pre>

<p>and</p>

<pre><code>pi@raspberrypi ~/Python-Arduino-Command-API $ lsusb
Bus 001 Device 002: ID 0424:9512 Standard Microsystems Corp.
Bus 001 Device 001: ID 1d6b:0002 Linux Foundation 2.0 root hub
Bus 001 Device 003: ID 0424:ec00 Standard Microsystems Corp.
Bus 001 Device 004: ID 045e:0719 Microsoft Corp. Xbox 360 Wireless Adapter
Bus 001 Device 005: ID 1a40:0201 Terminus Technology Inc. FE 2.1 7-port Hub
Bus 001 Device 006: ID 0bda:8176 Realtek Semiconductor Corp. RTL8188CUS 802.11n WLAN Adapter
Bus 001 Device 007: ID 046d:c52b Logitech, Inc. Unifying Receiver
Bus 001 Device 008: ID 2341:0043 Arduino SA Uno R3 (CDC ACM)
</code></pre>

<p>From the Raspberry Pi, I'm able to rotate the servo as a test clockwise for one second, counter-clockwise for one second, then stop the servo, with the following Python script:</p>

<pre><code>#!/usr/bin/env python
from Arduino import Arduino
import time

board = Arduino(9600, port='/dev/ttyACM0')

board.Servos.attach(9) # Declare servo on pin 9
board.Servos.write(9, 0) # Move servo to full speed, clockwise
time.sleep(1) # Sleep for 1 second
print board.Servos.read(9) # Speed check (should read ""0"")

board.Servos.write(9, 180)
time.sleep(1)
print board.Servos.read(9) # (Should read ""180"")

board.Servos.write(9, 90)
print board.Servos.read(9)
board.Servos.detach(9)
</code></pre>

<p>The output via the Raspberry Pi terminal reads:</p>

<pre><code>0
180
90
</code></pre>

<p>Although this only performs full-speed in both direction (as well as the calibrated ""stop"" speed of 90), I have successfully alternated from a full-speed to slower speeds, for example, going from 0 up to 90 in increments of 10.</p>

<p>From the Raspberry Pi, I'm able to send input from my Xbox controller to drive the servo with a small custom Python script I've created along with xboxdrv (which works flawlessly with other projects I'm doing):</p>

<pre><code>#!/usr/bin/python

from legopi.lib import xbox_read
from Arduino import Arduino

# To catch Ctrl+C
import signal
import sys

# The deadzone within which we ignore inputs, approximately 1/3 of total possible input
DEADZONE = 12000

def signal_handler(signal, frame):
    print ""Stopping Wrapper""
    sys.exit(0)

# Capture Ctrl+C so we can shut down nicely
signal.signal(signal.SIGINT, signal_handler)

print ""Starting Wrapper""
print ""Press Ctrl+C at any time to quit""

board = Arduino(9600, port='/dev/ttyACM0')
board.Servos.attach(9)
board.Servos.write(9, 90)

for event in xbox_read.event_stream(deadzone=DEADZONE):
    print ""Xbox event: %s"" % (event)

    # If the RB button it's being held, rotate the servo counter-clockwise at full-speed.
    # When the RB button is released, stop the servo.
    if(event.key=='RB'):
        if(event.value&gt;0):
            board.Servos.write(9, 180)
            print board.Servos.read(9)
        else:
            board.Servos.write(9, 90)
            print board.Servos.read(9)
        continue
</code></pre>

<p>This script runs, and I'm able to control the servo using the RB button on my controller. However, <strong>it eventually fails</strong> - sometimes after minutes, sometimes after seconds (rapid and intermittent input seemingly having no influence on expediting a crash). Input is no longer read by the script, the terminal comes to a halt, the servo freezes on whatever the last command given was (either spinning endlessly or stopped), and I'm forced to <kbd>Ctrl</kbd> + <kbd>C</kbd> out of the script. If I check to see if the Arduino is still connected to the Raspberry Pi, it shows that it has reconnected itself to the Raspberry Pi as ""ttyACM1"" (from /dev/ttyACM0 to /dev/ttyACM1):</p>

<pre><code>pi@raspberrypi ~/robotarm $ dir /dev/ttyA*
/dev/ttyACM1  /dev/ttyAMA0
</code></pre>

<p>Why does the Arduino reconnect itself? Is there some other way I should be processing this information? Distance to the wireless Xbox receiver is not a factor as all of these pieces are adjacent to one another for testing purposes. It will prove impossible to use this servo as a wheel for my robot if I'm constantly tending to this issue.</p>
","arduino raspberry-pi servos python"
"2274","Workable low-resolution object/target recognition pattern and library?","<p>I've spent quite some time researching this, but most of my Google search results have turned up academic research papers that are interesting but not very practical.</p>

<p>I'm working on a target/pattern recognition project** where a robot with a small camera attached to it will attempt to locate targets using a small wireless camera as it moves around a room. The targets will ideally be as small as possible (something like the size of a business card or smaller), but could be (less ideally) as large as 8x10 inches. The targets will be in the form of something easily printable.</p>

<p>The pattern recognition software needs to be able to recognize if a target (only one at a time) is in the field of vision, and needs to be able to accurately differentiate between at least 12 different target patterns, hopefully from maybe a 50x50 pixel portion of a 640x480 image.</p>

<p>Before playing with the camera, I had envisioned using somewhat small printed barcodes and the excellent <a href=""https://code.google.com/p/zxing/"" rel=""nofollow"">zxing</a> library to recognize the barcodes.</p>

<p>As it turns out, the camera's resolution is terrible - 640x480, and grainy and not well-focused. <a href=""http://i.imgur.com/MuPibcJ.png"" rel=""nofollow"">Here is an example still image.</a> It's not very well-suited for capturing barcodes, especially while moving. I think it could work with 8x10 barcodes, but that's really larger than I'm looking for. (I'm using this particular camera because it is tiny, light, cheap, and includes a battery and wi-fi.)</p>

<p>I'm looking for two things: a suggestion or pointer to an optimal pattern that I could use for my targets, and a software library and/or algorithm that can help me identify these patterns from images. I have NO idea where to start with the right type of pattern so suggestions there would really help, especially if there is a project out there that does something resembling this. I've found <a href=""http://opencv.org/"" rel=""nofollow"">OpenCV</a> and <a href=""http://robwhess.github.io/opensift/"" rel=""nofollow"">OpenSIFT</a> which both seem like potential candidates for software libraries, but neither seemed to have examples of doing the type of recognition I'm talking about. I'm thinking picking the right type of pattern is the big hurdle to overcome here, so any pointers to the optimal type of pattern would be great. Being able to recognize the pattern from all different angles is a must.</p>

<p>So far, my idea is to use patterns that perhaps look <a href=""http://i.imgur.com/jWU875q.jpg"" rel=""nofollow"">something like this</a>, where the three concentric color rings are simply either red, green, or blue - allowing for up to 27 unique targets, or 81 if I use 4 rings. From about 2 feet, the capture of a 3x3 inch target (from my computer screen) <a href=""http://i.imgur.com/JQcmBdv.png"" rel=""nofollow"">looks like this</a> which seems like it would be suitable for analysis but I feel like there should be a better type of pattern that would be more compact and easier to recognize - maybe just a plain black and white pattern of some sort with shapes on it?</p>

<p>Pointers to an optimal approach for this are greatly appreciated.</p>
","software computer-vision artificial-intelligence"
"2277","Once you understand motors, what's the next step?","<p>I've gone through tutorials on how to build circuits and control dc, stepper, and servo motors.  I may not understand everything about them internally, but i have a good basic foundation.  </p>

<p>Now i'm at a loss for where to go from here.  I'm more interested in learning how to make mechanical devices with them than just the electronics behind the devices.  While i know that they go hand in hand, i want to learn more about the mechanical aspects of using motors.  </p>

<p>I have in mind several ultimate goal projects that i want to work toward, like home automation, model rc vehicles, autonomous robots, etc...  But i'm sure that there is more to mechanics that i need to learn before i can jump into a project like that.  <strong><em>He who will learn to fly one day must first learn to stand and walk.</em></strong></p>

<p>Are there hobbyist mechanical starter kits or starter projects to learn how to make effective use of electric motors?  I don't necessarily need a specific product endorsement, but rather a general idea of what important concepts to learn and materials / projects to help me learn them.</p>

<p>My apologies if this question is too broad.  I can refine it if deemed necessary.</p>
","motor mechanism"
"2279","2D path following robot, converting XY axis path to input on wheels","<p>at the moment I am creating an android program, that will steer my simple, 3 wheel (2 motors, 1 for balance) robot to move online following the path drawn by user on his screen. The robot is operated through WiFi and has 2 motors that will react on any input signals.</p>

<p>Imagine user drawing a path for this robot on smartphone screen. It has aquired all the points on XY axis, every time beginning with (0,0). Still I have no idea, how to somehow ""convert"" just points, into voltage input to both motors. Signals will be sent in approx. 60Hz connection, so quite fast. Maybe not every single axis point will be taken into consideration, there will be surely some skips, but that is irrelevant, since this path does not have to be done perfectly by the robot, just in reasonable error scale.</p>

<p>Do you have any idea on how to make the robot follow defined axis points that overall create a path?</p>

<p>Edit 10.01:</p>

<p>The voltage will be computed by the robot, so input on both is between -255 and 255 and the velocity should increase or decrease lineary in those borders.
Additionaly, I would like to solve it as if there were perfect conditions, I don't need any feedback crazy models. Let's assume that all the data is true, no sensors and additional devices. Just XY axis path and required input (ommit wheel slide too).</p>
","wheeled-robot wifi two-wheeled"
"2283","Emulation of an Orrery","<p>Orrery is a clockwork model of the solar system. I am trying to emulate one in 2D. Now, to emulate, I need to know what goes on inside. Can someone please explain the basic principle behind the clockwork? Or direct me to a resource that will explain all the machinery inside a simple Orrery.</p>
","motor design"
"2289","Dynamics of parallel manipulator","<p>My task is to apply forces to control 3-dof parallel manipulator. Forces are applied to linear  actuators, friction is neglected.  End-effector of a robot is supposed to follow generated path; for this example, let it be a simple circle. So far I have made a simplified 3d model of robot and calculated inverse kinematics.</p>

<p><img src=""http://i.imgur.com/L8b00Iz.png"" alt=""3D model of robot""></p>

<p>Promoter of my engineering work don't really know how to do this, but he said that calculating forward dynamics is too complex and i shouldn't go that way. Could you tell me what will be the easiest way to go? </p>
","force dynamics manipulator"
"2290","KK2.0 Quad Stablility","<p>I'm running a kk2.0 + 4 20A Multistar ESCs + 4 EMax GF 2215-20 motors + 4 Slow Fly Props</p>

<p>After about a foot off the ground, the entire quadcopter starts wobbling like crazy (no auto-level). Any ideas?</p>

<p>I'll add some video if needed.</p>
","quadcopter stability"
"2295","Power Model for humanoids","<p>I am in the process of creating a Power Prediction Model for <a href=""http://en.wikipedia.org/wiki/HUBO"" rel=""nofollow"">the Hubo Robot</a>.
The Robot has 38 Degrees of Freedom and has a computer some sensors and motor boards. The motors are powered through Motor Boards. All these boards are powered through a main power board that exists at the robots chest.</p>

<p>My model should be able to predict the power for any trajectory of the robot. Say for instance if the robot raises its hand from 0 degrees to 180 degrees my model should be able to predict the power.</p>

<p>Heres an idea I came across. My idea was to equate the electrical torque to the mechanical torque of each joint.</p>

<p>For instance if the Right arm pitch moves from 0 to 180 degrees I can do as follows ?
$mgsin(\theta)= Kt*I$</p>

<p>However, I am not getting a proper prediction and the current value is way off than what we can read from a software installed in the robot. I know there are losses but even then its off. I was wondering if there are any other approaches or a fault in my approach.</p>

<p>And after I do this I can add all the joint currents for a specific trajectory and then give a estimate for total power consumption.</p>
","brushless-motor power inverse-kinematics motion-planning torque"
"2297","Quadcopter PID tuning","<p>In continuation of the question I asked here: <a href=""http://robotics.stackexchange.com/questions/2167/quadcopter-instability-with-simple-takeoff-in-autonomous-mode"">Quadcopter instability with simple takeoff in autonomous mode</a>
  ...I'd like to ask a few questions about implementing a basic PID for a quadrotor controlled by an APM 2.6 module. (I'm using a frame from 3DRobotics)</p>

<p>I've stripped down the entire control system to just two PID blocks, one for controlling roll and another for controlling pitch (yaw and everything else... I'd think about them later).</p>

<p>I'm testing this setup on a rig which consists of a freely rotating beam, wherein I've tied down two of the arms of the quadrotor. The other two are free to move. So, I'm actually testing one degree of freedom (roll or pitch) at a time.</p>

<p>Check the image below: here A, B marks the freely rotating beam on which the setup is mounted.
<img src=""http://i.stack.imgur.com/CbgYO.png"" alt=""enter image description here""></p>

<p>With careful tuning of P and D parameters, I've managed to attain a sustained flight of about 30 seconds. </p>

<p>But by 'sustained', I simple mean a test where the drone ain't toppling over to one side. Rock steady flight is still no where in sight, and more than 30 secs of flight also looks quite difficult. It wobbles from the beginning. By the time it reaches 20 - 25 seconds, it starts tilting to one side. Within 30 secs, it has tilted to one side by an unacceptable margin. Soon enough, I find it resting upside down </p>

<p>As for the PID code itself, I'm calculating the proportional error from a 'complimentary filter' of gyro + accelerometer data. The integral term is set to zero. The P term comes to about 0.39 and the D term is at 0.0012. (I'm not using the Arduino PID library on purpose, just want to get one of my own PIDs implemented here.)</p>

<p>Check this video, if you want to see how it works.</p>

<p><a href=""http://www.youtube.com/watch?v=LpsNBL8ydBA&amp;feature=youtu.be"">http://www.youtube.com/watch?v=LpsNBL8ydBA&amp;feature=youtu.be</a>
[Yeh, the setup is pretty ancient! I agree. :)]</p>

<p>Please let me know what could I possibly do to improve stability at this stage.</p>

<p>@Ian: Of the many tests I did with my setup, I did plot graphs for some of the tests using the reading from the serial monitor. Here is a sample reading of Roll vs 'Motor1 &amp; Motor2 - PWM input' (the two motors controlling the roll):</p>

<p><img src=""http://i.stack.imgur.com/rnyIi.png"" alt=""Roll vs Motor PWM input""></p>

<p>As for the input/output:</p>

<p><strong>Input:</strong> Roll and pitch values (in degrees), as obtained by a combination of accelerometer + gyro</p>

<p><strong>Output:</strong> PWM values for the motors, delivered using the Servo library's motor.write() function</p>

<hr>

<p><strong>Resolution</strong></p>

<p>I resolved the problem. Here's how:</p>

<ol>
<li><p>The crux of the issue lied in the way I implemented the Arduino program. I was using the write() function to update the servo angles, which happens to accept only integer steps in the argument (or somehow responds only to integer input, 100 and 100.2 produces the same result). I changed it to writeMicroseconds() and that made the copter considerably steadier.</p></li>
<li><p>I was adding up RPM on one motor while keeping the other at a steady value. I changed this to increase RPM in one motor while decreasing the opposing motor. That kinda keeps the total horizontal thrust unchanged, which might help me when I'm trying to get vertical altitude hold on this thing.</p></li>
<li><p>I was pushing up the RPM to the max limit, which is why the quadcopter kept losing control at full throttle. There was no room for the RPM to increase when it sensed a tilt.</p></li>
<li><p>I observed that one of the motor was inherently weaker than the other one, I do not know why. I hardcoded an offset into that motors PWM input.</p></li>
</ol>

<p>Thanks for all the support.</p>

<hr>

<p><strong>Source Code:</strong></p>

<p>If you're interested, here's the source code of my bare-bones PID implementation: <a href=""https://github.com/agnivsen/NoobCopter"">PID Source Code</a></p>

<p>Please feel free to test it in your hardware. Any contributions to the project would be welcome.</p>
","arduino pid quadcopter stability"
"2298","How much accuracy could I get position tracking with a 3-axis accelerometer and gyro sensor, and compass, and how would I do it?","<p>Given a 12' x 12' field (4m x 4m), a reasonably cheap 3-axis gyro sensor and accelerometer, and compass, I plan to design a device capable of tracking its position to sub-centimeter accuracy for a minute of motion or so.</p>

<p>The device has a holonomic drive system, capable of moving any direction at a maximum of about 8mph (3.6m/s), with a maximum acceleration of about 2g's. However, there are some simplifying constraints. For one, the field is nearly flat. The floor is made of a tough foam, so there is slight sinking, but the floor is flat except for a ramp of known angle (to a few degrees). The device will, excepting collisions, not be rising above the floor.</p>

<p>Accuracy is preferred over simplicity, so any mathematics required on the software side to improve the system would be welcomed.</p>

<p>Before I definitively choose accelerometers as the method of position tracking, though, I would like some idea of how much accuracy I could get, and the best ways of doing it.</p>
","kinematics accelerometer machine-learning"
"2299","Where to start for the software side of Robotics?","<p>I am a Computer Science student entering my last year of college. I'm pretty sure Robotics is what I want to eventually be doing based on my interests in AI and embedded systems. I've seen a lot of topics that covers Robotics such as: control theory, signal processing, kinematics, dynamics, 3D simulators, physics engines, AI, Big Data with machine learning. I'm hoping someone can point me in the right direction as to what I should be attempting to study in my interests of Robotics. I am not sure what other topics I have not mentioned that would be relevant. I would like to deal with the software side of Robotics, both AI and none AI.</p>

<p>My other question is about machine learning. I've seen researchers applying machine learning (deep learning/unsupervised learning specifically) to robotics but how do they do this? Is information and data transferred from the internals of the robot to an external computer that does the data processing? Machine learning requires a lot of data to predict. Is this the only way machine learning can be used in robotics (through an external computer)?</p>

<p>I hope someone can touch on some of the things I've mentioned, Thank you.</p>
","mobile-robot software artificial-intelligence programming-languages"
"2304","System for determining occupied seats in an auditorium","<p>I need an app that can do live monitoring of whether each seat in an auditorium is occupied,  so visitors can load the app and see where to sit.    </p>

<p>The auditorium has a relatively flat ceiling 4m high, and the seats are .5m wide. 
The hardware cost per seat needs to be $5.</p>

<p>I'm looking for all solutions.  Web cams, preasure sensors, sonars, lasers, arduino, pi, intel edison, anything. </p>

<p>Obviously there cannot be wires that people could trip over.  Sensors on the ceiling could have wired networking.  Sensors on the seat or floor would need to have wireless communication.  sensors on the ceiling would need to consider occlusion by people sitting in the seats (think, if there is an empty spot between 2 people, can the sensor see it as empty)</p>

<p>In the end, the data needs to be collected as a simple list of which chairs are occupied/open</p>

<p><img src=""http://i.stack.imgur.com/dGKyO.png"" alt=""enter image description here""></p>

<p>Possible solutions:</p>

<ul>
<li>rasberry pi's on the ceiling every 8 seats with a camera.   </li>
<li>pressure sensors under chair legs wired to pi's gpio</li>
<li>Drones flying around the auditorium :)</li>
</ul>

<p>Any ideas?</p>

<p><strong>Update (more constraints):</strong></p>

<ul>
<li>auditorium size is 400 seats</li>
<li>Installation costs should average 10 chairs per hour(400/10 = 40 hours)  </li>
<li>as the picture shows, chairs are cushioned</li>
<li>regular maintenance should take no longer than 30 min. per 2-hour event(eg, batteries)</li>
<li>hardware should last 100 sessions</li>
<li>for auditorium cleaning, it should be possible to ""disconnect"" and ""reconnect"" the chairs with 4 hours of labor.</li>
</ul>
","arduino sensors raspberry-pi computer-vision"
"2315","Overcorrecting Kalman Filter","<p>I'm trying to get an extended Kalman Filter to work. My System Model is:</p>

<p>$ x = \begin{bmatrix}
 lat \\
 long \\
 \theta
\end{bmatrix}$</p>

<p>where <em>lat</em> and <em>long</em> are latitude and longitude (in degree) and $\theta$ is the current orientation of my vehicle (also in degree).
In my Prediction Step I get a reading for current speed <em>v</em>, yaw rate $\omega$ and inclination angle $\alpha$:</p>

<p>$z = \begin{bmatrix}
 v \\
 \alpha\\
 \omega 
 \end{bmatrix}$</p>

<p>I use the standard prediction for the EKF with $f()$ being:</p>

<p>$
\vec{f}(\vec{x}_{u,t}, \vec{z}_t) = \vec{x}_{u,t} + 
 \begin{bmatrix}
  \frac{v}{f} * \cos(\theta) * \cos(\alpha) * \frac{180 °}{\pi * R_0} \\
  \frac{v}{f} * \sin(\theta) * \cos(\alpha) * \frac{180 °}{\pi * R_0} * \frac{1}{\cos(lat)} \\
  \frac{\omega}{f}
 \end{bmatrix}
$</p>

<p>$f$ being the prediction frequency, $R_0$ being the radius of the earth (modelling the earth as a sphere)</p>

<p>My Jacobian Matrix looks like this:</p>

<p>$
C = v \cdot \Delta t \cdot cos(\alpha) \cdot \frac{180}{\pi R_0}
$</p>

<p>$
F_J =
\begin{pmatrix}
  1 &amp; 0 &amp; -C \cdot sin(\phi) \cdot \frac{1}{cos(lat)} \\
  -C \cdot sin(\phi) \cdot \frac{sin(lat)}{{cos(lat)}^2} &amp; 1 &amp; C \cdot cos(\phi) \cdot \frac{1}{cos(lat)}\\
  0 &amp; 0 &amp; 1
\end{pmatrix}
$</p>

<p>As I have a far higher frequency on my sensors for the prediction step, I have about 10 predictions followed by one update.</p>

<p>In the update step I get a reading for the current GPS position and calculate an orientation from the current GPS position and the previous one. Thus my update step is just the standard EKF Update with $h(x) = x$ and thus the Jacobian Matrix to $h()$, $H$ being the Identity.</p>

<p>Trying my implementation with testdata where the GPS Track is in constant northern direction and the yaw rate constantly turns west, I expect the filter to correct my position close to the track and the orientation to 355 degrees or so. What actually happens can be seen in the image attached (Red: GPS Position Measurements, Green/blue: predicted positions): <img src=""http://i.stack.imgur.com/KCasT.png"" alt=""Red: GPS Position Measurements, Green/blue: predicted positions""></p>

<p>I have no Idea what to do about this. I'm not very experienced with the Kalman filter, so it might just be me misunderstanding something, but nothing I tried seemed to work…</p>

<p>What I think:</p>

<p>I poked around a bit: If I set the Jacobian Matrix in the prediction to be the identity, it works really good. The Problem seems to be that $P$ (the covariance Matrix of the system model) is not zero in $P(3,1)$ and $P(3,2)$. My interpretation would be that in the prediction step the Orientation depends on the Position, which does not seem to make sense. This is due to $F_J(2,1)$ not being zero, which in turn makes sense.</p>

<p>Can anyone give me a hint where the overcorrection may come from, or what I should look at / google for?</p>
","kalman-filter gps sensor-fusion"
"2321","Tracking 2D positioning with IMU Sensor","<p>I am using a miniature car and I want to estimate the position. We can not use GPS modules and most of the tracking systems that I saw, are using IMU senson with the GPS module. In our car we are able to find our exact correct location with image processing but for some parts that dont have enough markings we can not do this. So we want to use the IMU as backup for our positioning. so as long as the positioning is close is good for us.</p>

<p>And we are only interested in our 2D position since the car is on a flat ground.</p>

<p>I am using a IMU 9DOF sensor and I want to calculate my movement. I have seen some amazing works with IMU for tracking body movements but no code or simple explanation is anywhere about it. So basically I have the reading from accelerometer, gyro and magnetometer. I also have orientation in quarternions. From the device I am getting also the linear acceleration but even when I am not moving it in any direction the values are not 0 which is really confusing.</p>

<p>Can you please help me how to approach this?</p>

<p>Thanks in advance</p>
","sensors accelerometer gyroscope"
"2322","What can be the rating and specifications of dc motor used for making a quadcopter?","<p>I want to make a quadcopter for my final year project and I am willing to use DC motors as the four rotors of the quadcopter. Can any one guide me about the ratings for proper motor selection for my job.</p>
","quadcopter"
"2324","Kinematics of a 4 wheeled differential drive robots","<p>I have a 4 wheeled differential drive robot, like the Pioneer 3-AT. There are only two motors, one for left wheels and one for right wheels.</p>

<p>I want to send velocity commands to the robot, I'm using ROS and the standard commands are: [linear_velocity, angular_velocity].</p>

<p>I need to convert them into left and right velocities, from literature if I had 2 wheels I should do this:</p>

<p>$v_l = linear_v - \omega * |r|$</p>

<p>$v_r = linear_v + \omega * |r|$</p>

<p>where |r| is the absolute value of the distance from the wheels to the robot ""center"".</p>

<p>How should I take into account that I have 4 wheels?</p>
","wheeled-robot inverse-kinematics wheel"
"2326","problem with simulated sensor in Matlab?","<p>I'm simulating a sensor in 3D. The sensor should determine ($p, \theta, \phi$) from the origin  where $\theta$ is the rotation about z-axis and $\phi$ is the rotation about x-axis. The sensor is given position of a point($x, y, z$). This is what I did</p>

<pre><code>    p = sqrt(x^2 + y^2 + z^2);
theta = acos(z/p);   &lt;---- I'm guessing the problem here
  phi = atan2(y,x);
</code></pre>

<p>Now I need to get the Cartesian coordinates ($x',y',z'$). This is what I did</p>

<pre><code>    [p theta phi] = getmeasurement(x, y, z);
    x' = p*cos(theta)*sin(phi);
    y' = p*sin(theta)*sin(phi);
    z' = p*cos(phi); 
</code></pre>

<p>The sensor is working fine at the beginning but at a particular point it behaves strangely. I have the state vector to compare it with the measurement. I'm guessing that $\theta$ might be the problem. </p>

<hr>

<p>Edit: </p>

<p>I'm sorry for this mistake. The aforementioned calculations based on the following picture</p>

<p><img src=""http://i.stack.imgur.com/BYMzR.png"" alt=""spherical Coordinates""></p>

<p>So, the point will rotate first about z-axis ($\theta$) and then rotate about x-axis ($\phi$)</p>
","sensors sensor-error"
"2330","Is it possible to achieve fully autonomous route following using PX4FMU module?","<p>I have a quadcopter equipped with PX4FMU board. You may download its datasheet from <a href=""http://pixhawk.org/_media/modules/px4fmu-manual-v1.6.pdf"" rel=""nofollow"">HERE</a>.
I wonder whether it is possible to program the quadcopter to autonomously follow a path like circular motion without any human interference. Are the built-in sensors enough for this task?
I also wonder how accurate the built-in GPS is? I read that it gives coordinates with a radius of 5m as error.</p>
","quadcopter"
"2331","What is the Kalman Filter in the basics of its aspects?","<p>My question is very broad. However I would like a complete description to the very last detail in a way that a foreign exchange student would understand. I want to try my best to master the way the Kalman Filter works. Please be as through as you possibly can, and more.</p>
","kalman-filter"
"2336","learning (embedded) electronics","<p>I am an aerospace engineer (currently in grad school) and I really want to get into (embedded) electronics.
But I have this problem:
I understand the theory fairly well, I took an edX course in circuits and had no problem. I can build projects from the internet. However, I have a very hard hard time connecting the theory with the practical part, understanding why projects are done the way they are done and I have a hard time to design my own projects!</p>

<p>Please help!</p>

<p>I'd appreciate the following:
-General tips: how did you learn it? How is your workflow? What should I do? Which steps should I take?
-Books: which hands on books and websites can you recommend? I am looking for books and website that are practical but also explain the why
-Kits: What kits can you recommend that combine the theory with the practical?
-Anything you think is important</p>

<p>Thank you for your time!</p>
","electronics"
"2337","Monte-Carlo Localization","<p>I'm implementing Monte-Carlo localization for my robot that is given a map of the enviroment and its starting location and orientation. Mine approach is as follows:</p>

<ol>
<li>Uniformly create 500 particles around the given position</li>
<li>Then at each step:
<ul>
<li>motion update all the particles with odometry (my current approach is newX=oldX+ odometryX(1+standardGaussianRandom), etc.)</li>
<li>assign weight to each particle using sonar data (formula is for each sensor probability*=gaussianPDF(realReading) where gaussian has the mean predictedReading)</li>
<li>return the particle with biggest probability as the location at this step </li>
<li>then 9/10 of new particles are resampled from the old ones according to weights and 1/10 is uniformly sampled around the predicted position</li>
</ul></li>
</ol>

<p>Now, I wrote a simulator for the robot's enviroment and here is how this localization behaves:
<a href=""http://www.youtube.com/watch?v=q7q3cqktwZI"">http://www.youtube.com/watch?v=q7q3cqktwZI</a></p>

<p>I'm very afraid that for a longer period of time the robot may get lost. If add particles to a wider area, the robot gets lost even easier. </p>

<p>I expect a better performance. Any advice?</p>
","localization motion-planning sonar"
"2341","How do ultrasonic range finders detect objects at an angle?","<p>As far as I can tell, an ultrasonic rangefinder works by reflecting inaudible soundwaves off of objects and timing their return.  But if the object has a flat surface and is angled with respect to the line to the rangefinder, how does it detect that object?  Under what circumstances might it give a false distance or otherwise fail to detect the object?</p>
","sensors sonar"
"2345","Simple web interface with beaglebone black","<p>This is actually a very simple question, but I'm lost at the moment.</p>

<p>I was using a beaglebone black for a school project. It controls a bunch of motors and actuators,etc. We wrote everything in C++, and made libraries of functions. When a main program calls them, the functions run just fine.</p>

<p>Recently we have been told to demo our progress so far. The main program is nowhere near done, so we were thinking of some sort of web interface that can execute the complied C++ program on command. We were hoping to get the server hosted on the board, and access it via LAN from other PCs. But I've never done this before and have no idea where to start. Does node.js (with the 'bonescript') going to be of any help? Or is there a simpler way with basic HTML?</p>

<p>I only have a few days to figure it out, so I didn't want to waste time looking at the wrong methods.</p>
","embedded-systems"
"2347","The costs of using existing 6 axis kuka/abb robots and existing vision systems in picking and placing tasks","<p>I would like to use a Kuka/abb 6 axis robot and a machine vision system to pick and place a variety of metal drill bits in size ranges from 0.5mm (ascending up 0.5mm per cylinde) to 13mm in metric and then 1/16 of an inch ascending to 9/64 of an inch. The machine would not have to differentiate between the bits, niether drill bit would weight more than 1kg.</p>

<p>Crucially at the beginning and end of the picking and placing I would like to inspect the very tip of the cylinders to be inspected for a 118 degree chamfer on one end of the bit which should be present regardless of drill diameter of length. </p>

<p>I am lead to believe that if the drill bits are placed end up on a conveyor belt and always the the same place its relatively low cost but crucially if the kuka 6 axis robot has to find the drill bits then the cost increases dramatically, is this true?</p>
","industrial-robot"
"2348","What motor to use for reciprocating (reversive) movement","<p>I want to make a copy of this machine <a href=""https://www.youtube.com/watch?v=16x-_9pcWXU"" rel=""nofollow"">Fisher Price Soothing Motions™ Glider</a> and I'm wondering what motor to use? Simple DC motor with appropriate gearbox (slow rpm) or stepper motor?</p>

<p><a href=""http://www.youtube.com/watch?feature=player_embedded&amp;v=KvlVKIuXGuI"" rel=""nofollow"">Here</a> is another instance of this idea.</p>
","motor stepper-motor mechanism"
"2350","Electronic noses for detecting dog urine","<p>I have very limited experience with sensors or robotic components at all, and I hope you will excuse the lack of detail in this Question. </p>

<p>I want to set up posts around my yard with electronic noses that detect dog urine. I want to use this information to make a map of my yard from a dogs perspective. Is it possible with todays technology? What would it cost? There may be information that is very relevant to me, but that I'm not requesting. This is because of lacking insight. If there is something you think I should consider or research, please say so. </p>
","sensors electronics"
"2356","Line follower robot program","<p>I am working on a line follower robot as part of my Microelectronics project, and am confused over what sort of code to use to program the ""pic18f"" microcontroller I'm using. Can someone give me source code or a layout of the code and what should be in there?</p>
","c line-following"
"2357","Cons and pros of wireless technologies for rescue robot","<p>robotics enthusiasts!</p>

<p>I'm a member of a team which has to develop a mobile rescue robot to cooperate with firemen (e.g. on earthquake sites).</p>

<p>The problem we have is connection of a commander post with the robot. The robot has to enter buildings, so it is desirable that the connection can go through several decimeters of walls and have a reach 50-100 meters. On the other hand, we need to send a lot of data (camera images, point clouds, maps) which can easily eat 10 Mbps or more.</p>

<p>At this time we use 2.4 GHz WiFi for this connection. As for the speed, with direct visibility it seems to be sufficient, but only when a single robot is operating (we can use up to 3 non-overlapping channels, so in theory 3 robots can work together; but usually you have the environment messed up with home routers). We need at least 5 robots to be operating simultaneousely.</p>

<p>We have tried 5 GHz WiFi, but it has problems with penetrating walls, so it can only be used for UAVs.</p>

<p>My idea was to use some mobile connection technology like LTE. I found that LTE can be run on 800 MHz, which could be great for the wall penetration performance. I also found that the LTE's theoretical upload speeds (for clients) are 70 Mbps, but nobody says if it is on 2.6 GHz and how would it be changed when running LTE on 800 MHz.</p>

<p>Moreover, we cannot rely on some provider's coverage. I have found that you can build your own LTE transmitter for about €2000, which seems to be interesting to us. Maybe it is possible to build it even cheaper. But we think both 2.6 GHz and 800 MHz are regulated frequencies. However, the cooperation with firefighters could persuade local regulators to give an exception to us to setup our own small LTE base station.</p>

<p>And now to the question: do you think such setup would give better results than using WiFi? Or do you know of any other technologies that would help us to either increase the bandwidth or the wall penetration performance? What are their cons and pros?</p>
","wireless"
"2361","Explanation of the Kalman Filter","<p>I am a beginner in robotics, and I am learning about the Kalman filter. I do not seem to get it, though. I am a mathematician, and so it would be helpful if the Kalman filter could be explained in a mathematical method.</p>
","kalman-filter"
"2362","Actuator to control steam valve","<p>I have a steam radiator at home and it has a valve similar to the picture below.</p>

<p><img src=""http://i.stack.imgur.com/Ukvhd.jpg"" alt=""enter image description here""></p>

<p>Please note that the valve doesn't have grooves on top to attach things to.</p>

<p>I want to build something to turn it on and off depending on the temperature at certain points in the room.</p>

<p>I have that taken care of but cannot find a way to attach a actuator(actuator is the right word in the context I guess?) to turn the valve in both directions.</p>

<p>Also It is a rented apartment so I would like to avoid making any modifications to the radiator itself.</p>
","actuator valve"
"2363","Building a mobile camera platform","<p>I have zero experience with robotics, but I need to build a mobile platform for a streaming camera. The idea is that I'll plug in my Android phone into the pan/tilt unit on my wheeled robot and then drive and look around via WiFi. I have already solved all of the software, interface and controller issues, but I would appreciate some advice on how to build the wheeled platform.</p>

<p>My initial idea was to buy a cheap RC car, remove all electronics and replace them with my own. This approach almost worked. I purchased this New Bright F-150 Truck.  The size is good and there is plenty of storage space:</p>

<p><a href=""http://i.stack.imgur.com/rofI9.jpg"" rel=""nofollow""><img src=""http://i.stack.imgur.com/rofI9m.jpg"" alt=""New Bright F-150 Truck""></a>
<a href=""http://i.stack.imgur.com/QAlFU.jpg"" rel=""nofollow""><img src=""http://i.stack.imgur.com/QAlFUm.jpg"" alt=""enter image description here""></a></p>

<p>However, I quickly ran into a problem with this thing. I assumed that the front wheel would be turned by some kind of servo. Instead I found this nonsense:
<img src=""http://i.stack.imgur.com/LFdWC.jpg"" alt=""enter image description here"">
<img src=""http://i.stack.imgur.com/9Ouzy.jpg"" alt=""enter image description here""></p>

<p>That small gear shaft is not driven by a servo - it's a conventional motor, which spins until it is jammed at the extremes of travel. The wheels are straightened when power is removed by a small spring on the other side. This means that there is only one angle at which the wheels can be turned, and that angle is way too small for what I need. So using this RC car will not work.</p>

<p>Before I start buying more things, I would like to hear some opinions from more experienced people. Am I on the right track? Do I simply need to get a better RC car, or are they all designed like this? Perhaps there are other options that would be more suitable for what I am doing?</p>
","mobile-robot"
"2365","covariance matrix in EKF?","<p>I'm struggling with the concept of covariance matrix. 
$$
\Sigma
= 
\begin{bmatrix}
\sigma_{xx} &amp; \sigma_{xy} &amp; \sigma_{x \theta} \\
\sigma_{yx} &amp; \sigma_{yy} &amp; \sigma_{y \theta} \\
\sigma_{\theta x} &amp; \sigma_{\theta y} &amp; \sigma_{\theta \theta} \\
\end{bmatrix}
$$
Now, my understanding for $\sigma_{xx}$, $\sigma_{yy}$, and $\sigma_{\theta \theta}$ that they describe the uncertainty. For example, for $\sigma_{xx}$, it describes the uncertainty of the value of x. Now, my question about the rest of sigmas, what do they represent? What does it mean if they are zeros? I can interpret that if $\sigma_{xx}$ is zero, it means I don't have uncertainty about the value of x. </p>

<hr>

<p>Note, I'm reading <a href=""http://mitpress.mit.edu/books/principles-robot-motion"" rel=""nofollow"">Principles of Robot Motion - Theory, Algorithms, and Implementations</a> by Howie Choset et. al., which states that</p>

<blockquote>
  <p>By this definition $\sigma_{ii}$ is the same as $\sigma_{i}^{2}$ the variance of $X_{i}$. For $i ≠ j$, if $\sigma_{ij} = 0$, then $X_{i}$ and $X_{j}$ are independent of each other.</p>
</blockquote>

<p>This may answer my question if the rest of sigmas are zeros however, I'm still confused about the relationship between these variables for example $x$ and $y$. When does this happen? I mean the correlation between them. Or in other words, can I assume them to be zeros?</p>

<p>Another book namely <a href=""http://www.amazon.ca/FastSLAM-Scalable-Simultaneous-Localization-Robotics/dp/3540463992"" rel=""nofollow"">FastSLAM: A Scalable Method ...</a> by Michael and Sebastian which states </p>

<blockquote>
  <p>The off-diagonal elements of the covariance matrix of this
  multivariate Gaussian encode the correlations between pairs of state
  variables.</p>
</blockquote>

<p>They don't mention when the correlation might happen and what does it mean?</p>
","kalman-filter noise"
"2371","Compensating for Yaw in Lateral Quadcopter Movement","<p>I'm trying to make a quadcopter move laterally at a certain angle. I've been able to find the proper roll and pitch angles for this (that work with a yaw of 0°); how would I adjust these values to compensate for a different yaw?</p>
","quadcopter gyroscope movement dynamics"
"2373","Using vision for Monte-Carlo localization","<p>From each step of my vision code I am able to get around 400 coordinates of where the robot thinks the walls are</p>

<p>I want to integrate this into Monte-Carlo observation step.</p>

<p>I'm storing the map of the maze as a set of Line segments. What would be a nice way to implement the sensor update, i.e. given the position (x,y) of the robot what is the probability that it is found there given the above described coordinates of the walls.</p>

<hr>

<p>The main idea I currently have:</p>

<p>Transform points in polar coordinates. Then for each point (from vision output) compute a ray with this angle and find the first intersection with the maze. Now we have the predicted distance and real distance and we can compute the probability that this measurement is right. </p>

<p>The main drawback is that this is slow. For each point from vision output I have to iterate over all line segments to find the one with the closest intersection. The line segments number is around 50. So it gets to O(400*50*Particle number).</p>
","mobile-robot localization computer-vision"
"2374","Maximum angle between the camera pose to correctly estimate homography","<p>I want to capture two views of same scene. The scene consists of a set of objects kept on a table. From the two views, I wish to calculate homography for image matching. I want to know what is the maximum angle between the two views  such that the homography can be accurately calculated. Right now, I am capturing the images at around 60 degrees of angle, but unable to construct homography accurately.</p>
","kinect computer-vision stereo-vision"
"2376","Kinect for Xbox: SDK selection","<p>My application is basically about sound source localization and visual servoing. I selected Kinect as the main hardware.</p>

<p>I already know the basic differences between Kinect for <em>Windows</em> and Kinect for <em>Xbox</em>. I cannot access to windows version from my country (no reseller here in Turkey), but the xbox version is there at the stores. I am not sure about problem specific software selection.</p>

<p>I found out that the latest Kinect SDK supports sound source localization (and beamforming) using the built-in microphone array. <strong>Can I use that SDK within the xbox version? Or is there another SDK for xbox, having the same support?</strong> I am not sure because I also read that OpenNI does not provide the best audio API.</p>

<p>I will also apply some processing on image &amp; depth outputs, so I will be using OpenCV. I also want to use Qt for threading, GUI etc. So, another question: <strong>Is it possible to use the microsoft official kinect SDK within another IDE, not Visual Studio?</strong></p>
","kinect"
"2379","Water depth arduino sonar sensor","<p>Just like a fish finder finds the depth of the water directly beneath it, Im trying to find a sensor that I can purchase for the Arduino that does the same. would like it to check up to 20 ft at least, with high accuracy, +/- 10 or 15 cm. All the threads and info I've been finding are water level sensors not for water depth. So does anyone know of a sensor like this and where I can find one?</p>
","arduino microcontroller underwater"
"2382","Motion Model for Holonomic Robot","<p>We are working with an holonomic robot equipped with three (120 degree shifted) omnidirectional wheels. The relative movement is estimated by dead reckoning using wheel encoders. To improve this estimation we installed an gyroscope to measure the change in orientation. Furthermore the robot has a 270 degree laser range finder. </p>

<p>In order to solve the kidnapped robot problem we implemented a particle filter. In every step each particle is updated according to the odometry and gyroscope readings. Since these readings are distorted by noise we need a motion model to include these errors. As described in Probabilistic Robotics by Thrun (Page 118 - 143) there are two commonly used motion models (velocity motion model and odometry motion model). However these models seem to describe the behavior of differential drive robots not omnidirectional robots. I base this thesis on the fact that the error in relative y-direction is proportional to the error in orientation as far as the motion models by Thrun are concerned. This is appropriate for differential drive robots as the orientation and the heading of the robot are identical. For omnidirectional robots this assumption can not be made since the heading and the orientation are completely independent. Even if we assume perfect information about the robots orientation we can still obtain error in relative y-direction.</p>

<p>I would like to discuss if my assumption - that the velocity/odometry motion model fails for omnididrectional robots - is correct or not as i am not sure about that. Furthermore  i am curious if there are any other motion models for omnidirectional robots that might fit better.</p>
","mobile-robot localization motion particle-filter"
"2387","How can I start programming proto X quad","<p>I have bought a really small <a href=""http://www.ebay.com/itm/like/231129032058?lpid=82"" rel=""nofollow"">proto X quad</a> (it has a joystick which navigates the device) and I am looking for a way to send a signal to this thing from my computer.</p>

<p><img src=""http://i.stack.imgur.com/381Yl.jpg"" alt=""enter image description here""></p>

<p>So can anyone point me how can I turn on one of the propellers of this quad using my laptop (I have a decent knowledge in python/matlab/C# but hardware is a completely new world to me).</p>
","quadcopter programming-languages"
"2394","LIDAR solutions","<p>I am surprised by the price range of Lidar applications considering the simplicity of the design. I try to make a simple project that requires lidar for object recognitions etc. I wouldn't like to use Visual Recognition such as OpenCV. </p>

<p>Regardless of that I am trying to understand why Lidar solutions are so expensive you can see
that this 
<a href=""http://velodynelidar.com/lidar/hdlproducts/hdl32e.aspx"" rel=""nofollow"">http://velodynelidar.com/lidar/hdlproducts/hdl32e.aspx</a>
Small lidar sensor goes for 20,000$. </p>

<p>I strongly believe that Lidar is the next step in robotic applications but I am not sure why it is so EXCLUSIVE. </p>

<p>I have seen few projects that go for around 200$ but their performance is very bad. </p>

<p>I hope you can answer what makes a Lidar so expensive and what are some cheap systems a hobbyist can afford. </p>
","arduino computer-vision lidar"
"2396","Generic name for two-motor wheeled/tracked robots?","<p>Is there a generic name for the category of robots that move using two opposing wheels or tank-like treads?</p>
","mobile-robot differential-drive"
"2402","How to: Attach wheel encoder to motor?","<p>I have a couple of these DC motors</p>

<p><a href=""http://www.pololu.com/product/2202"" rel=""nofollow"">http://www.pololu.com/product/2202</a></p>

<p>Which have an extended motor shaft that sticks out the back and is 1mm diameter. </p>

<p>I'm having trouble trying to think of the best way to attach an encoder disk to this shaft.</p>

<p>I thought of getting a custom wheel 3d printed and make the opening 0.9mm so it will be a tight fit. But I don't know if is just to small?</p>

<p>I also though of taking the encoder disks from a PC mouse and drilling a 1mm / 0.9mm but its the same problem but with the added difficultly of trying to drill a small hole on a small thing.</p>

<p>So I wondered if anyone knows a better way, or of a made disk to attach. As I just can't find anything for a 1mm shaft</p>
","motor"
"2409","Connections on a Baby Orangutang B-328 board","<p>I am new to robotics and planning my first purchase.
I'm looking at the Baby Orangutang B-328. Here is information about the microcontroller: <a href=""http://www.hobbytronics.co.uk/baby-orangutan-328"" rel=""nofollow"">http://www.hobbytronics.co.uk/baby-orangutan-328</a>.</p>

<p>The pin headers come unmounted, so you have to do the soldering yourself. My problem is that I don't know what the pin connections are for. Here is a picture of the board:</p>

<p><img src=""http://www.ca.diigiit.com/image/cache/data/pololu/1220/Pololu-Baby-orangutan-b-328-6-500x500.png"" alt="""">.</p>

<p>Could someone briefly tell me what the different connections are for, or link a website that does?</p>
","microcontroller"
"2411","Robotic part to dispense candy","<p>I'm a complete newbie trying to build a simple robot that dispenses candy (M&amp;M, skittles, etc).  However, since I'm not familiar with the field, I'm having a hard time googling because I don't know the correct terms to search for.  I'm looking for a piece to build a robotic 'trap door' of sorts that will open for a specified amount of time to release candy.  What parts can I use and what is are called?  I've tried robotic lever, robotic door, etc with no luck.</p>
","design"
"2413","Why is analysis required to study robotics?","<p>I am studying Informatics and I am interested in doing a Masters in Robotics and I was checking out some unis and their courses and I saw that Robotics contains analysis and a lot of math.</p>

<p>Why is that?</p>
","software"
"2416","Low Amp FPV Quadcopter Motors","<p>Why do FPV Quadcopter Motors (usually the more expensive ones) draw lower amps than regular motors? </p>

<p>And why are they more squat(disk shaped) as opposed to normal motors which are about the same diameter and height?</p>
","motor quadcopter"
"2418","Quadcopter Props? Wood vs Plastic vs Carbon Fiber","<p>All the pro FPV builds and the more expensive quads don't seem to be using plastic props. Any reason for this?</p>
","quadcopter"
"2421","Just run program on NXT, not download?","<p>Is there any way I can simply run a program on the NXT, but not download it?<br />  I have all my programs already downloaded, and I am connecting with a USB cable to a MacBook Pro using the NXT-G interface.  Is there any way I can just run programs existing on the NXT from the computer, and not download them?  It's really increasing my robot's run time.
<br />
<br />  I am competing in Robocross in Science Olympiad, and my event is at noon.
<br />
Thank you.</p>
","nxt usb"
"2422","Is configuration space same as joint space?","<p>For robotic manipulator like the one on the picture:</p>

<p><img src=""http://www.learnartificialneuralnetworks.com/images/rcfig1.jpg"" alt=""robot arm""></p>

<p>are the configuration space and joint space equivalent? I am trying to understand difference between the two...</p>
","kinematics motion-planning forward-kinematics"
"2423","What do quadcopter propeller specifications mean?","<p>I'm trying to figure out the diameter of tri-blade propellers.</p>

<p>I found a 7x3x4.5 blade, I'm trying to understand the measurements. Is the '7' the length of the blade giving the prop a 10.5"" diameter? or is the 7 the total diameter?</p>
","quadcopter design"
"2424","Android phone + ADK + Arducopter APM 2.5 for autonomous quadcopter","<p>for a project for a robotic lab, i'd like to build an automous quadcopter able to follow a path and land on its own. I'd like to use an onboard android phone to do the image processing and recognition part, so that I avoid to send the video stream to a control station, process it and send back the commands.</p>

<p>As I need to use it in an indoor environment (so no GPS coordinate), I need the phone to guide the quadcopter giving it relative directions like FORWARD and after 1 sec STOP. This is something a normal pilot would do via the RC radio.</p>

<p>I already have a arducopter APM 2.5 and an arduino mega ADK and I was thinking to connect the phone to the ADK and then the ADK to the APM to guide the copter.</p>

<p>I think I have 2 options: either having the ADK to generate PPM/PWM signals as a RC receiver would do or use the mavlink protocol.</p>

<p>Which is the best/easiest solution?</p>

<p>Other info:</p>

<p>-I have already read checked some UAV related websites, but I couldn't find something close to what I want. In most of them, try to build a new type of controller, or use only ab Android phone + ADK. I'd like to stick to something already tested and known to work (as the APM &amp; arducopter software) and I don't want to use the phone as IMU as i don't trust its sensors</p>

<p>-I already have built the quad (a hexa actually)</p>

<p>-I have already set up the connection and protocol between the phone and the adk so i'm able to send commands like, i.e. forward, turn, hover etc...</p>

<p>-I have already checked the andro-copter project and similar ones.</p>

<p>-I might consider other platforms than the APM 2.5 if there's something easier to use</p>

<p>-It'd be nice to keep the RC receiver in the loop to regain control of the quad if something goes wrong.</p>
","arduino control quadcopter ardupilot"
"2426","How to programatically calibrate Turningy esc?","<p>I have a <a href=""https://hobbyking.com/hobbyking/store/__14630__Turnigy_TrackStar_18A_1_18th_Scale_Brushless_Car_ESC_.html"" rel=""nofollow"">Turnigy ESC</a> and I am controlling it from AVR. Now I need to calibrate it to set the range of the input.
With a servo tester I managed to calibrate it without any problems, more or less by following the user guide, but when I try to do the same procedure from code, the ESC starts beeping in some confused pattern and then enters programming mode.</p>

<p>My code looks like this:</p>

<pre><code>void calibrate_turningy_esc()
{
    servo_set16(SERVO_RANGE_TICKS);
    for (uint16_t i = 0; i &lt; 10000; ++i)
        _delay_ms(1);
    servo_set16(-SERVO_RANGE_TICKS);
    for (uint16_t i = 0; i &lt; 10000; ++i)
        _delay_ms(1);
    servo_set16(0);
    for (uint16_t i = 0; i &lt; 10000; ++i)
        _delay_ms(1);
}
</code></pre>

<p>where +SERVO_RANGE_TICKS is 2.2ms pulse length, -SERVO_RANGE_TICKS is 0.8ms pulse length and 0 is 1.5ms. The timeouts of 10s were measured during the manual calibration with a stopwatch.</p>

<p><strike>I have checked with an oscilloscope that the output servo signal looks the way I would expect it -- 10 seconds of 2.2ms pulses, 10 seconds of 0.8ms pulses and then 1.5ms pulses.</strike>
Edit: I made a mistake here, see my answer.</p>

<p>Do you have any idea what to change to calibrate the ESC?</p>
","c calibration esc avr"
"2433","What is minimum torque required for CNC stepper motors and spindle for aluminium milling?","<p>I am planning to buy CNC mechanical skeleton without motors, spindle and controller. I will be using the CNC mainly for aluminium milling. Are there any specifications for minimum torque requirement for stepper motors and spindle to perform aluminium milling ?</p>
","stepper-motor cnc"
"2434","Is it possible to make DIY clone of MakerBeam","<p>Is it possible to make clone of <a href=""http://www.makerbeam.eu/"" rel=""nofollow"">http://www.makerbeam.eu/</a> of some easy accessible material like:</p>

<ul>
<li>wood</li>
<li>plywood</li>
<li>OSB</li>
<li>MDF</li>
<li>HDF</li>
<li>others</li>
</ul>

<p>Using any type of CNC machine to mill some holes and rails in those materials may give sufficient results e.g. to make a prototype of 3D printer of such ""beams"".</p>

<p>Of course it won't be as rigid and durable but for making prototypes it may be good idea.</p>

<p>Just for reference: when reading this <a href=""http://www.lowtechmagazine.com/2012/12/how-to-make-everything-ourselves-open-modular-hardware.html"" rel=""nofollow"">http://www.lowtechmagazine.com/2012/12/how-to-make-everything-ourselves-open-modular-hardware.html</a> I've found this <a href=""http://bitbeam.org/"" rel=""nofollow"">http://bitbeam.org/</a>, this <a href=""https://www.google.com/search?q=grid+beam&amp;client=ubuntu&amp;hs=zAr&amp;channel=fs&amp;source=lnms&amp;tbm=isch&amp;sa=X&amp;ved=0CAcQ_AUoAWoVChMIiLHdqf2MyQIVQZoUCh1cHwV1&amp;biw=1215&amp;bih=927"" rel=""nofollow"">https://www.google.com/search?q=grid+beam&amp;client=ubuntu&amp;hs=zAr&amp;channel=fs&amp;source=lnms&amp;tbm=isch&amp;sa=X&amp;ved=0CAcQ_AUoAWoVChMIiLHdqf2MyQIVQZoUCh1cHwV1&amp;biw=1215&amp;bih=927</a> and this <a href=""http://www.gridbeam.com/"" rel=""nofollow"">http://www.gridbeam.com/</a>, and this <a href=""https://www.tetrixrobotics.com/"" rel=""nofollow"">https://www.tetrixrobotics.com/</a>.</p>
","mechanism"
"2435","JSP container for embedded systems","<p>There is this project I am working on which is using a BeagleBone and we need a JSP container to run on it. I was thinking of Tomcat but wanted to know if Tomcat is suitable for embedded systems.</p>

<p>Is it too resource-heavy? If yes, are there other lighter JSP containers? I know only of Tomcat and Jetty.</p>
","beagle-bone"
"2443","How to link ends of timing belt into loop","<p>When I buy some length of timing belt I don't know how to link ends of timing belt into loop.</p>

<p>So far I've found one way to do that (thanks to <a href=""http://www.lasersaur.com/"" rel=""nofollow"">http://www.lasersaur.com/</a>):
<a href=""http://www.flickr.com/photos/stfnix/8697962319/in/set-72157624491114826"" rel=""nofollow"">http://www.flickr.com/photos/stfnix/8697962319/in/set-72157624491114826</a></p>

<p>Any other ideas?</p>
","mechanism"
"2445","Transformation a robot in 2D?","<p>I'm watching this <a href=""http://www.youtube.com/watch?v=0wOp4k_lJvI"" rel=""nofollow"">video</a> at 36.00 min. The guy gave an example but I'm not sure what is the problem. He stated that if we want to move a robot then we should to the following </p>

<p>for inhomogeneous case, 
$$
x' = Rx + t \\
R = 
\begin{bmatrix}
cos\theta &amp; -sin\theta \\
sin\theta  &amp; cos\theta
\end{bmatrix}
$$ 
where $t$ is the translation vector and $x$ is the previous position. </p>

<p>in homogeneous case, 
$$
x' = 
\begin{bmatrix}
R &amp; \bf{t} \\
\bf{0}^{T} &amp; 1 
\end{bmatrix}
x
$$
Now, he gave an example in which 
$$
t = 
\begin{bmatrix}
1 \\
0 
\end{bmatrix}
, 
x = 
\begin{bmatrix}
0.7 \\
0.5 
\end{bmatrix}
$$
My solution is as the following in Matlab</p>

<pre><code>% inhomogeneous case
&gt;&gt; a = 45;
&gt;&gt; R = [cosd(a) -sind(a); sind(a) cosd(a)];
&gt;&gt; t = [1; 0];
&gt;&gt; x = [0.7; 0.5];
&gt;&gt; xnew = R*x + t
xnew =

    1.1414
    0.8485
</code></pre>

<p>For homogeneous case</p>

<pre><code>&gt;&gt; xn = [R t; 0 0 1]*[x ; 1]

xn =

    1.1414
    0.8485
    1.0000
</code></pre>

<p>Both have same result, but the guy got another result. What exactly he did is </p>

<pre><code>&gt;&gt; xf = [R x; 0 0 1]*[t ; 1]

xf =

    1.4071
    1.2071
    1.0000
</code></pre>

<p>Why he did switch $t$ and $x$? I'm aware of the issue that he is trying to calculate the velocity but in fact he is computing the position. This mistake in the notation won't affect the final result. </p>

<hr>

<p>Second question, why he assumed that the forward movement of the robot in the above example should be
$$
t = \begin{bmatrix}1\\0\end{bmatrix}
$$
? He said that because the robot always in the forward movement move in +x axis. Why this is the case? The movement in robot's frame is determined based on the direction of the robot and the distance the robot travels which is specified as hypotenuse length. </p>
","mobile-robot"
"2446","RC Helicopter Connect with computer","<p>I am planing to control my RC helicopter with my computer. I have experience of programming in .Net. Could we use .Net to control RC helicopter? 
From where can I start this project?</p>
","control radio-control"
"2453","Quadrocopter first build: how to tell if components play well together?","<p>I'm building my first quadrocopter, and I'm trying to come up with a parts list that is suitable for a first build. </p>

<p>I will use this to <strong>learn how to fly a quadrocopter manually</strong> (lots of crashes!), and to do some experiments with running AI for piloting it.</p>

<p>A couple questions about the below list of parts:</p>

<ol>
<li><p><strong>Is this a good choice for a first build?</strong> Are we missing any crucial parts?</p></li>
<li><p><strong>Do these components work together?</strong> Is this <strong>battery strong enough</strong> to fuel all the components that need power?</p></li>
</ol>

<p><em>Here's the current list of parts:</em></p>

<ul>
<li>Frame - 450 mm</li>
<li>Propellers - 10x4.5"", two pairs</li>
<li>Motor (4x) - 900kv brushless outrunner motor; max current: 18A; ESC: 25-30A; cell count:3s-4s lipoly</li>
<li>Electronic speed controllers (4x) - 20A constant current, 25A burst current; battery: 2-4S lipoly</li>
<li>Battery - 3300mAh lipoly, 11.1v, 3 cell; constant discharge: 30C, peak discharge: 40C. charge plug: JST-XH. 3300 mAh x 30C = 99 amps?</li>
<li>Charger - lipoly, 50W, 6A, 12v power supply</li>
<li>Power supply - input: AC 100-240v 50/60Hz; output: DC15v 5A</li>
<li>Arduino board</li>
<li>Gyroscope for arduino</li>
<li>Accelerometer for arduino</li>
<li>GPS sensor for arduino?</li>
<li>RC transmitter for arduino</li>
<li>RC controller</li>
</ul>
","arduino motor sensors quadcopter"
"2456","How are Interference Avoidance and Collision Avoidance different?","<p>Someone told me when explaining about a controller module named <code>CollisionDetector</code> that it only checks self-<code>interference</code> and moves accordingly without detecting <code>collision</code>. To me both sounds the same. How are they different? Thanks.</p>
","untagged"
"2458","What should I study if I want to get into robotics?","<p>I understand this is a broad question but I'm taking the risk to ask anyway.  Robotics, from what I can tell so far, is a detailed, diverse, and thorough field. However, if there are better areas of research to invest time into, what would those areas be?</p>
","research"
"2462","Connecting a Raspberry Pi to a roomba via an FTDI cable","<p>I have a Raspberry Pi with <a href=""https://www.sparkfun.com/products/9718"" rel=""nofollow"">this</a> FTDI cable and a Roomba 560. The Roomba has an SCI port to allow for control of the roomba via serial. I installed the PySerial library on the pi and send valid commands to Roomba, but the roomba doesn't respond. I have the TXD of the cable attached to the TXD of the roomba, the RXD on the cable wired to the RXD on the roomba, and a ground on the cable wired to the ground on the roomba (everything in  it's respective port). I do not have power going from the cable to the roomba or vice-versa.</p>

<p>What I can't figure out is why the commands aren't working. There's no error message upon running the python code. <a href=""http://www.irobot.com/images/consumer/hacker/Roomba_SCI_Spec_Manual.pdf"" rel=""nofollow"">This</a> is the information sheet for the Roomba's SCI port.</p>

<p>Code:</p>

<pre><code>import serial

ser = serial.Serial('/dev/ttyUSB0')
# this is the defualt Roomba baud rate
ser.baudrate = 57600

# Start SCI - puts into safe mode
ser.write(chr(128))
# Enable full mode
ser.write(chr(131))
# Spot clean
ser.write(chr(134))

print 'Done'
</code></pre>
","mobile-robot software wheeled-robot"
"2465","Alternatives to Primesense depth cameras?","<p>I'm looking for low-cost depth cameras (less than 1000 USD) with a range of more than 3 meters.</p>

<p>Currently, I have found only SoftKinetic DS-311 that meets these requirements. </p>

<p>Here are some other low-cost cameras that I found, but they have a short range:</p>

<ul>
<li>pmd[vision] camboard nano</li>
<li>SoftKinetic DS-325</li>
</ul>

<p>and others with a long range but high cost</p>

<ul>
<li>Panasonic D-Imager</li>
<li>pmd[vision] CamCube 3.0</li>
<li>SwissRanger SR4500</li>
<li>Odos Imaging Real.iZ-1K</li>
</ul>
","sensors kinect computer-vision"
"2470","GPS Amplifier - Is this reliable?","<p>I'm multimedia developer who is searching for a way to get GPS signal inside buildings/structures. Is amplification a reliable way to fix this GPS signal issue?</p>

<p>Will a ""GPS Amplifier"" work as perfectly as using GPS outside?</p>
","wireless gps"
"2476","Benefits of the Number of Propellers","<p>I am planning on creating a quad-copter with my Arduino that I have. I have created a few land robots before but no aerial vehicles, so this is all new to me. I was looking on the Internet for different models, and I see that most robots have 4 propellers. I have also seen a few hexacopters (?) and octocopters but that many propellers can't get a but out of hand. Does having 4 propellers the best and most efficient thrust to weight ratio, or will 3 propellers/arms work better?</p>
","quadcopter"
"2478","Ros ~ Android sensor driver","<p>I try to connect android device and computer over ROS. 
<a href=""http://wiki.ros.org/android_sensors_driver"" rel=""nofollow"">http://wiki.ros.org/android_sensors_driver</a></p>

<p>this tutorial explains good. I download the application on android device and set the ROS_MASTER_URI.. When I run the application, the phone shut down and this node is not seen in 'rosnode list'</p>

<p>Is there anyone who experience similar error?</p>
","ros"
"2480","looking for a circular track and bearing with a spindle","<p>I've really tried to find something online that's suitable but what I'm after is</p>

<p>I've two concentric circular rings, one of which has a diameter about 10mm smaller than the larger .</p>

<p>The rings themselves are in the region of 300mm diameter.</p>

<p>I'm trying to find a way to connect the two together and allow the smaller to 'slide' in a circular rotational way within the larger one.</p>

<p>I'm also trying to let the 2 rings pivot vertically in relation to each other - the intention being of producing a gyroscopic-esque motion.</p>

<p><a href=""http://i.stack.imgur.com/n4jO3.jpg"" rel=""nofollow""><img src=""http://i.stack.imgur.com/n4jO3.jpg"" alt=""diagram of it""></a></p>

<p>What type of bearings/tracks/spindles would suffice?</p>
","tracks linear-bearing"
"2481","writing a simple program for processing RGBD video with OpenNI","<p>I would like to write a simple program which processes the depth feed from an Asus xtion depth sensor using OpenNI. The sensor will be fixed like a CCTV camera and count multiple targets moving around. </p>

<p>The processing would involve some simple functions on each frame (background subtraction, level sets, connected components filter), and some multi-target tracking across frames. I have searched the web, but it is hard to see how best to get started (and I'm also quite new to programming in C). </p>

<p>Can anyone recommend any existing code that can help to get started / any libraries which would be suitable for this real-time application? Or perhaps there is some opensource code which already does such a thing? Would really appreciate any pointers from anyone with experience. Thanks!</p>
","openni"
"2486","VAL language and velocity control of industrial robot","<p>In my bachelor, I programmed CNC machines. Now, working with an Industrial robot arm, I learn that their programming languages are mostly similar. VAL is a typical example, for instance:</p>

<pre><code>PROGRAM PICKPLACE
  1. MOVE P1
  2. MOVE P2
  3. MOVE P3
  4. CLOSEI 0.00
  5. MOVE P4
  6. MOVE P5
  7. OPENI 0.00
  8. MOVE P1
.END
</code></pre>

<p>Most of the cases, control of a robot arm is similar to this example. Cleary, move end-effector to a point with a given pose.</p>

<p>But... is there any way that I can control the end-effector (EE) speed? An example is ""Move EE to P1 with time duration T1"", or ""Move EE to P1 with velocity V1"" (I could have only seen defining for joint rotational velocity) In other way of speaking, I can command the EE to move from P0 to P1 but cannot control the duration of that traverse which is necessary in cases of EE velocity control</p>

<p>This is the programming manual for my robot mediafire.com/?agl76pi7t7v4hjv. The velocity control I'm talking about is not joint velocity but end-effector velocity. But EE_screw = robot_Jacobian*joint_vel which means to control EE velocity, it resolves in control joint velocity. About the inverse kinematic, I've already programmed a module to solve the robot</p>

<p>the experienced in robotics programming and VAL please help! I've stuck in this problem for months</p>
","control manipulator"
"2488","Can motion model noise be zero?","<p>Can I assume the noise of motion model to be zero? If so, what are the consequences of doing so?</p>
","kalman-filter noise ekf"
"2489","VeX - Keeping arm at an angle","<p>So my team made a Vex robot for the <a href=""http://www.vexrobotics.com/wiki/Toss_Up"" rel=""nofollow"">toss-up competition</a>, but we need the arm up during the autonomous. The problem is that it's too heavy to stay up on its own. I was going to use encoders to count what angle the arm is at. I was going to use this code, but I'm not sure if there's a better way.</p>

<pre><code> while(MotorEncoder[rightMotor] &lt; 1000)
  {
    motor[rightMotor] = 80;
    motor[leftMotor] = 80;
  }
</code></pre>

<p>Would anyone recommend a better solution or is this the best way? This is untested by the way.</p>
","arm"
"2491","How are color sensors used for line following?","<p>I have to build a line following robot that will be able to detect a selected colored line on the floor and start following it.</p>

<p>How do color sensors do this after detecting the specific colored line?</p>
","mobile-robot sensors"
"2494","SLAM for Autonomous car","<p>I am working on SLAM for autonomous car like vehicles with 2D lasers and IMU (deriving odometry).</p>

<p>I would like to know how efficient is using the existing SLAM algorithms (for example: gmapping in ROS based on rao blackwellized particle filter).</p>

<p>till now i find MAP are high in volume and speed of vehicle is high and most importantly computational time compared to Mobile robots.</p>

<p>Are there  any other important factors to consider for car like vehicles in using SLAM algorithm.</p>

<p>thank you.</p>
","mobile-robot localization slam"
"2498","Forward Kinematics Two fixed standard wheels","<p><img src=""http://i.stack.imgur.com/G3Jxp.png"" alt=""enter image description here""></p>

<p>What would the equations be for the robot's angular and linear velocity at P and also P2? I think I'm doing it wrong...</p>

<p>WL = left wheels angular velocity
WR = right wheels angular velocity</p>

<p>For P I had for example the linear velocity = (1/3)*r*WL + (2/3)*2r*WR</p>

<p>Am I on right track?</p>
","kinematics wheel"
"2499","Can inverse dynamics control be regarded as a function?","<p>I know that inverse kinematics ($p \rightarrow q$, p: desired pose of the end-effector, q: joint angles) is not a function because there might be multiple joint angle vectors q that result in the same pose p.</p>

<p>By inverse dynamics control I mean the mapping $(q, \dot{q}, \ddot{q}) \rightarrow u$ (u: required torques. I am not very experienced with these kind of problems. Is the mapping a function, i.e. for each triple $(q, \dot{q}, \ddot{q})$ there is a <em>unique</em> solution u? My intuition says it is. But I am not sure. If there is not, would it always be possible to obtain a solution by averaging two or more solutions?</p>
","dynamics"
"2504","Jacobian method for inverse kinematics","<p>I have big problem. I have to solve inverse kinematics for a manipulator with 6-DOF using jacobian method. From what I know to do that I need to have matrix of transformation and Denavit–Hartenberg parameters, which both I have. But I am not a mathematician, and descriptions I find on the web are not even a bit understandable to me. So I would love if you could give me an example of how to solve my problem.</p>

<p>The Denavit-Hartenberg parameters are:
$$
        \begin{matrix}
        \alpha &amp; l &amp; \lambda &amp; \theta\\
        90 &amp; 150 &amp; 0 &amp; var(-69) \\
        0 &amp; 610 &amp; 0 &amp; var(85) \\
        90 &amp; 110 &amp; 0 &amp; var(-52) \\
        -90 &amp; 0 &amp; 610 &amp; var(62) \\
        90 &amp; 0 &amp; 113 &amp; var(-60) \\
        0 &amp; 0 &amp; 78 &amp; var(-108) \\
        \end{matrix}
$$</p>

<p>The values in theta are values to get the following matrix of transformation, and values I want to get with this jacobian method. And those values are in degrees.</p>

<p>Matrix of transformation:
$$
        \begin{matrix}
        0.7225 &amp; 0.0533 &amp; 0.6893 &amp; 199.1777\\
        -0.2557 &amp; -0.9057 &amp; 0.3381 &amp; -500.4789\\
        0.6423 &amp; -0.4206 &amp; -0.6408 &amp; 51.6795\\
        0 &amp; 0 &amp; 0 &amp; 1 \\
        \end{matrix}
$$</p>

<p>I would be most greatful, if someone could walk me through how to solve it in simple language.</p>
","inverse-kinematics"
"2507","robotics as a beginner","<p>I want to begin robotics.So as a beginner what micro-controller would be convenient?Arduino or PIC?what type of robots can be built with arduino or PIC? Should I start from just a line-following vehicle? </p>
","arduino microcontroller beginner"
"2508","Looking for a way to scan cylindrical objects","<p>Can anyone recommend a commercial or solid, reliable DIY solution for scanning cylindrical objects? I've seen a couple of simple hacks for flatbed scanners, but I'm looking for something I could make or buy for a commercial project that work reliably.</p>

<p>Many thanks</p>
","stepper-motor"
"2510","Slam and Vision (good resources)?","<p>I would like to know if there is a good source that combines Slam problem with vision. From mathematical perspective, there are numerous resources that handle SLAM ,however, I didn't find a good source that focuses on slam and vision.  </p>
","kalman-filter slam ekf"
"2511","Find the difference between two consecutive sensor readings in real-time in C","<p>I am working on a Micromouse and it has three sensors. Call it S1,S2 and S3. For now, I have to use S1. The idea is this S1 controls the left motor and S3, the right motor. S2 will detect wall in the front. </p>

<p>Anyways, I am trying to write a code in C for the dsPIC30F4011 MCU which would continuously read Sensor values and after reading two consecutive values, it will compare the two values. Read happens every 0.1ms.</p>

<p>The Flow of the Code is as follows:</p>

<pre><code>// Initialize timer for generating interrupts every 0.1ms
// Pseudo-Code
void __attribute__((interrupt, auto_psv)) _T1Interrupt(void)
{
  int count = 0;      //Read Sensor1 and store value in Sensor1Value
  Sensor1Value = Sensor1;
  int i = count++;

  //Now this is part where I am lost :(
  // I want to do this
* diff_S1Value = Sensor1Value(i = n+1) - Sensor1Value(i = n); // n is in the mathematical sense, like n+1 is 2 and n is 1

  // So I want to compare the new value with the previous sensor value
  if (diff_S1Value != 0) // Checks if the difference is zero
  { //Duty Cycle of PWM that controls the Speed of the Motor
    Sensor1Value = Sensor1Value(i = n+2) + or - diff_S1Value;
    PDC1 = float k/Sensor1Value;
  }
}
</code></pre>

<p>So if you look at the line with the *, how do I compare two sensor values in real-time every 0.1ms ? Let me know if one wants to more info!!</p>
","c micromouse"
"2519","How to measure displacement, cheaply and without using an accelerometer?","<p>Motion is known to be confined in a sphere with radius of about 0.5m, and resolution doesn't have to be very high (5cm is enough). The device will actually be incorporated in a toy designed for kids.</p>

<p>I tried implementing that with an accelerometer but the estimated displacement drifted away 100s of meters per minute.</p>

<p>Is there some other solution, maybe involving electrical or magnetic fields? It's important that the sensor costs no more than a few bucks.</p>

<p><strong>Edit:</strong> The device should not be attached to anything mechanical and its movement is in 3d (a kid moves the toy freely).</p>
","sensors imu"
"2524","How to calculate the power required to drive a fan","<p>I need to specify a fan motor combination and wondered if there are formulas that can work this out? The fan we are using is a crossflow fan:</p>

<p><img src=""http://i.stack.imgur.com/zenu6.jpg"" alt=""cross flow fan""></p>

<p>So I'm assuming the power required to drive it is derived from the number of blades, dimensions of blades (including angle of attack), dimension of barrel/wheel and the speed in RPM.</p>

<p>Is this possible or does it need to be worked out practically with experimental measurements etc?</p>

<p>Hopefully this is the correct stack for this question, if not then mods please feel free to edit/close. </p>
","motor power"
"2526","Neuromorphic Engineering and Robotics","<p>I have been into a boggling paper research on neuromorphic engineering and its implications on robotics applications, lately. It is relatively a less applied field and full of academic papers and difficult to skim easily :)</p>

<p>There are many ongoing projects applying analog or digital circuitry design to implement neurosynaptic simulations of the brain. Consumer oriented products like IBM <a href=""http://www.research.ibm.com/cognitive-computing/neurosynaptic-chips.shtml?lnk=ushpls1"" rel=""nofollow"">Synapse</a> and Qualcomm's <a href=""http://www.qualcomm.com/media/blog/2013/10/10/introducing-qualcomm-zeroth-processors-brain-inspired-computing"" rel=""nofollow"">Zeroth</a> focus on digital hardware whereas academic research like Standford's <a href=""http://www.stanford.edu/group/brainsinsilicon/neurogrid.html"" rel=""nofollow"">neurogrid</a> or ETC Zurich's Human Brain Project focus more on actual brain study using analog hardware.</p>

<p>If anybody is following this engineering field, can he/she spread more light on it and explain it's <strong>implications, methodologies and toolsets</strong> to the community, in detail?</p>

<p>PS : Regarding toolsets, I'm talking about the most feasible engineering methodologies to commit to the field.</p>
","mobile-robot artificial-intelligence research machine-learning"
"2530","Help sending serial command to Roomba","<p>I have a Raspberry Pi hooked up to a Roomba 560's serial port. While going over <a href=""http://www.irobot.com/images/consumer/hacker/Roomba_SCI_Spec_Manual.pdf"" rel=""nofollow"">the spec</a>, I noticed movement controls weren't as simple as I expected. I can't send bytes larger than 255, but, according to the spec, to go straight I have to send 8000. How does this work?</p>

<p>EDIT: My solution was the following three functions:</p>

<pre><code>import serial
import time

def start():
    #Open serial connection
    global ser
    ser = serial.Serial('/dev/ttyUSB0', 115200)
    # this is the defualt Roomba baud rate
    # Start SCI - puts into safe mode
    ser.write(chr(128))
    # Enable the safe mode
    ser.write(chr(131))
    # this is required or the command may fail
    time.sleep(1)

# This makes the serial command fit the big-endian notation
def make4(num):
    num = num[2:]
    z2a = 4-len(num)
    for q in range(z2a):
        num = '0' + num
    return '\\x'+num

def move(vel, rad):
    # Init move command
    ser.write(chr(137))

    # velocity
    vhex = hex(vel)
    vhex = make4(vhex)

    #radius
    vrad = hex(rad)
    vrad = make4(vrad)

    # send to roomba
    ser.write(vhex)
    ser.write(vrad)
</code></pre>
","mobile-robot raspberry-pi serial irobot-create roomba"
"2531","Why is ROS not a real-time operating system?","<p>ROS is not real-time OS.
After reading the architecture of ROS, I am unable to realize why is ROS not real-time? What part of the architecture or what design decision is causing that?</p>
","ros"
"2542","What do we call a coupling with infinite range of rotation","<p>In most situations, range of motion is limited by the fact that we need to carry power or information past a joint. So, past a certain point there are either cables in the way, or the cables would stretch so much that they would either prevent further movement or break.</p>

<p>However, if we situate the conductors in concentric rings around or within the rotor shaft, we can have a joint that can rotate forever while keeping in contact with any modules on the other side.</p>

<p>What do we call this mechanism? Does it even have a name?</p>
","motor motion joint"
"2545","arduino serial mixing incoming commands","<p>In my project i'm aiming to control a quadruped robot from my android phone using raspberry pi as a middle device (web server). In order to make sure that the server on RPi working fine i googled and got an app that sends a specific character whenever a button is clicked and the arduino job here is simply to receive it from serial port and blink a led! (so easy huh?)
<strong>but the problem here is that i noticed that some leds are blinking when i click a button not assigned with them!</strong> this can be a disaster if you are controlling a robot! 
Does anybody know the reason of this and the solution? </p>
","arduino serial"
"2552","Best Way to Sense Rubiks Cube Movements","<p>I was wondering what was, in your opinion, the best way to study the different motions of a Rubiks Cube.
I want to be able to recognize what face moved and in what direction. </p>

<p>Would I be able to get the direction and the face with accelerometers/gyro and if yes how many would I need?
If you have ever used a Leapmotion or Kinect, is it possible to achieve this using those?</p>
","accelerometer motion"
"2553","Capacitive touch input robot to remote access iPad","<p>I'd like to buy a capacitive touch input robot in order to remote access my iPad but I'm having trouble describing a correct kind of robot. </p>

<p>I would like to keep lag down to an additional 60ms so that it is still a high quality interface. </p>

<p>I would like to have a robotic arm equipped with a capacitive pen that moves to places on the ipad screen based on the mouse or I'd like a array of capacitive pens that emulate the touch of a user. </p>

<p>I guess I'd use Squires software reflect and the mirror function but I'm open to using an SHD camera with the robotic arm and a pixel sensor array with the array of capacitive pens. </p>

<p>Does this make sense? How could I improve the design? What materials would I need to build it myself. Assuming ready built arm? How could I build an array of capacitive touch micro pens?</p>
","robotic-arm"
"2555","Any image transfer protocol for wireless serial transfer?","<p>I want to send image over wireless serial communication. I am planning to capture images using either raspberry pi or stm32 mcu using DCMI and then transfer image using wireless serial communication module such as <a href=""https://www.sparkfun.com/products/10419"" rel=""nofollow"">Xbee</a> or <a href=""http://store.3drobotics.com/products/3dr-radio"" rel=""nofollow"">3DR radio</a> which can provide air data rate upto 250Kbps at baud rate of 115200</p>

<p>I would like to know if there is any protocol which can send a jpeg compressed image as a wireless serial data.</p>
","raspberry-pi serial communication wireless"
"2556","How to rotate covariance?","<p>I am working on an EKF and have a question regarding coordinate frame conversion for covariance matrices. Let's say I get some measurement $(x, y, z, roll, pitch, yaw)$ with corresponding 6x6 covariance matrix $C$. This measurement and $C$ are given in some coordinate frame $G_1$. I need to transform the measurement to another coordinate frame, $G_2$. Transforming the measurement itself is trivial, but I would also need to transform its covariance, correct? The translation between $G_1$ and $G_2$ should be irrelevant, but I would still need to rotate it. If I am correct, how would I do this? For the covariances between $x$, $y$, and $z$, my first thought was to simply apply a 3D rotation matrix, but that only works for a 3x3 submatrix within the full 6x6 covariance matrix. Do I need to apply the same rotation to all four blocks? </p>
","kalman-filter"
"2559","Are propellers dangerous?","<p>Aren't all propellers super dangerous? How are these startups like Hex and Pocket Drone selling drones as 'kid-friendly' to consumers? What happens if a kid puts his finger in a propeller's movement space while its flying? </p>
","quadcopter"
"2563","Connecting Wifi bee with Xbee USB adapter","<p>I started to follow ""Get started with the Wifi Bee"" tutorial in <a href=""http://www.dfrobot.com/wiki/index.php/WiFi_Bee_V1.0_%28SKU%3aTEL0067%29"" rel=""nofollow"">Wifi bee v1.0 wiki page</a>. 
I am using Xbee USB adapter v2.0,Wifi Bee-RN-XV,mini USB cable as listed in <strong>Tools needed</strong>.
When i conneted the Xbee USB adapter v2 to my computer via mini USB cable Wifi bee didn't light up (but USB adapter did light up). Then i followed all the steps till number 4, which is "" Send AT command $$$ to the wifi Bee and it will reply ""CMD"" to indicate that it enter the command mode properly"". When i sent command, it didn't reply anything. I typed other commands like <strong>show net</strong> and  <strong>scan</strong>, it didn't reply either.</p>

<p>When i tried <strong>Arduino Server example</strong> in the wiki page, Wifi bee lighted up. But it gave me some strange ip address with port 200, and when i entered that ip address to my browser address, browser couldn't find the page.</p>

<p>So my question is does Xbee Usb adapter need some extra power sources? Or it just doesn't fit with Wifi Bee? I don't think the problem is with usb cable, 'cause my computer found the device. </p>
","wifi usb"
"2564","How to use TOA (Time Of Arrival) to measure 3-axis location of a wireless device?","<p>I need to read the location of my device within a 1m radius sphere, with accuracy of 5-10cm.</p>

<p>The device is handheld and wireless, and currently communicates using a Bluetooth v4 chip. I could add an RF transmitter on the moving part and a few stationary receivers at the base.</p>

<p>What components should I look into? What would be the cheapest way to triangulate it?</p>
","sensors imu"
"2570","Do servo motor specifications take into account the gear ratio inside?","<p>I am looking at buying a servo motor for a an application that must be able to lift 4-5 lb at a rotational speed of approximately 1rpm. The servo motor listed here <a href=""http://www.robotshop.com/ca/en/hitec-hs755mg-servo.html"">http://www.robotshop.com/ca/en/hitec-hs755mg-servo.html</a> states a stalling torque of 200 oz-in. Is this torque rating at the horn of the servo motor or the torque rating of the actual motor before any gear reduction is done?</p>

<p>Is this motor sufficiently strong for my application?</p>
","motor servos servomotor"
"2571","Sensor for tracking relative position of human","<p>I am making a robot that needs to continuously track the relative position of a human, up to 15 meters away and with at least 300 degrees coverage. Currently I am using a <a href=""https://www.hitechnic.com/cgi-bin/commerce.cgi?preadd=action&amp;key=NSK1042"" rel=""nofollow"">Hitechnic IRSeeker v2 sensor</a> and made a beacon wristband with 6 TV remote IR LEDs. But the maximum distance I can get is around 3 meters.</p>

<p>I ordered some 3 watts IR LEDs to boost the power, but the size of the wristband will be a problem because it will not run on a CR2032 battery.</p>

<p>I also bought some IR remote receivers. But I am not sure if the reflection from the wall will give false results.</p>

<p>Is what I am trying to do possible? Is a beacon 15m away feasible using this technology? </p>

<ul>
<li>If it is, then what do I need to modify in my current implementation?</li>
<li>If not, are there any other technologies that I should be considering to track the relative position or direction of human, 15 meters away with at least 300 degrees coverage?</li>
</ul>
","sensors"
"2575","PID control against sine wave error","<p>I'm writing a PID to control a toy car that follows a black line on a circuit. I've tuned my PID and it works at high speed for all the circuit except the winding section. For that, the error signal looks like a sine wave, and the toy car steers too much. I would like it to go close to straight, is it possible?</p>

<p><strong>Edit:</strong> My car sees 100 grey points in a line ahead, and the difference between the darkest point and the middle of the visual range is the error signal. My output is the angle of a servo on the front wheels of the car, while the speed of the back motors is constant.</p>

<p>The desired performance would be to oscillate with an amplitude less than the amplitude of the winding road, and the actual performance is that the car steers close to the sine line for one period, and at the next max amplitude it under steers. Sorry, I can't provide graphs right now but I'll try to add some in the next days.</p>

<p>Is there a formula for adjusting the PID constants for the desired PID bandwidth?</p>
","pid"
"2580","Fit robot simulator to robot","<p>I have odometry data $(x, y, angle)$ of a real two-wheeled robot, who received control commands $(forward speed, angular speed)$.</p>

<p>Now I want to code a motion model (in C++ (/ROS)) which should follow the same trajectory, given the same control commands.</p>

<p>Normally, the kinematics should look something like this:</p>

<p>$$ 
\begin{align} 
v_{fwd} &amp;= control_{fwd} \\
v_{ang} &amp;= control_{ang} \\
x &amp;= x_{old} + 0.5(control_{fwd} + v_{fwd,old}) * \cos(angle) * dt \\
y &amp;= y_{old} + 0.5(control_{fwd} + v_{fwd,old}) * \sin(angle) * dt \\
angle &amp;= angle_{old} + 0.5(control_{ang} + v_{ang,old}) * dt
\end{align} 
$$</p>

<p>And I thought about just setting </p>

<p>$$ 
\begin{align} 
v_{fwd} &amp;= control_{fwd} + k_1 v_{fwd,old} + k_2 v_{fwd,old}^2 + k_3 v_{ang,old} + k_4 v_{ang,old}^2 \\
v_{ang} &amp;= \text{ ...analog...} \\
x, y, angle &amp;\text{ unchanged}
\end{align} 
$$</p>

<p>and then just search the minimum of the squared distance of the computed trajetory to the real one - depending on the values of $k_i$. This would mean either a good optimization algorithm or just brute-forcing / randomly testing a lot of values.</p>

<p>Is this the way to go here? I tried the 2nd approach, but the results so far are not that good. </p>

<p>So, as you might guess now, I'm pretty new at this, so any help is appreciated.</p>
","wheeled-robot kinematics algorithm"
"2587","Reading data from D+ and D- pins of a USB","<p>So I have this optical mouse with me, which has a PAN3504DL-TJ optical sensor. It has a USB interface and when I looked up the internet, all I could find was tutorials using A2501 or sensors in those lines and it has pins like SCLK and SDI but I don't have them instead I have D+ and D-. I understand that these are the data pins so what I did was take two wires and plug them into my Analog Pins of my dsPIC30F4011 and read data from it. After setting up UART communication and transmitting data, all I get are numbers running continuously.</p>

<p><img src=""http://i.stack.imgur.com/Gu8lW.png"" alt=""Screenshot of Termite receiving information from the Analog Pins which are connected to the D+ and D- of the Optical Sensor""></p>

<p>What I want to do is to read coordinates over the analog pins as the mouse aka the sensor moves on a surface. I would use this for position control for my robot.
So my question is how do I read coordinates from the Optical Sensor over the D+ and D- lines through my Analog Pins ?</p>
","sensors"
"2588","Robotics StackExchange vs ROS Answer","<p>Robotics Stackexchange vs. ROS Answer:
What is better and for what purpose?</p>
","ros robotc"
"2591","Denavit–Hartenberg parameters of a robot with spherical wrist","<p>What are valid values for the Denavit-Hartenberg parameters $d$ and $a$ (sometimes called $r$) of the last 3 links of a robot with spherical wrist?</p>

<p>From <a href=""http://www.eng.utah.edu/~cs5310/chapter4.pdf"" rel=""nofollow"">this reference</a>, ""A spherical joint can be represented by three consecutive rotary joints with intersecting rotation axes.""</p>

<p>So the retrictions should be:</p>

<p>$L_{n-2}$ ($d$ arbitrary, $a=0$)</p>

<p>$L_{n-1}$ ($d = 0$, $a=0$)</p>

<p>$L_{n}$ ($d = 0$, $a=0$)</p>

<p>But in <a href=""http://www.dis.uniroma1.it/~deluca/rob1_en/WrittenExamsRob1/Robotics1_10.07.07.pdf"" rel=""nofollow"">this exam</a> I have found on the internet, It says that the KUKA robot has spherical wrist, and $d$ of the last joint is different to $0$. Would $d\neq0$ in the last link still yield a spherical wrist?</p>
","kinematics joint"
"2593","Sourcing Motors for larger robots","<p>I have been wanting to build larger robots and r/c cars for some time now, but one issue I have had is trying to find larger motors, in the range of electric wheelchair motor size. I found one set on ebay but I am trying to find a more reliable source for these.</p>

<p>To make my question more clear, I am looking for a reliable source(s) for medium size electric motors around the size and power rating of a typical electric wheelchair motor</p>
","motor"
"2594","24v dc to 12vdc converter","<p>I have built a r/c car that runs on 2 30AH 12V DC deep cycle batteries. The motors are 24v motors that will each draw around 15A at full power. My  motor controller can handle this, as well as reclaiming braking energy. </p>

<p>This is my way of saying that i have a 24v power system. Now my issue is that I want to run a 12v device on this 24v service. I do not want to have the hassle of another battery to maintain so i would like to power it off the main batteries. All the BECs and other converters that i have found only supply around 1 amp while the device i am looking at powering will take around 4-5A 12v DC. Does anyone know of a device that will do this.</p>
","bec"
"2595","Control Arduino firmata with HC-05","<p>I'm using the <a href=""https://github.com/rwaldron/johnny-five"" rel=""nofollow"">johnny-five</a> library to control an Arduino Uno running StandardFirmata. I have a HC-05 bluetooth module that I want to use to wirelessly control firmata, but have yet to get it working.</p>

<p>I used <a href=""http://www.instructables.com/id/Modify-The-HC-05-Bluetooth-Module-Defaults-Using-A/"" rel=""nofollow"">http://www.instructables.com/id/Modify-The-HC-05-Bluetooth-Module-Defaults-Using-A/</a> to configure the board for 57600 baud rate: <code>AT+UART=57600,0,0</code>. I'm able to send various AT commands and read back the results in my serial monitor.</p>

<p>I followed <a href=""http://www.instructables.com/id/Use-your-android-phone-sensors-on-the-arduino-/?ALLSTEPS"" rel=""nofollow"">http://www.instructables.com/id/Use-your-android-phone-sensors-on-the-arduino-/?ALLSTEPS</a> to wire up the voltage divider, to make my Arduino's TX operate at 3.3 going into the HC-05's RX.</p>

<p>I've tried running the HC-05 in master, slave, and slave-loop. It only makes a BT connection in slave, which is default.</p>

<p>When I run my johnny-five script, here's the output:</p>

<pre><code>± node jf-simple.js
1394412173445 Board Connecting...
1394412173447 Board -&gt; Serialport connected
waiting for board to be ready
1394412273448 StandardFirmata A timeout occurred while connecting to the Board.
Please check that you've properly loaded StandardFirmata onto the Arduino
1394412273448 Board Closing: firmata, serialport
</code></pre>

<p>I've more-than-triple-checked everything. Uploaded firmata many times. Firmata works fine over  USB. Also, I have been able to get this working in the past with an HC-06.</p>

<ul>
<li>Am I missing something?</li>
<li>What are some good debugging techniques to figure out why it won't connect to Firmata?</li>
</ul>
","arduino troubleshooting"
"2596","Atlas Robot Reference","<p>Boston Dynamics keeps making great robots, however, I dont see any papers that they publish.  Although now I can find papers on people using the ATLAS robot, I can not find an original paper detailing the robot or its mechanics designs.  Is there a reference for the robot, should I use youtube videos?</p>
","design mechanism humanoid"
"2597","Software libraries for parsing sensor data","<p>What software libraries are there for assisting the general problem of parsing a stream of sensor data?</p>

<p>We use various sensors like LIDARs and GPSINS units that provide messages in proprietary binary formats, and have to write drivers for each one. Even though there's a lot of similar concepts used in each sensor (like a general purpose datagram for all messages, consisting e.g. of start/end sentinels, length specifications and a checksum, and then a variety of well-defined message formats for the payload), it ends up being a lot of tedious work to develop a driver each time.</p>

<p>I'd love a solution where I can write out packet/message specifications in some format, and have a library that finds &amp; extracts valid messages from a stream, and provides them in a simple data structure format.</p>

<p>I'm not too fussed about what language, but basically want a general purpose datagram parsing library. There's a lot of customisation with sensors, maybe some odd format parsing, and probably some initial configuration to start the data stream, so this is really something I want as a library for processing the data in real-time that can be used as part of a driver/application.</p>

<p>Everything I find is either too basic (the low level tools for interpreting individual elements, so still lots of time spent extracting individual elements explicitly), or too specific (i.e. parsers written specifically for one particular protocol).</p>

<p>As a concrete example, consider NMEA messages:</p>

<ul>
<li>There's a basic outer datagram (starts with <code>$</code> followed by message name, then comma separated data, and ends with <code>*</code>, checksum and line terminating character)</li>
<li>Data is in ASCII so needs to be parsed to binary for computational use</li>
<li>Outer datagram allows for validation and removal of incomplete/corrupted messages</li>
<li>Message name &amp; content would be further parsed for consumption</li>
<li>Field names can be specified for ease of use</li>
</ul>

<p>A 'GPGLL' message might be turned from <code>$GPGLL,4533.21,N,17739.11,W,113215.22,A*31</code> into a programmatic data structure containing latitude, longitude, UTC timestamp and its validity.</p>
","sensors software driver"
"2600","Need Troubleshooting help regarding Arduino Uno & HC-06 Bluetooth connection problem","<p>I just bought Arduino Uno and HC-06, I hooked up the connections:</p>

<ul>
<li>5V Bluetooth &rarr; 5V Arduino</li>
<li>GND Bluetooth &rarr; GND Arduino</li>
<li>TDX Bluetooth &rarr; RX &rarr;1</li>
<li>RDX bluetooth &rarr; TX &rarr; 0</li>
</ul>

<p>Here are the pictures:</p>

<p><a href=""http://i.stack.imgur.com/58xGC.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/58xGCm.png"" alt=""HC-06""></a>
<a href=""http://i.stack.imgur.com/5k9Em.jpg"" rel=""nofollow""><img src=""http://i.stack.imgur.com/5k9Emm.jpg"" alt=""Arduino""></a></p>

<p>My problem is that I cannot seem to search for the Bluetooth connection on my Laptop or on my phone.
Is there something wrong I am doing here?</p>
","arduino"
"2604","Differential drive trajectory following control","<p>I have a robot platform with differential drive which knows it's position and orientation.
Lets say that the space through which the robot moves is known and it has only static obstacles. The task is to move the robot from point A and heading alpha (on which it currently stands) to point B and heading beta on the map.</p>

<p>Lets also say that I can obtain a reasonable trajectory (in relation to the turning abilities of the robot). As both the robot and the sensors are inert, what are some general approaches for controlling such a robot to follow the path? It should of course be kept in mind that the final task is to reach the point B without colliding with the obstacles and not the perfect trajectory following.</p>

<p>I hope the question is not too general.</p>
","control pid navigation differential-drive"
"2606","Where to ask NXP LPC1343 / ARM Cortex M3 related questions","<p>I am a beginner to robotics and embedded systems. Consequently I have a lot of questions related to the toolchain and how things are going together like how to debug or how to connect a bluetooth module.</p>

<p>I already tried <a href=""http://electronics.stackexchange.com/"">http://electronics.stackexchange.com/</a> and it did not work out for me.</p>

<p>Any ideas where I can get help with my LPC1343 related questions?</p>
","microcontroller arm embedded-systems"
"2609","Arduino Operational Frequency","<p>Just wanted to clarify some pretty basic Arduino concept:</p>

<p>If I put this code into an arduino board:</p>

<pre><code>double start, endTime;
long int counter = 0;

void setup()
{
  Serial.begin(19200);
start = micros();
}


void loop()
{
  endTime = micros();
  counter++;

  //Point A

  if((endTime - start) &gt; 1000000)
  {
    Serial.println(counter);
    counter = 0;
    start = micros();
  }

}
</code></pre>

<p>...I see >38000 value in my serial monitor (for the 'counter' variable).</p>

<p>However, the moment I put in some heavy calculation in 'Point A', the value drops to 150 - 170. This is expected, I guess.</p>

<p>My question is: Is the only way to push up the operational frequency lies in optimising the code/calculation? Or, is there some other way I can get faster execution?</p>
","arduino"
"2610","function of PIDControl #pragma config() directive in robotC","<p>I am trying to sync motors on a VEX Cortex based robot and have had mixed success using the encoders with position control.  I noticed that the motor setup directive</p>

<pre><code>#pragma config(Motor, port2, motorA, tmotorVex393, PIDControl, encoder, encoderPort, I2C_1, 1000)
</code></pre>

<p>has a parameter ""PIDControl"" but I cannot find any documentation as to what it actually does.  </p>

<p>I see on the encoder documentation page <a href=""http://www.vexrobotics.com/wiki/index.php/Intergrated_Motor_Encoders"" rel=""nofollow"">here</a> that the encoder provides velocity output, but it is not apparently built into the API.  So my question is two fold:</p>

<p>1) What does the ""PIDControl"" directive actually do?</p>

<p>2) How can I use the encoder to control the speed of the motors?</p>
","robotc"
"2613","What is a suitable model for four-wheeled differential drive rigid-body robots?","<p>I found a model for 2-wheeled robots here:</p>

<p><a href=""http://robotics.stackexchange.com/questions/106/what-is-a-suitable-model-for-two-wheeled-robots/134#134"">What is a suitable model for two-wheeled robots?</a></p>

<p>How should I adapt it to a 4-wheeled setting?</p>
","wheeled-robot kinematics"
"2619","Building A Servo Tester To Measure Peak/Stalled Amp Draw","<p>Since finding data on stalled and in use under load (not free) amp draw for servos seems impossible, I want to build/create my own servo tester.</p>

<p>All I really want to know is how much amps the servo is drawing at idle, at movement under load, and at stalled/full position. I think that should cover all the bases relative to amp usage on the servo but if not, please let me know what I am missing.</p>

<p>Here are my questions:</p>

<ol>
<li><p>I am going to need a power supply for an exact 4.8V and 6.0V since that seems to be the standard measurement voltages. </p></li>
<li><p>I'll need some way to accurately measure the amp draw.</p></li>
<li><p>I'll need some way to control the servo movement.</p></li>
</ol>

<p>Is that it? What am I missing and if anyone has any suggestions please let me know and thanks for the help.  This seems to be uncharted waters for those in the RC hobby area but someone in the robotics field may have been down this path.</p>
","rcservo"
"2620","How to explain bandwidth of a measurement to a noob?","<p>I am working on a system which is measuring a force. The specification is to have a 500Hz bandwidth on the measurement. </p>

<p>Now I was trying to explain this 500Hz bandwith to my mom and I could not really explain it easily.</p>

<p>What is the most easy way to explain the term bandwidth of a measurement to someone without control engineering background?</p>
","control"
"2628","Project Idea for a AI related project","<p>I am a Computer Science final year undergraduate student.Until now,I used to shirk away from robotics as I believed that it is more related to electrical and mechanical aspects.But my interest in robotics grew by seeing some demos and I seriously want to make a robot which involves AI by teaming up with interbranch students of college.So what is the best project I can pick up as a beginner in robotics and AI and some experience in Computer Science so I can apply AI Machine learning concepts so that it learns something.How to start something?</p>
","artificial-intelligence machine-learning"
"2631","Quadrocopter build - Do these parts look fine?","<p>I am completely brand new to quadrocopter building. I am currently about to start building a Quad. I have done a little bit of research and was thinking of buying the following parts:</p>

<ul>
<li>KK2.1 Hobbyking Flight Controller</li>
<li>Turnigy H.A.L Quadcopter Frame</li>
<li>4 x NTM Prop Drive    35-30 1100kv / 380w</li>
<li>Turnigy 9X 9CH</li>
<li>Turnigy Plush 40A ESC</li>
<li>Slow Fly Prop Left</li>
<li>Slow Fly Prop Right</li>
<li>Quad Power Dist Board</li>
<li>Turnigy 5Ah 3S25C LiPo</li>
</ul>

<p>What do you think of these parts/Do you have any complete builds with instructions that you would recommend instead?</p>

<p>Thanks</p>
","motor sensors quadcopter multi-rotor"
"2635","How much can realistically be drawn from a 25C max 50C battery?","<p>I am using 8 brushless motors for an octocopter. Each motor can be run at maximum 30A. I use 4 batteries in parallel. How high C number is needed?</p>

<p>$$\frac{30*8}{4*2} = 30C$$</p>

<p>When running the motors at 100% load, it will draw 30C from each battery. Can a 25C with max 50C be used, or will it run hot?</p>

<p>Additionaly, how many ampere hours can be drawn from a 5000mAh battery before it's empty? Many 12V car batteries can only be drawn for 60% of their stated capacity before they need to be charged.</p>
","battery"
"2637","What is meant by a speed profile?","<p>When researching robots, micro mouses etc I've often come across people taking about generating ""speed profiles"" and how to calculate them. Also profiles for acceleration , deceleration , turning etc. Also trapezoidal profile?</p>

<p>But I can't seem to find exactly what is meant by this. The how or why also.</p>

<p>So what is a ""profile"" in this sense and why you would need one?</p>
","control"
"2639","Raspberry Pi for two wheel robot?","<p>I want to create a two wheel remote controlled robot. I have worked out a lot of the logic with regards to balancing. I have started to read up on motor control and arduino vs beagleboard black vs raspberry pi.</p>

<p>Is the multitasking nature of a full linux OS a problem I need to be concerned with for this application?</p>

<p>I expect that I have to adjust the motors at least 20 times per second, but I don't think a slight variation in the update loop interval to be a problem. Possibly, I will face problems if I need to do PWM myself?</p>

<p>Basically, the way I plan to make the robot work is by using an accelerometer to have a reference to where down is. The robot will autonomously work to keep the direction of the accelerometer down. The remote control will simply adjust the readings from the accelerometer, and the balancing loop will then react as if the robot is falling and accelerate the wheels.</p>
","arduino control raspberry-pi real-time"
"2641","What is the response time of an Arduino Nano?","<p>I want to make a circuit that powers a transistor when a sound above a set threshold is reached. (Trigger a flash for high speed photography.)</p>

<p>How long will the response time be?</p>
","arduino"
"2642","What is the best way to fuse measurements from IMU, LIDAR, and Encoder information in some recursive bayesian filter?","<p>I am doing SLAM with a four wheeled (2-wheel drive) differential drive robot driving through some hall way. The hallway is not flat everywhere. And the robot turns by spinning in place, then traveling in the resulting direction. The SLAM algorithm does not need to run online.</p>

<p>The robot takes measurements from a strap down IMU/gyro measuring <code>(ax,ay,az,wx,wy,wz)</code>, where <code>ax</code> refers to acceleration the x direction and <code>wx</code> measures angular acceleration about the x-axis. The LIDAR scans the hall way with a 270-degree arc and measures ranges and angles. However, so far as I know the hall way has no discernable features except when it corners</p>

<p>I need to find the best way to fuse the proposed action measured by the encoder with IMU and LIDAR data. It makes sense to me that I could fuse yaw from IMU with encoder data to get a better sense of heading, but how should I incorporate LIDAR data? </p>

<p>In essence, what is the appropriate <em>measurement model</em> and how should I incorporate noise into the <em>motion model</em>? Beside just adding some gaussian noise at some <code>(0,σ)</code>?</p>

<p><strong>Addendum</strong></p>

<p>This somewhat orthogonal to the question but just as confusing to me. Currently I am using a particle filter to do SLAM, and I am a little confused about whether to represent uncertainty in angular acceleration in the particles themselves. I see two options:</p>

<ol>
<li><p>A separate navigation filter using EKF (or anything really) to find a vector of ""best-estimate"" angular acceleration matrix first, then use this matrix as absolute truth for the particle filter. So that any drift in the particles is <em>not</em> from uncertainty in angular acceleration.</p></li>
<li><p>Incorporate the uncertainty into the particle drift themselves. This option appears more sensible but I am not sure what a principled way to do this is. </p></li>
</ol>
","kalman-filter slam particle-filter"
"2643","Can a rigid-prop quadcopter hover upside-down?","<p>Most small quadcopters use rigid rotors with some fixed pitch.
In principle, I can imagine it might be possible for such a rigid-prop quadcopter to hover upside-down,
but that apparently requires reversing the direction of rotation of all 4 motors.</p>

<p>(This is very different from the way some ""standard"" single-rotor model helicopters can hover upside-down by continuing to spin that rotor in ""the same direction"", but moving the swash plate to give negative blade pitch).</p>

<p>Is it possible for a rigid-prop quadcopter to hover upside-down?</p>

<p>When I build a quadcopter so it can switch from flying upright to flying upside-down and back again in mid-flight, what do I do differently than a normal quadcopter designed to always fly right-side-up?</p>

<p>(Related: <a href=""http://electronics.stackexchange.com/questions/2771/can-you-run-a-bldc-motor-backwards-without-damage"">""Can you run a BLDC motor backwards without damage?""</a> )</p>
","motor quadcopter multi-rotor"
"2644","DC motor with encoder","<p>Can anyone help me out here with this DC motor, especially the encoder part? I tried searching around for its datasheet but its as short as 1 page and the only spec I get are:</p>

<p>Encoder:
1 pulse/revolution</p>

<p>It has 2 connection on the bottom, and I guess they are for driving the motor, but they say nothing about the 3 connections wires below.</p>

<p><img src=""http://i.stack.imgur.com/yj8sz.jpg"" alt=""enter image description here""></p>
","motor quadrature-encoder"
"2649","Kinect point cloud + Pcduino. Will it work?","<p>I'm a newbie to Mircroprocessors (PcDuino, for example) and I wanted to know if Kinect can be integrated with the pcduino, before I go and buy the board. I know in terms of connectors etc what might be required. My concern is regarding the hardware required to run the Kinect.</p>

<p>To elaborate more, I'll explain my current system: I have a system working on my laptop that uses a Kinect to extract unorganized point cloud data using ""Processing"" IDE which interacts with Kinect using openni drivers. My Matlab code then processes this information to detect obstacles and specific objects (can also be done using C++). </p>

<p>I want to build such a system for a robot, but using pcduino as the processing module. This means that the Kinect will connect to the pcduino using one of its usb ports. I'll power the Kinect using battery and a converted power adapter. Since pcduino can run Linux (Ubuntu) I (think I) can easily convert my laptop code into whatever the Ubuntu requires. The only concern I have is if there were any problems associated with using depth sensors with mini pc boards in terms of hardware capabilities of mini pc boards? I know that mini pc boards are not as fast as a PC, so the processing would be slower, but I'm not concerned with the speed, atleast for the time being.</p>

<p>One problem I encountered while using kinect, even on a PC is that the point cloud drivers in openni won't initiate the point cloud data stream, unless there was a GPU in the PC; the exact same code runs perfectly on a PC with a dedicated GPU. However, I do know that pcduino has a GPU chip (OpenGl ES2.0). Would the kinect work on this?</p>

<p>I searched online but the closest thing I could find is <a href=""http://hackaday.com/2013/05/29/charlotte-the-hexapod-with-3d-vision/"" rel=""nofollow"">this</a> which does not elaborate how the integration of Raspi and Asus Xtion works. I'm not too picky about the boards, anything that would work with a kinect is fine with me, although I like the pcduino since it has arduino headers and built in wi-fi etc. </p>

<p>Any additional pointers can also be helpful. Please let me know if I need to elaborate on anything more.</p>

<p>Thanks in advance</p>
","microcontroller kinect"
"2652","A few questions about my first quadcopter build","<p>I'm planning to order parts for my first quadcopter build and I had a few questions. <a href=""https://docs.google.com/document/d/1jJsP1noISRmEyQZirJVCQK5455tRXtAwk2ZPOVDElxk/edit?usp=sharing"" rel=""nofollow"">Here</a> is my parts list. I'm crossing my fingers that they are all compatible, and I'm pretty sure they are. I have two questions:</p>

<ol>
<li>Do I need a Power Distribution Board and if so, what does it do?</li>
<li>Where on my flight controller do I attach my radio receiver? </li>
</ol>
","quadcopter microcontroller radio-control"
"2657","Could piezoelectric sensors be crushed?","<p>I have found some load sensors (piezoelectric) that measure relatively small weights (on the order of ~ grams). That's what I need!</p>

<p>However...
Around my robot, there will occasionally be bursts of extremely high pressure. These bursts do not need to be measured... they just wash over.
The pressure appears, to the sensor, to be a ~ 2,000+ kg </p>

<p>Question:
Are these sensors likely to break or fatigue? I realize piezos do not measure via deformation, but still... that's a big load!</p>

<p>Maybe I should just order a few and try...</p>
","sensors"
"2661","Kalman Filter when states are not observable at the same time?","<p>I have a system that I can make a strong kinematic model for, but my sensors send readings at unpredictable times. When I say unpredictable, I am not just saying the order the readings will arrive, I also mean that sensors are able to sleep when they do not see a significant change. When an input arrives for any given sensor, that information can be used to infer the states of many other sensors based on my model.</p>

<p>At first, it seemed like a Kalman Filter was exactly what I needed because I could make a prediction of all of the states of the system and then update those states when one piece of information comes in and repeat this process until a good estimate of the system as a whole was determined. However, after reading over Kalman filters, it looks like they assume that every state will be updated on a regular basis. Is there a way the Kalman filter can be modified for when you are unsure about what input will come in next and you are also unsure how much time will elasped before the next input arrives? Please note that in my case, once the information arrives, I will know the source of the input as well as the time that has elapsed since the last update, I just won't be able to predict these two things beforehand.</p>
","kalman-filter"
"2667","Wine yard robotics?","<p>My friend has acquired a (small) wine yard to discover how much work it is tending and harvesting the grapes.</p>

<p>Now we are musing to enlist robotic help. The vine stocks are usually connected by a stiff wire, so some dangling machine might be feasible.</p>

<p>We are looking for references/inspirations for an agricultural robotic vine assistant. Any ideas?</p>
","mobile-robot robotic-arm"
"2668","Removing quadcopter drift to the side","<p>I wrote my own quadcopter firmware which is based on some older code. This code shall stabilize the copter to be always in equilibrium. The model is behaving relatively nice. I can control it with my laptop.
However I noticed, that the copter is hovering to the side (if not manually controlled), likely because of wind, not well balanced or turbulence. </p>

<p>My idea was maybe to fuse GPS and accelerometer data to implement a function which shall help to hold the position. But this will likely only work if I have a hold altitude function, because changes in pitch or roll change the height, because the thrust is changed slightly. This is why I recently added a routine which shall allow to hold the altitude. </p>

<p>Is someone having experiences with this? I mean with avoiding side drifts of the model because of whatever by software? The problem is in my opinion, that I don't know whether the position change is wanted (by remote control) or not. Additionally it is hard to localize the correct position and calculate the distance caused by drift from it (just with GPS, but this is not precise).</p>

<pre><code>void hold_altitude(int_fast16_t &amp;iFL, int_fast16_t &amp;iBL, int_fast16_t &amp;iFR, int_fast16_t &amp;iBR, 
                   const int_fast32_t rcalt_m) 
{
  // Enhance the performance:
  // This function is only needed for (semi-)autonomous flight mode like:
  // * Hold altitude
  // * GPS auto-navigation
  if(_RECVR.m_Waypoint.m_eMode == GPSPosition::NOTHING_F) {
    return;
  }

  // Return estimated altitude by GPS and barometer 
  float fCurAlti_cm       = _HAL_BOARD.get_alti_m() * 100.f;
  // Estimate current climb rate
  float fBaroClimb_cms    = _HAL_BOARD.get_baro().climb_rate_ms * 100;
  float fAcclClimb_cms    = _HAL_BOARD.get_accel_ms().z * 100;
  // calculate the 
  float fAltStabOut      = _HAL_BOARD.m_rgPIDS[PID_THR_STAB].get_pid(fCurAlti_cm - (float)(rcalt_m*100), 1);
  float fBarAcclOut      = _HAL_BOARD.m_rgPIDS[PID_THR_ACCL].get_pid(fAltStabOut - fBaroClimb_cms, 1);
  float fAccAcclOut      = _HAL_BOARD.m_rgPIDS[PID_THR_ACCL].get_pid(fAltStabOut - fAcclClimb_cms, 1);
  int_fast16_t iAltOutput = _HAL_BOARD.m_rgPIDS[PID_THR_RATE].get_pid(fAltStabOut - (fBarAcclOut + fAccAcclOut), 1);
  // Modify the speed of the motors
  iFL += iAltOutput;
  iBL += iAltOutput;
  iFR += iAltOutput;
  iBR += iAltOutput;
}
</code></pre>

<p>Copter control:</p>

<pre><code>// Stabilise PIDS
float pit_stab_output = constrain_float(_HAL_BOARD.m_rgPIDS[PID_PIT_STAB].get_pid((float)rcpit - vAtti.x, 1), -250, 250);
float rol_stab_output = constrain_float(_HAL_BOARD.m_rgPIDS[PID_ROL_STAB].get_pid((float)rcrol - vAtti.y, 1), -250, 250);
float yaw_stab_output = constrain_float(_HAL_BOARD.m_rgPIDS[PID_YAW_STAB].get_pid(wrap180_f(targ_yaw - vAtti.z), 1), -360, 360);

// is pilot asking for yaw change - if so feed directly to rate pid (overwriting yaw stab output)
if(abs(rcyaw ) &gt; 5.f) {
  yaw_stab_output = rcyaw;
  targ_yaw = vAtti.z; // remember this yaw for when pilot stops
}

// rate PIDS
int_fast16_t pit_output = (int_fast16_t)constrain_float(_HAL_BOARD.m_rgPIDS[PID_PIT_RATE].get_pid(pit_stab_output - vGyro.x, 1), -500, 500);
int_fast16_t rol_output = (int_fast16_t)constrain_float(_HAL_BOARD.m_rgPIDS[PID_ROL_RATE].get_pid(rol_stab_output - vGyro.y, 1), -500, 500);
int_fast16_t yaw_output = (int_fast16_t)constrain_float(_HAL_BOARD.m_rgPIDS[PID_YAW_RATE].get_pid(yaw_stab_output - vGyro.z, 1), -500, 500);

int_fast16_t iFL = rcthr + rol_output + pit_output - yaw_output;
int_fast16_t iBL = rcthr + rol_output - pit_output + yaw_output;
int_fast16_t iFR = rcthr - rol_output + pit_output + yaw_output;
int_fast16_t iBR = rcthr - rol_output - pit_output - yaw_output;
// Hold the altitude
hold_altitude(iFL, iBL, iFR, iBR, rcalt);

hal.rcout-&gt;write(MOTOR_FL, iFL);
hal.rcout-&gt;write(MOTOR_BL, iBL);
hal.rcout-&gt;write(MOTOR_FR, iFR);
hal.rcout-&gt;write(MOTOR_BR, iBR);
</code></pre>
","arduino quadcopter"
"2671","Compliance control for a single link robot in matlab","<p>What exactly is active compliance control in robotics joint? 
Why Is it used ? 
How can I write a program to simulate the compliance control in matlab for a single robotic link or single robotic joint ?
I have to develop an algorithm for torque control.
I have to sense the torque and give feedback to BLDC motor which is supposed to apply some controlled torque. 
I also have some unclear understanding of few things: Lets say I have single joint two link systems, How would this system behave when I have applied the compliance control algorithm at the joint? How will I test it? I mean if I apply some external torque what should it do so that I understand that it is in compliance control mode.
Here is a related paper.
<a href=""http://www.thehandembodied.eu/pdf/ICCAS.pdf"" rel=""nofollow"">http://www.thehandembodied.eu/pdf/ICCAS.pdf</a></p>
","control robotic-arm"
"2672","MCBL Controller through RS232","<p>I'm trying to use the <a href=""http://www.micromo.com/manuals/sites/en/steuerungen/mcbl_3006_s_rs.html"" rel=""nofollow"">MCBL Controller</a> by Faulhaber to control my motor. I'm trying to program some sort of driver on linux using the serial connection and libserial. But it does not seem to be working for now. </p>

<p>I'm using the usb to RS232 converter like this one:</p>

<p><a href=""http://i.stack.imgur.com/jA8vW.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/jA8vWm.png"" alt=""Cable""></a></p>

<p>I'm wondering if it's well supported by libserial. I've read that yes but does anyone have any experience with it?</p>
","serial communication"
"2673","Passive ego-motion estimation vs active","<p>I am doing research of ego-motion estimation and positioning in 6DoF space. And I found that apparently all systems are based on active RGB-D sensors, like Kinect. I understand, that such sensors provide greater accuracy, and requires less computational resources.</p>

<p>But if such systems will be used, for example, for augmented reality or robot navigation, how they are going to solve the problem of the interference of signals from different systems, operating in the same space? If many people will wear AR glasses with active sensors - they will interfere with each other, aren't they?</p>

<p>Are there big commercial projects, that use passive visual odometry with multiple camera units and IMU sensors? I found some good papers on this topic, but I have not found commercial application of such technology. I am going to make research of passive odometry method for AR, but is it actually a problem with active depth sensors, that i described earlier?</p>

<p>UPD:
The main question:</p>

<p>Is passive odometry, based on video flow analysis and IMU, worth to make deep research in this topic, or active sensors - is our future, and the signal mix is not a big deal, and passive odometry is a dead end of such kind of technology? Because it will be not very useful to make research in useless technology...</p>
","kinect sensor-fusion odometry"
"2676","How Can I Measure the Torque Value of a Servo?","<p>I know there must be a ""tool"" that can measure oz-in of torque.  I do not want to trust what the servo manufacturers state on their site for torque values so I want to test them for myself.</p>

<p>Anyone know what tool I can use to do this?  I have used fishing scales before, but I need something more sensitive than that and my units are pretty small such as around 20 oz-in.</p>

<p>Thanks.  </p>
","servos"
"2677","Autonmous surveillance vehicle","<p>I am planning to build an autonomous all terrain surveillance robot using raspi, which is the better option Computer vision or ultrasonic sensing to avoid obstacles? <br> And i want to transmit the video recording to base station.</p>
","raspberry-pi computer-vision ultrasonic-sensors"
"2680","How to decide about the length of a robotic arm, the base of it and the torque?","<p>I am designing  a remote controlled robot which will have a base with three wheels, two of them will be simple wheels at the back of the base and the third will be a ball wheel at the front. It will have a robotic arm which will have a gripper to hold objects up to 1kg. I have designed the arm like this</p>

<p><img src=""http://i.stack.imgur.com/8BH53.png"" alt=""enter image description here""></p>

<p>What I want to ask is how to calculate the length of the arm, the base of the  robot, the torque and also which motor to use. Please suggest to me if there is a better solution for designing the robot.I am a robot enthusiast and I am designing a robot for the first time.</p>
","motor control design robotic-arm"
"2683","Altitude hold for quadcopter with Accelerometer and Barometer","<p>I wonder currently how to implement an altitude control for a quadcopter. I have atm just a barometer/GPS and an accelerometer. </p>

<p>Barometer and GPS are relatively straight forward implemented, but not very precise and slow. For the accelerometer readout, I remove the constant 9.81 m/s² acceleration by a low pass filter. Then I take this data and calculate out of it the climb-rate(in cm per s). I know the speed approximation by this way is not so great. However I don't know a better approach so far.</p>

<p>For the calculation of the motor speeds I use atm two PIDs (STAB and RATE). </p>

<p>I coded the example shown below, without much testing so far. I believe it will not work out in a smooth and nice way. E. g. instead of the speed calculated of the accelerometer I could use the climb-rate of the barometer. However for low altitudes and small changes I do need very likely the accelerometer. </p>

<p>ArduPilot seems to use somehow in a different way both with a third PID for the acceleration. I believe they calculate the height difference like me. Then they use maybe for STAB-Pid the barometer climb rate (not like me the accelerometer) and calculate with acceleration data another output. Unfortunately I don't know how exactly, or whether there are other methods.</p>

<p>Does someone know the exact layout to implement with a barometer and accelerometer an altitude hold function. I mean I am really not sure whether my ideas would be correct. Maybe I can post some options later.</p>

<p>My PIDs:</p>

<pre><code>m_rgPIDS[PID_THR_STAB].kP(1.25);  // For altitude hold

m_rgPIDS[PID_THR_RATE].kP(0.35);  // For altitude hold
m_rgPIDS[PID_THR_RATE].kI(0.15);  // For altitude hold
m_rgPIDS[PID_THR_RATE].imax(100); // For altitude hold
</code></pre>

<p>Code for altitude hold:</p>

<pre><code>// Stabilizing code done before

float fCurAlti_cm       = _HAL_BOARD.get_alti_m() * 100.f;        // Barometer and GPS data
float fAcclClimb_cms    = _HAL_BOARD.get_accel_mg_ms().z * 100;   // Accelerometer output in cm per s (gravitational const. corrected)

// calculate the difference between current altitude and altitude wanted
float fAltStabOut       = _HAL_BOARD.m_rgPIDS[PID_THR_STAB].get_pid(fCurAlti_cm - (float)(rcalt_m*100), 1);
// Rate it with climb rate (here with accelerometer)
int_fast16_t iAltOutput = _HAL_BOARD.m_rgPIDS[PID_THR_RATE].get_pid(fAltStabOut - fAcclClimb_cms, 1);

// Modify the speed of the motors
iFL += iAltOutput;
iBL += iAltOutput;
iFR += iAltOutput;
iBR += iAltOutput;
</code></pre>
","sensors quadcopter accelerometer ardupilot"
"2687","Need help calculating the thrust on quadcopter motors","<p>I'm trying to calculate the lifting capability of my four quadcopter motors. I tried using eCalc but it doesn't have battery I'm using. Are there any equations to keep in mind for doing these calculations? Here are some relevant details:</p>

<p>Battery: 2200mAh 3S 25~50C LiPo</p>

<p>ESC: 25A</p>

<p>Motor: 1240kV Brushless</p>

<p>Propeller: 8x4</p>

<p>Any help would be much appreciated, thanks!</p>
","quadcopter"
"2692","Using 3DR Radio to communicate ArduPilot Data","<p>I am trying to send some data over to my PC from the Arduipilot, I used a Normal USB connection to send over a recurring string like this:-</p>

<pre><code>const AP_HAL::HAL&amp; hal = AP_HAL_BOARD_DRIVER;

void setup()
{
   hal.uartA-&gt;begin(38400);
}

void loop()
{
    hal.scheduler-&gt;delay(20);
    hal.console-&gt;println(""Recd_String"");
}
AP_HAL_MAIN();
</code></pre>

<p>I receive the string just fine when I open a serial monitor with baud of 38400 bits/sec. But, when I remove the USB port and plug in the 3DR radio module to the ardupilot and the PC, it gives me garbage. I know that the 3DR radios use MAVLink communication protocol, but I was wondering if it's possible to change this protocol and use a normal SPI so that I receive the data in the same format I receive when connected via USB.</p>

<p>If this is not possible, is there a way to convert this garbled data from the radio module to a useful string. 
It would be greatly appreciated if someone can help me with this.</p>
","ardupilot radio-control"
"2694","Set CANopen Node ID of Ingenia Pluto DC Servo Drive","<p>Does anybody know how to configure the node ID of an Ingenia <a href=""http://www.ingeniamc.com/En/-Pluto-compact-dc-servo-drive-canopen"" rel=""nofollow"">Pluto</a> DC Servo Drive?</p>

<p>I've got a request out to their support team, but perhaps somebody here is already familiar with these drive boards.</p>

<p>I do have Ingenia MotionLab 2.7.2, but it does not ship with documentation and the MotionLab user manual on the site is out of date (I had previously been looking through the hardware documentation, but it turns out the info was in MotionLab documentation; although the instructions for previous versions no longer seem to apply to 2.7.2).</p>
","control can"
"2695","What are the options for a thin light source (e.g. LED)?","<p>I'm looking to make or buy something resembling an LED, that would be thin (about 0.5mm or less) and cheap (&lt;0.1$ in mass production).</p>

<p>Any suggestions?</p>
","arduino electronics"
"2696","An architecture for testing autonomous flight and sensors","<p>I'm designing a simple autopilot software on top of Ardupilot, my goal is to possibly interface an Raspi on top of ArduPilot Mega (APM). I am stuck on setting up a simulation environment using either V-Rep or Gazebo. 
The quadcopter will have basic sensors plus advanced sensors. basic sensors talks directly with ArduPilot, while advanced sensors talks with my own autopilot software. I am trying to wrap my head around a feasible setup to test the software while using ArduPilot Mega in the Hardware-In-The-Loop. I am planning on having three stages of Simulation:</p>

<p>Stage 1. Simulate quadcopter physics in Gazebo/V-Rep, run ArduPilot software and my autopilot software in a VM (not sure if it's even do-able)</p>

<p>Stage 2. Simulate quadcopter physics in computer, run my autopilot software in a VM, and run APM in a hardware-in-the-loop fashion.</p>

<p>Stage 3. deploy my autopilot onto Raspi and interface with APM then run both hardwares in Hardware-in-the-loop fashion.</p>
","quadcopter simulator"
"2698","Random number generation for Particle Filter","<p>I implemented a bootstrap Particle filter on C++ by reading few Papers and I first implemented a 1D mouse tracker which performed really well. I used normal Gaussian for weighting in this exam. </p>

<p>I extended the algorithm to track face using 2 features of Local motion and HSV 32 bin Histogram. In this example my weighing function becomes the probability of Motion x probability of Histogram. (Is this correct).</p>

<p>Incase if that is correct than I am confused on the resampling function. At the moment my resampling function is as follows:</p>

<p>For each Particle N = 50;</p>

<ol>
<li>Compute CDF</li>
<li>Generate a random number (via Gaussian) X</li>
<li>Update the particle at index X</li>
<li>Repeat for all N particles.</li>
</ol>

<p>This is my re-sampling function at the moment. Note: the second step I am using a Random Number via Gaussian distribution for get the index while my weighting function is Probability of Motion and Histogram. </p>

<p>My question is: Should I generate random number using the probability of Motion and Histogram or just the random number via Gaussian is ok.</p>
","mobile-robot localization particle-filter tracks"
"2700","Battery system with and without mains voltage attached","<p>I'm working on a project where mains voltage will sometimes be disconnected, and the system will have to run on battery for as long as possible before doing a (safe) shutdown.  The desired behavior is exactly like a laptop battery system:</p>

<ol>
<li>When mains voltage is connected, charge the battery and power the system from mains</li>
<li>When mains voltage is disconnected, power the system from the battery</li>
<li>Prevent the battery system from supplying current when the batteries are discharged below a certain voltage (to prevent damage).</li>
</ol>

<p>Is there a name for this type of system, or a name for the feature(s) that I should be looking for when I look at chargers?  (If it matters, this system will be 12V, so I'm looking at 14.8V Lithium battery options.)</p>
","power battery"
"2701","2D Map representation of GPS coordinates in degrees","<p>I want to implement my own GPS navigation for a quad-copter.
I can calculate and filter the GPS coordinates (latitude and longitude in degrees). </p>

<p>I believe the easiest approach for me would be, to calculate the change of the heading of the quad-copter from the current attitude to the destination point and let it fly straight on after turning. </p>

<p>However I am not sure about the 2D representation of the latitude/longitude-GPS coordinates (for a round earth to a 2D map system when calculating the heading change). How big is the expected error? Or is there none?</p>
","navigation gps"
"2702","How does one calculate the angular motion of each node in a robotic arm","<p>A robotic usually consists of joints with sections of possibly varying width connected together. Considering we know how much each is bent and the length of each section, and their location in 3D space (not local coordinates) at time zero; how do we determine how much each joint should rotate to goto position B from position A. Both A and B are defined in world cartesian coordinates.</p>

<p>Now each joint can move in terms or all at once, so should all joints move simultaneously or in turns?</p>
","kinematics robotic-arm"
"2704","Quadcopter forward speed","<p>I am trying to better understand the dynamics of forward flight in multirotors.</p>

<p>Assuming I have a quadcopter with 4 motor/propeller combinations capable (each) of a propeller pitch speed of, say, SpeedMax= 100 mph.</p>

<p>In forward horizontal flight, the quadcopter will pitch down at a certain angle, let's say AlphaP, from horizontal. If AlphaP is, say, 45 degrees, and drag is neglected, wouldn't the quadcopter be capable of a max theoretical speed of sin (45)* SpeedMax ~ 70Mph?</p>

<p>Also, seems to me AlphaP cannot go all the way to 90 degree (quadcopter flying like a plane), as at that point the propellers would not produce any  upward thrust to maintain the copter aloft given there is no wing loading as available in a plane.  If drag was to be neglected, what factors would the optimum AlphaP be depended on, and what would that angle be, for maximum speed?</p>
","quadcopter"
"2706","How do I get started in Computer Vision?","<p>I want to know how to write and run the correct code. I understand that I need to download OpenCV (which I have), but when I try to compile sample code - for example, Blob Detection - it doesn't compile. I am just very confused on the process of what you need to do to get something to show up on the screen. 
I know my question is really vague, but I have such a bad understanding of Computer Vision that I don't really know how to describe my problem. Hopefully discussing more will be able to help me. </p>

<p>Please help me! I have been searching the Internet for 2 hours now and I am just lost in a sea of information...</p>
","computer-vision"
"2708","Multiple position estimates fusion","<p>I have a system in which I have two separate subsystems for estimating robot positions.
First subsystem is composed of 3 cameras which are used for detecting markers the robot is carrying and which outputs 3 estimates of the robot's position and orientation.
The second subsystem is a system which is located on the robot and is measuring speed on the two points of the robot. By numerically integrating those two I can get an estimate on the robot's position and orientation (because I am tracking two points at once).</p>

<p>The first system is less accurate but the second system drifts. First system gives output about once a second while the second one gives output much more frequently (100-200 times per second).</p>

<p>I assume there must be a better approach than to just reset the position with the first system's estimate (as it is not 100% accurate), but to also use the accumulated position from the second sensor system and fuse that with the new data from the first system. Also, there is a question how to fuse 3 estimates of the first system? There must be a better way than pure average as it might happen that the two estimates are exactly the same and the third one is completely different (meaning that it is probably more wrong)?</p>

<p>Do you have any fusion algorithms to recommend to use in such a system? I know about Kalman filter, but I am having trouble figuring out how to use it as the two systems output data at different frequencies.</p>

<p>I hope the question is clear enough, what is the best approach to fuse the estimates into a more correct and accurate estimate?</p>

<p>Thanks</p>
","sensors localization kalman-filter sensor-fusion"
"2712","Power issues involving Raspberry Pi","<p>I have a Raspberry Pi (Model B) attached to a Roomba. I'm using <a href=""http://www.adafruit.com/products/1065"" rel=""nofollow"">this part</a> to bring the unregulated 17V+ power of the Roomba down the 5V/1A of the Pi. The problem is that the Pi will randomly reboot and cause peripherals (such as the bluetooth adapter) to freak out and not work. We can sometimes drive it around for a little while before it reboots, other times it happens almost immediately.</p>
","mobile-robot raspberry-pi"
"2713","How to connect Arduino Uno3, L293D motor driver and 3 colour sensors together?","<p>I am trying to build an autonomous multi coloured lines following robot.</p>

<p>The parts I have bought so far include:</p>

<ol>
<li>Arduino Uno 3.</li>
<li>3 Colour Sensors (taos tcs 230).</li>
<li>L293DNE motor driver.</li>
<li>Robot chassis including 2 dc motors, 2 wheels and 1 caster wheel.</li>
</ol>

<p>I am trying to figure on how to connect all these components together (for example: arduino to the colour sensor, l293d to motor).  How do I connect it in order for the motor to rotate in both directions?  Do I need to solder anything?</p>
","mobile-robot"
"2714","Precision we can expect of an ultrasound-based localisation system","<p>I'm considering building an absolute, indoor robot-positioning system based on ultrasound Time Of Flight. Transducers will be ordinary, narrow-band, 40 kHz ones.</p>

<p>Based on your experience, what is the best exactitude and precision we can achieve with such a system?</p>

<p>I'm aware that the answer to such a question will depend on many factors, both hardware and software (if applicable), but I'm not asking about the performance of one solution or another, but about the intrinsic limitations of the ultrasound technology.</p>
","localization ultrasonic-sensors"
"2715","How to implement a distance proximity sensor with wider range","<p>More in the line of robotics observing their environment, I'm trying to implement a proximity sensor that can sense objects in front of it to a least up to $-30^ \circ \space$ to   $\space+30^ \circ $ of it's direction of propagation.</p>

<p>There are only two ways I can think of</p>

<ol>
<li>Multiple Infrareds.     Con: more spacious</li>
<li>Fast-Motor.  Con: expensive in money and time-complexity wise</li>
</ol>

<p>I'm currently using a <a href=""https://www.sparkfun.com/datasheets/Sensors/Infrared/gp2y0a02yk_e.pdf"" rel=""nofollow"">Proximity Sensor with up to 10ft distance capability</a></p>
","sensors"
"2719","H bridge for rover","<p>I am building a 6 wheeled rover, one set of 3 wheels will work together and other set of 3 wheels will run together, so current in each side may vary from <strong>3A-15A(blocked rotor)</strong> and <strong>6V</strong> i want to make a H-bridge(for controlling direction and speed(i ll use PWM) so i will require <strong>two such H-bridges</strong>. what is the <strong>copper track thickness</strong> i should use in <strong>proteus</strong> for making the design or else shall i go for <strong>manual soldering</strong> replacing <strong>tracks with wires</strong>. Can anybody suggest a design which is relatively easy to design with some <strong>protection circuit</strong> in it (for MCU pins isolation) or suggest any suitable motor controllers from TI or any company which can be apt to my problem</p>
","motor control"
"2720","What are the pros and cons of fictitious play","<p>I've been looking for articles and topics the Fictitious Play learning algorithms for my presentation
I haven't found it's pros and cons
Is there a book or something that I can benefit from?
Thanks</p>
","algorithm"
"2723","SCARA Arm Lead Screw Choices","<p>I'm thinking about building a <a href=""http://en.wikipedia.org/wiki/SCARA"" rel=""nofollow"">SCARA Arm</a> to lift moderate loads(5lbs) with a high degree of accuracy. I want a relatively quick and inexpensive Z axis gantry, and I was thinking about using a lead screw with dual linear rail. Trouble is I'm not certain the linear velocity will be fast enough.</p>

<p>What's the best method of choosing the lead screws and the associated nut, given a desired linear velocity of 10inches/second and a NEMA stepper motor driving it?</p>
","robotic-arm mechanism"
"2724","What's a good pose estimation method for high precision (<5mm per-axis) solutions at short range (<50cm)?","<p>I'm trying to get a 6DOF pose solution for an object that'll be between 10 and 50 cm from a fixed point. I want to avoid putting too much special hardware on the object, but extra hardware on the fixed side is fine.</p>

<p>I've been looking into two general methods:</p>

<ul>
<li><p><strong>fiducial markers</strong> There are several software packages with different types of markers, but I haven't been able to find any information about them regarding precision or accuracy in short-range pose sensing.</p></li>
<li><p><strong>ultrasonics</strong> I've found some commercial systems that do 6DOF pose sensing (e.g. <a href=""http://hexamite.com/"" rel=""nofollow"">hexamite</a>), but they're expensive and require you to put transmitters on the object.</p></li>
</ul>
","sensors ultrasonic-sensors pose"
"2726","What's the difference between CAN's Motor Max Velocity vs. Profile Max Velocity?","<p>CAN301/402 provides Max Motor Speed (0x6080,0x00) and Max Profile Velocity (0x607F,0x00). In profiled motions, the maximum speed is limited to the lower of these two values. In non-profiled motions, the maximum speed is limited to Max Motor Speed.</p>

<p>What is the intended purpose of Max Profile Velocity, rather than only providing Max Motor Speed and using that everywhere instead?</p>
","motor control can"
"2729","Is there an analytical solution for inverse kinematics of a 6 DOF serial chain?","<p>Let's take a 6 DOF robotic structure. It's consisting of the 3 DOF global structure for the position - and the 3 DOF local structure for the orientation of the endeffector.</p>

<p>If the last 3 axis (of the local structure) are coincident in one point, the inverse kinematics can be solved analytically by decomposing it into a position- and orientation-problem. </p>

<p>But is it possible to solve the inverse kinematics analytically if the last 3 axis are <strong>NOT coincident</strong> in one point? I've read several papers that claim that due to high non-linearity of the trigonometric functions and motion complexity in 3D-space, a 6 DOF serial chain cannot be solved analytically. </p>

<p>Does anybody know if this is right?</p>
","inverse-kinematics"
"2730","How to use scan command in Arduino WifiBee","<p>We want to find available WiFi networks near.
So in tutorial there is command scan, which is send from CoolTerm program from PC.
<a href=""http://www.dfrobot.com/wiki/index.php/WiFi_Bee_V1.0_(SKU:TEL0067)"" rel=""nofollow"">http://www.dfrobot.com/wiki/index.php/WiFi_Bee_V1.0_(SKU:TEL0067)</a>
Now we want to write program to Arduino which will do same operation, how it can be done?</p>
","arduino wifi"
"2731","Using robotic simulator for prediction step in probabilistic localization approaches","<p>Probabilistic localization approaches like Kalman or Monte Carlo benefit from an accurate prediction step. The more accurate the prediction step, the more accurate is the belief of the robots pose. In most approaches probabilistic motion models are applied, mainly because robot dynamics are more difficult to model. Still some approaches rely on dynamic models in order to increase the accuracy.</p>

<p>Therefore, I was wondering if it’s reasonable to utilize a robotic simulator like V-REP or Gazebo for the prediction step. The advantages I see in doing so are the following:</p>

<ul>
<li>the robots kinematic is solved by default, simply through modeling it in the robotic simulator</li>
<li>the robots dynamics are taken into account</li>
<li>nonlinear behaviors like slippage or collision can be modelled up to a certain extend</li>
<li>the robots workspace is taken into account, by modeling its environment (if the robot drives against a wall previous models would predict it behind the wall, which won’t happen in a robotic simulator)</li>
</ul>

<p>With the shown advantages I hope to achieve a more accurate prediction.</p>

<p>However there might be some problems using a robotic simulator. For a start it has to ensure real time behavior and there will be delay in the prediction due to the communication with the simulator.</p>

<p>I was looking for some papers which pick up on that idea but couldn’t find any. Are there any approaches similar to my idea? If not, are there any reasons why nobody is using a robotic simulator for the prediction? What are your opinions about my proposal?</p>
","mobile-robot localization motion simulator"
"2744","What different sensing approaches are used in the current batch of indoor 3D cameras?","<p>I'm aware of the PrimeSense camera powering the Kinect. Are more advanced sensor types available now in the &lt; $500 range? For example, has there been any sort of game-changer in structured light techniques? Do decent flash lidar cameras exist now?</p>
","sensors kinect cameras lidar"
"2747","Robot start-up movement problems","<p>I have created robot using a robot <a href=""http://www.hobbyking.com/hobbyking/store/__44606__DG012_ATV_4WD_ATV_Multi_Chassis_Kit_with_Four_Rubber_Tyres.html"" rel=""nofollow"">chassis kit</a> from hobbyking. At first when testing the robot connected to USB power source and the wheels lifted above the ground everything seemed to be OK. Then, when I tried to power the robot with batteries I encountered a problem with starting the movement.</p>

<p>The robot hardly starts to move even when I power it with 100% of power - sometimes I have to push it a little bit in order to start driving.</p>

<p>As a newbie I don't know whether it is a power source (battery) or motors problem.</p>

<ul>
<li>There are 4 motors with torque of 800gf.cm min in the chassis. </li>
<li>The gear ratio is 48:1 and to power the motors </li>
<li>I used two serially connected Li-ion batteries and dc-dc regulator which limits the voltage output to 5V. </li>
<li>The power is regulated with dual H-bridge motor driver. </li>
</ul>

<p>According to specifications, the maximum free running current for a single motor is 250mA and I have read that the stall current is 3-8x running current.</p>

<p>Anyway, the problem is that the robot has problems with starting-up driving and I don't know how whether the motors are even powerful enough to move the robot or it is a power source problem or perhaps the obstacle could be solved with appropriate power regulation (ramp). </p>

<p>How can I solve this problem?</p>

<p><a href=""http://i.stack.imgur.com/1Mgjy.jpg"" rel=""nofollow""><img src=""http://i.stack.imgur.com/1Mgjym.jpg"" alt=""enter image description here""></a></p>
","motor battery movement"
"2748","Maximum Distance using Ultrasonic sensor Arduino","<p>What is the maximum distance (of say , a car ) you could measure using an ultrasonic sensor that would be compatible with arduino? Is there any sensor(ultrasonic or not) that could measure the distance of a car , say upto 50 meters that can be used with arduino?</p>
","arduino ultrasonic-sensors"
"2754","What is the Name of the part I'm describing","<p>I'm looking for a part that will do a particular function.  It has to be able to move along one axis and tell me the force that is being exerted on it.</p>

<p>Kind of how like a piston moves inside an engine (one axis of movement) except that something will be pushing at the top of the piston and I need to know how hard its pushing.  Another difference is that the piston won't be constantly moving back in forth, but needs to be able to receive commands like.  <code>move x centimeters forward</code> and then remain stationary at its new position.</p>

<p>I know to make this it would involved a sensor and something that can exert force but what is the name of the described machine?</p>

<p>Edit #1 - Response to Matthew Gordon</p>

<p>The piston would have to move between 0-6 centimeters.  The form factor would be small, ideally smaller than the palm of your hand.  (Smaller=better) The forces it would have to deal with are comparable to the forces exerted on a bicycle by its chain.  I'm a math/cs person not engineering so I don't know technical terms for these kinds of things off the top of my head.  It would have to be real time sensor reading, but the volume of data could be processed by a phone.  Would have to working in conjunction with wireless communication, probably Bluetooth, but I'd have to look into the latency requirements to be sure.</p>
","sensors mechanism force-sensor"
"2758","What are the advantages of using the Denavit-Hartenberg representation?","<p>When one wants to model a kinematic chain and in particular define the frames attached to each body, it is common to use the <a href=""http://en.wikipedia.org/wiki/Denavit%E2%80%93Hartenberg_parameters"" rel=""nofollow"">Denavit-Hartenberg parameters</a>.</p>

<p>What are the advantages of this representation?</p>

<p>I can understand the interest of having a normalized representation but does it impact the algorithms performance?
The algorithm is not trivial to implement, what gain can we expect from this instead of, for instance, just fixing reference frames by hands (i.e. arbitrarily) like this is done in many robotics formats such as <a href=""http://wiki.ros.org/urdf"" rel=""nofollow"">URDF</a>.</p>
","kinematics"
"2760","Computing inverse kinematic with jacobian matrices for 6 dof manipulator","<p>I'm trying to calculate the inverse kinematic for an 6 dof manipulator.</p>

<h3>Task:</h3>

<p>A target point $p_{target} = (x,y,z)^T$ and the orientation $o_{target} = (a, b, c)^T$ are given and I want to get the angle configuration $q = (q_1, q_2, q_3, q_4, q_5, q_6)^T$ for my robot.</p>

<h3>Method:</h3>

<p>For that I try to implement the Jacobian method (with the transposed jacobian matrix) with <a href=""http://graphics.cs.cmu.edu/nsp/course/15-464/Fall09/handouts/IK.pdf"">this</a> guide and followed the pseudocode at slide 26. But instead using the pseudoinverse of the Jacobian matrix I used the transposed one.</p>

<p>I'll try to compute the Jacobian matrix numerically and analytically, but didn't get a solution (endless loop) for any of them. Here's how I retrieve the Jacobian:</p>

<ul>
<li><p>Numerically: </p>

<pre><code>private void calculateMatrixNumerically()
{
    var deltaTheta = 0.01;
    var newAxisConfiguration = new AxisConfiguration(
        this.currentAxisConfiguration.joints[0] + deltaTheta,
        this.currentAxisConfiguration.joints[1] + deltaTheta,
        this.currentAxisConfiguration.joints[2] + deltaTheta,
        this.currentAxisConfiguration.joints[3] + deltaTheta,
        this.currentAxisConfiguration.joints[4] + deltaTheta,
        this.currentAxisConfiguration.joints[5] + deltaTheta
        );

    var ePos = this.kinematic.CalculateDirectKinematic(newAxisConfiguration);

    for (var column = 0; column &lt; this.Columns; column++)
    {
        this.M[0, column] = (this.currentPos.Point.X - ePos.Point.X) / deltaTheta;
        this.M[1, column] = (this.currentPos.Point.Y - ePos.Point.Y) / deltaTheta;
        this.M[2, column] = (this.currentPos.Point.Z - ePos.Point.Z) / deltaTheta;
        this.M[3, column] = (this.currentPos.Orientations[0].A - ePos.Orientations[0].A) / deltaTheta;
        this.M[4, column] = (this.currentPos.Orientations[0].B - ePos.Orientations[0].B) / deltaTheta;
        this.M[5, column] = (this.currentPos.Orientations[0].C - ePos.Orientations[0].C) / deltaTheta;
    }
}
</code></pre></li>
<li><p>Analytically:</p>

<pre><code>private void calculateMatrixAnalytically()
{
    var peMatrix = calculateJointPositions();
    var zMatrix = calculateZ();

    for (var column = 0; column &lt; this.Columns; column++)
    {
        double[] p_p = new double[3];
        for(var row = 0; row &lt; zMatrix.Rows; row++)
        {
            p_p[row] = peMatrix.M[row, this.Columns-1] - peMatrix.M[row, column];
        }

        this.M[0, column] = zMatrix.M[1, column] * p_p[2] - zMatrix.M[2, column] * p_p[1];
        this.M[1, column] = zMatrix.M[2, column] * p_p[0] - zMatrix.M[0, column] * p_p[2];
        this.M[2, column] = zMatrix.M[0, column] * p_p[1] - zMatrix.M[1, column] * p_p[0];
        this.M[3, column] = zMatrix.M[0, column];
        this.M[4, column] = zMatrix.M[1, column];
        this.M[5, column] = zMatrix.M[2, column];
    }
}

/// &lt;summary&gt;
/// Calculate the positions of every joint.
/// &lt;/summary&gt;
/// &lt;returns&gt;The Matrix with the joint coordinate for each joint.&lt;/returns&gt;
private Matrix calculateJointPositions()
{
    Matrix jointPositions = new Matrix(3,6);
    Position pos;

    for (var joint= 0; joint&lt; this.currentAxisConfiguration.joints.Count(); joint++)
    {
        pos = this.kinematic.CalculateDirectKinematic(this.currentAxisConfiguration, joint);
        jointPositions.M[0, joint] = pos.Point.X;
        jointPositions.M[1, joint] = pos.Point.Y;
        jointPositions.M[2, joint] = pos.Point.Z;
    }

    return jointPositions;
}

private Matrix calculateZ()
{
    // (z0^T-z1^T-z2^T-...-z6^T)
    var ksEnd = Kinematics.TranslateRobotToWorld();
    var zMatrix = new Matrix(3, 6);

    for (var column = 0; column &lt; this.currentAxisConfiguration.joints.Count(); column++)
    {
        for (var row = 0; row &lt; zMatrix.Rows; row++)
            zMatrix.M[row, column] = Math.Round(ksEnd.M[row, 2], 7);

        ksEnd = ksEnd.Multiply(
            Kinematics.TranslateCoordinateSystem(
            Robo.theta[column] + this.currentAxisConfiguration.joints[column],
            Robo.d[column],
            Robo.alpha[column],
            Robo.a[column])
            );
    }
    return zMatrix;
}
</code></pre></li>
</ul>

<p>Here is the implementation of the Pseudocode:</p>

<pre><code>        do
        {
            jacob = JacobiMatrix.GetJacobi(currentPosition, currentAxisConfiguration);
            jacobiTranspose = jacob.getTransposeMatrix();

            // deltaE = (x2-x1, y2-y1, z2-z1, a2-a1, b2-b1, c2-c1)    
            deltaE = Position
                .GetDistanceVector(currentPosition, targetPosition);

            deltaThetas = jacobiTranspose.Multiply(deltaE).
                                    Scale(beta);

            for (var axis = 0; axis &lt; deltaThetas.Rows; axis++ )
                currentAxisConfiguration.joints[axis] += deltaThetas.M[axis, 0];

            currentPosition = this.CalculateDirectKinematic(currentAxisConfiguration);
        } while (Math.Abs(Position.Distance(currentPosition, targetPosition)) &gt; epsilon);
</code></pre>

<p>where $beta = 0.5$ and $epsilon = 0.0001$</p>

<h3>Problems:</h3>

<p>The transformation matrice should be fine, because it behaves good for the forward kinematic.
In my opinion the Jacobian matrix must be somehow wrong. I'm not sure if it is correct how I put the orientation data in the numerical calculation. For the analytical computation I didn't have any clue what could be wrong.
I would be thankful for any advice. An explicit example for calculating the Jacobian would also be very helpful. </p>
","inverse-kinematics manipulator"
"2761","Is it possible to record only incoming data with realterm?","<p>I am trying to test a sensor circuit I'm working on. Essentially, I am using <a href=""http://realterm.sourceforge.net/"" rel=""nofollow"">RealTerm</a> to send commands to the microcontroller and it is returning the value read by the sensor. </p>

<p>When logging to a file in RealTerm, I noticed the commands being sent were showing up as well as the data being returned. I was wondering if anyone knew a way to record only the incoming data using RealTerm, and not the outgoing commands. Any suggestions would be greatly appreciated. Unfortunately, there is no way around using RealTerm specifically because of a company policy. </p>
","serial"
"2767","NXT Segway problem. Need advice/help","<p>I'm attempting to build a segway robot using a gyrosensor and accelerometer.</p>

<p>I'm having trouble getting the robot to remain standing, for some reason, and I can't identify the problem.
Here's what I know:</p>

<p>The gyroscope API for the lejos NXT platform is here:</p>

<p><a href=""http://www.lejos.org/nxt/nxj/api/"" rel=""nofollow"">http://www.lejos.org/nxt/nxj/api/</a></p>

<p>By using timestamps and angular velocity, the project attempts to infer the angle of the robot.  The API suggests that in order to be accurate, it must be polled 100 times per second (or every 10ms on average).</p>

<p>The problem is that simply polling the gyrosensor takes 4ms.
Polling the accelerometer takes 10ms.</p>

<p>The dimensions of the robot:
Height: 28cm
wheel circumference : 13.25cm
Radius of a wheel, given the circumference:  2.1cm</p>

<p>The accelerometer is mounted on the top of the robot (at approximately 28cm from the ground, 26cm from axis of rotation)</p>

<p>In order to keep the correction amount linear (as opposed to trying to correct an arbitrary angle) , I translate the angle of the robot to a distance to travel along the ground to ""right"" the robot.  This might be a bit naive, and I'm open to suggestion here.  Basically it's just the horizontal distance calculated using a right-angle triangle with the angle of the robot at the top and hypotenuse of 28cm.</p>

<p>If that's not clear, it's essentially the horizontal distance from the top of the robot and the bottom of the robot.</p>

<p>Right now my main concern is the amount of drift the gyroscope seems to be experiencing.  Given the fact that with the NXT java software package, it's nearly impossible to poll 100 times per second, the amount of error accumulated by the gyroscope is fairly large.</p>

<p>Finally, I've implemented a PID control system.  The thing I'm not clear about with respect to this system is the integral and derivative of error must be calculated given a set of values.  Say, the last 20 error measurements recorded.</p>

<p>If the amount of past errors recorded is a variable, and the PID constants are variable, and the speed of the wheels is a variable, it seems this problem begs for some kind of automated optimization.  But how to do it?  If I set the speed to 120 RPM (roughly the max of the NXT servos) and take the past 20 errors for calculating the integral and derivative of the error, will it be possible to optimize the PID constants successfully?  Or must all 5 variables be tuned together?</p>

<p>Thanks ahead for any insight on the problem.</p>
","accelerometer gyroscope nxt"
"2768","differential drive PID controller","<p>I have a differential drive robot that works fine (good PD parameters) driving at say 1 m/s. Now, if it speeds up (to 1.2 m/s) it starts wobbling again. What would be a good strategy for a controller that is able to cope with the whole speed range of 0 - 4 m/s?</p>

<p>edit 14th of April:</p>

<p>The robot is a line follow robot but I do not see how this would be related to my question since a robot following a trajectory would have the same problem. </p>

<p>I recently talked to other developers of differential drive robots and they are facing similar issues e.g. they told me that they need to adjust PID parameters once the battery is not fully charged hence the robot drives at a different speed.</p>

<p>I do not know if you guys are into youtube, but if your are really interested in my robot this link would be helpful: <a href=""https://www.youtube.com/watch?v=vMedNPhXlEo"" rel=""nofollow"">https://www.youtube.com/watch?v=vMedNPhXlEo</a></p>

<p>PID parameters are: P 0.31, D 0.59, I 0.00</p>

<p>PID controller programmed using C:</p>

<pre><code>  // note: the inner wheel turns backwards for narrow curves
  // cte is -128..128 depending on the robots position
  // relative to a trajectory / black line 
  /** Execute the PID controller and update motor speeds */
  void PID()
  {
    int32_t steer;
    int32_t cte;
    cte = 128 - get_segment_center(0);
    // Compute PID equation
    steer = (int)(
      -P * (float)cte 
      -D * (float)(cte - diff_cte) / (float)PERIOD_MS
      -I * (float)int_cte
    );
    if (steer &lt; -5)
    {
      // turn left
      turn = -1;
      uXbot_move(MAX_SPEED + steer, MAX_SPEED);
    } 
    else if (steer &gt; 5)
    {
      // turn right
      turn = 1;
      uXbot_move(MAX_SPEED, MAX_SPEED - steer);
    }
    else
    {
      // go straight
      turn = 0;
      uXbot_move(MAX_SPEED, MAX_SPEED);
    }
    diff_cte = cte;
    int_cte += cte;
  }
</code></pre>
","pid differential-drive"
"2769","How to turn a rover 90 degrees using wheel encoders?","<p>I have a four wheel DC rover with two optical wheel encoders.  I'm executing rover turns by controlling the direction of wheel motion on either side of the rover.  To turn left, all wheels on the left rotate backwards while all right wheels rotate forward.  This allows to rover to remain relatively in the same position while turning.  The directions are reversed to do a right turn.  </p>

<p>How can I use these two sensors to execute as close to a 90 degree turn as possible without fusing additional sensor data?</p>
","sensors motion"
"2771","Point tracking from a mobile robot","<p>How can I track a fixed point $P=(x_P, y_P)$  from a moving robot?</p>

<p>Coordinates of $P$ are relative to the state/pose of the robot (x axis looks forward the robot and y axis is positive on the right of the robot).
Suppose that the initial robot state/pose is at $S_{R}=(x_R, y_R, \theta_R)$.
The next frame (namely after $\Delta t$) with the applied control $(v, \omega)$ the robot is at state $S_{R'}=(x_{R'}, y_{R'}, \theta_{R'})$.</p>

<p>Where (I set the axes as OpenCV):</p>

<p>$x_{R'} = x_R + v cos(\theta_R) \Delta t $ </p>

<p>$y_{R'} = y_R + v sin(\theta_R) \Delta t $ </p>

<p>$\theta_{R'} = \theta_{R} + \omega\Delta t$</p>

<p>The question is: which are the coordinates $(x_P', y_P')$ of the same point $P$ relative to $S_{R'}$?</p>

<p><img src=""http://i.stack.imgur.com/TgM1h.png"" alt=""enter image description here""></p>

<p>As visible in the picture, I know the transformation from the initial state to the next state of the robot and the coordinate of P in reference to the initial state</p>

<p>$$
t = \begin{pmatrix}
cos(\theta_{R'}) &amp; -sin(\theta_{R'}) &amp; x_{R'}\\
sin(\theta_{R'}) &amp; cos(\theta_{R'}) &amp; y_{R'}\\
0 &amp; 0 &amp; 1\\
\end{pmatrix}
$$</p>

<p>Please correct me if I made some mistakes!</p>

<p>Thank you, any help is appreciated.</p>
","mobile-robot kalman-filter tracks"
"2772","Quadcopter Throttle and PID mixing to Motor Speed","<p>I've been writing some quad copter software and I am not sure what the best way is to map the throttle and PID inputs to ESC power.</p>

<p>My throttle range is 0-1 and my PID outputs are 0-1. My ESC's have a range of 1060us to 1860us.</p>

<p>I have mapped the motor speeds like this:</p>

<pre><code>_motorPower[0] = map((_rcConstrainedCommands[3] + _ratePIDControllerOutputs[1] + _ratePIDControllerOutputs[2] + _ratePIDControllerOutputs[0]), 0.0, 4.0, 1060, 1860);
_motorPower[1] = map((_rcConstrainedCommands[3] + _ratePIDControllerOutputs[1] - _ratePIDControllerOutputs[2] - _ratePIDControllerOutputs[0]), 0.0, 4.0, 1060, 1860);
_motorPower[2] = map((_rcConstrainedCommands[3] - _ratePIDControllerOutputs[1] - _ratePIDControllerOutputs[2] + _ratePIDControllerOutputs[0]), 0.0, 4.0, 1060, 1860);
_motorPower[3] = map((_rcConstrainedCommands[3] - _ratePIDControllerOutputs[1] + _ratePIDControllerOutputs[2] - _ratePIDControllerOutputs[0]), 0.0, 4.0, 1060, 1860);
</code></pre>

<p>This works but if my quad is perfectly level (i.e. the PID outputs are 0) and I apply full throttle (1.0) then map this to ESC power I will only get quarter power (1260us).</p>

<p>How should I be doing this so that if my throttle is on max then I get max power? If my throttle is half (0.5) then I should get half power plus the PID values etc.</p>

<p>Can anyone help me with this?</p>

<p>Thanks
Joe</p>
","motor quadcopter pid pwm esc"
"2774","Datasheet for Taos TCS3200 GY-31","<p>Can someone please post the datasheet for the colour sensor mentioned above. All i can find is for TCS3200</p>
","sensors"
"2781","How to know what motor/ESC/propeller combination will work for a quadcopter?","<p>I am preparing for my first quadcopter build and need to know how to tell what motors/ESC's/propellers will work with each other. I also would like to know how to tell what the motors would be capable of carrying/how much thrust they have. I would like to put a camera on this copter. I cannot find anywhere a straight answer to this question.</p>

<p>The ones I currently think are the ones I want are:</p>

<p>ESC:
<a href=""https://www.hobbyking.com/hobbyking/store/__25365__Turnigy_Multistar_30_Amp_Multi_rotor_Brushless_ESC_2_4S.html"" rel=""nofollow"">https://www.hobbyking.com/hobbyking/store/__25365__Turnigy_Multistar_30_Amp_Multi_rotor_Brushless_ESC_2_4S.html</a></p>

<p>Motor: <a href=""https://www.hobbyking.com/hobbyking/store/__28112__Turnigy_D3530_14_1100KV_Brushless_Outrunner_Motor_US_Warehouse_.html"" rel=""nofollow"">https://www.hobbyking.com/hobbyking/store/__28112__Turnigy_D3530_14_1100KV_Brushless_Outrunner_Motor_US_Warehouse_.html</a></p>

<p>Propeller: 11inch</p>

<p>This copter needs to be able to carry a camera (~go pro)</p>

<p>TLDR: How does one match ESC's/Motors/Propellers, and how to tell if they can get the job done?</p>

<p>(ESC - Electronic Speed Control)</p>
","quadcopter brushless-motor esc multi-rotor"
"2782","Wire gauge for hobby robot question","<p>So, i'm making my second ever hobby robot(I'm 15) and am planning on soldering my own connectors for the battery, sensors, Arduino, etc. It will be a small mobile robot. Anyways, I was wondering what a good gauge of stranded hookup wire would be good for that purpose. Thanks!!</p>
","mobile-robot"
"2787","Position and Object Data Tracking","<p>For a class project, I'm working with a weight stack:</p>

<p><img src=""http://i.stack.imgur.com/LSgxa.png"" alt=""weight stack""></p>

<p>I'm trying to simultaneously measure:</p>

<ol>
<li>the position of a moving weight stack</li>
<li>the value of the weight based on a calibrated/preloaded position in the stack, not via load sensor. (e.g. think a stack of plate weights where the sensor knows in advance that 1 plate = 10lbs, 2 plates = 20lbs, etc.)</li>
</ol>

<p>The weight stack and the base camp chip/sensor/laser would be within two feet of the weight stack, so I don't need anything overly strong. My requirement is that it is small/unobtrusive and cost effective. I've looked into a few options, but I'm not an engineer so I'm not sure if I am on the right track.</p>

<p>How would you do this? Is there any research that I could check out?</p>
","sensors"
"2794","Crimp or Solder to LiPo Battery?","<p>So, I'm planning out my second hobby robot(I'm 15). I am planning on using a 7.4V LiPo battery and my idea is to solder on a 3-pin header to connect to the electronics. Anyways, should I solder to crimp terminals and then attach it to the pack? Or should I solder directly to the battery leads Keep in mind I have a decent background in hobby electronics and am just starting with robotics. My soldering skills are also decent!</p>

<p>Thanks!!</p>
","battery"
"2798","Why does iRobot not sell the Create in Europe?","<p>I'm trying to find a good beginners platform to use ROS with, and I came across the iRobot Create. To my surprise, they do not sell in Europe. Why is that?</p>
","mobile-robot ros irobot-create"
"2800","PID output does not reach setpoint precisely enough","<p>I'm developing/tuning a software PID for a quadcopter. For now I'm only trying to stabilise the pitch angle using the front and back motors, and I'm only looking at Kp. The motors have a control resolution: input variations need to reach a threeshold to have any effect at all.</p>

<p>The process output does reach the setpoint, but not precisely enough for my requirements. There is no steady-state error (aka droop), the hunting range is centered on the setpoint, just too wide for my requirements. Also the instability is not an oscillation, but more of a random drift which needs to be large enough before the PID attempts to correct it.</p>

<ul>
<li>With a lower Kp the output needs to diverge from the setpoint significantly before the error is big enough for the PID to attempt to correct it.</li>
<li>With a higher Kp the PID oscillates.</li>
</ul>

<p>I could not find a reasonable compromise.</p>

<p>I'm thinking about applying the cuberoot function (or similar) to the error before feeding it to the PID: that way small errors should be significant enough for the PID to attempt to correct them, and large errors would be reduced and might not trigger oscillations. I suppose someone must have been through this before: is this a good solution? Are there any better alternatives?</p>

<p>This is not a steady-state error (aka droop) or oscillation issue as far as I can tell: please don't suggest using Ki or Kd</p>

<p><strong>EDIT</strong>: I have clarified the problem description and suggested using cuberoot rather than logarithm which was a bad idea indeed.</p>

<p>Thanks,
Marc.</p>
","pid"
"2801","what is the minimum number of RC channels required to control Quad Copter?","<p>Most of the blogs/website say we need minimum of Four channels  for a quadcopter (pitch, roll, throttle, yaw): </p>

<ol>
<li>One channel for throttle</li>
<li>second channel for turning right and left.</li>
<li>third channel for pitching forward and backward.</li>
<li>fourth one for rolling left and right.</li>
</ol>

<p><img src=""http://blog.oscarliang.net/wp-content/uploads/2013/10/5-channel-transmitter-diagram.gif"" alt=""Radio Transmitter Image""></p>

<p>But looking at the RC transmitter , i see that at a time you can change a maximum of two sets  data ( left and right joy stick) .  Even if you want to send Rudder and Throttle information at the same , can it not be sent in the same packet ?</p>

<p>My understanding is  two channels should be sufficient  to control quad copter. Please provide more clarity on this.</p>
","quadcopter radio-control"
"2802","Radio-control over dozens of kilometers and mountains","<p>I am wondering what technology should I use to transmit data (enough for controlling the robot and receiving video) over dozens of kilometers and mountains ?</p>
","radio-control"
"2807","robot control law - control theory vs optimal control","<p>For a robot, say path planning in particular, what are the pros and cons of choosing classical control theory or optimal control (LQR for example) ?</p>
","control motion-planning"
"2812","Role of Neuromorphic Computing and Quantum Computing in the field of Robotics and AI","<p>I asked a similar kind of question some time ago (<a href=""http://robotics.stackexchange.com/questions/2526/neuromorphic-engineering-and-robotics"">Neuromorphic Engineering and Robotics</a>)</p>

<p>Since then, many things have come to the point of revelation. A <a href=""http://journal.frontiersin.org/Journal/10.3389/fnins.2013.00118/full"">road-map for neuromorphic computing</a> was revealed recently; It proposes the analog way of computation, to solve advanced computer vision problems. IBM and Qualcomm are also working on the similar project though on the digital side. Memristor technology is slated to come very soon.</p>

<p>The question I am asking here is <strong>How is the Robotics community working to adopt the technology?</strong> This question opens the domain for other pressing questions which have been answered cryptically since the 1980s.</p>

<p>Are neuromorphic computers good for <em>mission critical precise robots</em>, like that on Mars? Can we use neuromorphic systems on <em>Avionics systems</em>? How is neuromorphic processing going to help us solve the problems on NLP, and knowledge processing? <em>Aren't quantum computers very similar to neuromorphic computers in ideology?</em> If neuromorphic robots gain traction, will digital hardware still be required?</p>

<p>It would be really nice if someone could explain all points, because answers in various but sparsely related research papers are very cryptic.</p>
","microcontroller computer-vision machine-learning research"
"2822","Robot kit suggestions","<p>I want to develop a toy project that will allow me to move object around the house. Because I am interested in the programming of the robot and not actually build it, I would like to use some sort of programmable ""starter kits"" (like lego mindstorm) to get started. While I do not have everything figured out yet, here is a list of specs that I expect my ideal kit should have: </p>

<ul>
<li>The ability to lift object (Object as big as 4'' or 10 centimeters)</li>
<li>The ability to distinguish objects by their colors. It should have some stort of color sensors.</li>
<li>Obviously it should be able to move on a smooth surface.</li>
<li>Obstacle detection. Should have sensors for obstacle detection</li>
<li>Extra: Maybe remotely controllable.
Can someone please suggests the cheapest kit I should use for this? 
Thanks</li>
</ul>
","mobile-robot kit"
"2826","How much should I expect a Kalman filter to converge?","<p>I am learning about Kalman filters, and implementing the examples from the paper <a href=""https://www.cs.cornell.edu/Courses/cs4758/2012sp/materials/MI63slides.pdf"" rel=""nofollow"">Kalman Filter Applications - Cornell University</a>.</p>

<p>I have implemented example 2, which models a simple water tank, filling at a constant rate. We only measure the tank level, and the Kalman filter is supposed to infer the fill rate.</p>

<p><img src=""http://i.stack.imgur.com/w9SEw.png"" alt=""Kalman Filter Example, filling a water tank.""></p>

<p>According to the model, the fill rate is a constant, so I assumed that over time, the Kalman filter would converge more and more accurately (and with less and less noise) on the correct fill rate. However, the amount of noise in the fill rate never seems to reduce after the first few iterations:</p>

<p><img src=""http://i.stack.imgur.com/zNd1Q.png"" alt=""Kalman Filter example, Filling a water tank at a constant rate.""></p>

<p>This graph shows how the fill rate part of the state vector changes over the course of 1000 iterations of the simulation.</p>

<p>Adjusting the Measurement Variance Matrix seems to have very little effect on the fill rate noise.</p>

<p>Also, the Kalman gain vector and State Variance matrix seem to be constant throughout the simulation. I assumed that the State Variance would reduce as the filter became more and more confident in its state estimate.</p>

<p>Questions:
- Is this graph what I should expect to see?
- Should the Kalman Gain vector and State Variance matrix change over time in this situation?</p>
","kalman-filter"
"2828","Click button short vs long button presses Arduino","<p>I am using the <a href=""https://code.google.com/p/clickbutton/wiki/Usage"" rel=""nofollow"">clickbutton</a> library from Arduous and am having some problems implementing it. As it stand now the code just runs the servo clockwise and I'm not sure what I did wrong. Basically I want the servo if pressed for a short period of time to move according to an exponential function, and if pressed according to a long period of time to move at a regular pace.</p>

<pre><code>#include &lt;Servo.h&gt;
#include ""ClickButton.h""

Servo myservo; // create servo object to control a servo

// CONSTANTS

// PINS
const int crServo = 12; // sets pin 12 as servo
const int buttonPinCW = 2; // sets pin 2 as button; CW =&gt; clockwise =&gt; FOCUS FAR
const int buttonPinCC = 3; // sets pin 3 as button; CC =&gt; counterclockwise =&gt; FOCUS NEAR
const int ledPinB = 4; // sets pin 10 as LED
const int ledPinG = 5; // sets pin 10 as LED
const int ledPinR = 6; // sets pin 10 as LED

const int t = 1;  // slow down

// SERVO PROPERTIES
const int crSpeedDefault = 100; // is the stay still position, motor should not turn
const int crSpeedCW = 107; // turns the motor full speed clockwise
const int crSpeedCC = 87; // turns the motor full speed counter-clockwise

// SET BUTTON STATES
ClickButton buttonCW(buttonPinCW, LOW);
ClickButton buttonCC(buttonPinCC, LOW);

void setup()
{
  myservo.attach(crServo); // attaches the servo on pin 12 to the servo object
  //pinMode (buttonPinCW, INPUT); // sets button as input
  //pinMode (buttonPinCC, INPUT); // sets button as input
  pinMode (ledPinB, OUTPUT); // sets led as output
  pinMode (ledPinG, OUTPUT); // sets led as output
  pinMode (ledPinR, OUTPUT); // sets led as output
  myservo.write(crSpeedDefault); // default servo to crSpeedDefault
  startup();
}

int startup() {
  //blinker(2, ledPinB);
  //blinker(1, ledPinG);
  //blinker(1, ledPinR);
}

void blinker(int count, int pin) {
  for (int x = 0; x &lt; count; x++)
  {
    digitalWrite(pin, HIGH);
    delay(1000);
    digitalWrite(pin, LOW);
    delay(1000);
  }
}

void loop()
{
  buttonCW.Update();
  buttonCC.Update();
  int buttonCWClicks = 0;
  int buttonCCClicks = 0;
  if (buttonCW.clicks != 0) buttonCWClicks = buttonCW.clicks;
  if (buttonCC.clicks != 0) buttonCCClicks = buttonCC.clicks;
  //buttonStateCW = digitalRead(buttonPinCW);
  //buttonStateCC = digitalRead(buttonPinCC);
  // clockwise rotation
  if (buttonCW.depressed == true) {
    if (buttonCWClicks == 1) {
      digitalWrite(ledPinR, HIGH);
      float speed = crSpeedCW;
      Serial.print(""CLOCKWISE-ROTATION \n"");
      for (int i = 0; i &lt; t * 5; i++) {
        speed += ((float)crSpeedDefault - speed)/ 10;
        Serial.print(speed);
        Serial.print(""\n"");
        myservo.write((int)speed);
        delay(100);
      }
      myservo.write(crSpeedCW);
    } 
    else if (buttonCWClicks == -1) {
      digitalWrite(ledPinR, HIGH);
      myservo.write(crSpeedCW);
    }
  } 
  else if (buttonCC.depressed == true) {
    if (buttonCCClicks == 1) {
      digitalWrite(ledPinG, HIGH);
      float speed = crSpeedCC;
      Serial.print(""COUNTER-CLOCKWISE-ROTATION \n"");
      for (int i = 0; i &lt; t * 5; i++) {
        speed += ((float)crSpeedDefault - speed) / 10;
        Serial.print(speed);
        Serial.print(""\n"");
        myservo.write((int)speed);
        delay(100);
      }
      myservo.write(crSpeedCC);
    } 
    else if (buttonCCClicks == -1) {
      digitalWrite(ledPinR, HIGH);
      myservo.write(crSpeedCW);
    }
  } 
  else {
    myservo.write(crSpeedDefault);
    digitalWrite(ledPinR, LOW); 
    digitalWrite(ledPinG, LOW);     // turn the LED off by making the voltage LOW
  }
}
</code></pre>
","arduino"
"2829","Differential Drive Robot on uneven surfaces","<p>So I am building a differential drive robot and I want it to autonomously drive in a straight line on an uneven surface. I know I need a position and velocity PID. As of now, I am deciding on which sensors to buy. Should I use optical encoders, accelerometers, or something else?</p>

<p>I wanted to go with accelerometers due to the error encoders would face due to slippage, but I am not sure.</p>

<p>Some enlightenment would help!</p>
","sensors control pid differential-drive"
"2832","Servos power supply in Quadruped Robot","<p>I'm facing a problem while building my quadruped robot which is figuring out the efficient power supply needed for the 12 servos. I'm using 12 MG995 tower pro servos powered by 2 lithium batteries 2x3.7v (about 8 volts) with 2200 mA . I really don't know if that enough for the servos or something else is needed to be added(i hardly fitted the 2 batteries into the robot's body) 
any suggestions please?</p>
","power battery servomotor walking-robot"
"2834","Dead reckoning on a car-like robot with a gyro and only one encoder","<p>Recently I began to build a car-like robot and I stumbled upon dead reckoning. I use one motor for steering and one for traction. I want to be able to get the position of the robot. From what I have read 2 encoders should be used. But I am curious if you can use only one encoder on the motor shaft to get distance  and a gyro + accelerometer to get the orientation of the robot.</p>
","mobile-robot motor gyroscope encoding"
"2837","What kind of motor control can I implement if I cannot use an Encoder?","<p>Every time I see a PID control for a motor, it involves an Encoder, so the algorithm knows the real position of the motor or wheel.
But with <a href=""http://www.ebay.es/itm/331143350493?ssPageName=STRK%3aMEWNX%3aIT&amp;_trksid=p3984.m1497.l2649"" rel=""nofollow"">the robot I have</a>, I cannot use an encoder. I only have ONE optocoupler per wheel which can count how many degrees the wheel has moved. I can use it to increment a counter, but the counter ALWAYS increment (if the wheel moves forward or if the wheel moves backward).</p>

<p>The first moment I saw it as an inconvenient was when I studied the <a href=""http://brettbeauregard.com/blog/2012/01/arduino-pid-autotune-library/"" rel=""nofollow"">Arduino PID Autotune Library</a>. In the first figure, I would not see decrements on the INPUT. </p>

<p>My objective is to move a little two-wheels robot small segments driven by simple trajectories separated over time by a complete stop (straight 10 cm, stop, move right 90 degrees, stop, straight until detect obstacle...)</p>

<p>Could you suggest me some kind of ideas?
The first idea I have is to transform the PID position control to a speed control (which is more convenient for the feedback loop I have) and keep a counter of the traveled distance to inform the speed control when to stop.</p>
","arduino motor control pid"
"2841","What approaches should I consider to create rotating a turret","<p>First a bit of background, I am planning to make a highly maneuverable airship controlled by four thrust vectored propellers. I don't want to rely on a rudder and forward momentum for turns but instead be able to maneuver with direct prop thrust. I want to be able to point each prop anywhere within a half sphere/dome. So two axis, 360 degrees traversal for the forward/back/up/down and 180 degrees for the left/right. The nearest thing I can think of is a ball turret similar to <a href=""http://en.wikipedia.org/wiki/Ball_turret"" rel=""nofollow"">this ball turret</a> but instead of a gun, have a motor and propeller. The turret can rotate infinitely through 360 degrees, but the gun rotates through 90 degrees.</p>

<p>My first thought was for a servo for both axis, but they are limited in range and I would like the 360 axis to be able to rotate continuously. This would allow for the turret to rotate to the desired angle using the shortest path. </p>

<p>My question is, What do I need to be able to rotate the turret and still know what angles the turret is currently pointing?</p>
","sensors servos"
"2847","Accelerometer, gyro, and magnetometer sensor fusion in 2d","<p>I have not yet build this so this is basically a theoretical question. I am still wrestling with some C code to manage i2c communication etc. </p>

<p>When I originally said ""I have not build this"" I meant that the robot is in what could be called a ""design phase"". For the sake of my question lets assume for a moment that the whole robot consists of just one IMU sensor. It moves magically (no motors that create a lot of noise in the sensor measurements). With theoretical I mean I am interested in the mathematics and algorithms involved in solving this problem. What I call IMU sensor provides raw accelerometer, gyro, and magnetometer measurements.</p>

<p>Lets say our tiny robot travels on a snooker table (3569 mm x 1778 mm). I believe this is sufficiently small to call it 2d. Now, sensor fusion should be much easier (faster, consume less resources) than in 3d, right? </p>

<p>I would like to measure the velocity and if possible the position. 
With velocity I mean at a given point of time I need to know the current velocity of the robot moving over the snooker table. Velocity is in the range of 0 - 5 m/s.
With position I mean at a given point of time I need to know the current position of the robot on the snooker table (x, y, heading).</p>

<p>I hope this will be possible since the robot should be able to identify some landmarks and use this information to reduce position errors.</p>

<p>When I originally said ""I hope this will be possible"" I meant to express that I already am aware of the fact that real sensor data is noisy and errors accumulate quickly. Using landmarks I will / or will not be able to manage to reduce the error in the position estimates. But this is NOT part of my question.</p>

<p>I am about to improve my linear algebra knowledge. So I am confident to manage some matrix multiplications, inversions and such.</p>

<p>My question is for some ideas, references on measuring velocity and position in 2d using <a href=""http://www.pololu.com/product/2468"" rel=""nofollow"">an IMU sensor like this one</a>. </p>

<p>A little side question: I just figured that this question is probably too theoretical for robotics.SE. If you know any forum that is more focused on mathematical / algorithmic side of robotics please let me know.</p>
","localization imu sensor-fusion"
"2848","CTL port in a motor controller","<p>When I look at my <a href=""http://www.ampflow.com/motor_controller.htm"" rel=""nofollow"">160A motor controller</a>, it has a port that is called ""CTL.""
What does CTL stand for? Is that a sort of protocol like RS232?</p>
","motor"
"2849","Our quadcopter goes forward instead of hovering in place. How to correct it?","<p>I'm trying to do a quadcopter with some friends and we have a problem. It goes forward instead of hovering in place. We made a video to explain it, you can see it <a href=""https://www.youtube.com/watch?v=iH-BB3ADuus"">here</a>.</p>

<p>As you can see, the quadcopter flight and go forward when I don't touch the controller. I need to correct it to go backward and it goes forward again.</p>

<p>We use the kk2.1.5. </p>

<blockquote>
  <p>The HobbyKing KK2.1.5 Multi-Rotor controller is a flight control board for multi-rotor aircraft (Tricopters, Quadcopters, Hexcopters etc). Its purpose is to stabilize the aircraft during flight. To do this it takes the signal from the 6050MPU gyro/acc (roll, pitch and yaw) then passes the signal to the Atmega644PA IC. The Atmega644PA IC unit then processes these signals according the users selected firmware and passes control signals to the installed Electronic Speed Controllers (ESCs). These signals instruct the ESCs to make fine adjustments to the motors rotational speed which in turn stabilizes your multi-rotor craft.</p>
</blockquote>

<p>We made some test. As you can see in the video, we placed the battery backward to be sure there is no weight against. When we check values in the debug mode, all values are at 0 when nothing is pressed.</p>
","quadcopter uav multi-rotor"
"2850","Need specifications to operate this stepper motor with RPi or Arduino","<p>Here is the <a href=""http://imgur.com/a/oFeJ5"" rel=""nofollow"">disassembled stepper motor that I'm working with</a>:</p>

<p>Contains the photo of the motor, and the label that's on the bottom of the motor.</p>

<p><a href=""http://i.imgur.com/UGPG0t5.jpg"" rel=""nofollow""><img src=""http://i.imgur.com/UGPG0t5m.jpg"" alt=""disassembled stepper motor""></a>
<a href=""http://i.imgur.com/nJsyKFl.jpg"" rel=""nofollow""><img src=""http://i.imgur.com/nJsyKFlm.jpg"" alt=""disassembled stepper motor""></a></p>

<p>I need to identify this stepper motor that was retrieved from scrap for a project. Budget constraints force us to use the scrap motor. I tried to drive this using a L298 H Bridge, but I couldn't find the right bit sequences to get this running smoothly. I also tried to search for a specifications sheet in the internet with the label, unsuccessfully.</p>

<p>I'm using either an RPi or Arduino board to run this.</p>

<p>I just need a pin diagram and the specifications of the motor, if you guys have seen this type before.</p>
","arduino raspberry-pi stepper-motor"
"2852","How to get prevent twisting of cables","<p>I am planning to create a motor turret described <a href=""http://robotics.stackexchange.com/questions/2841/what-approaches-should-i-consider-to-create-rotating-a-turret"">in this question</a>. But to simplify the problem, I'm thinking of a wind turbine with a generator in the main head that can rotate freely through 360 degrees to face the wind. How would I prevent the power and sensor wires coming from the head and down the shaft from twisting? </p>
","sensors wiring"
"2854","Create set of drones to fly in patterns","<p>How would one go about making a number of drones fly in a preset pattern or formation. for example have them rotating around a point.</p>

<p>something like this.
<a href=""https://www.youtube.com/watch?v=ShGl5rQK3ew"" rel=""nofollow"">https://www.youtube.com/watch?v=ShGl5rQK3ew</a></p>
","quadcopter"
"2860","How long can a VEX pneumatic arm be?","<p>How long can a vex pneumatic piston be?</p>
","robotic-arm arm"
"2865","Simplest way to do object tracking with a 2D range finder sensor?","<p>My current class assignment is to program a robot through a course that includes two moving obstacles – other robots moving at constant speed around a region the one robot must get to. Since the other robots are moving at a constant pace alongside a predictable path, my robot can just stop at the border of the region, wait until the others pass by and then proceed. The robot can use a 2D laser range scanner to sense its surroundings.</p>

<p>Given these restrictions, what is the simplest object tracking algorithm I could use? I am thinking of something along these lines:</p>

<ol>
<li>Collect two laser readings (2D point clouds) <code>A</code> and <code>B</code> with a suitable time gap between them;</li>
<li>Apply <a href=""http://en.wikipedia.org/wiki/DBSCAN"" rel=""nofollow"">DBSCAN</a> to <code>A</code> and <code>B</code>, producing the cluster lists <code>A'</code> and <code>B'</code>; </li>
<li>Generate a list <code>P</code> of pair-wise matches of the clusters in <code>A'</code> and <code>B'</code>, maybe using the <a href=""http://en.wikipedia.org/wiki/Hungarian_algorithm"" rel=""nofollow"">Hungarian algorithm</a>;</li>
<li>Discard from <code>P</code> any pairings whose difference falls within a threshold;</li>
<li>Calculate direction and magnitude of movements from the distance between the centers of mass of each cluster pair.</li>
</ol>

<p>The reason for choosing DBSCAN and the Hungarian algorithm is that I already have both implemented and in use elsewhere; and the difference between clusters could be measured as the distance between their centers of mass.</p>

<p>Do you think this solution would work for my problem? Do you have any suggestions on better and/or simpler ways to solve it?</p>
","mobile-robot sensors algorithm rangefinder"
"2868","Differential Drive Robot Control","<p>Edited: I have a differential drive robot that needs to drive down a hall and stay in the center. I have 4 ultra sonic sensors, 2 on each side. Currently, I thought of implementing pure pursuit and having a lookahead distance etc. but I am not sure if this is the correct approach. I have all the lower level PIDs for motor control working, I just need some help in choosing a path planning algorithm? Will pure pursuit work to my needs? OR do people have any suggestions.</p>
","control pid differential-drive"
"2870","Find the right actuator to control the flow of flour","<p>I'm building a circuit to control the flow of flour. The basic idea is to open that actuator (Possibly valve?) to let a specific amount go through and then close it. The tube should have a diameter of 1cm max.</p>

<p>I wonder what is the right actuator to use? Maybe a valve is the right one? Any other solution? Also, it would be great if you pointed out some suitable actuators that I can buy online.</p>
","actuator"
"2871","SLAM without data association?","<p>I would like to build 2D EKF-SLAM in openGL. I've implemented the entire virtual environment in which there is a robot that moves in 2D and there are some landmarks(feature-based map). I have the motion and observation models. Also, I've implemented the sensors with Gaussian noise. Now, I would like to use MRPT to build SLAM. At this point, I don't want to use data association that is the robot moves and detects its pose and landmarks and discard the previous data which means I only concern with the current state vector. My question is Is it possible to build SLAM without data association? Please suggest me some articles so that I can enrich my background about only this issue. </p>
","slam ekf"
"2873","How to calibrate differential drive?","<p>I'm building a robot with differential drive. I've reached the point when I can drive it around on remote control and I'm trying to get the localization working. Now I would like to exactly measure parameters of the robot.</p>

<p>Model of the robot I'm using has two wheels spaced $b$ meters, each wheel has a distance per encoder tick $s_L$ and $s_R$ and variance standard deviation of the driven distance $\sigma_L$ and $\sigma_R$.
When moving, distances are random variables from the following distributions: $d_L \sim t_{L}s_{L}N(1, \sigma_L^2)$ and $d_R \sim t_{R}s_{R}N(1, \sigma_R^2)$. Later this model might expand a little bit.</p>

<p>What is a good way to measure the parameters?</p>

<p>I found a way to measure $b$, $d_L$ and $d_R$ (added that as an answer), but I have no idea how to measure the standard deviations.</p>

<p>The model will be used as a prediction input in MCL, so I don't need covariance matrix for localization.</p>
","localization calibration differential-drive"
"2875","Connect sensors to beaglebone/arduino in a complex robot","<p>I'm building the biggest robot I've ever done. The hardware I have so far is as follows:</p>

<ul>
<li>HCR-Platform from DFRobot as a base</li>
<li>x2 12V 146Rpm DC motor with two phase hall encoder</li>
<li>x7 Sharp 2Y0A21 IR sensors</li>
<li>x6 URM37 ultrasonic sensor</li>
<li>x4 IR ground sensor</li>
<li>Microsoft Kinect</li>
</ul>

<p>Right now I'm only using a RoMeo board (arduino 328 compatible) to drive the motors and process the PIDs for the wheels + steering and also access to all the sensors (except for the kinect). I have a BeagleBone Black running linux that is intended to be the main brain connected to the RoMeo using the RS232 port and do the processing of the Kinect + wifi access.</p>

<p>I started thinking about connecting the sensors to the Beagle board directly so I don't need to waste time sending commands to the arduino board to read the sensors and that yielded the first issue, beagle board works on 3.3V instead of 5V used on the sensors. </p>

<p>After this I thought to create a board with voltage divisors to connect the sensors there and then using a ribbon cable to connect this new board to the beaglebone. I couldn't find any 2x23 IDC male connector to create the ""interface cable"" between the two boards so the beaglebone option is out as I don't want to have tons of jumper cables all over the place.</p>

<p>This morning I thought about all this again and I researched about GPIO boards over USB and found three from Numato, only one works on TTL 5V and has 8 pins so I would need a few of them to use all the sensors so unless I design my own board this option is out too.</p>

<p>At this point I'm quite confused in terms of what's the best hardware I could use to drive this beast. Right now I think I should use a Intel NUC with linux for the Kinect, wifi and usb link to a custom made sensor board. This custom made board will work on TTL 5V, provide a power bus for the sensors and will interface all the ""low level"" sensors using USB as link. I also thought about an FPGA for the custom board but I'm not sure if this would help me or if it's worth the effort of learning how to use it.</p>

<p>What are your thoughts on this? Any idea on how this issues are solved on ""complex"" robots?</p>
","arduino mobile-robot sensors computer-vision beagle-bone"
"2876","Why linkage-based haptic devices are much more common than cable (tension)-based ones?","<p>According not only to <a href=""http://www.hitl.washington.edu/people/tfurness/courses/inde543/READINGS-03/BERKLEY/White%20Paper%20-%20Haptic%20Devices.pdf"">this link</a>, but in my humble opinion to common sense as well, cable-based haptic devices have got lower inertia and are less complicated mechanically. I also believe that controlling them is not that big deal - the inverse kinematics should be quite straightforward. Moreover, the play should be easy to compensate - if there occurs any at all, since the cables are tensioned. Cables should also be easy - ? just a guess from me - to be equipped with strain gauges or to become strain gauges themselves, allowing to enhance control capabilities of a device.</p>

<p>Where am I wrong? Why is that the links-based systems (e.g. PhaNTom or Falcon, although that latter has got cable transmission), especially with impedance control, are the only I seem to be able to buy? Is it because of cable elongation (creep)? Or too constrained workspace (esp. angles)?</p>
","sensors mechanism"
"2879","What to use as electrovalve?","<p>I want do make following installation (blowing bottle tops as music instrument):</p>

<pre><code>V - valve
B - bottle


 ___________________
|mini air compressor|
 -------------------
         |
   ______|_______
  |  |  |  |  |  |
  V  V  V  V  V  V
  |  |  |  |  |  |
  B  B  B  B  B  B
</code></pre>

<p>And I want to use and rc servo as electro valve (throttle) to control air flow for each bottle. Is there any other way to do that?</p>
","rcservo mechanism valve"
"2882","How do I add an external library to the Rock Framework?","<p>The <a href=""http://rock-robotics.org"" rel=""nofollow"">Rock</a> framework already includes a lot of software libraries. However, I would like to add an existing external library so that I can use it for my component development. What is the preferred way of doing that?</p>
","software rock"
"2885","Can you identify the construction material/system used in the pic?","<p>What's the name of the ""big meccano"" used in the photo below to construct all the cabinets and racks?</p>

<p>It appears to be an aluminium cut-to-length system of 4-way rails. I've seen it used many times and assume it has a well-known brand-name to those that know.</p>

<p><img src=""http://i.stack.imgur.com/HG4sp.jpg"" alt=""Inside Audi facility""></p>

<p>Photo taken from theverge.com and was a feature about how Audi are building a new car.</p>
","frame"
"2889","Static equilibrium for 7 dof manipulator","<p>I have a 7 dof manipulator (Kuka LBR4+) and would like to calculate the joint torques needed to keep the arm in a static equilibrium. In most books the transposed jacobian is used to map the forces applying on the end effector to the joint torques.</p>

<p>$\tau = J^T\cdot F$</p>

<p>That however doesn't take the mass of the links into account. Is there a way to calculate the needed torques for a given configuration so that, assuming an ideal case, by setting these torques the arm will be in a static equilibrium?</p>

<p>cheers</p>

<p><strong>EDIT:</strong></p>

<p>For everybody interested, i found a solution to this problem in <em>Introduction to Robotics - Third Edition</em> by John J. Craig on Page 175-176. It is done with the aid of the iterative Newton-Euler dynamics algorithm. The actual trick is, to set all velocities and accelerations to zero except for the base acceleration. The base acceleration will be $^0 \dot v_0 = G$, where G has the magnitude of the gravity vector but points in opposite direction. This is equivalent to saying that the base of the robot is accelerating upwards with 1 g and this upward acceleration causes exactly the same effect on the link as gravity would.</p>
","torque manipulator"
"2890","Filling 30mL bottles with food-grade liquid","<p>A project has been given to me at work, with no schematics or idea of where it was going.  I need to fill 5 30ml bottles at a time with a food-grade liquid.</p>

<p>Based on the parts I have, I think the design was going to use a air agitated pressure pot tank which is used for spraying paint, which would work if we weren't using food grade liquid, so right off the bat I cant use that.</p>

<p>The main parts that I can use are an Allen Bradley micrologix plc, 2 pneumatic cylinders, a couple solenoids, start and stop buttons. </p>

<p>My question is: to fill a 30ml bottle with this liquid, would a positive displacement pump with a vfd be the best way of slowing the pdp down enough to fill the 5 30ml bottle at a time?</p>

<p>I do have a little experience with this particular plc so the ladder logic is not the issue, its the figuring out the specs for the pump and the motor.  Any input would be very helpful also any links would be great. At this point im trying to determine if this is a huge waste of time and money or should I just go buy a filling machine for $3-5000.     </p>
","electronics"
"2891","Outputting a precise voltage in millivolts on Arduino Mega","<p>So I need to output a varying voltage off an <a href=""http://arduino.cc/en/Main/ArduinoBoardMega2560"" rel=""nofollow"">Arduino Mega</a> in a range of 17 to 32 millivolts, which I've attempted to do by sending a PWM signal off the board into a low-pass filter which steps down the voltage.</p>

<p>This works, but the problem is that Arduino's <a href=""http://arduino.cc/en/Reference/AnalogWrite"" rel=""nofollow"">analogWrite</a> function accepts a value of 0 - 255 to represent the duty cycle of the PWM which isn't precise enough. A value of 1 yields around 20 millivolts and a value of 2 yields around 40 millivolts. Is there some way to have a duty cycle that is more precise than the 0 - 255 range like 0 - 1023 (I think even this isn't really precise enough)? Or is there a better way to get precise voltage output?</p>

<p>The mega is running on and outputting a max voltage of 5 volts, and the low pass filter contains an 11 kiloohm resistor and a 1 microfarad capacitor.</p>
","arduino pwm"
"2894","What are some good cheap, silent, motors for mannequin robots and what kind of controller should I use?","<p>What are some good cheap, silent, motors for mannequin robots and what kind of controller should I use?</p>

<p>I'm creating mannequin robots that require 24 motors: 2 neck, 4 shoulder, 2 elbow, 4 wrist, 2 waist, 4 hip, 2 knee and 4 ankle motors. The mannequins will be bolted via a horizontal post from the lower back to a wall. This means they can run, dance, and pose. I may also do something with the horizontal beam so the robots can do side flips, and moves like the twist, jumps and crouches. They will not stand up, but be bolted as I've described to a wall. They could even spin around completely to face the wall.</p>

<p>I've tried using 6v hobby servos for the joins. These are too weak for lifting fibreglass mannequin body parts. They are too loud. They also make noise when poses are held, and they drop and smash when power is shut off. I have been using a handheld remote control for testing with only 5 channels. The robot must be programmable though. Lets say the objective is to make a mannequin dance to 'Thriller' like Michael Jackson. </p>

<p>I am open to using kinnect technology as the controller (so that a dancer can simply dance in front of my robot, who can copy and remember) but I'm also open to controllers that allow me to force the mannequin into a pose at specific time codes in the song. If necessary, I am also willing to program the poses using some kind of lighting desk type controller (such as tech crews use in rock concerts to sync everything to go with the music).</p>

<p>I have noticed that a power drill or winch is very loud whereas my fan is very quiet. I live in an apartment in which neighbours can hear footsteps from other apartments. I would not dare turn on a drill at 4am because it would wake up everyone in the whole building, but I would have no guilt in turning on a fan. I need my robot to be as quiet as a fan. The voltage does not really matter for this project. I'm happy to use up to 240v from the wall socket.</p>

<p>Please let me know which motors and controllers are best for my mannequin robots, taking cost into account. Thanks so much for any help :)</p>

<p><img src=""http://i.stack.imgur.com/1myof.jpg"" alt=""Two standard mannequins - these will be cut at certain joints to allow for mobility.""></p>
","microcontroller robotic-arm motion humanoid"
"2897","How to read data from i2c using i2cget?","<p>I'm new to embedded devices and am trying to understand how to use i2cget (or the entire I2C protocol really).</p>

<p>I'm using an accelerometer MMA8452, and the datasheet says the Slave Address is 0x1D (if my SAO=1, which I believe is referring to the I2C bus being on channel 1 on my raspberrypi v2).</p>

<p>From the command line, I enter</p>

<pre><code>sudo i2cget -y 1 0X1d
</code></pre>

<p>It returns</p>

<pre><code>0X00
</code></pre>

<p>I think that means I'm attached to the correct device.</p>

<p>So now, I'm trying to figure out how do I get actual data back from the accelerometer?</p>

<p>The i2c spec says</p>

<pre><code>i2cget [-y] i2cbus chip-address [data-address [mode]]
</code></pre>

<p>So I have tried</p>

<pre><code>sudo i2cget -y 1 0x1D 0x01
</code></pre>

<p>where 0x01 is the OUT_X_MSB. I'm not sure entirely what I'm expecting to get back, but I figured if I saw some data other than 0x00, I might be able to figure that out.</p>

<p>Am I using ic2get wrong? Is there a better way to learn and get data from i2c?</p>

<p>The datasheet for my accelerometer chip is at <a href=""http://dlnmh9ip6v2uc.cloudfront.net/datasheets/Sensors/Accelerometers/MMA8452Q.pdf"" rel=""nofollow"">http://dlnmh9ip6v2uc.cloudfront.net/datasheets/Sensors/Accelerometers/MMA8452Q.pdf</a></p>
","raspberry-pi i2c"
"2899","Salvaging a bunch of laptop battery packs","<p>I'm working on my first robot project. I previously used a 12V 6Ah sealed lead acid battery, but recently I aquired some 15 ASUS Li-Ion battery packs, each of them 14.8V and either 2200 mAh or 4400 mAh. The laptops have been discarded, and some of the battery packs seem to be dead.</p>

<p>The battery packs have an 8 pin connector. Inside, I assume there's a bunch of 18650-cells and some electronics.</p>

<p>My robot can handle 14.8 V directly.</p>

<p>How can I use these batteries? How can I charge them without the laptops? I'm a little put off by the idea of taking the 18650-cells out of the packs and rebuilding my own battery pack and charging system, but if that's what's needed I have to do it.</p>

<p>The packs are marked ASUS A41-A3 for the 2200 mAh ones, and ASUS A42-A3 for the 4400 mAh ones.</p>
","battery"
"2902","Solutions for Finding Position and Heading in a Multi-Level House","<p>I was wondering if you could reccomend possible solutions for locating a robot within a multilevel house.  What seems obvious to me is that need an altitude sensor to derive the story the robot, and a compass sensor to derive the heading.  However I was wondering what I could use to locate the robots xy position in the house.  If this requirement is unclear, imagine that I have to map a dot representing my robot position to an image of the current floor from the top.<br>
My original idea was to use GPS, however as I need submeter accuracy that would be incredibly expensive.  I also considered Monti-Carlo localization, however that requires no obstruction between sonar sensors and walls.  It is also a significant task programically.  I had an idea to place 3 wireless beacons of some sort on the vertexes of an equilateral triangle surrounding the house, then triangulate my position using distance from each beacon.  However, I have no idea how I would go about this hardware-wise.  Do any of these ideas seem viable, and if so do you have suggestions on how to implement them? Otherwise, can you reccomend an easier or cheaper alternative?  My platform is essentially an arduino hooked up to sensors and motor drivers connected to java on a laptop over serial.  Thanks.</p>
","arduino sensors localization gps"
"2904","Using pic as webserver?","<p>I want to show information about temperature over the Internet using a sensor and PIC.
How can I do using PIC?  Is there any way for the PIC 16f628A or 18F to do it?
Does anyone hame some literature about it? Schematics?</p>
","microcontroller"
"2905","Grasshopper effect on a quadcopter with kk2.1.5","<p>We have builded a quadcopter that use the flight manager kk2.1.5 with the latest firmware. When we increase the throttle, it flight. When we keep the hand on the stick we are able to maintain it but when we don't touch to the throttle, it goes up and down.</p>

<p>You can see an example on <a href=""http://youtu.be/N0BhK1tS2YQ"" rel=""nofollow"">this video</a>.</p>

<p>We have tried different values for PID but we don't know what is the best for us. We have a large quadcopter with medium propellers (may be too small). </p>

<p>Does the weight of the quadpcoter or the width of the propellers can be a factor? What can be the problem?</p>
","quadcopter multi-rotor"
"2908","Simon K firmware when the IMU outputs at 50HZ","<p>I am building a Quadcopter using the Sparkfun Razor IMU which outputs the Roll, Pitch and Yaw axes values at 50 Hz, which limits the operations of the controller(implemented on Arduino IMU), to 50 Hz mx itslef. Please tell me if flashing the ESCs(EMax 40A) with the Simon K firmware can do me any good. </p>

<p>I'll be grateful. :)</p>
","arduino quadcopter imu"
"2915","Effect of adding a Pole and Zero to PID","<p>I am confused about how adding a D (which adds a zero to the complete system) decreases the speed of the system. But when we normally add a zero to the system, it causes the system to overshoot.</p>

<p>The same goes for the I part of the PID. Normally when we add a pole to the system, it has less overshoot, but at the same time the integrator increases the overshoot!</p>

<p>How can I make sense out of this inverse relation?</p>
","control pid"
"2918","Suggestions for stepper motor controllers","<p>I am working on a project where a dslr camera will be rotated on a tripod on 2 axes. I'm definitely using nema 17 motors as those are what I have. The motors will rotate 30 degrees every 5 seconds in normal usage so speed is not a requirement. The weight of the camera is 1170g and I'm using 3d printed parts for the remainder of the mount. I tried running nema 17 stepper motors off a Adafruit Motor Shield V2 but the whole thing overheats (battery, driver, and motor). By the way, the motor will be controlled by an Arduino. I need to find another motor controller to use. I looked on ebay and things like this came up for 20 dollars which seems too good to be true. </p>

<p>My question is what motor driver should I use for this project as I have little experience with them outside of Arduino shields</p>

<p><img src=""http://i.stack.imgur.com/hPqyQ.jpg"" alt=""A motor controller""></p>
","arduino motor cameras stepper-driver"
"2919","How do i use the Nicolas ziegel approach if my system never becomes unstable?","<p>How do i use the Nicolas Ziegel approach when the root locus plot of my system never becomes marginally stable , for any gain (unless it is negative).. ??</p>

<p>How do i estimate my ultimate gain value????</p>
","control pid"
"2920","understanding the PID controller","<p>I am trying to understand the effects of P, I and D constants in a PID controller on a system.</p>

<p>As far I've understood, P and I make the system 'faster', and D makes it 'slower'(which I read in books), but I don't actually understand what makes it go 'fast' or 'slow'.</p>

<p>How an integrator causes overshoot and all things like that. It makes sense that the P part causes overshoot, since it adds a gain. But what is the integrator doing? I want some kind of mathematical understanding on how all these parameters affect the system.</p>

<p>I know how they work individually, but I'm having a hard time understanding, how it affects the system as a whole. For example, how does a Zero added to the system lead to decrease in overshoot, but when normally adding a zero to a system would create more overshoot.</p>
","pid"
"2921","Controlling digital servos","<p>Many websites say that analog servo motors work on 50Hz and digital servo motors work on 300Hz. My question is, does this difference apply only to the inner control loop of the servo or does the user of the digital servo actually provide 300Hz PWM signal? To rephrase, are all (most) servos including digital ones controlled with 50Hz PWM, or are digital ones specifically controlled with 300Hz PWM? Thanks</p>
","rcservo pwm servomotor"
"2925","Why does a ID controller not exist?","<p>Why doesn't a PID only consisting of ID exist?</p>
","control pid"
"2930","Is it important to have good PI settings when we running in self-level with kk2?","<p>I have a kk2.1.5 and I fly with the self-level on. In the kk2, there are two menus. One to set PI-settings and another with selflevel-settings. Both enable to set P-Gain and I-Gain.</p>

<p>Is it important to have good PI-settings when the self-level is on, or is setting good values in selflevel-settings sufficient?</p>
","multi-rotor"
"2931","Quad copter attitude control","<p>I have built a quad copter completely from scratch (electronics, mechanics and software). 
I am now at the point where all my sensor data looks correct and when I tilt the quad copter the correct motors increase and decrease.</p>

<p>I have been trying to tune the PIDs for a couple of days now, in rate mode it stays level and rotates at roughly the correct degrees per second when I give it a command.</p>

<p>In stability mode a lot of the time it just spins around the axis and when I did get it stable it kept rotating from upright to upside down and then maintaining an upside down flat position. I have come to the conclusion that I am either doing something completely wrong or I have some + - signs mixed around somewhere.</p>

<p>Would anyone who is knowledgeable about quad copter control code be able to take a look at what I have done and how it works as I'm really struggling to work out what needs to change and what I should try next.</p>

<p>My flight control code is posted below, the other relevant classes are hardware.h and main.cpp</p>

<pre><code>#include ""mbed.h""
#include ""rtos.h""
#include ""hardware.h""


//Declarations
float Constrain(const float in, const float min, const float max);
float map(float x, float in_min, float in_max, float out_min, float out_max);
void GetAttitude();
void Task500Hz(void const *n);
void Task10Hz();


//Variables
float _gyroRate[3] ={}; // Yaw, Pitch, Roll
float _ypr[3] = {0,0,0}; // Yaw, pitch, roll
float _yawTarget = 0;
int _notFlying = 0; 
float _altitude = 0;
int _10HzIterator = 0;
float _ratePIDControllerOutputs[3] = {0,0,0}; //Yaw, pitch, roll
float _stabPIDControllerOutputs[3] = {0,0,0}; //Yaw, pitch, roll
float _motorPower [4] = {0,0,0,0};




//Timers
RtosTimer *_updateTimer;


// A thread to monitor the serial ports
void FlightControllerThread(void const *args) 
{  
    //Update Timer
    _updateTimer = new RtosTimer(Task500Hz, osTimerPeriodic, (void *)0);
    int updateTime = (1.0 / UPDATE_FREQUENCY) * 1000;
    _updateTimer-&gt;start(updateTime);

    // Wait here forever
    Thread::wait(osWaitForever);
}


//Constrains value to between min and max
float Constrain(const float in, const float min, const float max)
{
    float out = in;
    out = out &gt; max ? max : out;
    out = out &lt; min ? min : out;
    return out;
}


//Maps input to output
float map(float x, float in_min, float in_max, float out_min, float out_max)
{
    return (x - in_min) * (out_max - out_min) / (in_max - in_min) + out_min;
}


//Zeros values
void GetAttitude()
{
    //Take off zero values to account for any angle inbetween the PCB level and ground
    _ypr[1] = _ypr[1] - _zeroValues[1];
    _ypr[2] = _ypr[2] - _zeroValues[2];

    //Swap pitch and roll because IMU is mounted at a right angle to the board
            //Gyro data does need swapping - done within freeIMU class
    float pitch = _ypr[2];
    float roll = _ypr[1];
    _ypr[1] = pitch;
    _ypr[2] = roll;
}


void Task500Hz(void const *n)
{
    _10HzIterator++;
    if(_10HzIterator % 50 == 0)
    {
        Task10Hz();
    }


    //Get IMU data
    _freeIMU.getYawPitchRoll(_ypr);
    _freeIMU.getRate(_gyroRate);
    GetAttitude();

    //Rate mode
    if(_rate == true &amp;&amp; _stab == false)
    {
        //Update rate PID process value with gyro rate
        _yawRatePIDController-&gt;setProcessValue(_gyroRate[0]);
        _pitchRatePIDController-&gt;setProcessValue(_gyroRate[1]);
        _rollRatePIDController-&gt;setProcessValue(_gyroRate[2]);

        //Update rate PID set point with desired rate from RC
        _yawRatePIDController-&gt;setSetPoint(_rcMappedCommands[0]);
        _pitchRatePIDController-&gt;setSetPoint(_rcMappedCommands[1]);
        _rollRatePIDController-&gt;setSetPoint(_rcMappedCommands[2]);

        //Compute rate PID outputs
        _ratePIDControllerOutputs[0] = _yawRatePIDController-&gt;compute();
        _ratePIDControllerOutputs[1] = _pitchRatePIDController-&gt;compute();
        _ratePIDControllerOutputs[2] = _rollRatePIDController-&gt;compute();
    }
    //Stability mode
    else
    {
        //Update stab PID process value with ypr
        _yawStabPIDController-&gt;setProcessValue(_ypr[0]);
        _pitchStabPIDController-&gt;setProcessValue(_ypr[1]);
        _rollStabPIDController-&gt;setProcessValue(_ypr[2]);

        //Update stab PID set point with desired angle from RC
        _yawStabPIDController-&gt;setSetPoint(_yawTarget);
        _pitchStabPIDController-&gt;setSetPoint(_rcMappedCommands[1]);
        _rollStabPIDController-&gt;setSetPoint(_rcMappedCommands[2]);

        //Compute stab PID outputs
        _stabPIDControllerOutputs[0] = _yawStabPIDController-&gt;compute();
        _stabPIDControllerOutputs[1] = _pitchStabPIDController-&gt;compute();
        _stabPIDControllerOutputs[2] = _rollStabPIDController-&gt;compute();

        //If pilot commanding yaw
        if(abs(_rcMappedCommands[0]) &gt; 0)
        {  
            _stabPIDControllerOutputs[0] = _rcMappedCommands[0];  //Feed to rate PID (overwriting stab PID output)
            _yawTarget = _ypr[0];
        }

        //Update rate PID process value with gyro rate
        _yawRatePIDController-&gt;setProcessValue(_gyroRate[0]);
        _pitchRatePIDController-&gt;setProcessValue(_gyroRate[1]);
        _rollRatePIDController-&gt;setProcessValue(_gyroRate[2]);

        //Update rate PID set point with desired rate from stab PID
        _yawRatePIDController-&gt;setSetPoint(_stabPIDControllerOutputs[0]);
        _pitchRatePIDController-&gt;setSetPoint(_stabPIDControllerOutputs[1]);
        _rollRatePIDController-&gt;setSetPoint(_stabPIDControllerOutputs[2]);

        //Compute rate PID outputs
        _ratePIDControllerOutputs[0] = _yawRatePIDController-&gt;compute();
        _ratePIDControllerOutputs[1] = _pitchRatePIDController-&gt;compute();
        _ratePIDControllerOutputs[2] = _rollRatePIDController-&gt;compute();
    }

    //Testing
    _ratePIDControllerOutputs[0] = 0; // yaw
    //_ratePIDControllerOutputs[1] = 0; // pitch
    _ratePIDControllerOutputs[2] = 0; // roll


    //Calculate motor power if flying
    if(_rcMappedCommands[3] &gt; 0.1 &amp;&amp; _armed == true)
    {
        //Constrain motor power to 1, this means at max throttle there is no overhead for stabilising
        _motorPower[0] = Constrain((_rcMappedCommands[3] + _ratePIDControllerOutputs[1] - _ratePIDControllerOutputs[2] + _ratePIDControllerOutputs[0]), 0.0, 1.0);
        _motorPower[1] = Constrain((_rcMappedCommands[3] + _ratePIDControllerOutputs[1] + _ratePIDControllerOutputs[2] - _ratePIDControllerOutputs[0]), 0.0, 1.0);
        _motorPower[2] = Constrain((_rcMappedCommands[3] - _ratePIDControllerOutputs[1] + _ratePIDControllerOutputs[2] + _ratePIDControllerOutputs[0]), 0.0, 1.0);
        _motorPower[3] = Constrain((_rcMappedCommands[3] - _ratePIDControllerOutputs[1] - _ratePIDControllerOutputs[2] - _ratePIDControllerOutputs[0]), 0.0, 1.0);


        //Map 0-1 value to actual pwm pulsewidth 1060 - 1860
        _motorPower[0] = map(_motorPower[0], 0.0, 1.0, MOTORS_MIN, 1500); //Reduced to 1500 to limit power for testing
        _motorPower[1] = map(_motorPower[1], 0.0, 1.0, MOTORS_MIN, 1500);
        _motorPower[2] = map(_motorPower[2], 0.0, 1.0, MOTORS_MIN, 1500);
        _motorPower[3] = map(_motorPower[3], 0.0, 1.0, MOTORS_MIN, 1500);
    }


    //Not flying
    else if(_armed == true)
    {
        _yawTarget = _ypr[0];

        //Set motors to armed state
        _motorPower[0] = MOTORS_ARMED;
        _motorPower[1] = MOTORS_ARMED;
        _motorPower[2] = MOTORS_ARMED;
        _motorPower[3] = MOTORS_ARMED;

        _notFlying ++;
        if(_notFlying &gt; 500) //Not flying for 1 second
        {
            //Reset iteratior
            _notFlying = 0;

            //Reset I
            _yawRatePIDController-&gt;reset();
            _pitchRatePIDController-&gt;reset();
            _rollRatePIDController-&gt;reset();
            _yawStabPIDController-&gt;reset();
            _pitchStabPIDController-&gt;reset();
            _rollStabPIDController-&gt;reset();
        }
    } 
    else
    {
        //Disable Motors
        _motorPower[0] = MOTORS_OFF;
        _motorPower[1] = MOTORS_OFF;
        _motorPower[2] = MOTORS_OFF;
        _motorPower[3] = MOTORS_OFF;
    }

    //Set motor power
    _motor1.pulsewidth_us(_motorPower[0]);
    _motor2.pulsewidth_us(_motorPower[1]);
    _motor3.pulsewidth_us(_motorPower[2]);
    _motor4.pulsewidth_us(_motorPower[3]);
}


//Print data
void Task10Hz()
{
    int batt = 0;
    _wirelessSerial.printf(""&lt;%1.6f:%1.6f:%1.6f:%1.6f:%1.6f:%1.6f:%1.6f:%d:%1.6f:%1.6f:%1.6f:%1.6f:%1.6f:%1.6f:%d:%d:%d:%d:%1.6f:%1.6f:%1.6f:%1.2f:%1.2f:%1.2f:%1.2f:%1.8f:%1.8f:%1.8f:%1.8f:%1.8f:%1.8f:%1.8f:%1.8f:%1.8f:%1.8f:%1.8f:%1.8f:%1.8f:%1.8f:%1.8f:%1.8f:%1.8f:%1.8f&gt;"",
    _motorPower[0], _motorPower[1], _motorPower[2], _motorPower[3], _ypr[0], _ypr[1], _ypr[2], batt, _ratePIDControllerOutputs[0], _ratePIDControllerOutputs[1], _ratePIDControllerOutputs[2], _stabPIDControllerOutputs[0], _stabPIDControllerOutputs[1], _stabPIDControllerOutputs[2], _armed, _initialised, _rate, _stab, _gyroRate[0], _gyroRate[1], _gyroRate[2], _rcMappedCommands[0], _rcMappedCommands[1], _rcMappedCommands[2], _rcMappedCommands[3], _yawRatePIDControllerP, _yawRatePIDControllerI, _yawRatePIDControllerD, _pitchRatePIDControllerP, _pitchRatePIDControllerI, _pitchRatePIDControllerD, _rollRatePIDControllerP, _rollRatePIDControllerI, _rollRatePIDControllerD, _yawStabPIDControllerP, _yawStabPIDControllerI, _yawStabPIDControllerD, _pitchStabPIDControllerP, _pitchStabPIDControllerI, _pitchStabPIDControllerD, _rollStabPIDControllerP, _rollStabPIDControllerI, _rollStabPIDControllerD);    
}
</code></pre>

<p>The whole program can be seen on my mBed page at <a href=""http://mbed.org/users/joe4465/code/QuadMK6/"" rel=""nofollow"">http://mbed.org/users/joe4465/code/QuadMK6/</a></p>

<p>If you need any more info or something explaining let me know.</p>

<p>If anyone can point me in the right direction or has any idea of what I should try next it would be very much appreciated.</p>

<p>Thanks
Joe</p>
","quadcopter pid imu"
"2933","Does RoboRIO support Java 8?","<p>My FRC team will recently upgrade from <a href=""http://en.wikipedia.org/wiki/CompactRIO"" rel=""nofollow""> CompactRIO </a> to RoboRIO.</p>

<p>CompactRIO only supports up to Java 1.4.
What version of Java does RoboRIO?</p>
","microcontroller"
"2935","How PID affect the Root locus of a close loop transfer function","<p>I am trying to understand how a PID controller moves the poles and zeros of an transfer function. 
I've been playing a bit with it, but aren't able to see some kind of connection. </p>

<p>I mean that P and I rises the Overshoot which would mean that the damping ratio gets smaller, thereby should  away from the real axis. </p>

<p>and D should should do the opposite, but it doesn't seem to be true with the examples i've used.. am i doing something wrong??</p>

<p>Well i kind of just want a general knowlegde of how it affect second order systems. </p>
","control"
"2936","Best strategy for area scanning using little sensing bots","<p>I'm currently working on a school project about simulating robots scanning an area, but it has been a struggle to find what strategy the robots should use.  Here are the details:</p>

<p>I am given a certain amount of robots, each with a sensing range of $r$.  They spawn one after another.  Their task to scan a rectangular area.  They can only communicate with each other when they are within communication range. </p>

<p>I am looking for the best strategy, (i.e. time efficient solution) for this. 
Any reply or clue to the strategy will be appreciated. </p>
","mobile-robot sensors coverage"
"2937","how does poles and zeroes affect the step response of an transfer function?","<p>I have this close loop transfer function:
<a href=""http://snag.gy/Vh1SM.jpg"" rel=""nofollow""><img src=""http://i.snag.gy/Vh1SM.jpg"" alt=""close loop transfer""></a></p>

<p>It overshoots, but why? The poles are placed such that that the damping = 1, so why the overshoot?</p>
","control"
"2939","What does this ""inverse"" peak mean? (step function)","<p>I identified my system and now I am trying to tune PI regulator since I think I do not need D.</p>

<p>I came across this graph while Matlabing and I do not know what does it mean.</p>

<p>I am using pidtune() to get my P and I values. (I think computation is all correct, I made model in simulink to confirm). Anyway see my picture and arrow is pointing at what I do not understand. Why is my system going below zero first?</p>

<p>It is supposed to be water flow regulator.</p>

<p><img src=""http://i.stack.imgur.com/36usV.png"" alt=""enter image description here""></p>

<p>Transfer function:
$$ \frac{-0.311s + 0.05548}{s^2 + 0.06882s + 0.0007626}$$</p>

<p>Continuous-time PI controller in parallel form:
$$K_p + K_i * \frac{1}{s}$$</p>

<p>With $K_p = 0.256$, $K_i = 0.000342$</p>
","pid"
"2940","LQR design with low effort","<p>I am trying to implement a controller for an inverted pendulum using LQR (with MATLAB command lqr(A,B,Q,R)). The problem is that the motors are relatively weak, so I tried to increase R, but simulations show that the effort is still very high. How can I reduce the effort?</p>
","control design"
"2945","How does a robot efficiently store a map it makes?","<p>From what I understand, you can create a map using sensors and storing values in an array.  For example, the array can have 0's for places not visited, and 1's for places visited, or 0's for open spaces and 1's for occupied spaces, etc.  </p>

<p>The map I'm thinking of making will have about 1000 x 2000 members.  This means I need an array of 2 million members.  That seems like a lot.  If I have eight attributes about each member (like temperature, light level, sound level, etc.), then I have 16 million pieces of information to capture.  </p>

<p>Is this correct, or is there a better way to create a map for a robot using software?</p>
","software mapping"
"2948","PID tuning method based on Pole placement","<p>Is it possible to determine PID parameter using pole placement. 
I mean by solving the ch. eq. of close loop transfer functions  which consists of either P,PI,PD or PID controllers??</p>

<p>Because i've tried it, an eventhough i am getting my poles at the locations I want the systems does not act as I assumed.</p>

<p>an example. I want my system to be overdamped and to have settling time less than 1 sec. which means that i want my poles to lie on the real axis, and to be less than -4. </p>

<p>$$G(s) =\frac{10.95 s + 0.9574}{s^2 + 0.09149 s +  6.263*10^{-6}}$$</p>

<p>With P = 0.1, I= 0.617746, d =  0.0147173
I get a close loop system which is 
$$G_cl(s) =  \frac{0.1612 s^4 + 1.109 s^3 + 6.86 s^2 + 0.5914 s}{  0.1612 s^4 + 2.109 s^3 + 6.952 s^2 + 0.5914s}$$</p>

<p>But looking at it's step response I see it creates overshoot, which i cannot justify due to an step input...</p>
","control pid tuning"
"2949","Actuation of three reciprocating blades via conversion of rotary to linear","<p>Sorry if this is out of robotics, but this seemed the closest.
A motor has to be used to rotate three cranks. They are in the same plane, but each should not run at the same time.</p>

<p>Only one that occupies a central position, corresponding to the motor, should rotate. The discs are mounted in rectangular frame that can be slid to bring the required disc to the central position.</p>

<p>Can anyone suggest a method to achieve this? (Basically coupling and uncoupling the motor shaft from whatever disc is in the centre.)</p>
","motor mechanism"
"2961","McKibben artificial muscles and the 400:1 ratio","<p>I was recently reading that those artificial muscles had the highest power/weight ratio while electric motors only have a ratio of less than 100:1. As electrical engineer I have never worked with pneumatics before and do not have a big idea about air pumps.
My question is, how much will this ratio change if we consider an autonomous system. 
On one side, the McKibben actuators + air pumps and valves + energy source   and on the other side, electric motors + energy source. As an example, let suppose that somebody found a way to model and control the artificial muscles as good as the electric motors and make two autonomous walking leg models, which one would have a higher resulting power/weight ratio ?</p>
","control power actuator torque battery"
"2963","How do I learn about Arduino/raspberry-pi based robotics on my own?","<p>I am interested in learning how to build a dynamic quadcopter, and hope to be fairly proficient with Arduino/raspberry-pi. What resources and/or practices might you recommend? </p>
","arduino raspberry-pi beginner"
"2964","Quadcopter PID output","<p>I'm trying to develop a control system to a quadcopter and one of my options is to use a PID controller (which I think is the most used method).</p>

<p>From what I've read, the commom strategy is to run a PID algorithm to each axis. My question is: how the PID output is converted to PWM signals?</p>

<p>I'm asking that because the three axes and the four rotors depend on each other. I mean, if I slow down a couple of rotors which are opposite to each other then the quadcopter will move down in a vertical axis. But if I speed one of them and slow down the other, the quadcopter will rotate in a different axis.</p>

<p>So we cannot isolate each axis and associate them with a single rotor or a pair of those. And because of that, how can a PID output (which is associated to an axis) can be converted to PWM signals to the rotors?</p>

<p>Is that a MIMO system?</p>
","quadcopter pid"
"2966","What parts do I need for a strong robotic arm?","<p><a href=""https://www.youtube.com/watch?v=MPhhYlIq294"" rel=""nofollow"">https://www.youtube.com/watch?v=MPhhYlIq294</a></p>

<p>I have found this robotic arm on the internet, and I was wondering if someone can tell me what parts I need to build it or similar. The video says it can lift a small cat.</p>

<p>I need to be able to program it, so I'll need a controller.</p>

<p>Please recommend specific motor, controller links that you recommend. Thanks :)</p>
","robotic-arm"
"2968","proper tuning method of an cascade controller?","<p>Would a control system consisting of 2 PID controller one plant  would be considered as an cascade controller??</p>

<p>And how come would a proper tuning method be?</p>

<p>As far i've googled it seems to me that only best method is to manually do it, one by one. </p>

<p>this is how my system looks like 
<a href=""http://snag.gy/rJH2J.jpg"" rel=""nofollow"">http://snag.gy/rJH2J.jpg</a></p>
","control"
"2970","How can I dynamically control the amount of torque necessary to rotate something for a test rig?","<p>I'm looking to build a test rig for a robot that rotates a 1"" diameter pipe 180º (roll not yaw or pitch). I am currently testing a motor's performance when subjected to various kinds of PWM (high duty cycle, low duty cycle, etc). I would like to characterize how this performs under various loads.</p>

<p>Is there a simple mechanical mechanism I can attach to a fixture and insert into or around the pipe that lets me control how easy or difficult it is to rotate? </p>

<p>I am thinking of something like a drill bit chuck that fits inside the pipe and expands or a circular clutch that clamps down around pipe to add resistance when one tightens a thumbscrew. I would like to go from no resistance to full stop for a 4N•m motor. I would like to be able to test how 'sticky' the pipe is using a torque wrench.</p>

<p>I imagine this would be very simple but I can't think of something that would do this!</p>
","motor torque"
"2972","Transfer function of DC motor being unstable due to a controller?","<p>I am having a hard time grasping the concept of a DC motor  with load being unstable, and stable due to a controller </p>

<p>My confusions appears as I am trying to design a controller for one using Z-N method, and the transfer function i've identified using matlab tells me that my DC-motor always will be stable. </p>

<p>Which makes sense, since feeding it constant voltages, will lead to a constant veloicty. </p>

<p>But to use the z-n approach the system has be able to become unstable, and since this isn't possible i am getting confused if a motor are able to become motor for which i am to design a controller for. </p>

<p>The question in simplicity, 
how come can a controller make a motor, (if the motor itself cannot (due to pole zero plot)) unstable.  </p>
","motor control"
"2976","Selecting Precise High Torque Motor and Motor Controller for use with Arduino Mega 2560 R3","<p>My current project calls for 6 motors (50 mechanical watts), and an assortment of sensors.  I selected the Arduino Mega 2560 R3 to use as the basis for the project.  I am having trouble determining the best motors and motor controllers for the project.<br>
The motors need to be very precise and very controllable, meaning I need to make very small changes to the speed of the motor several times a second.  The motors will follow a position vs. time curve.  I broke up the position vs time graph into 100 hundred sections.  I found the acceleration(degrees per second^2) of each section, and I want to put it in to the array and format it as such: </p>

<p>float arr[] = {position 1, acceleration between position 1 and 2, position 2, acceleration between position 2 and 3, position 3, etc...}</p>

<p>Here is an idea of how I plan to program the motor:</p>

<pre><code>           for (int i=0; i&lt;100-2; i+=2){  
              //I will add a function that sets the acceleration of the motor
              motor1.setAcc(arr[i+1]);
              //I will create a function that gets the motor position from a potentiometer or rotary encoder
              while (motor1.getPos()&lt;arr[i+2] || motor1.getPos()&gt;arr[i+2]){}
              }
</code></pre>

<p>I am planning to run the motors in a PID loop so it will not be as cut and dry as the loop above.  </p>

<p>I saw the spark fun AutoDriver stepper motor controller, I thought the accompanying library would save me a lot of work. I very quickly noticed that it had some major limitations.  First it did not allow me to very the acceleration while the motor was running, for example: if I call:</p>

<pre><code>             kneeMotorDriver.run(FWD, 2000);
</code></pre>

<p>I have to call softStop() or hardStop() otherwise the AutoDriver will keep telling me its busy, my interpretation (please correct me if I am wrong) is I cannot change any of the drive commands (i.e.) I cannot vary the acceleration.  The AutoDriver does have other functions but they require a target step, I do not want to give a target step because I don't want to be limited by the steps (1.8 degrees on the stepper I have) I may want to stop at 34.6 degrees, but with the Auto Driver I cannot.  </p>

<p>The auto driver is also limited by its 3A per phase current limit.</p>

<p>Just to summarize:
I need a Mega to control 6 motors with a minimum output of 50 mechanical watts, I need very precise control, I am planning on using a potentiometer or rotary encoder.  The motors will run in a PID loop with several sensors.  I need to be able to vary the speed several times a second.</p>

<p>All 6 motors will be executing different motions and they need to be able to run very consistent over time, (i.e) when motor 1 is at 2 degrees motor 2 must be at 18 degrees.</p>

<p>I am still open to the idea of a stepper motor I can live with having to stop on a step but I would really prefer not too.</p>

<p>Any and all help would be greatly appreciated.</p>

<p>Thanks,
Joel </p>
","motor"
"2980","Definition of (or determine whether something is) a robust controller?","<p>I am bit uncertain how I should interpret the definition of an robust controller. </p>

<p>As far I've understood, the closed loop system including the controller has to have a high gain for frequencies where disturbance appears, and decay at frequencies higher than the work area, or noise.  Both of these can be determined using a bode plot, thereby determining the robustness of my closed-loop system. </p>
","control"
"2981","Acrylic/plastic cutting services","<p>I want to cut acrylic pieces that will be assembled into the body of a robot. What are some recommendations for acrylic/plastic cutting services? Does laser cutting produce the best results?</p>
","manufacturing"
"2982","Performance/memory considerations for pathfinding lookup tables on RobotC for a small set of paths","<p>I'm writing a C code generator geared toward RobotC and complex tasks for an FTC team, and was wondering about some performance and storage concerns:</p>

<ol>
<li>How much memory is available for my program's data? It'll be mostly pre-defined lookup tables, generally in the form of multidimensional arrays.</li>
<li>How much NXT memory is available for my program itself? As in, roughly how much code can I expect to fit into a single RobotC compiled program?</li>
<li>How quickly do programs execute, generally? Looking at disassembly most of my generated lines correspond to 2-4 opcodes.</li>
</ol>

<p>Based on these, I'm trying to make a decision of precomputation vs runtime pathfinding.</p>

<p>I'm using NXT/Tetrix. My major interest at this point with these questions is for pathfinding. I plan to have a 64x64 grid and be running Djisktra's A* algorithm with a heuristic function that assigns a penalty to turns and is as close to consistent as possible (not sure if consistency/monotonicity is doable with the turn penalty).</p>

<p>Roughly 8 paths would be cached if I decide to use the pre-cached lookup tables.</p>

<p>Instead of a set, I'll probably use a boolean array for the set of nodes visited. The fact that I'm working with a square layout will allow me to use a 2D array for the map needed to reconstruct the path.</p>

<p>I'd love some feedback and answers to my question if anyone has any. Thanks!</p>
","nxt robotc"
"2983","Securing a disc/wheel to a shaft","<p>I am in a situation where I need to secure a 20"" wheel made of 3/4"" thick MDF (of my own making) onto a 1-1/4"" precision-ground steel shaft, and this joint needs to be strong enough to convey a rather large amount of torque, about 575-600 in-lbs, effectively a 5hp motor driving the shaft at 500-600 rpm.</p>

<p>I don't have a milling machine or any metalworking tools, so my preferred option of 'milling flats onto the shaft' is out. My second option was to attempt to increase the surface area of the wheel's bore and then use an appropriate adhesive such as JB Weld.</p>

<p>Is JB weld a suitable solution to this problem or is there a better method of fastening that doesn't involve modifying the steel shaft at all?</p>
","wheel"
"2987","Can I reuse the hall sensors in a brushless motor as an encoder?","<p>I have upgraded the motors in my robotic arm to sensored, brushless RC car motors. The hope was to reuse the Hall sensors to double as a rotary encoder, by tapping 2 Hall sensors and treating the 2 bits as a quadrature signal (a crude quadrature since 2 of the 4 states will be longer than the other 2).</p>

<p>This works when none of the motor phases are powered and I just rotate the motor manually. But once the stator coils are energized, the encoder no longer counts correctly: When running at low power, the counting is correct, but when running under high power, the count is monotonic (only increases or decreases) no matter if I run in reverse or forward.</p>

<p>I'm almost certain this is because of the stator coils overpowering the permanent magnets on the rotors. So is there still a way to use the Hall sensors as an encoder?</p>

<p>Sorry if this is an obvious question. I'd love to research this problem more if I had more time.</p>

<p><strong>Update:</strong>
I've measured the wave forms with my DSO quad and see the expected 120 degree separated signals (the measurement for phase C gets more inaccurate over time because I only had 2 probes, so I measured phases A &amp; B first, then A &amp; C, and then merged them.</p>

<p>When ESC speed is 0.1:
<img src=""http://i.stack.imgur.com/tXBod.png"" alt=""speed = 0.1""></p>

<p>When ESC speed is 0.3:
<img src=""http://i.stack.imgur.com/4D4Na.png"" alt=""speed = 0.3""></p>

<p>Previously, I was using a hardware quadrature counter (EQEP module on a BeagleBone). At speed=0.3, this was counting backwards no matter if I do forward or reverse!</p>

<p>I then implemented quadrature counting on an LPC1114FN28 uController. The result was still bad at high speeds (count didn't change at all). The logic was:</p>

<pre><code>void HandleGPIOInterrupt()
{
  const uint8_t allowableTransitions[4][2] = {1, 2, 3, 0, 0, 3, 2, 1};
  static int prevState = -1;
  int state = phaseA | (phaseB * 2)
  if (prevState != -1)
  {
    if (allowableTransitions[prevState][0] == state)
    {
       ++rotations;
    }
    else if (allowableTransitions[prevState][1] == state)
    {
      --rotations;
    }
  }
  prevState = state;
}    
</code></pre>

<p>Then I got the idea to change the code to not update prevState until an expected state happens (to deal with glitches):</p>

<pre><code>  int state = phaseA | (phaseB * 2)
  if (prevState != -1)
  {
    if (allowableTransitions[prevState][0] == state)
    {
       ++rotations;
       prevState = state;
    }
    else if (allowableTransitions[prevState][1] == state)
    {
      --rotations;
      prevState = state;
    }
    else
    {
        // assume transition was a glitch
    }
  }
  else
    prevState = state;
</code></pre>

<p>Now the counting finally is correct in both directions, even at speeds higher than 0.3!
But are there really glitches causing this? I don't see any in the waveforms?</p>
","brushless-motor encoding hall-sensor"
"2988","Voice control solution for Linux robot?","<p>I wanted to present a voice-controlled robot in my lab's upcoming demo contest. My robot is essentially a x86 Ubuntu notebook resting on top of a two-wheeled platform, so in principle any solution available on Linux would do.</p>

<p>I looked into <a href=""http://julius.sourceforge.jp/en_index.php"" rel=""nofollow"">Julius</a>, but it seems the only comprehensive acoustic model available for it is aimed at the Japanese language – which coincidentally I <em>can</em> speak a little, but apparently not clearly enough to produce anything beyond garbled text. I also tried the <a href=""https://gist.github.com/alotaiba/1730160"" rel=""nofollow"">Google Speech API</a>, which has a decent selection of languages and worked very well, but requires Internet access. Finally there is <a href=""http://cmusphinx.sourceforge.net/"" rel=""nofollow"">CMU Sphinx</a>, which I haven't yet tested, but I'm afraid might have a problem with my accent (I'm a nativa Brazilian Portuguese speaker, and apparently there is no such acoustic model available for it).</p>

<p>Is that all there is to it? Have I missed any additional options? As you may have guessed, my main requirement is support for my native language (Brazilian Portuguese), or failing that, good performance for English spoken with foreign accents. A C++ API is highly desirable, but I can do with a shell interface.</p>
","mobile-robot linux speech-processing digital-audio"
"2990","innovation step ekf localization?","<p>Let's say we have a bunch of observations $z^{i}$ from sensor and we have a map in which we can get the predicted measurements $\hat{z}^{i}$ for landmarks. In EKF localization in correction step, should we compare each observation $z^{i}$ with the entire predicted measurement $\hat{z}^{i}$?, so in this case we have two loops? Or we just compare each observation with each predicted measurement?, so in this case we have one loop. I assume the sensor can give all observations for all landmarks every scan.  The following picture depicts the scenario. Now every time I execute the EKF-Localization I get $z^{i} = \{ z^{1}, z^{2}, z^{3}, z^{4}\}$ and I have $m$, so I can get $\hat{z}^{i} = \{ \hat{z}^{1}, \hat{z}^{2}, \hat{z}^{3}, \hat{z}^{4}\}$. To get the innovation step, this is what I did 
$$
Z^{1} = z^{1} - \hat{z}^{1} \\
Z^{2} = z^{2} - \hat{z}^{2} \\
Z^{3} = z^{3} - \hat{z}^{3} \\
Z^{4} = z^{4} - \hat{z}^{4} \\
$$
where $Z$ is the innovation. For each iteration I get four innovations. Is this correct? I'm using EKF-Localization  in this book <a href=""http://rads.stackoverflow.com/amzn/click/0262201623"">Probabilistic Robotics</a> page 204. </p>

<p><img src=""http://i.stack.imgur.com/Ht7Yw.jpg"" alt=""""> </p>
","sensors localization ekf"
"2998","Why does microstepping give less torque?","<p>I am experimenting with using a stepper motor for a robotics project. I'd like to use microstepping to give a better resolution and smoother movement, but I have noticed that the finer the microsteps, the lower the torque from the motor. Why is this?</p>

<p>For reference I'm using the Allegro Micro <a href=""http://www.allegromicro.com/~/media/Files/Datasheets/A4988-Datasheet.ashx"" rel=""nofollow"">A4988</a> motor driver, and a bipolar stepper motor.</p>
","stepper-motor torque stepper-driver"
"3006","How can I achieve long distance, high quality 3D Scans on a mobile robot?","<p>Am going to be competing in Robocup Rescue in Thailand next year - I was too busy to pull off a campaign for Brazil this year :(</p>

<p>Will be using CUDA powered GPUs, Kinect/Xtions, and ROS as the primary navigation system, but I need a sensor for long range scanning - at least 25 meters. It is probably overkill for the competition, but I want it to be used in other real world applications. It will need to be very robust, fairly light, high resolution, and proven. The cheaper the better, but high quality is a must.</p>

<p>Have read this question, but I need something that is available and proven now:
<a href=""http://robotics.stackexchange.com/questions/2744/what-different-sensing-approaches-are-used-in-the-current-batch-of-indoor-3d-cam"">What different sensing approaches are used in the current batch of indoor 3D cameras?</a></p>

<p>A similar question was asked before, but closed:
<a href=""http://robotics.stackexchange.com/questions/2394/lidar-solutions"">LIDAR solutions</a>. The <a href=""http://www.robotshop.com/en/hokuyo-urg-04lx-laser-rangefinder.html"" rel=""nofollow"">suggestion</a> was good , but I need something with a lot more range:</p>

<p>At the moment am probably going to go with a the RobotEye <a href=""http://www.ocularrobotics.com/products/lidar/re05/"" rel=""nofollow"">RE05</a> or <a href=""http://www.ocularrobotics.com/re08-3d-laser-scanner"" rel=""nofollow"">RE08</a> 3D-LiDAR:</p>

<p>Here is a paper that descibes how this sensor can be used on a mobile robot: <a href=""http://www.araa.asn.au/acra/acra2012/papers/pap125.pdf"" rel=""nofollow"">www.araa.asn.au/acra/acra2012/papers/pap125.pdf</a></p>

<p>Does anyone have any alternative techniques, or suggestions of a sensor that can achieve similar results?</p>
","mobile-robot ros slam kinect lidar"
"3007","Industrial Robotic Arm","<p>I had the opportunity to work for a factory/company that is in the domain space of production and they want to use a robotic arm for part of the production line.</p>

<p>They want basically a robotic arm with payload of about 2 Kgs or more and an arm length of more than 1600mm</p>

<p>I have researched a few companies like Kuka.com but I am not sure what I should be looking for when making suggestions and researching for it.</p>

<p>Are there any suggestions you can give me on few good points to be careful about with robotics arms? Any innovating companies out there I should consider? How is an installation done and if I should find a supplier for it etc. Please enlighten me. </p>
","robotic-arm mechanism"
"3013","Does it matter if my electronic speed controllers are close to my brushless motors?","<p>I have built several quadcopters, hexacopters, and octacopters. This means that between the flight controller (I use 3DR APM2.6 or Pixhawk) and the motors there are heavy duty power wires as well as a servo-style cable carrying a PWM control signal for the ESC. Three short heavy-duty wires then connect the motor to the ESC, one for each phase.</p>

<p>Several times I've heard or read people saying that the electronic speed controllers (ESCs) should be mounted far away from the flight controller (FMU seems to be the abbreviation en vogue) and close to the motors. I think the idea is that this cuts down on interference (I'm not sure what sort) that could be emitted by the long ESC -> motor wires that would be required if you have the ESCs all at the center of the aircraft. Another consideration is that ESCs can be cooled by propellers if they are right under the rotor wash, as mine usually are.</p>

<p>So, I've always mounted ESCs close to motors, but realized that design could be much simpler if ESCs are mounted centrally. So, my question is: what are the pros and cons of mounting ESCs close to the motor versus close to the FMU?</p>
","brushless-motor multi-rotor esc"
"3016","Tried Normal Distributions Transform with my own files (in correct PCD format) and it throws errors, why?","<p><a href=""http://pointclouds.org/documentation/tutorials/normal_distributions_transform.php#normal-distributions-transform"" rel=""nofollow"">http://pointclouds.org/documentation/tutorials/normal_distributions_transform.php#normal-distributions-transform</a></p>

<p>I've used this program with the sample PCD's given and it came out correctly. This was confirmed by experienced users on here. Now I'm trying to use my own pcd's. I didn't want to bother changing the program so I just changed the names to room_scan1 and room_scan2. When I attempt to use them, I get this error:</p>

<blockquote>
  <p>Loaded 307200 data points from room_scan1.pcd Loaded 307200 data
  points from room_scan2.pcd Filtered cloud contains 1186 data points
  from room_scan2.pcd normal_distributions_transform:
  /build/buildd/pcl-1.7-1.7.1/kdtree/include/pcl/kdtree/impl/kdtree_flann.hpp:172:
  int pcl::KdTreeFLANN::radiusSearch(const PointT&amp;, double,
  std::vector&amp;, std::vector&amp;, unsigned int) const [with PointT =
  pcl::PointXYZ, Dist = flann::L2_Simple]: Assertion
  `point_representation_->isValid (point) &amp;&amp; ""Invalid (NaN, Inf) point
  coordinates given to radiusSearch!""' failed. Aborted (core dumped)</p>
</blockquote>

<p>This is the program I compiled: <a href=""http://robotica.unileon.es/mediawiki/index.php/PCL/OpenNI_tutorial_1%3a_Installing_and_testing#Testing_.28OpenNI_viewer.29"" rel=""nofollow"">http://robotica.unileon.es/mediawiki/index.php/PCL/OpenNI_tutorial_1:_Installing_and_testing#Testing_.28OpenNI_viewer.29</a></p>

<p>Before you suggest it, I will let you know I already changed all of the PointXYZRGBA designations to just PointXYZ. It threw the same error before and after doing this. The thing that confuses me is that I looked at my produced PCD files and they seem to be exactly the same as the samples given for NDT.</p>

<p>Mine:</p>

<pre><code>2320 2e50 4344 2076 302e 3720 2d20 506f
696e 7420 436c 6f75 6420 4461 7461 2066
696c 6520 666f 726d 6174 0a56 4552 5349
4f4e 2030 2e37 0a46 4945 4c44 5320 7820
7920 7a0a 5349 5a45 2034 2034 2034 0a54
5950 4520 4620 4620 460a 434f 554e 5420
3120 3120 310a 5749 4454 4820 3634 300a
4845 4947 4854 2034 3830 0a56 4945 5750
4f49 4e54 2030 2030 2030 2031 2030 2030
2030 0a50 4f49 4e54 5320 3330 3732 3030
0a44 4154 4120 6269 6e61 7279 0a00 00c0
7f00 00c0 7f00 00c0 7f00 00c0 7f00 00c0
</code></pre>

<p>Sample from NDT page:</p>

<pre><code>2320 2e50 4344 2076 302e 3720 2d20 506f
696e 7420 436c 6f75 6420 4461 7461 2066
696c 6520 666f 726d 6174 0a56 4552 5349
4f4e 2030 2e37 0a46 4945 4c44 5320 7820
7920 7a0a 5349 5a45 2034 2034 2034 0a54
5950 4520 4620 4620 460a 434f 554e 5420
3120 3120 310a 5749 4454 4820 3131 3235
3836 0a48 4549 4748 5420 310a 5649 4557
504f 494e 5420 3020 3020 3020 3120 3020
3020 300a 504f 494e 5453 2031 3132 3538
</code></pre>

<p>Does anyone have any ideas?</p>
","kinect computer-vision openni"
"3017","Programming language?","<p>In general, what is a good programming language for robotics? I am a starting robo nerd and don't know anyone who would know things like this.</p>
","mobile-robot wheeled-robot programming-languages"
"3021","Directional hearing for Linux robot?","<p>I want to give my Linux robot the ability to locate a sound source and drive towards it. I am reading a <a href=""http://cdn.intechopen.com/pdfs-wm/10567.pdf"" rel=""nofollow"">paper</a> on sound localization that seems to cover the theory well enough, but I'm at a loss as to how do I implement it. Specifically I would like to know:</p>

<ol>
<li>How do I connect two microphones to a Linux PC?</li>
<li>How do I record from two microphones simultaneously?</li>
<li>Is there any library of sound processing algorithms (similar to how OpenCV is a library of computer vision algorithms) available for Linux?</li>
</ol>
","mobile-robot digital-audio linux"
"3026","world files for simulating roads and tracks","<p>Hello I wanted to simulate a busy urban road,similar to <strong>Darpa Urban Challenge</strong> for an autonomous <strong>self-driving-car</strong>. I'm in search of simulators for that.</p>

<p>I've seen gazebo since its integration with ROS is easier but editing world files or indeed creating them itself is difficult. In <strong>torcs</strong> simulator I have seen many world files but not many sensors.  I don't want much physics in my simulation. I want a light weight simulator(for checking out path planning on an urban road) and in which creating roads are easier. </p>

<p>I've even searched for gazebo sdf files similar to urban city but in vain.</p>
","simulator gazebo"
"3027","How frequently should a PID controller update?","<p>I am developing a quadcopter platform on which will be extended over the next year. The project can be found on <a href=""https://github.com/micavdtu/MICAV"" rel=""nofollow"">Github</a>. Currently, we are using an Arduino Uno R3 as the flight management module.</p>

<p>At present, I am tuning the <a href=""https://github.com/micavdtu/MICAV/blob/master/mav_pilot/PID.cpp"" rel=""nofollow"">PID loops</a>. The PID function is implemented as:</p>

<pre><code>int16_t pid_roll(int16_t roll)
{
    static int16_t roll_old = 0;
    int16_t result = 
    (KP_ROLL * roll) + 
    (KI_ROLL * (roll_old + roll)) +
    (KD_ROLL * (roll - roll_old))
    ;
    roll_old += roll;
    result = constrain(result, PID_MIN_ROLL, PID_MAX_ROLL);
    return -result;
}
</code></pre>

<p>I am having trouble interpreting the system response on varying the constants. I believe the problem is related to the questions below.</p>

<ul>
<li>How frequently should a PID controller update the motor values? Currently, my update time is about 100-110 milliseconds. </li>
<li>What should be the maximum change that a PID update should make on the motor thrusts? Currently, my maximum limit is about +-15% of the thrust range.</li>
<li>At what thrust range or values, should the tuning be performed? Minimum, lift off, or mid-range or is it irrelevant?</li>
</ul>
","quadcopter pid"
"3029","Alternative way to perform Pole zero cancellation?","<p>I've read a lot places that making a controller which cancels the unwanted pole or zero is not good designing practice for designing a controller.. </p>

<p>It should make the system uncontrollable which off course isn't wanted. </p>

<p>But what alternatives do i have.. ??</p>

<p>considering i have a system in which all poles and zeros lies on RHP. </p>
","control design"
"3032","jacobian of Abb irb140 robot","<p>Can someone please help me with the jacobian matrix equations for Abb irb140 robot. Or an easy way by which I can derive it given the DH parameters. I need it to implement some form of control that am working on. Thanks</p>
","control robotic-arm kinematics dh-parameters"
"3035","How to make one robot follow the other in parallel formation","<p>This is quite a basic question. I'm practising robot programming with VRep. I have 2 K3 robots in the scene. One robot follows a predefined path. I want the second robot to move ""in parallel"" with the first one so they keep same orientation and same distance at all time. When there is a turn, I want the follower to slow/accelerate a little to keep the parallel.</p>

<p>In my implementation, I use wireless communication. The first robot will periodically ""tell"" the second about its speed, orientation. The second will use these parameters to calculate two speed to its two wheel. But when I run it, it doesn't work. The orientation of the follower is wrong. The distance is not maintained. I was totally confused.</p>

<p>I think this is quite a rudimentary task. There must be some practise to follow. Can somebody help to provide some ideas, references? That will be highly appreciated! </p>
","kinematics"
"3040","""Smooth"" inverse kinematics model for 2-wheeled differential drive robot","<p>I have been reading about kinematic models for nonholonomic mobile robots such as <a href=""http://en.wikipedia.org/wiki/Differential_wheeled_robot"" rel=""nofollow"">differential wheeled robots</a>. The texts I've found so far all give reasonably decent solutions for the forward kinematics problem; but when it comes to inverse kinematics, they weasel out of the question by arguing that for every possible destination pose there are either infinite solutions, or in cases such as $[0 \quad 1 \quad 0]^T$ (since the robot can't move sideways) none at all. Then they advocate a method for driving the robot based on a sequence of straight forward motions alternated with in-place turns.</p>

<p>I find this solution hardly satisfactory. It seems inefficient and inelegant to cause the robot to do a full-stop at every turning point, when a smooth turning would be just as feasible. Also the assertion that some points are ""unreachable"" seems misleading; maybe there <em>are</em> poses a nonholonomic mobile robot can't reach by maintaining a single set of parameters for a finite time, but clearly, if we vary the parameters over time according to some procedure, and in the absence of obstacles, it <em>should</em> be able to reach any possible pose.</p>

<p>So my question is: what is the inverse kinematics model for a 2-wheeled differential drive robot with shaft half-length $l$, two wheels of equal radii $r$ with adjustable velocities $v_L \ge 0$ and $v_R \ge 0$ (i.e. no in-place turns), and given that we want to minimize the number of changes to the velocities?</p>
","mobile-robot inverse-kinematics"
"3043","Are there any problems with a variable frequency PID?","<p>I am working on a quadrotor and am trying to solve the problems <a href=""http://robotics.stackexchange.com/questions/3027/how-frequently-should-a-pid-controller-update"">described here</a>. In attempts to bring the refresh rate to 100 Hz, I did an analysis of the functions  and most of the time 35+ ms is being taken by the RC receiver input function. To tackle this, I have decided on two solutions:</p>

<ul>
<li>Use interrupts (<code>PinChangeInt</code> library) instead of <code>pulseIn</code></li>
<li>Reduce the frequency of pilot input</li>
</ul>

<p>The second solution which is much simpler is to simply read the pilot input once in $(n+1)$  PID updates. So, for $n$ times, we have a update time of $8\;ms$, and for the $(n+1)^{th}$ time, we have an update time of T ms. $n$ will be around $10$.</p>

<p>This will create a system that will run on average in $(n*8 + T)/(n+1)\; ms$.</p>

<p>Now, how does a dual/variable frequency affect the PID system? Does the system behave as if working at the effective frequency? I have been searching for some time but I cannot find anything that discusses such a situation. </p>
","pid"
"3049","How to define conditions for state-machines in roby?","<p>I am searching for a way that allows me to wait for some conditions on ports before applying a new state.</p>

<p>My concrete Problem:
I want to make sure that my AUV aligns to the right pipeline. Therefore before starting the pipeline-tracking, I want to check for the current system heading.</p>

<p>My current state-machine looks like this:</p>

<blockquote>
  <p>find_pipe_back = state target_move_def(:finish_when_reached => false ,
  :heading => 1 ...)</p>
  
  <p>pipe_detector = state pipeline_detector_def</p>
  
  <p>pipe_detector.depends_on find_pipe_back, :role => ""detector""</p>
  
  <p>start(pipe_detector)
  forward pipe_detector.align_auv_event, success_event</p>
</blockquote>

<p>roughly I am looking for a way to condition the last-forward.</p>
","rock syskit"
"3052","Gears in Autodesk Inventor are looking weird","<p>I use Autodesk inventor professional 2014. I design my gears using the design accelerator. However, whenever I create gear trains, parts of certain gears become transparent. This seems completely random because sometimes if I zoom in or out or when I pan or orbit, the gears look normal again.
I have experienced this problem using both the default and other material types.
I also have ensured that each of these gears are enabled.</p>

<p>Here are some example pictures
<img src=""http://i.stack.imgur.com/T0PRQ.png"" alt=""This is a picture of my problem"">
<img src=""http://i.stack.imgur.com/GaIvo.png"" alt=""This is another picture""><img src=""http://i.stack.imgur.com/e9zWS.png"" alt=""A third picture""><img src=""http://i.stack.imgur.com/nrGWd.jpg"" alt=""enter image description here"">
Any help or suggestions will be greatly appreciated.</p>
","design errors"
"3058","Why cannot we find EtherCAT shields?","<p>I have a riddle about EtherCAT in mind and I'd like to have your point of view about it...</p>

<p>With the rise of open platforms and hardware, and easily accessible embedded machines, it is now rather straightforward to install a RT system such as Xenomai on a raspberry PI, or a beagleboard black, or whatever cheap platform you prefer...</p>

<p>Now to connect these a RT bus would be really cool (e.g. EtherCAT...).</p>

<p>Hence my question: every hobbyist face the same problems with RT communication, so is there any good reason why there does not exist any open EtherCAT shield for raspberry PI or beagleboards? It would solve so many problems...</p>

<p>Any thoughts on why? Any idea?</p>
","communication"
"3062","How to connect a servo motor and a crank shaft","<p>I have a standard 5v </p>

<p><a href=""http://i.stack.imgur.com/6Iv4e.jpg"" rel=""nofollow""><img src=""http://i.stack.imgur.com/6Iv4em.jpg"" alt=""enter image description here""></a></p>

<p>I am using the horn it came with it, I mean this piece: </p>

<p><img src=""http://i.stack.imgur.com/aQJxo.jpg"" alt=""enter image description here""></p>

<p>Like the long arm in the middle. Each of its holes are 1mm diameter.</p>

<p>Then I have a 3d printed crankshaft I did:</p>

<p><img src=""http://i.stack.imgur.com/i5JGD.png"" alt=""enter image description here""></p>

<p>Its holes are also 1 mm. So while the servo horn is attached to the servo, I attach this crank to the horn in order to lift or lower a small scructure.</p>

<p>What I am not sure is hot to connect these 2 pieces (horn and 3d printed crankshaft). So far I have been using a paper clip, and at both end of it I placed 2 blobs of tin using a soldering iron. This has worked for nearly a year, but today I failed, and I was wondering if there's something more specific for my problem, which seems something common.</p>

<p>I have seen some people use something called Dubro EZ connector, but it seems an overkill, plus it won't have space for my 3D printed piece. Some people seems to use a clevis pin, but I cannot find any with a diameter of less than 1.</p>

<p>So my question is, how can I fix it? What can I put at both ends to stop if from slipping away? I have already tried simple things like simply bending it.</p>
","arduino mechanism rcservo"
"3063","Why should I still use EKF instead of UKF?","<p>The Unscented Kalman Filter is a variant of the Extended Kalman Filter which uses a different linearization relying on transforming a set of ""Sigma Points"" instead of first-order Taylor series expansion.</p>

<p>The UKF does not require computing Jacobians, can be used with discontinuous transformation, and is, most importantly, more accurate than EKF for highly nonlinear transformations.</p>

<p>The only disadvantage I found is that ""the EKF is often slightly faster than the UKF"" (Probablistic Robotics). This seems negligible to me and their asymptotic complexity seems to be the same.</p>

<p>So why does everybody still seem to prefer EKF over UKF? Did I miss a big disadvantage of UKF? </p>
","mobile-robot localization kalman-filter ekf"
"3066","What are some generally accepted lift capacity guidelines for multirotors?","<p>e.g. what general multicopter configurations would be generally accepted as recommendations to lift 0.5kg, 1kg, 2kg, 4kg, etc.</p>

<p>Is there any general correlation between number of motors on a similar sized frame and lift capacity?</p>
","quadcopter power"
"3069","Quadcopter degrees of freedom","<p>It might be kind of a stupid question but how many degrees of freedom are there in a typical quadcopter? I say some saying 4 and some saying 6. The difference stands in translation throughout the other 2 axis (horizontal ones). Being strict to what you can directly tell the quadcopter to do, only 4 movements are possible since you cannot apply a pure lateral force. But you can tilt to start a lateral movement and align the body right after and let it hover in a horizontal axis, theoretically. So, formally, how many degrees of freedom should I consider to exist?</p>
","quadcopter"
"3071","Mahalanobis distance between 2 line features","<p>I am implementing the <strong>ATLAS SLAM</strong> framework for a ground robot, using EKF Slam for local maps and using line segment features. The line segment features can be abstracted to their respective lines <code>[d,α]</code> where <code>d</code> and <code>α</code> represent the distance and angle in the distance-angle representation of lines. </p>

<p>In the given framework, there is a local map matching step where lines of the local maps will be matched, and there is a need for a distance metric between 2 lines. The mahalanobis distance is suggested in the literature, however strictly a mahalanobis distance is between a single measurement and a distribution and not between 2 distributions.</p>

<p>How do I find the mahalanobis distance between line 1 <code>[d1,α1]</code> with covariance matrix <code>S1</code> and line 2 <code>[d2,α2]</code> with covariance matrix <code>S2</code>?</p>

<p>In the <strong>EKF Algorithm</strong> from the book Probabilistic Robotics by Sebastian Thrun, there is a computation during the feature update step, where it looks like the covariances (of a new measurement and an existing measurement) are multiplied to give a resultant covariance matrix, and then the inverse is used in the Mahalanobis distance computation. </p>

<p>That would be similar to </p>

<pre><code>Mahalanobis_Distance = [d2-d1,α2-α1] * Inverse(S1*S2) * [d2-d1,α2-α1]'
</code></pre>

<p>Is that correct?</p>
","ekf mapping"
"3073","Data association with ekf?","<p>Given part of the following algorithm in page 217 <a href=""http://rads.stackoverflow.com/amzn/click/0262201623"" rel=""nofollow"">probabilistic robotics</a>, this algorithm for EKF localization with unknown correspondences </p>

<p>9. for all observed features $z^{i}  = [r^{i} \ \phi^{2} \ s^{i}]^{T} $</p>

<p>10. &nbsp; &nbsp; for all landmarks $k$ in the map $m$ do</p>

<p>11. &nbsp; &nbsp; &nbsp; &nbsp; $q = (m_{x} - \bar{\mu}_{x})^{2} + (m_{y} - \bar{\mu}_{y})^{2}$</p>

<p>12. &nbsp; &nbsp; &nbsp; &nbsp; $\hat{z}^{k} = \begin{bmatrix}
\sqrt{q} \\ 
atan2(m_{y} - \bar{\mu}_{y}, m_{x} - \bar{\mu}_{x} ) - \bar{\mu}_{\theta} \\ 
m_{s} \\ \end{bmatrix}$</p>

<p>13. &nbsp; &nbsp; &nbsp; &nbsp; $ \hat{H}^{k} = \begin{bmatrix} 
h_{11} &amp; h_{12} &amp; h_{13} \\
h_{21} &amp; h_{22} &amp; h_{23} \\
h_{31} &amp; h_{32} &amp; h_{33} \\ \end{bmatrix}  $</p>

<p>14. &nbsp; &nbsp; &nbsp; &nbsp; $\hat{S}^{k} = H^{k} \bar{\Sigma} [H^{k}]^{T} + Q $</p>

<p>15. &nbsp; &nbsp; endfor </p>

<p>16. &nbsp; &nbsp; $ j(i) = \underset{k}{\operatorname{arg\,max}} \ \ det(2 \pi S^{k})^{-\frac{1}{2}} \exp\{-\frac{1}{2} (z^{i}-\hat{z}^{k})^{T}[S^{k}]^{-1} (z^{i}-\hat{z}^{k})\} $</p>

<p>17. &nbsp; &nbsp; $K^{i} = \bar{\Sigma} [H^{j(i)}]^{T} [S^{j(i)}]^{-1}$</p>

<p>18. &nbsp; &nbsp; $\bar{\mu} = \bar{\mu} + K^{i}(z^{i}-\hat{z}^{j(i)}) $</p>

<p>19. &nbsp; &nbsp; $\bar{\Sigma} = (I - K^{i} H^{j(i)}) \bar{\Sigma} $</p>

<p>20. endfor</p>

<p>My question is why the second loop ends in the line 15. Shouldn't it end after the line 19. I've checked the <a href=""http://robots.stanford.edu/probabilistic-robotics/errata.html"" rel=""nofollow"">errata</a> of this book but nothing about this issue. </p>
","localization ekf data-association"
"3079","Typical method for integrating a neural net into a PLC","<p>How would one typically integrate a neural network into an online automation system?</p>

<p>As an example, we have developed a neural network that predicts a difficult to measure variable within a reactor using multiple sensors. We then use this predicted variable to tell the automation system to, for example, increase/decrease the stirrer speed.</p>

<p>How would someone implement this idea into a commercial system. Would they develop a function block that can simulate the neural network? Would they run a software on the server that reads and writes to the PLC control tags?</p>
","sensors control"
"3080","Arduino Quadcopter using bluetooth shield and android phone","<p>I need help on how to go about building a quadcopter software from scratch with the available tools I have with me. I don't have a transmitter radio therefore the only way I can do remote control is using an android phone with the itead studio bluetooth shield that I was recently given.
How can I use the existing open source software, i.e aeroquad or arducopter. The following are parts that I have:-</p>

<ol>
<li>Arduino Uno </li>
<li>Bluetooth shield </li>
<li>Four brushless motors </li>
<li>Q450 frame Four</li>
<li>ESC Turnigy </li>
<li>MPU6050</li>
</ol>
","arduino quadcopter"
"3088","Different Particle Filter min and max particle numbers give almost the same result","<p>I'm using <a href=""http://wiki.ros.org/amcl"" rel=""nofollow"">amcl</a> package in <a href=""http://wiki.ros.org"" rel=""nofollow"">ROS</a> to localize a mobile robot. I've changed <code>min_particles</code> and <code>max_particles</code> several times then calculated the output difference with odomotry to evaluate these parameters. The table below demonstrate results; As you see, there is no notable change in the output and if you ignore the first row of the table, output variance is small. 
<img src=""http://i.stack.imgur.com/NMSy8.jpg"" alt=""enter image description here"">
<br/>And this is the Particle Filter output on the map:<br/>
<img src=""http://i.stack.imgur.com/tiD4j.jpg"" alt=""enter image description here""></p>
","localization particle-filter"
"3090","How to request a specific Mavlink packet from Ardupilot?","<p>I'm developing a program for communicating with Ardupilot using Mavlink. I've generated code based on the Mavlink definition for Ardupilot, and I have the basic communication working.</p>

<p>What I can't figure out, is how to request Ardupilot to send a specific Mavlink message. I'd like Ardupilot to send me Mavlink message <a href=""https://pixhawk.ethz.ch/mavlink/#ATTITUDE"" rel=""nofollow"">Attitude (#30)</a> every second. How can I do this?</p>
","ardupilot"
"3091","How to overwrite default git source in autoproj?","<p>I want to overwrite the git source of a package in autoproj. That package is by 
default on gitorious and I forked it on spacegit to apply specific patches.
According to the autoproj documentation (<a href=""http://rock-robotics.org/stable/documentation/autoproj/customization.html"" rel=""nofollow"">http://rock-robotics.org/stable/documentation/autoproj/customization.html</a>), I set the new repo in the overrides.yml by:</p>

<pre><code>  - control/orogen/&lt;package&gt;:
    url: git://spacegit.dfki.uni-bremen.de/virgo/orogen-&lt;package&gt;.git
</code></pre>

<p>But if I inspect the remotes of the newly checked out package, only the 
fetch url is adapted to spacegit whereas the push url still points to 
the default gitorious repo:</p>

<pre><code>$ git remote -v
autobuild   git://spacegit.dfki.uni-bremen.de/&lt;project&gt;/orogen-&lt;package&gt;.git (fetch)
autobuild   git@gitorious.org:/rock-control/orogen-&lt;package&gt;.git (push)
</code></pre>

<p>How can I overwrite both the fetch and the push source of a package in the 
overrides.yml?</p>
","rock"
"3098","Controlling a system with delayed measurements","<p>Assume I have a rather simple system I want to control, but all sensor measurements exhibit considerable time delay, i.e.:</p>

<p>$z_t = h(x_{(t-d)}) \neq h(x_t)$ </p>

<p>With my limited knowledge about control, I could imagine the following setup:</p>

<ul>
<li>One observer estimates the delayed state $x_{(t-d)}$ using control input and (delayed) measurements.</li>
<li>A second observer uses the delayed observer's estimate and predicts the current state $x_t$ using the last control inputs between delayed measurement and current time.</li>
<li>The second observer's estimate is used to control the system.</li>
</ul>

<p>Can I do any better than that? What is the standard approch to this problem? And is there any literature or research about this topic?</p>
","sensors control sensor-fusion sensor-error"
"3101","Velocity Model Motion in Matlab (Probabilistic Robotics)","<p>I want to implement the velocity motion model in Matlab. According to <a href=""http://rads.stackoverflow.com/amzn/click/0262201623"" rel=""nofollow"">Probabilistic Robotics</a> page 124, the model is as following </p>

<p>\begin{align*}
\hat{v}      &amp;= v + sample(\alpha_{1} v^{2} + \alpha_{2} w^{2}) \\
\hat{w}      &amp;= w + sample(\alpha_{3} v^{2} + \alpha_{4} w^{2}) \\
\hat{\gamma} &amp;= sample(\alpha_{5} v^{2} + \alpha_{6} w^{2}) \\
x' &amp;= x - \frac{\hat{v}}{\hat{w}} sin \theta + \frac{\hat{v}}{\hat{w}} sin(\theta + \hat{w} \Delta{t}) \\
y' &amp;= y + \frac{\hat{v}}{\hat{w}} cos \theta - \frac{\hat{v}}{\hat{w}} cos(\theta + \hat{w} \Delta{t}) \\
\theta' &amp;= \theta + \hat{w} \Delta{t} + \hat{\gamma} \Delta{t}
\end{align*}</p>

<p>where $sample(b^{2}) \Leftrightarrow \mathcal{N}(0, b^{2})$. With this kind of variance $\alpha_{1} v^{2} + \alpha_{2} w^{2}$, the Kalman Gain is approaching singularity. Why?  </p>
","mobile-robot kinematics motion motion-planning noise"
"3102","6 DOF Robotic Arm","<p>We are building a 6 DOF robotic arm as a college project and we've almost finished the designs. The problem is with the controls. We still havent thought on how to control the arm, as in , software gui interfaces , etc. Any suggestions on this ? Also, is there any simulation software for Simulating and testing robotic arms ? </p>
","control arm"
"3104","EKF localization known correspondences","<p>I'm facing problems with this <a href=""http://rads.stackoverflow.com/amzn/click/0262201623"" rel=""nofollow"">book</a> and it is the only book that discusses localization in depth. The results that I'm getting makes no sense. I've read a lot of papers, majority of them copy the localization algorithm from this book. My question here is why $\bar{\mu}$ and $\bar{\Sigma}$ are being changed every iteration?? I'm using them to get the predicted measurements in lines 11- 13, so they should be fixed. </p>

<p>9. for all observed features $z^{i}  = [r^{i} \ \phi^{i} \ s^{i}]^{T} $ do </p>

<p>10. &nbsp; $j = c^{i}$ </p>

<p>11. &nbsp; &nbsp; &nbsp; &nbsp; $q = (m_{x} - \bar{\mu}_{x})^{2} + (m_{y} - \bar{\mu}_{y})^{2}$</p>

<p>12. &nbsp; &nbsp; &nbsp; &nbsp; $\hat{z}^{i} = \begin{bmatrix}
\sqrt{q} \\ 
atan2(m_{y} - \bar{\mu}_{y}, m_{x} - \bar{\mu}_{x} ) - \bar{\mu}_{\theta} \\ 
m_{s} \\ \end{bmatrix}$</p>

<p>13. &nbsp; &nbsp; &nbsp; &nbsp; $ \hat{H}^{i} = \begin{bmatrix} 
h_{11} &amp; h_{12} &amp; h_{13} \\
h_{21} &amp; h_{22} &amp; h_{23} \\
h_{31} &amp; h_{32} &amp; h_{33} \\ \end{bmatrix}  $</p>

<p>14. &nbsp; &nbsp; $\hat{S}^{i} = H^{i} \bar{\Sigma} [H^{i}]^{T} + Q $</p>

<p>15. &nbsp; &nbsp; $K^{i} = \bar{\Sigma} [H^{i}]^{T} [S^{i}]^{-1}$</p>

<p>16. &nbsp; &nbsp; $\bar{\mu} = \bar{\mu} + K^{i}(z^{i}-\hat{z}^{i}) $</p>

<p>17. &nbsp; &nbsp; $\bar{\Sigma} = (I - K^{i} H^{i}) \bar{\Sigma} $</p>

<p>18. endfor</p>

<p>19. $\mu = \bar{\mu}$ </p>

<p>20. $\Sigma = \bar{\Sigma}$</p>

<p>Please suggest me other books that discuss EKF localization in depth. </p>
","localization ekf"
"3105","Will a 20amp ESC run a Turnigy 2213-980?","<p>I'm a noobie just starting out and trying to come up what I need to build my first quadrocopter, I just wanted to run something by people with some experience before I commit to buying anything.</p>

<p>Would <a href=""http://www.hobbyking.com/hobbyking/store/__27775__Turnigy_Multistar_20_Amp_Multi_rotor_Brushless_ESC_2_4S_OPTO_.html"" rel=""nofollow"">this esc</a> be fine for running <a href=""http://www.hobbyking.com/hobbyking/store/__39036__Turnigy_Multistar_2213_980Kv_14Pole_Multi_Rotor_Outrunner.html"" rel=""nofollow"">this motor</a>? As I understand it the ESC should be rated for slightly above what the max amps are for the motor?</p>

<p>On top of that, should <a href=""http://www.hobbyking.com/hobbyking/store/__7636__ZIPPY_Flightmax_2200mAh_3S1P_25C_EU_warehouse_.html"" rel=""nofollow"">this battery</a> be able to run all of the motors without any issue?</p>
","quadcopter brushless-motor multi-rotor esc battery"
"3108","How to control a function with two different inputs","<p>How might I be able to control one function (like brightness control of an <a href=""http://en.wikipedia.org/wiki/Light-emitting_diode"" rel=""nofollow"">LED</a>) with two different triggers (like a tactile switch and an IR remote)?</p>

<p>I am trying to be able to control the brightness with switches as well as IR remote when desired.</p>
","control"
"3110","ROS: Best practices?","<p>i'm going to build a small Robot System and it seems like that ROS serves a nice framework to control and program the System.
However i am wondering which is the best practice to manage the components of my Robot. Does it make sense to put all the sensors in one Node?
Should i only put the sensors of the same type in one node or is it better so have one node for one sensor? 
Is it a good practice to have some kind of handler node, which takes input from sensors and steers the corresponding actuators or should the actuator nodes and sensor nodes communicate directly?</p>

<ol>
<li><p>Fused sensor nodes and actuator nodes with handler
<img src=""http://i.stack.imgur.com/wkjDK.jpg"" alt=""1. Fused sensor nodes and actuator nodes with handler""></p></li>
<li><p>Single sensor and actuator nodes with handler
<img src=""http://i.stack.imgur.com/WZkGN.jpg"" alt=""enter image description here""></p></li>
<li><p>direct communication
<img src=""http://i.stack.imgur.com/MyCez.jpg"" alt=""enter image description here""></p></li>
</ol>

<p>For me i guess the best is to have some kind of handler, which handles the communication between sensors and actuators and have one node for each element of the Robot (like in Fig.2). Because like that the system loosely coupled and can be extended easily, but I want to know what your opinion is.</p>

<p>Greetings</p>
","control ros"
"3116","Problems using syskit monitors -> failed emission of the foo event of","<p>I had just tested my first monitor, which results in the following error
regarding the suggestion in <a href=""http://robotics.stackexchange.com/questions/3049/how-to-define-conditions-for-state-machines-in-roby"">How to define conditions for state-machines in roby?</a></p>

<p>unfortunately i ran into a runtime error, i don't know whether this is a bug or if i misuse the monitor...</p>

<pre><code>16:28:27.564 (Roby) = failed emission of the weak_signal event of Pipeline::Detector:0x71f5cf0
16:28:27.564 (Roby) = Backtrace
16:28:27.564 (Roby) |
16:28:27.564 (Roby) | /home/auv/dev/tools/roby/lib/roby/task.rb:663:in `emitting_event'
16:28:27.564 (Roby) | /home/auv/dev/tools/roby/lib/roby/task_event_generator.rb:46:in `emitting'
16:28:27.564 (Roby) | /home/auv/dev/tools/roby/lib/roby/event_generator.rb:628:in `emit_without_propagation'
16:28:27.564 (Roby) | /home/auv/dev/tools/roby/lib/roby/execution_engine.rb:1017:in `block (2 levels) in event_propagation_step'
16:28:27.564 (Roby) | /home/auv/dev/tools/roby/lib/roby/execution_engine.rb:648:in `propagation_context'
16:28:27.564 (Roby) | /home/auv/dev/tools/roby/lib/roby/execution_engine.rb:1015:in `block in event_propagation_step'
16:28:27.564 (Roby) | /home/auv/dev/tools/roby/lib/roby/execution_engine.rb:559:in `block in gather_propagation'
16:28:27.565 (Roby) | /home/auv/dev/tools/roby/lib/roby/execution_engine.rb:648:in `propagation_context'
16:28:27.565 (Roby) | /home/auv/dev/tools/roby/lib/roby/execution_engine.rb:559:in `gather_propagation'
16:28:27.565 (Roby) | /home/auv/dev/tools/roby/lib/roby/execution_engine.rb:1014:in `event_propagation_step'
16:28:27.565 (Roby) |/home/auv/dev/tools/roby/lib/roby/execution_engine.rb:783:in `block in event_propagation_phase'
16:28:27.565 (Roby) | /home/auv/dev/tools/roby/lib/roby/execution_engine.rb:761:in `gather_errors'
16:28:27.565 (Roby) | /home/auv/dev/tools/roby/lib/roby/execution_engine.rb:779:in `event_propagation_phase'
16:28:27.565 (Roby) | /home/auv/dev/tools/roby/lib/roby/execution_engine.rb:1426:in `process_events'
16:28:27.565 (Roby) | /home/auv/dev/tools/roby/lib/roby/execution_engine.rb:1940:in `block (2 levels) in event_loop'
16:28:27.565 (Roby) | /home/auv/dev/tools/roby/lib/roby/support.rb:176:in `synchronize'
16:28:27.565 (Roby) | /home/auv/dev/tools/roby/lib/roby/execution_engine.rb:1939:in `block in event_loop'
16:28:27.565 (Roby) | /home/auv/dev/tools/roby/lib/roby/execution_engine.rb:1917:in `loop'
16:28:27.565 (Roby) /home/auv/dev/tools/roby/lib/roby/execution_engine.rb:1917:in `event_loop'
16:28:27.565 (Roby) | /home/auv/dev/tools/roby/lib/roby/execution_engine.rb:1797:in `block (3  levels) in run'
16:28:27.565 (Roby) =
</code></pre>

<p>Don't know whether this is a bug, or if i had miss-used the monitor... 
here is the action_state_machine i'm using:</p>

<pre><code>183     describe(""Find_pipe_with_localization"").
184         optional_arg(""check_pipe_angle"",false)
185     action_state_machine ""find_pipe_with_localization"" do
186         find_pipe_back = state target_move_def(... some long stuff here ... )
187         pipe_detector = state pipeline_detector_def
188         pipe_detector.depends_on find_pipe_back, :role =&gt; ""detector""
189         start(pipe_detector)
190
191         pipe_detector.monitor(
192             'angle_checker', #the Name
193             pipe_detector.find_port('pipeline'), #the port for the reader
194             :check_pipe_angle =&gt; check_pipe_angle). #arguments
195             trigger_on do |pipeline|
196                 angle_in_range = true
197                 if check_pipe_angle
198                     angle_in_range = pipeline.angle &lt; 0.1 &amp;&amp; pipeline.angle &gt; -0.1
199                 end
200                 state_valid = pipeline.inspection_state ==  :ALIGN_AUV || pipeline.inspection_state == :FOLLOW_PIPE
201                 state_valid &amp;&amp; angle_in_range #last condition
202             end. emit pipe_detector.success_event
# for non-monitor use, this works if the above is commented out
203 #        forward pipe_detector.align_auv_event, success_event
204 #        forward pipe_detector.follow_pipe_event, success_event
205
206         forward pipe_detector.success_event, success_event
207         forward pipe_detector,find_pipe_back.success_event,failed_event #timeout here on moving
208     end
</code></pre>
","rock syskit"
"3117","Device that can push out independent pin points?","<p>I'm looking for a device that can push out independent pinpoints from something similar to a Pin Point Impression Toy. I'm looking to create a 3D image from for example my computer. Does anybody know the name of such a device or can point me in the right direction of making one? </p>

<p>I've been looking now for a while, but I'm having some slight problems finding a good way to describe it as a search term.</p>

<p>I'm sorry if this is the wrong forum.</p>
","3d-printing"
"3118","Observation Model Jacobian for Fixed Transforms","<p>Let's say I have a hypothetical sensor that provides, for example, velocity estimates, and I affix that sensor at some non-zero rotational offset from the robot's base. I also have an EKF that is estimating the robot's velocity.</p>

<p>Normally, the innovation calculation for an EKF looks like this:</p>

<p>$$ y_k = z_k - h(x_k) $$</p>

<p>In this case, $h$ would just be the rotation matrix of the rotational offset. What are the ramifications if instead, I pre-process the sensor measurement by rotating $z_k$ by the inverse rotation, which will put its coordinates in the frame of the robot? Can I then safely just make $h$ the identity matrix $I$?</p>
","kalman-filter ekf"
"3121","Where can I buy heavy-duty Omni Wheels?","<h2>Where can I buy multi-directional omni wheels?</h2>

<p>I'm specifically looking at something which can support in <strong>excess of 100kg/wheel, so around 400kg in total</strong>. Also, a possible mission profile would include a 300 meter excursion outdoors on asphalt path, so they should be a little durable. The only ones I can find online are small ones for experimenting.</p>

<p><img src=""http://4.bp.blogspot.com/-tB_edZOgyPc/TvCsA0_ebAI/AAAAAAAA5pg/cV4ggfOYUWs/s1600/Omniwheel-grabcad.JPG"" alt=""pic""></p>
","wheel"
"3124","PWM PID control for small 2 watts brushed DC motor","<p>It is ""good enough"" for PID output directly controls, without further modelling, the PWM duty cycle?</p>

<p>Logic behind the question is, </p>

<p>In case of pure resistance heater, PWM duty cycle percentage directly relates to power (on off time ratio). So, direct control is appropriate.</p>

<p>However, motor has two additional effects,</p>

<p>a) with considerable <strong>inductance</strong>, initial current is smaller and ramping up over time</p>

<p>b) as RPM gradually ramping up, after time constant of mechanical inertia etc, increasing <strong>back EMF</strong> will reduce current</p>

<p>Will it be wise to ignore the above two effects and still expect a reasonably good outcome? </p>

<p>Application is 6 volts, 2 watt DC brushed motor, gear 1:50, 10000 RPM no load, PWM frequency 490Hz, driving DIY 1kg robot.</p>
","motor pid pwm"
"3125","What is the actual application for manufactuing, with robot arm autonomous object classification system","<p>I’m a graduate student, and we're doing a project that is going to introduce a robot arm into manufacturing. Our goal is to build up an autonomous object classification system. We already have the software and hardware required for the task, but we have no idea if there is any existing manufacturing scenario where we can apply the system and really improve the efficiency or save human resources.</p>

<p>Here is some info about the robot arm:
For the hardware part, the robot arm is with 7 dof and 5kg payload (the weight of the end effector is not counted). Besides, the end effector is a 1.5kg 3-fingered robot hand with 2kg payload. The workspace is approximately a sphere with 0.9m diameter.</p>

<p>For the software part, we have programming by touch, by which human can drag the robot and record the desired pose. Besides, we have PCL object recognition that can recognize the object and its pose in the scene. Lastly, we have online trajectory generator and dynamic obstacle avoidance that can improve the safety when the robot corporates with human.</p>

<p>Since we know few about manufacturing, we hope that someone can give us a hint about the scenario and an actual application where we can apply this system. </p>
","robotic-arm manufacturing"
"3132","EKF localization is approaching singularity. Are my sensors too noisy?","<p>I'm getting this warning from Matlab about Kalman Gain. </p>

<pre><code>Warning: Matrix is close to singular or badly scaled.
         Results may be inaccurate. RCOND = 9.996841e-19. 
</code></pre>

<p>The problem is coming from high variance of the measurement model. My question is here Does EKF work with high noise in sensor?</p>
","localization ekf"
"3133","Installing OpenNI on Pcduino","<p>Following my <a href=""https://robotics.stackexchange.com/questions/2649/kinect-point-cloud-pcduino-will-it-work"">previous question about pcduino+kinect</a>, decided to go ahead and buy the pcduino I wish to run my robot with (kinect+pcduino+shields). However I'm having trouble getting started: I tried installing OpenNI, NITE and SensorKinect however OpenNI installation fails (I haven't even gotten to installing NITE and SensorKinect yet so no idea if that would work). I tried a bunch of pointers (<a href=""http://daybydaylinux.blogspot.com/2012/12/how-to-compile-openni-and-sensorkinect.html"" rel=""nofollow"">here</a> and <a href=""http://www.andol.info/hci/1924.htm"" rel=""nofollow"">here</a>). For example the error I get if I follow link 1 is:</p>

<blockquote>
  <p>ubuntu@ubuntu:~/kinect/OpenNI/Platform/Linux/CreateRedist \$ sudo ./RedistMaker.Arm </p>
  
  <p>Target: Linux-Arm</p>
  
  <p>Version: 1.5.7.10</p>
  
  <p>Num of compile jobs: 0</p>
  
  <ul>
  <li>Building OpenNI...</li>
  </ul>
  
  <p>Common/CommonDefs.mak:36: <em>*</em> Cross-Compilation error. Can't find ARM-J1_CXX and >ARM-J1_STAGING.  Stop.</p>
  
  <p>failed to execute: make PLATFORM=Arm-j1 -C </p>
  
  <p>/home/ubuntu/kinect/OpenNI/Platform/Linux/CreateRedist/../Build clean  </p>
  
  <p>/home/ubuntu/kinect/OpenNI/Platform/Linux/CreateRedist/Output/BuildOpenNI_clean.txt</p>
  
  <p>Cleaning Failed!</p>
  
  <p>ubuntu@ubuntu:~/kinect/OpenNI/Platform/Linux/CreateRedist$ </p>
</blockquote>

<p>After someone suggested it, I tried removing the -mfloat-abi=softfp option but that didn't help. There seems to be some compiling/linking issue due to float types which I'm not able to figure out. In link 1 the author mentions to remove the 'calc_jobs_number()' but that does not work and I get similar error. Also similar problem exists for link 2 above</p>

<p>If I follow link 2, 'make' won't work and will give the following error:</p>

<blockquote>
  <p>/usr/bin/ld: error: ../../Bin/Arm-Release/libOpenNI.so uses VFP register >arguments, ./Arm-Release/tinyxmlparser.o does not
  /usr/bin/ld: failed to merge target specific data of file ./Arm-Release/tinyxmlparser.o</p>
</blockquote>

<p>Another approach would be to use simpleCV instead of OpenNI on pcduino as someone else claims it has worked before. However I've never used simpleCV with Kinect before so unless it's not radically different I prefer using OpenNI.</p>

<p>Any suggestions as to why I might be getting these errors are appreciated. Any other pointers for solving the problem of installing OpenNI on Pcduino would be welcome.</p>

<p>Please let me know if you need more details about anything else.
Thanks in advance</p>
","kinect arm openni"
"3134","libfreenect simplecv integration?","<p>I installed SimpleCV and libfreenect on pcduino (running lbuntu). I separately verified that simpleCV reads my USB webcam well and libfreenect (glview tutorial) gives me depth and rgb correctly, albeit and a pathetic framerate. What I want is to call cam = Kinect() in simplecv but when I do that, I get the warning ""You dont seem to have the freenect library installed. This will make it hard to use a kinect"". Although this is a warning I get an error if I then do cam.getDepth(), which says ""NameError: global name 'freenect' is not defined"".</p>

<p>How do I let simplecv know that I've installed libfreenect?</p>
","kinect"
"3137","How to use quaternions to feed a PID quadcopter stabilization loop?","<p>I'm making a quadcopter. I have set up a PID loop to stabilize it to a given Euler angle (pitch and roll). The problem arises when the roll approaches 90 degrees (45 degrees and up). The values don't make sense anymore, as it approaches the gimbal lock. I intend to make it do complex maneuvers like looping etc., which exceeds the 45 degree roll limit.</p>

<p>How can I use quaternions to overcome this problem? (I get quaternions from the MPU-9150.) I have read many articles on the matter of quaternions, but they all talk about rotations in 3D software, and tweening between two rotation points. This makes little sense as I do not know imaginary numbers and matrices.</p>
","quadcopter pid stability"
"3139","Denavit-Hartenberg convention","<p>There are two different conventions that can determine <strong>DH parameters</strong>.   What is the difference between <strong>Craig's</strong> [1, Sec 3.4] convention and the <strong>Spong</strong> [2, Sec. 3.2] convention?
I know that both methods must have the same response.</p>

<p>[1]: Craig, John J. <em>Introduction to robotics: mechanics and control</em>. Addison-Wesley, 1989.</p>

<p>[2]: Spong, Mark W., Seth Hutchinson, and Mathukumalli Vidyasagar. <em>Robot modeling and control</em>. Wiley, 2006.</p>
","forward-kinematics dh-parameters"
"3144","Inverse Kinematics of Parallel Manipulator (Delta Robot)","<p>Let me start off by saying that I am currently going to university majoring in computer engineering.  I love software/hardware and I especially love robotics and I want to apply my knowledge of software/hardware in robots.  I have never taken a formal class on robotics, so I don't really know where to start or how to approach the mathematics that robots entail.  </p>

<p>Currently, I am interested in calculating the inverse kinematics of a delta robot. To clarify a bit more, I am trying to determine the required joint angles that will position the end-effector of the delta robot to a specific location given some x,y,z coordinate.  The delta robot that I will be basing my design off of is shown in the image below.</p>

<p><img src=""http://i.stack.imgur.com/mr8hD.jpg"" alt=""enter image description here""></p>

<p>Based off of some research that I have been doing for the past few days, I found that the sort of mathematics involved are usually like those of Denavit-Hartenberg parameters, Jacobian matrices, etc.  I am going to be honest, I have never encountered Denavit-Hartenberg parameters or Jacobian matrices and I don't even know how to apply these to solve the kinematics equations and let alone find the kinematics equations.  Most of the articles that I have read, mainly deal with serial manipulator robots and the mathematics in finding the kinematics equations of those serial manipulators.  I couldn't really find any good material or material that was easy to understand given my current situation on parallel manipulators.  </p>

<p>I wanted to ask my question here in the hopes that someone in the community could direct me to where I can start on learning more on obtaining the inverse kinematics equations of parallel manipulators and solving those equations.</p>

<p>Any help will be much appreciated.</p>

<p>Thank you.</p>
","kinematics inverse-kinematics"
"3145","How to brake a brushed dc motor, belt-driven linear actuator to within 0.5mm of an end stop","<p>I will have a belt-driven linear actuator, consisting a gantry-plate riding on two rails. I'm thinking of using a brushed dc motor.</p>

<p>The gantry will move from home position to the right (outbound) at 1m/s. The mass of the gantry will vary from 3Kg to 6Kg. On the return home (inbound) one must avoid spillage of contents which may require soft start/stop or simply a slow return to home.</p>

<p>In the outbound case, what I'd like to know is how,in a practical sense, do you brake the mass and bring the gantry to a stop, ensuring that the gantry plate always comes to rest to within 0.5mm of an end plate?</p>

<p>I'm clearer how I can ensure the gantry stops to within 0.5mm of the home position, because I can use a PWM ramp to slowly decelerate.</p>

<p>I'm wanting to avoid using an MCU. Just want to use an IC with switches and potentiometers.</p>

<p>You can also use math if you want to explain.</p>

<p>Of course, one seeks to begin to arrest the mass as close to the end stop in the outbound case as one can without problems.</p>

<p>Thanks.</p>
","motor"
"3150","Rock envire - vizkit3d : Change environment visualization (envire lib) from ruby script","<p>I am using a ruby script to connent the Multi Layer Surface Map of the velodyne_slam component to the vizkit3D visualization.</p>

<p>The visualizazion plugin is loaded like this:
envireViz = Vizkit.default_loader.EnvireVisualization</p>

<p>It is possible to get the MLSVisualisation object from the EnvireVisualization in order to set visualization properties (like colors etc.) from the ruby script?</p>

<p>Rubys introspection abilities didn't help a lot here...</p>
","rock"
"3152","autoproj snapshot with git detached HEAD","<p>I need to search in the git history of a couple of packages to 
get back to a working state for a demo. I am searching by checking out 
commits manually until I found the commits of all effected packages that 
work together.</p>

<p>By checking out commits manually, I will get into the detached HEAD state:</p>

<p>$ git checkout 995e018
-> You are in 'detached HEAD' state. [...]</p>

<p>To save the current state of all packages, a snapshot is created:</p>

<p>$ autoproj snapshot demo_working</p>

<p>Now the demo_working/overrides.yml will pin the commit where the HEAD is 
pointing to (e.g. 5e2e3a259) instead of the commit that I chose manually 
for the package (995e018).</p>

<p>Is this the desired behaviour? In my opinion a snapshot should store the 
current state of all my git repositories meaning that I can also select 
commits manually.</p>
","rock"
"3155","One of the best ways to numerically integrate the velocity?","<p>I need to get position $x$ from integrating velocity $v$. One could use 1st order Euler integration as</p>

<p>$x_{t+1} = x_t + \delta * v_t.$</p>

<p>However, doing so leads to errors proportional to sampling time $\delta$. Do you know any more accurate solution please?</p>
","kinematics"
"3159","Cannot launch iRobot Create. Powers down upon minimal launch?","<p>I just got an iRobot iCreate base and I've followed the instructions given in  ROS Tutorials to setup the turtlebot pc and the workstation. I could successfully ssh into username@turtlebot through workstation so I'm assuming that is all good. I had an issue with create not able to detect the usb cable which I solved using the detailed answer given for question <a href=""http://answers.ros.org/question/46790/failed-to-open-port-devttyusb0/"" rel=""nofollow"">here</a>. This solved the problem of ""Failed to open port /dev/ttyUSB0"" that I was facing before. </p>

<p>Now the next step would be to ssh into the turtlebot (which I've done) and use <code>roslaunch turtlebot_bringup minimal.launch</code> to do whatever the command does (I've no idea what to expect upon launch). But apparently something's amiss since the create base chirps and then powers down after showing <code>[kinect_breaker_enabler-5] process has finished cleanly</code> as output and the log file location (see output below), but I dont see a prompt. I checked the battery and that's charged so that's not the problem. <strong>Following</strong> is the terminal output.</p>

<pre><code>anshul@AnshulsPC:~$ roslaunch turtlebot_bringup minimal.launch
... logging to /home/anshul/.ros/log/9d936a6a-fbdc-11e3-ba6b-00265e5f3bb9/roslaunch-AnshulsPC-5038.log
Checking log directory for disk usage. This may take awhile.
Press Ctrl-C to interrupt
Done checking log file disk usage. Usage is &lt;1GB.

started roslaunch server http://128.110.74.233:48495/

SUMMARY
========

PARAMETERS
 * /cmd_vel_mux/yaml_cfg_file
 * /diagnostic_aggregator/analyzers/digital_io/path
 * /diagnostic_aggregator/analyzers/digital_io/startswith
 * /diagnostic_aggregator/analyzers/digital_io/timeout
 * /diagnostic_aggregator/analyzers/digital_io/type
 * /diagnostic_aggregator/analyzers/mode/path
 * /diagnostic_aggregator/analyzers/mode/startswith
 * /diagnostic_aggregator/analyzers/mode/timeout
 * /diagnostic_aggregator/analyzers/mode/type
 * /diagnostic_aggregator/analyzers/nodes/contains
 * /diagnostic_aggregator/analyzers/nodes/path
 * /diagnostic_aggregator/analyzers/nodes/timeout
 * /diagnostic_aggregator/analyzers/nodes/type
 * /diagnostic_aggregator/analyzers/power/path
 * /diagnostic_aggregator/analyzers/power/startswith
 * /diagnostic_aggregator/analyzers/power/timeout
 * /diagnostic_aggregator/analyzers/power/type
 * /diagnostic_aggregator/analyzers/sensors/path
 * /diagnostic_aggregator/analyzers/sensors/startswith
 * /diagnostic_aggregator/analyzers/sensors/timeout
 * /diagnostic_aggregator/analyzers/sensors/type
 * /diagnostic_aggregator/base_path
 * /diagnostic_aggregator/pub_rate
 * /robot/name
 * /robot/type
 * /robot_description
 * /robot_pose_ekf/freq
 * /robot_pose_ekf/imu_used
 * /robot_pose_ekf/odom_used
 * /robot_pose_ekf/output_frame
 * /robot_pose_ekf/publish_tf
 * /robot_pose_ekf/sensor_timeout
 * /robot_pose_ekf/vo_used
 * /robot_state_publisher/publish_frequency
 * /rosdistro
 * /rosversion
 * /turtlebot_laptop_battery/acpi_path
 * /turtlebot_node/bonus
 * /turtlebot_node/port
 * /turtlebot_node/update_rate
 * /use_sim_time

NODES
  /
    cmd_vel_mux (nodelet/nodelet)
    diagnostic_aggregator (diagnostic_aggregator/aggregator_node)
    kinect_breaker_enabler (create_node/kinect_breaker_enabler.py)
    mobile_base_nodelet_manager (nodelet/nodelet)
    robot_pose_ekf (robot_pose_ekf/robot_pose_ekf)
    robot_state_publisher (robot_state_publisher/robot_state_publisher)
    turtlebot_laptop_battery (linux_hardware/laptop_battery.py)
    turtlebot_node (create_node/turtlebot_node.py)

auto-starting new master
process[master]: started with pid [5055]
ROS_MASTER_URI=http://128.110.74.233:11311

setting /run_id to 9d936a6a-fbdc-11e3-ba6b-00265e5f3bb9
process[rosout-1]: started with pid [5068]
started core service [/rosout]
process[robot_state_publisher-2]: started with pid [5081]
process[diagnostic_aggregator-3]: started with pid [5102]
process[turtlebot_node-4]: started with pid [5117]
process[kinect_breaker_enabler-5]: started with pid [5122]
process[robot_pose_ekf-6]: started with pid [5181]
process[mobile_base_nodelet_manager-7]: started with pid [5226]
process[cmd_vel_mux-8]: started with pid [5245]
process[turtlebot_laptop_battery-9]: started with pid [5262]
[WARN] [WallTime: 1403641073.765412] Create : robot not connected yet, sci not available
[WARN] [WallTime: 1403641076.772764] Create : robot not connected yet, sci not available
[kinect_breaker_enabler-5] process has finished cleanly
log file: /home/anshul/.ros/log/9d936a6a-fbdc-11e3-ba6b-00265e5f3bb9/kinect_breaker_enabler-5*.log
</code></pre>

<p><strong>Following</strong> is the log file: /home/anshul/.ros/log/9d936a6a-fbdc-11e3-ba6b-00265e5f3bb9/kinect_breaker_enabler-5*.log output:</p>

<pre><code>[rospy.client][INFO] 2014-06-24 14:20:12,442: init_node, name[/kinect_breaker_enabler], pid[5538]
[xmlrpc][INFO] 2014-06-24 14:20:12,442: XML-RPC server binding to 0.0.0.0:0
[rospy.init][INFO] 2014-06-24 14:20:12,443: ROS Slave URI: [http://128.110.74.233:51362/]
[xmlrpc][INFO] 2014-06-24 14:20:12,443: Started XML-RPC server [http://128.110.74.233:51362/]
[rospy.impl.masterslave][INFO] 2014-06-24 14:20:12,443: _ready: http://128.110.74.233:51362/
[xmlrpc][INFO] 2014-06-24 14:20:12,444: xml rpc node: starting XML-RPC server
[rospy.registration][INFO] 2014-06-24 14:20:12,445: Registering with master node http://128.110.74.233:11311
[rospy.init][INFO] 2014-06-24 14:20:12,543: registered with master
[rospy.rosout][INFO] 2014-06-24 14:20:12,544: initializing /rosout core topic
[rospy.rosout][INFO] 2014-06-24 14:20:12,546: connected to core topic /rosout
[rospy.simtime][INFO] 2014-06-24 14:20:12,547: /use_sim_time is not set, will not subscribe to simulated time [/clock] topic
[rospy.internal][INFO] 2014-06-24 14:20:12,820: topic[/rosout] adding connection to [/rosout], count 0
[rospy.core][INFO] 2014-06-24 14:20:20,182: signal_shutdown [atexit]
[rospy.internal][INFO] 2014-06-24 14:20:20,187: topic[/rosout] removing connection to /rosout
[rospy.impl.masterslave][INFO] 2014-06-24 14:20:20,188: atexit
</code></pre>

<p>From the logs, I could tell something told the create to power down. And since the log is named with 'kinect', I tried minimal.launch w/ and w/o kinect attached to the turtlebot pc. It doesn't make any difference. </p>

<p>Any clue what I might be missing? Or is this the way bringup works (I guess not)?</p>
","ros irobot-create"
"3160","iRobot Create without ROS?","<p>Is it possible to control the Create without using any ROS whatsoever? I know it has all these serial/Digital I/O pins that connect to ROS which controls it using drivers/libraries. But how hard would it be to do so using, say, a PCduino?</p>

<p>I'm asking this because I'm having trouble launching the create using ROS (<a href=""https://robotics.stackexchange.com/questions/3159/cannot-launch-irobot-create-powers-down-upon-minimal-launch"">question</a>)</p>
","irobot-create"
"3162","BS2 inconsistant pin state when connected to wire?","<p>I have a BS2 mounted on a Parallax Board of Education Rev D.
I was trying to use a wire to determine whether a control was pressed. 
however, whenever there's a wire connected the state seems to fluctuate between 1 and 0 instead of staying one or the other. when connected to the desired button it still exhibits this behavior but has the added quality of switching to zero when the button is pressed. ideally it will stay zero while the buttons pressed and 1 when it's not, but instead it flickers between 1 and 0 when unpressed. 
what causes this behavior and why does it occur even when the wire is not connected to anything except the bus?
the code used to get the state is </p>

<pre><code>DO
DEBUG CRSRXY,0,3,
""P5:"", BIN1 IN5,
LOOP
</code></pre>
","microcontroller"
"3164","Autodesk Inventor 2013: Rounding only at specific edge","<p>I am using Autodesk Inventor 2013 and I need to round a component of a device. I want to round the green marked edges, but not the red marked. But when I click ""round"", then the bottom edge will always be added to the rounding and I cannot de-select it. Any hints how to solve this problem?</p>

<p><img src=""http://i.stack.imgur.com/nR3wW.png"" alt=""Rounding problem in AutoDesk Inventor 2013""></p>
","design"
"3166","Maximum likelihood estimator (ML Data Association) EKF","<p>This question is an extension to <a href=""http://robotics.stackexchange.com/questions/3073/data-association-with-ekf"">my previous problem (Data association with ekf)</a>. My problem here is in the line 16 in the aforementioned link. </p>

<p>16. &nbsp; &nbsp; $ j(i) = \underset{k}{\operatorname{arg\,max}} \ \ det(2 \pi S^{k})^{-\frac{1}{2}} \exp\{-\frac{1}{2} (z^{i}-\hat{z}^{k})^{T}[S^{k}]^{-1} (z^{i}-\hat{z}^{k})\} $</p>

<p>When I compute this line, I'm getting huge number <code>1.0e+09 * 3.5230</code>. This is probability density function. Why is the pdf getting bigger than 1 in a huge way?</p>
","localization ekf"
"3170","Path comparison","<p>Problem: the cartesian position of an end effector (no orientation) of a robot arm is recorded, say, every millisecond (the time steps can not be changed), during a motion. The robot arm is commanded the same path but with different velocities. So I get different trajectories. I want to calculate the deviation of the paths, which is the distances of equivalent points of two paths. The problem is to find equivalent points. Since the two velocities are different the comparison at the same time steps of the trajectories makes no sense. I can assume that the paths underlying the trajectories to be compared are rather similar. The deviation for the ideal path being smaller than 1% of a typical length dimension of the path. I want to detect deviations of much lass than that. </p>

<p>I have to map the timestamp of the recorded points to the path length, and make comparison of points at the same path length. But of course also the path lengths differ for different paths, so any deviation would distort the result for all later points. How can I compensate for this ? </p>

<p>Is there a reliable algorithm ? Where can I find information ?</p>

<p>Note: time warp algorithms (even memory optimized ones) are out of the game because of memory consumption. </p>
","localization robotic-arm"
"3172","Making a Gripper Changer for a Robotic Arm","<p>How do you make a gripper changer for a robotic arm like <a href=""https://www.youtube.com/watch?v=_8ovd4khIBM"" rel=""nofollow"">this</a>? I don't see how you could connect power/control wires or what you use to hold the gripper to the arm.</p>
","robotic-arm"
"3173","Velocity Control via Vibration","<p>I am working on a robot that has an accelerometer. This accelerometer measures the vibration of the robot. When the robot hits a certain vibration, I would like it to slow down in order to reduce the vibration. I thought about a PID controller, but I don't think it would work. Does anybody have some input on different types of controllers I can use? </p>

<ul>
<li>Mechaman</li>
</ul>
","pid accelerometer navigation"
"3178","Controlling a system with PID that resists backdrive","<p>I'm controlling the angular position of a pendulum using a DC motor with a worm gearbox. Mechanically, worm gears are impossible to backdrive.</p>

<p>Using a PID controller on a pendulum system with a regular DC motor (no worm gear), the integrator would help the motor find the appropriate constant power setting to overcome gravity so the pendulum can hold any arbitrary position. With the worm gear, however, there is no need to apply constant power to the motor once the desired position is achieved. Power to the motor can be cut off and the worm gear will resist gravity's force to backdrive the pendulum to the lowest gravity potential.</p>

<p>It seems to me, then, that the integrator of the PID algorithm will cause large overshoots once the desired position is achieved. I want the integrator initially to help control the pendulum to the desired position. But once the position is achieved, I'd need the integrator to turn off.</p>

<p>The only solution I can come up with is to test for a special condition in the PID algorithm that checks if the position has been reached AND the angular speed is small, then instantaneously reset the integrator to zero. Is there a better way to handle the integrator in a system that resists backdrive?</p>

<p><strong>** EDIT *</strong></p>

<p>When I originally worded my question, I was mostly just interested in the academic approach of backdrive resistance in a PID loop. But it'll help if I explain the actual mechanism I'm building. The device is a robotic arm that rotates on a car window motor. It will also occasionally pick up and drop small weights at the end of the arm. Manufacturing variability in motors and the difference in drive torque when picking up the small weights led to me consider a PID loop.</p>
","pid"
"3181","How to calculate robot hand positions using Roll, Pitch angles","<p>i want to calculate humanoid robot hand position with given shoulder roll, pitch angles and elbow roll angle.</p>

<p>I'm able to calculate elbow position using rotation matrix which includes shoulder angles.</p>

<p>But i dont know how to calculate hand position using elbow position and elbow roll angle.</p>

<p>Can you propose a method to calculate hand position? </p>
","robotic-arm forward-kinematics"
"3183","Adding external magnets to a DC motor","<p>Is it possible to strengthen permanent magnet DC motors by simply attaching extra magnets on the outside of the motor casing - adding to the magnetic field?</p>

<p>If this is not possible, the question becomes; what happens if I replace the magnets inside the motor with better magnets?</p>

<p>I know that the coils will not handle more current than they currently do, so what will the net effect on the motor be?</p>
","motor"
"3188","Where to go to purchase parts for XY Plotter","<p>I am trying to build a 2ft square an XY Plotter. I have seen three designs so far: 1)Rack and Pinion 2)Threaded Screw 3) belt-driven. all these use a stepper motor to drive the system.</p>

<p>Each one has their obvious pros and cons but correct me if i am wrong, i believe the rack and pinion system is the most sturdy and easiest to put together.</p>

<p>I googled for Rack and pinion but all i get is industrial websites. Is there any place that sells cheaper rack and pinion sets for hobbyists? The payload of the XY Table is an eletro-magnet that isn't extremely heavy (maybe a half kilogram at most).</p>

<p>So obviously the motor must be strong enough to move anothe rack which will be significantly heavier than the payload.</p>

<p>This is my first real robotics project so i am new to all this.</p>
","stepper-motor motion actuator"
"3200","Manipulator link applied torque","<p>I want to implement a manipulator link using a physic library. I can only apply some torque to the centre of mass, but the torque should be applied at the beginning of the link.</p>

<p>Shifting a reference frame from the centre of mass and recalculating inertia tensor in the new frame is not a problem, neither is recalculating a new torque, based on the change of distance, but I think it is not the correct solution.</p>

<p>In short, how can I scale a torque of a control signal applied at the beginning of the link to a torque of a physic simulation applied to the centre of mass. Thanks.</p>
","simulator torque"
"3206","Current-limiting stepper motors for RepRap","<p>I have been working on a robot project for a while. Now I am tired of finding parts that just does the job, so it is time to do create parts.</p>

<p>A 3D printer will do the trick for many parts, but 3D printers share a lot with a CNC mill in terms of control and parts. So my question is this:</p>

<p>I am building a Reprap style printer, but I will use more heavy duty parts and motors, hoping to make a aluminum capable 3 axis mill later. I found some bipolar NEMA 23 stepper motors at 1.9 Nm and 3 Amps per coil. According to the reprap.org website, they recommend NEMA 17 and low voltage. Seems to me that they use voltage to limit the current.</p>

<p>Can I build a reprap, and use current limiting stepper drivers with an Arduino and some software I find online, and get away with these large stepper motors? Or am I in for a lot of trouble?</p>
","arduino stepper-motor stepper-driver cnc reprap"
"3207","Best microphone for speech recognition tasks","<p>I made several tests with different setups in order to achieve an acceptable speech recognition quality. It works well when I push a button to activate it but now I want it to be automatically activated when a user speaks. This is a big problem, especially when I only use the energy of the audio signal to guess when the user is speaking. That is why I thought about using a headset and not a distant microphone. In a headset the microphone is very close to the users mouth and it is easier to make correct guesses about when the user is speaking. Now my question is if bluetooth sets used with mobile phones also have such a property. They are not long enough and their microphone is not positioned exactly in front of the mouth. Is there a possibility that such devices can also capture some speech/noise from a distant user? Is there a significant difference in the signal energy coming from the user's speech and a 1 meter distant person's speech?</p>
","digital-audio speech-processing"
"3208","Electronic Speed Control Concepts","<p>I am a programmer who has never worked with electronics before. I am learning the concepts and hoping to build a quadcopter, with the control software entirely written by me. Motor control seems to be the most important part.</p>

<p>Is it true that the typical brushless DC motor and ESC (Electronic Speed Control) can only approximately control the speed?  That's because the ESC seems to have only a very approximate idea how fast the motor is revolving. This still works for a PID (Proportional Integral Derivative) controller because it gets indirect feedback from say a gyroscope whether the motor is going fast enough and so it can tell the ESC to make it revolve ""even faster"" or ""even slower"", and that's good enough.</p>

<p>Is my understanding in the above paragraph correct?</p>

<p>If so, I wonder whether a servo motor that can inform about its current rate of rotation could help do away with the ESC entirely? I feel that if the microcontroller can receive an input about motor speeds and send an output requesting a certain speed, it would not need the ESC. But I am not sure how servo motors work -- what happens immediately after you request 100rpm when say they were at 80rpm?</p>

<p>Since they cannot adjust the immediately, should the microcontroller immediately adjust other motors to account for the fact that not all motors are at 100rpm yet?  Does that imply that the microcontroller should only request very small deltas from the currently measured speed, so that the period of deviation from desired state is negligible?</p>

<p>In the latter model, of requesting only very small deltas from currently measured speed, the algorithm seems like it would not really be PID since there is no way to control the acceleration? But may be requesting the servo to go from 80rpm to 100rpm causes it to reach 81rpm much faster than requesting it to go from 80rpm to 81rpm?</p>

<p>I feel I know so little I cannot put my finger on it more precisely, but I hope this gives an idea of the concepts I am struggling to absorb.</p>

<p>To summarize, the questions are:</p>

<ul>
<li>can a servo (brushless dc) motor allow doing away with ESC?</li>
<li>does a servo motor accept control inputs such as ""revolve at 100rpm""?</li>
<li>does a servo motor offer an output saying ""i am at 80rpm now""?</li>
<li>does a servo motor at 80rpm go to 81rpm faster if it is requested to revolve at 100rpm versus at 81rpm?</li>
<li>the less precise questions implicit in the text above.</li>
</ul>

<p>(crossposted from <a href=""http://electronics.stackexchange.com/questions/118236/electronic-speed-control-concepts"">electronics.stackexchange</a>)</p>
","motor pid brushless-motor esc servomotor"
"3209","Can active sensor data be fed into an Autodesk Inventor Simulation?","<p>I'd like to drive the position of various components within a virtual assembly based on sensor data being collected in real time from an external device. Does Inventor support such a setup?</p>

<p>The goal is to match the relative movements of the components on screen to the real-world counterpart. For example, a absolute rotary encoder records the current angle of a physical joint and the virtual joint is rotated to match. Is this feasible?</p>

<p>My past searches for information on this have turned up empty; perhaps because I'm using the wrong search terms. Most results point to irrelevant mechanical stress simulations.</p>
","sensors kinematics"
"3210","Why production lines are so huge and power hungry?","<p>I'm thinking of starting my adventure in area of professional manufacturing. When I started to look onto machines I figured out that they are build somehow like in the 70s: huge footprint, big 3kW electric motors etc.</p>

<p>Is there any explanation why they are build in that way?</p>

<p>The only one I can think of is: they were developed long time ago and if it worked, it stays as it is.</p>

<p>BTW: If you know other place where to ask this question please let me know!</p>
","manufacturing"
"3216","Is this gear design feasible?","<p>I came up with an idea and am working with a mechanical engineer to design and prototype the idea but I keep sketching out my own ideas in the process and I just came up with this.. I'm quite sure this is not an idea he'll go with but I'm just kinda curious whether or not this would actually be feasible.  Or for all I know it's already common place, or totally stupid...  I dunno.</p>

<p>What do you think?</p>

<p><img src=""http://i.stack.imgur.com/JtLgo.jpg"" alt=""enter image description here""></p>
","mechanism movement"
"3218","Repairing non-lubricated linear actuator","<p>I have a Chinese CNC mill (CNC3020T, though several different devices go under this name), and its Z axis was very imprecise, often being randomly off position by as much as 0.5mm. I've disassembled the linear actuator and discovered several problems with it.</p>

<p>First problem is that they apparently forgot to lubricate the linear ball bearings. I make this conclusion because the rails have a set of grooves ground into them, and after wiping the rails with a tissue the only thing that is left is the finely powdered metal, with no traces of oil or other lubricant.</p>

<p><img src=""http://i.stack.imgur.com/eIQYD.jpg"" alt=""rails"">
<img src=""http://i.stack.imgur.com/bf2b4.jpg"" alt=""bearings""></p>

<p>Second problem is the nut. I expected to see a ballnut, but in reality it is just a piece of threaded PTFE! The leadscrew rotates smoothly in it, but there is quite some lateral movement, i.e. I can tilt it slightly without any opposing force.</p>

<p><img src=""http://i.stack.imgur.com/8KVRS.jpg"" alt=""PTFE nut""></p>

<p>Third problem is the overall mounting. In the picture below, the top left screw has been sheared in the factory and then they hid their mistake by tapping a larger thread and putting in a shorter screw that doesn't actually hold anything in the top plate.  So the whole assembly was fixed in three, rather than four, points. However, the remaining screw was quite tight.</p>

<p><img src=""http://i.stack.imgur.com/MSvwB.jpg"" alt=""assembly""></p>

<p>So my closely related questions are:</p>

<ul>
<li>Is the assembly even salvageable? How do I verify that linear ball bearings, the PTFE nut are relatively undamaged?</li>
<li>Can I just rotate the rails by 45° to get smooth surface again?</li>
<li>What do I lubricate the linear bearings with? Do I clean them before lubrication? I have an ultrasonic cleaner.</li>
<li>Any other advice on maintenance of the whole assembly? There may be something that I missed.</li>
</ul>
","actuator linear-bearing"
"3222","Predicting the impact point of a moving object","<p>Suppose we have a moving object (<i>a horizontal projectile motion as one of the most basic examples</i>). Is there any way to predict where it will hit finally? Please note that I'm looking for a machine learning method not a closed form solution.</p>

<p>Although we can track the motion, using Kalman filter, That is only applicable when we want to predict the new future<i>(As far as I'm considered)</i>. But I need to predict the ultimate goal of a moving object.</p>

<p>To better express the problem let see the following example:</p>

<p>Suppose a goalkeeper robot that of course uses filtering methods to smooth the ball motion. It needs to predict if the ball is going to enter the goal or not, before it decide to catch the ball or neglect it to go out.</p>

<p><b>Input data is a time series of location and velocity [x,y,z,v].</b></p>
","machine-learning"
"3225","Android Vibrating based on Arduino devices","<p>I want to make a simple device that causes my cellphone to vibrate for 30 seconds when my phone is 10 feet away from it. How would I go about doing that. How small could I make the device?</p>
","arduino"
"3227","Finding a Hydraulic Actuator to be controlled through a MCU","<p>I'm researching potential actuators I can use on a project i'm doing. I'm designing a creeper (platform for rolling under vehicles) that can lift you up just like the operation of those hospital beds. The creeper will have a joystick that will control up and down motion as well as the option to drive it forward,backwards,left and right.</p>

<p>I need an actuator that will <strong>support an average weight of 250lbs</strong> that would be able to lift a body of that weight. I was thinking of a hydraulic actuator but i'm not sure if these exist. <strong>I can very well have two actuators to share the load also.</strong> <strong>However, I need to control these actuators through a micro-controller unit</strong>. I'm planning on using a Raspberry Pi because I have an abundance of them mainly, but i'll be researching other potential units. </p>

<p>Therefore, my <strong>MAIN</strong> question is where can I find an actuator that would be a good fit for this type of project that can be integrated with a micro-controller unit? Does anyone have experience with this type of project or any important details I need to take into consideration that I'm not thinking of?   </p>
","microcontroller actuator"
"3231","Up to what force is a servo motor a reasonable choice as an actuator?","<p>I'm working on an application where I need to apply a linear or angular force to operate a linkage mechanism, but I don't (yet) know what amount of force I will need. I anticipate that it will be less than 4.5 kg (44 N). The travel distance on the linkage input should be less than 15 cm.</p>

<p>As I look through available servos, they seem to exist firmly in the scale-model realm of remote control vehicles, and as such I am uncertain if any will be suitable for my application. For example, one of Futaba's digital servos, the mega-high torque <a href=""http://www.futaba-rc.com/servos/digital.html"" rel=""nofollow"">S9152</a>, is listed at 20 kg/cm.</p>

<p>From what I understand, this means that at 1 cm from the center of the servo shaft, I can expect approximately 20 kg force. If I wanted 15 cm of travel distance I would need roughly a 10.6 cm radius, which would diminish the applied force to 20 / 10.6 = 1.9 kg, well below the 4.5 that might be required.</p>

<p><strong>Question:</strong></p>

<p>Is my understanding and calculation even remotely accurate? Should I be looking at other types of actuators instead of servos? They seem to become prohibitively expensive above 20 kg/cm torque. <em>(For the purposes of this project, the budget for the actuator is less than $250 US.)</em></p>

<p>For my application, I'd like to have reasonable control over intermediate positions across the travel range, good holding power, and fairly fast operation. For this reason I have dismissed the idea of using a linear actuator driven by a gearmotor and worm drive.</p>

<p>I am relatively new to robotics in the usage of motorized actuators, but I've used pneumatic cylinders for many years. For this application, I can't use pneumatics.</p>

<p><strong>Edit:</strong></p>

<p>Per comments, some additional constraints that are important:</p>

<ul>
<li><strong>Linkage Details:</strong> The linkage is a planar, one degree-of-freedom, part of a portable system (similar to a scissor lift mechanism). It is for a theatrical effect where the motion is amplified and force reduced (speed ratio and mechanical advantage are &lt; 1).</li>
<li><strong>Power:</strong> It will be carried by a person. As such, the actuation needs to be battery-operated, as no tubing or wiring can tether the person. Tubing or wiring that is self-contained is okay. Because this is a portable system, battery-power will be used. The control system will be designed specifically for an appropriate actuator. Rechargeable batteries up to 12V will most likely be employed. Actuators could operate on as high as 24V. Ideally a motor would not exceed 1-2 amperes draw, but as it is not in continuous operation, this is not a hard limit.</li>
<li><strong>Not Pneumatic:</strong> I've considered pneumatic actuation, using CO2 cartridges, for example, but the client would prefer not to use pneumatics. Also, the ability to stop/hold at intermediate points in the motion range is desirable, and somewhat more complicated to do with pneumatic actuators.</li>
<li><strong>Speed:</strong> An ideal actuator will be able to move the input coupling 15 cm in 1-2 seconds.</li>
<li><strong>Weight:</strong> Weight constraints are not well-defined. As it will be carried by a person, it should be moderately lightweight. The actuator itself should probably be less than 1kg, but certainly this can vary. (The rest of the mechanism will probably be 6-8 kg.)</li>
<li><strong>Size:</strong> The primary size constraint is that everything must fit within a space measuring no more than 500 x 500 x 120 mm (H x W x D). The linkage mechanism extends from and collapses outside the enclosure, parallel to the width.</li>
<li><strong>Noise:</strong> The quieter the better, but noise is the least priority.</li>
</ul>

<p>Servos seemed like the best choice for the job, but they don't seem to be available with the sort of torque I need.</p>
","servomotor"
"3233","Designing compatible spur gears for a robot gearbox","<p>I'm trying to increase the torque on the output shaft of my robot's gearbox.  I have a motor with a pinion attached to it with 8 teeth.  I want to create a gear with 33 teeth that will mesh with the pinion that I currently have.  I've got access to a 3D printer to make the gear, but I don't know how to design the second gear so that it will mesh properly.</p>

<p>What parameters do I need to know about the first gear (8 teeth) to ensure that the second gear (33 teeth) will mesh correctly?  How do I translate these parameters into the design of the second gear?</p>
","motor differential-drive"
"3234","Robot interaction language","<p>Is there any well documented robot interaction language? I would imagine something like taking a user's speech in English, parsing it using some natural language processing like NLTK or Stanford NLP and then building a new sentence understandable by the robot. Does something like this already exists?</p>

<p>I recently found ROILA <a href=""http://roila.org/language-guide/"" rel=""nofollow"">http://roila.org/language-guide/</a> but it seems like it is a whole different language and not just a reformulation of sentences using English words with less grammatical complexity.</p>
","speech-processing"
"4238","I need a software that will help me track passage and identify fish in clear water","<p>I am in charge of studying passage of different species of fish (six species) between lakes in Patagonian Andean range. We've been thinking of deploying video cameras underwater, but we'd need software that would control the cameras and record images only when the video adequately changes so as to avoid having to continuously check the video.</p>

<p>If the software is also capable of recognizing the species that would even be better.</p>
","software"
"4239","Good method for Retuning a PID After Detecting Oscillation","<p>Given a PID controller with an anti-windup, what are some practical ways to retune the controller once oscillation has been caused and detected? I have access to the magnitude and period of the oscillation.</p>

<p>I don't want to use the <a href=""http://en.wikipedia.org/wiki/Ziegler%E2%80%93Nichols_method"" rel=""nofollow"">Ziegler-Nichols method</a>; rather I'd like a method that allows me to specify a phase/gain margin as I am returning the system. </p>

<p>Could someone recommend me towards a book/article or theory?</p>
","control pid"
"4247","Tilt-compensated motor output to keep altitude for quadcopter","<p>The propellers of a multicopter produce thrust. Unfortunately the thrust is the smaller, the more the copter is tilted. I was currently wondering whether there is an established method to calculate how much the overall thrust has to be modified to hold the current altitude, based on the current attitude.</p>

<p>This is the way a calculate the motor output so far. rol/pit/yaw-output already ran through the PIDs. </p>

<pre><code>// Calculate the speed of the motors
int_fast16_t iFL = rcthr + rol_output + pit_output - yaw_output;
int_fast16_t iBL = rcthr + rol_output - pit_output + yaw_output;
int_fast16_t iFR = rcthr - rol_output + pit_output + yaw_output;
int_fast16_t iBR = rcthr - rol_output - pit_output - yaw_output;
</code></pre>
","quadcopter"
"4255","Software to simulate mechanics of production line","<p>Is there any software where I can simulate production line elements (joints, motors, springs, actuators, movement)? For example I want to simulate mechanism to unwind paper from big roll to weld it later with bubble foil and finally make bubble foil envelope, mechanism will look like this:</p>

<p><a href=""http://i.stack.imgur.com/WVROy.jpg"" rel=""nofollow""><img src=""http://i.stack.imgur.com/WVROy.jpg"" alt=""enter image description here""></a></p>

<p>I need it as simple as possible and preferably free.</p>
","mechanism simulator"
"4256","IMU based acceleration parameters for differential drive robot","<p>I have a differential drive robot whose motors are virtually quiet while driving on a completely flat surface, but the motors make a lot of noise when on a incline. This is likely due to the correction required to maintain speed with the high inertial load where the robot cannot accelerate fast enough for the PID to keep up.</p>

<p>But I noticed that some of the noise is related to acceleration, and the higher the acceleration, the smaller the amount of noise I hear, or the smaller the time the same level of noise lasts (up to a certain acceleration limit, otherwise the motors get really noisy again).</p>

<p>I am trying to find out of how to use an IMU that I have a available in order to change the acceleration based on how steep the path's incline is.</p>

<p>Any documentation (papers, tutorials, etc) about motion planning related to this topic that you can point me to?</p>
","ros imu differential-drive noise"
"4261","Stereo camera baseline not needed for calibration?","<p>I am doing stereo camera calibration as described in <a href=""http://blog.martinperis.com/2011/01/opencv-stereo-camera-calibration.html"" rel=""nofollow"">this blog post</a>. I wonder I do not need to input camera baseline for the calibration. The fact probably goes back to some very basic mathematics of triangulation. Can someone explain?</p>
","computer-vision calibration stereo-vision"
"4263","Visibility Graph Toolbox for Python","<p>I'm searching for a python toolbox/library to do visibility graph based motion planning. I have searched on the internet, but couldn't find anything. I'm probably missing out...</p>

<p>Is there any package, you can recommend me?</p>
","motion-planning python"
"4265","Determing limits of rotation in a robot workspace","<p>How to determine the limit range of end effector orientation (Roll-Pitch-Yaw) at one specific point(XYZ)?I had derived Forward/Inverse kinematic. I'm making a program for 6DOF articulated robot arm so that the user can know the limit of tool rotation in Global axis(Roll-Pitch-Yaw) at a certain point.</p>
","robotic-arm"
"4266","Are stereo camera calibration data standardized?","<p>Is there a standard format of how stereo calibration data (various matrices, usually saved in XML) are stored? Can I load calibration data generated say from a OpenCV script in C to another OpenCV script say in C++ or to completely different software where I create disparity image?</p>
","computer-vision calibration stereo-vision"
"4270","Solenoid to launch a ping pong ball","<p>I've been looking for ideas on how to launch a ping pong ball a small distance (&lt; 1 metre) for a game. Solenoids look like they might be useful but I'm not 100% on what force/type I need. I can mount it under a base and have the balls roll over it, with a pin pushing the ball up a ramp to it's target.</p>

<p>As it's only a ping pong ball, it should be light. I was considering something like this: <a href=""http://www.adafruit.com/product/412"" rel=""nofollow"">http://www.adafruit.com/product/412</a></p>

<p>Am I along the right lines? Or should I go back to the drawing board.</p>
","motor"
"4271","PD controller in C#","<p>I am currently building a line-following mobile robot. I've done all my image processing work in C#, and now I am in the control phase. I am looking for a PD controller program written in C# to start with. I've searched a lot but without success. My robot is not an Arduino based, it has a motherboard with a Core i3 CPU, and I am using a Camera not an LDR sensor.</p>
","control algorithm"
"4277","What is the difference between screw and wrench in rigid body motion?","<p>A screw is defined by a six dimensional vector of forces and torques. It can represent any spatial movement of a rigid body (<a href=""https://en.wikipedia.org/wiki/Screw_theory#Basic_concepts"" rel=""nofollow"">as written here</a>). But I don't get the following distinction between screw and wrench: </p>

<blockquote>
  <p>The force and torque vectors that arise in applying Newton's laws to a rigid body can be assembled into a screw called a <strong>wrench</strong>. </p>
</blockquote>

<p>It seems to be some kind of contextualisation but in what way?</p>
","dynamics theory"
"4281","What type of mechanism is this?","<p><img src=""http://i.stack.imgur.com/J9yMY.jpg"" alt=""enter image description here""></p>

<p>Held and rotated by the knurled ends, one in each hand, the silver spokes rise and fall in order for the assembly to rotate. What is it, some companies' salesmen show tool? Found in an old building, unit has no markings.</p>
","design joint"
"4285","sparse matrix in EKF SLAM","<p>I've successfully done with EKF Localization Algorithm with known and unknown correspondences that are stated in ""Probabilistic Robotics"". The results make perfect sense,so I can estimate the position of a robot without using GPS or odometry. Now I've moved to EKF-SLAM with known correspondences in the same book. I don't understand this matrix </p>

<p>$$
F_{x,j} = 
\begin{bmatrix}
1  &amp; 0  &amp; 0 &amp; 0  \cdots  0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 \cdots 0 \\
0  &amp; 1  &amp; 0 &amp; 0  \cdots  0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 \cdots 0 \\
0  &amp; 0  &amp; 1 &amp; 0  \cdots  0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 \cdots 0 \\
0  &amp; 0  &amp; 0 &amp; 0  \cdots  0 &amp; 1 &amp; 0 &amp; 0 &amp; 0 \cdots 0 \\
0  &amp; 0  &amp; 0 &amp; 0  \cdots  0 &amp; 0 &amp; 1 &amp; 0 &amp; 0 \cdots 0 \\
0  &amp; 0  &amp; 0 &amp; \underbrace{0  \cdots  0}_{3j-3} &amp; 0 &amp; 0 &amp; 1 &amp;  \underbrace{0 \cdots 0}_{3N-3j} \\
\end{bmatrix}
$$
What is exactly the bottom of this matrix? The following
$$
F_{x,j} = 
\begin{bmatrix}
0  \cdots  0 &amp; 1 &amp; 0 &amp; 0 &amp; 0 \cdots 0 \\
0  \cdots  0 &amp; 0 &amp; 1 &amp; 0 &amp; 0 \cdots 0 \\
\underbrace{0  \cdots  0}_{3j-3} &amp; 0 &amp; 0 &amp; 1 &amp;  \underbrace{0 \cdots 0}_{3N-3j} \\
\end{bmatrix}
$$
Is it as following (assuming N = 3)
$$
F_{x,j} = 
\begin{bmatrix}
1 &amp; 0 &amp; 0 &amp; 1 &amp; 0 &amp; 0 &amp; 1 &amp; 0 &amp; 0 \\
0 &amp; 1 &amp; 0 &amp; 0 &amp; 1 &amp; 0 &amp; 0 &amp; 1 &amp; 0 \\
0 &amp; 0 &amp; 1 &amp; 0 &amp; 0 &amp; 1 &amp; 0 &amp; 0 &amp; 1\\
\end{bmatrix}
$$
Or 
$$
F_{x,j} = 
\begin{bmatrix}
0 &amp; 0 &amp; 0 &amp; 1 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 \\
0 &amp; 0 &amp; 0 &amp; 0 &amp; 1 &amp; 0 &amp; 0 &amp; 0 &amp; 0 \\
0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 1 &amp; 0 &amp; 0 &amp; 0\\
\end{bmatrix}
$$
where ones' represent a specific landmark. </p>
","slam ekf"
"4287","Kalman filter Issue - GPS Odometry Fusion","<p>I am working on estimating a robots pose using Odometry and GPS.</p>

<p>My first problem is that all kinematic model i have seen for a differential drive robot proposes using the displacement of the left and right wheels to evaluate the robots next pose. However, in my situation the robot i have only spits out current X and Y pose relative to the starting point of the movement. can i use this as my state estimate
P = [x,y]T</p>

<p>P = [x0,y0] + [dx,dy] where dx and dy are change in respective coordinates gotten from the robots odometry.</p>

<p>if the above is posible how do i calculate the state covariance Q of the filter.</p>

<p>For GPS, how do i evaluate the covariance R; i have tried to collect multiple reading of latitude and longitude from a fixed point but i dont know if this is righ and i just dont get evaluate the covariance from these data (feeling dumb).</p>

<p>Thank you in anticipation.</p>
","mobile-robot kalman-filter gps odometry"
"4288","Tilt-compensated compass - at my wits' end","<p>I'm a bit at my wits' end here - I'm trying to build a tilt compensated compass for my autonomous sailboat (<a href=""https://github.com/kolosy/ArduSailor"" rel=""nofollow"">ardusailor</a>!). I'm using an InvenSense MPU9150. Originally, I used the built-in fusion support on the sensor to get a quaternion, pull the yaw/pitch/roll angles from that, and then use this formula to do the tilt compensation:</p>

<pre><code>float heading = atan2(-(mz * s_phi - my * c_phi), mx * c_theta + my * s_theta * s_phi + mz * s_theta * c_phi);
</code></pre>

<p>where the various s_angle is sin(angle) and c_angle is cos(angle). That didn't work. I tried using a vector-based approach stolen from <a href=""https://github.com/pololu/lsm303-arduino/tree/master/LSM303"" rel=""nofollow"">here</a>. That didn't work. Then, I took away the tilt compensation, and just did an uncompensated atan2(Yh,Xh), and that produced very strange result as well. </p>

<p>Basically, as I rotate the sensor about the z axis, the value rotates between 70 and -10 degrees, completing a full circle (i.e. as i make a 360 degree rotation, it starts at 70, gets to -10, and then back up to 70). 70 is at about 0* magnetic, 10 is at about 180, 0 is at about 70-80.</p>

<p>I see the same behavior from an HMC5883L magnetometer chip as well. The thing is, looking at raw values, I get magnetic values that seem fine, and hard and soft iron offsets are in place:</p>

<p><img src=""http://i.stack.imgur.com/a81c6.png"" alt=""enter image description here""></p>

<p>top row is corrected for offsets (using an ellipsoid fit method), bottom is raw. The numbers may look skewed, but they aren't - the scales aren't all the same. Graphs are, in order, x:y, y:z, x:z</p>

<p>What could this be?</p>
","compass magnetometer"
"4295","EKF Localization when robot is in parallel with a landmark.","<p>I'm facing a real weird problem with EKF Localization. The filer gives me wrong error every time the robot is in parallel with a landmark. I've debugged the code many times but failed to solve the problem however I found out where is exactly the problem occurs. The following picture shows the scenario. The robot moves in a circular motion. There are four landmarks. I have indicted in the picture where the filer gives me wrong angle for the estimated state.  As you see, when the robot is in parallel with all landmarks, I got a wrong angle for the estimated robot's pose. </p>

<p><img src=""http://i.stack.imgur.com/eEnwn.png"" alt=""enter image description here""></p>

<p>This is another picture shows how the estimated angle is wrong where the red circle is the estimated robot's pose and the blue one is the actual robot's pose. </p>

<p><img src=""http://i.stack.imgur.com/rjlEn.png"" alt=""enter image description here""></p>

<p>I did also track the problem numerically. What I found out is that the estimated measurement of landmark # 4 is in the opposite direction of the actual measurement of landmark # 4. </p>

<pre><code>i = 1 &lt;---- landmark 1 &lt;200,0&gt;

est_robot =
    6.4545
   21.1119
    0.1246

Zobs =
  194.9271
   -0.2208
    1.0000

Zpre =
  194.6936
   -0.2333
    1.0000

real_robot =
    6.2069
   20.9946
    0.1188

Mubar =
    6.2844
   21.7029
    0.1201

i = 2 &lt;---- landmark 2 &lt;200,200&gt;

est_robot =
    6.2844
   21.7029
    0.1201

Zobs =
  263.8102
    0.5982
    2.0000

Zpre =
  263.2785
    0.6239
    2.0000

real_robot =
    6.2069
   20.9946
    0.1188

est_robot =
    6.2901
   21.0100
    0.0155

i = 3 &lt;---- landmark 3 &lt;-200,200&gt;
est_robot =
    6.2901
   21.0100
    0.0155

Zobs =
  273.0734
    2.2991
    3.0000

Zpre =
  273.1173
    2.4114
    3.0000

real_robot =
    6.2069
   20.9946
    0.1188

est_robot =
    6.2840
   21.0462
    0.0259

i = 4 &lt;---- landmark 4 &lt;-200,0&gt;

est_robot =
    6.2840
   21.0462
    0.0259

Zobs =
  207.2696
    3.1272 &lt;--- the actual measurement of landmark 4
    4.0000

Zpre =
  207.3548
   -3.0658  &lt;--- this is the problem. (it should be 3.0658)
    4.0000

real_robot =
    6.2069
   20.9946
    0.1188

est_robot =
    6.0210
   20.8238
   -0.5621
</code></pre>

<p>and this is how I computed the angles. </p>

<p>For the actual measurements,</p>

<pre><code>Zobs = [          sqrt((map(i,1) - real_robot(1))^2 + (map(i,2) - real_robot(2))^2)        ;
            atan2(map(i,2) - real_robot(2), map(i,1) - real_robot(1)) - real_robot(3);
                                                                     i];


     % add Gaussian noise 
     Zobs(1) = Zobs(1) + sigma_r*randn();
     Zobs(2) = Zobs(2) + sigma_phi*randn();
     Zobs(3) = i;
 Zobs(2) = mod(Zobs(2), 2*pi);

 if (Zobs(2) &gt; pi) % was positive
    Zobs(2) = Zobs(2) - 2*pi;
 elseif (Zobs(2) &lt;= -pi) % was negative
    Zobs(2) = Zobs(2) + 2*pi;
 end
</code></pre>

<p>For the predicted measurements</p>

<pre><code>q    = (map(i,1) - est_robot(1))^2 + (map(i, 2) - est_robot(2))^2;
    Zpre = [                                                             sqrt(q);
            atan2(map(i,2) - est_robot(2), map(i,1) - est_robot(1)) - est_robot(3);
                                                                              i];

     if (Zpre(2) &gt; pi) % was positive
        Zpre(2) = Zpre(2) - 2*pi;
     elseif (Zpre(2) &lt;= -pi) % was negative
        Zpre(2) = Zpre(2) + 2*pi;
     end 
</code></pre>
","localization ekf"
"4296","Calculate object distance with camera","<p>Firstly I'm unsure whether this question belongs here or on another SE site (but I'll wing it for now).</p>

<p>I've recently been given the job of connecting up a 'smart camera' to a setup where a robotic arm will pick and place objects from point A to point B. The <em>real</em> application for the camera is to check if the objects are out of alignment to their supposed positions.</p>

<p>However I am curious to see if there is any way I can calculate the distance of an object given that I already know the objects actual size. Naturally the camera will see the object as bigger when closer and smaller when farther away but how can I turn this information into depth/distance from the camera?</p>

<p>I have not yet started using the camera. For now it is just an idea. I will assume that I can calculate what percentage of the view frame is taken up by the object.</p>

<p>For example if I have an object of uniform shape, I know that from dist1 it takes up 75% of the view frame and from dist2 it takes up 45% of the view frame.
<img src=""http://i.stack.imgur.com/WiWTh.png"" alt=""Basic concept""></p>

<p>Should this prove to be possible I imagine that it could have a number of different applications. /Anyway any feedback is appreciated. Thanks! ( :</p>
","localization calibration cameras"
"4297","Which micro-controler/processor to be used for autonomous stereo vision robot system?","<p>I am very new two robotics, however I have a working stereo algorithm which I want to combine with a SLAM algorithm. I am developing this system for an other application but I decided integrating it on a robot first and testing it might be a good way to get used to the system and test its behaviour in a realistic environment. (rather than testing it in some kind of software simulator only) However, I want the system to be autonomous and running on-board of the rover. </p>

<p>The system I am talking about will consist of:</p>

<p>a stereo camera
a rover with wheels with one motor each 
possibly some kind of sensor that ""measures"" the movement, e.g. how much the wheels turned
maybe some distance sensor </p>

<p>Appart from this it's only software
the stereo software is already developed, the SLAM algorithm not. Therefore it is currently impossible to say how much RAM it needs. I am currently running the stereo-vision only on an i7 in approx. 1s.</p>

<p>Now my question:
as mentioned I have no idea about robotics, and also my electronics knowledge is limited, so I have no idea what I need for this robot when it comes to the processor and the electronics. </p>

<p>I read some stuff about the Raspberry Pi and Arduino boards but I have no idea what to make from this. I am afraid that a Arduino will not be able to handle the computational load of the stereo vision and the SLAM algorithm but I read that Raspberry Pis are not the first choice when interfacing with sensors is needed (in this case my stereo cameras). Also I found the Leika kit which is a robotics kit for the Raspberry Pi. Maybe this would be a good option for me?</p>

<p>Maybe an entirely different system would be even more advisable? </p>

<p>Possibly someone else build an equally complex system before and can give me some advise form his/her experience? </p>
","arduino mobile-robot raspberry-pi slam stereo-vision"
"4299","Arduino with two Linear Actuators, two ACS714 Current Sensors, and an L298N Motor Driver setup","<p>I am using the L298N motor driver to drive two HAD1 linear actuators (12V each and a no-load drive current of ~950mA each)</p>

<p><strong>Linear Actuator</strong>: <a href=""http://www.alibaba.com/showroom/mini-linear-actuator-had1.html"" rel=""nofollow"">http://www.alibaba.com/showroom/mini-linear-actuator-had1.html</a></p>

<p><strong>Motor Driver</strong>: is a L298N dual-h-bridge motor-driver controller board-module for arduino robot</p>

<p>I am also using a current sensor per motor to get feedback of what the motor is doing (only sensors I have available, but I can detect of the motors are moving or stopped). I am using two ACS714 current sensors. The supply voltage for each is 4.5V to 5.5V and Supply Current is 10mA to 13ma:</p>

<p><strong>Current Sensor</strong>: is an ACS714 current sensor.</p>

<p>And Here is the circuit diagram that I made for my actual setup (an Arduino UNO, two current sensors, to linear actuators, and one motor drive):</p>

<p><strong>Circuit Diagram</strong>: 
<img src=""https://www.dropbox.com/s/ouxfrcutw5lcj6a/Actuator.jpeg?dl=1"" alt=""""></p>

<p>Will this setup work? Will I have enough current/power coming out of the 5V of the arduino to power both the L298N logic and the two ACS714 sensors?</p>
","arduino control actuator current circuit"
"4303","autonomous obstacle detecting quadcopter","<p>Is it possible to build a quadcopter which can detect obstacles and thereby avoiding them in order to reach its destination?
If so,how could it avoid the obstacles and how can the destination be set</p>
","quadcopter"
"4307","Stepper does not turn","<p>I always wanted to have a CNC to make PCB quickly at home. </p>

<p>Finally, I got a <a href=""http://rads.stackoverflow.com/amzn/click/B002ARTLUG"" rel=""nofollow"">7x7 kit</a> from zentools recently and put it together. I attached a battery powered screw driver to 2nd shaft of the stepper and moved the each axis all the way back and forward before wiring. All 3 axis moves smoothly, I can turn the steppers even by hand. Every piece works smoothly, no mechanical jam.</p>

<p>I decided to use <a href=""https://github.com/grbl/grbl"" rel=""nofollow"">GRBL</a> as controller software. Tested the software without the shield or stepper (qv: <a href=""http://cadduino.wordpress.com/2013/11/18/testing-grbl-in-arduino-board-without-the-motors/"" rel=""nofollow"">Testing GRBL in Arduino Board without the steppers</a>) I use <a href=""https://github.com/winder/Universal-G-Code-Sender"" rel=""nofollow"">Universal Gcode Sender</a> to communicate with GRBL.</p>

<p>I got an <a href=""http://blog.protoneer.co.nz/arduino-cnc-shield/"" rel=""nofollow"">Arduino CNC Shield</a> for Arduino UNO, put it together, attached to Arduino UNO, re-tested GRBL, it worked. </p>

<p>I used Reprep's <a href=""http://reprap.org/wiki/Stepper_wiring#.22pair.22_wires_on_6_wire_motors"" rel=""nofollow"">Stepper wiring</a> article to connect stepper to the driver, wired 1 stepper to the stepper driver (X axis). Powered the shield with 20V 17.5Amp (350W) DC Regulated Power supply. (It was the power adaptor for an old 17"" notebook. Notebook died, I kept the adaptor)</p>

<p>When the move 5 steps command (G1 X5) was sent, stepper makes a small move in the direction and then makes a grinding noise. (<a href=""https://www.youtube.com/watch?v=cNiFV6u2nSk"" rel=""nofollow"">Can be seen on Youtube</a>) </p>

<p>I tried switching 1st pair's cables, using another stepper driver (3 drivers), turning the potentiometer to increase the current, but still no luck.</p>

<p>I attached 2 photos of the cnc and the controller and controller unit.</p>

<p>I tried everything I can think of, any suggestions?</p>

<p><img src=""http://i.stack.imgur.com/Ubgr5.jpg"" alt=""CNC"">
<img src=""http://i.stack.imgur.com/hu2nb.jpg"" alt=""Controller (the DC adapter at the bottom)""></p>
","stepper-motor cnc"
"4308","What is the difference between the two different types of Mecanum wheels?","<p>While looking at Mecanum wheels, I noticed that there are two different designs that are popular.</p>

<p>One type holds the rollers in between the wheels frame, and the other holds the rollers from the center.</p>

<p>Is there a significant advantage to using one over the other?</p>
","wheel"
"4312","Low-cost centimeter accurate satellite positioning (GNSS/GPS)","<p>I am looking for a cheapest possible GPS setup with a centimeter precision without much HW hacking. I am not able to produce my PCB or do any soldering (though I would do that if there is no other way) so a kind of a easy-to-assemble setup would be welcome. I know about the <a href=""http://swiftnav.com/piksi.html"" rel=""nofollow"">$900 Piksi thing</a> but that is still too expensive for me. It seems like cm precision should be possible for much less - like employing a 50 USD raw GPS sensor with an antenna and ordinary PC with RTKLIB software.</p>

<p>I am not sure if it is better to use two GPS sensor setup for RTK (one base station and one for rover) or whether I can get the corrective DGPS data elsewhere (my region is Czech Republic - there seems to be <a href=""http://czepos.cuzk.cz/"" rel=""nofollow"">national grid</a> here allowing to stream correction data for reasonable cost).</p>

<p>My application will be in a passenger car so I will not be limited with power source - no low power needed although that would be nice. I will be using the position readings within OpenCV - so I need to get the data into C/C++ code. The application is data collection so I can use raw GPS post-processing.</p>
","gps"
"4313","PID Integration over not constant dt (∆time)","<p>Is integration over not constant dt (∆time) a possible thing? Let's say you have a PID loop with differentiating frequency, can the integral part of it still work? (Assuming you know the dt from the last iteration)</p>

<p>Could I just use a variable dt (∆time) in my calculation and the PID principle would still function correctly?</p>
","quadcopter pid real-time"
"4316","Compact design - building from off-the-shelf components","<p>I want to build a small cylindrical arm, with a main 360º angular servo on the longitudinal axis, and a secondary angular servo with variable speed in a trasversal axis that rotates with the main angular one. The secondary needs to receive data and power from a slip ring across the main servo, since it must be able to rotate freely and must not be binded by wirings. The width of the cylindrical arm must be below 0.4 cm</p>

<p>I've reviewed the market for off-the-shelf servos and there are a few that could fit the bill for the main servo, I know where to obtain the slip ring required, but it beats me where to obtain the secondary servo, since the space limitations demand that it is really small (&lt; 0.2 cm) and the smallest I've been able to find on internet are 0.5 cm</p>

<p>Any suggestions are greatly welcome!</p>
","servomotor"
"4318","Cascading PID DC Motor Position & Velocity Controllers","<p>I'm trying to build a robot with a differential drive powered by two DC Motors. First I implemented a PID Controller to control the velocity of each motor independently. Estimated the TF using the MATLAB's System Identification Toolbox, of the open loop system by the acquiring the velocity of each wheels encoder in function of the PWM signal applied by an Arduino microcontroller. All went well and i successfully dimensioned the PID gains for this controller.</p>

<p>What I'm trying to accomplish now is to control the exact (angular) position of the DC Motor. I thought in cascading a PID controller in the input of the other already implemented. So this way, I can give a position to the first controller, which will be capable of generate an output reference to the second (velocity) controller so it generates the appropriate PWM value signal to drive the DC Motor accordingly.</p>

<p>Will it work? Is that a good approach? Or should I try to implement a different controller which outputs the PWM signal in response to a position reference signal?</p>

<p>Many thanks for your attention and I hope somebody can help me with these doubts.</p>
","arduino control microcontroller pid wheeled-robot"
"4323","Exoskeleton Drive System Help","<p>I am currently working on an exoskeleton.  The exoskeleton is going to help kids with cerebral palsy learn to walk 4 years sooner than traditional therapy.  Currently we are using 2 Ame 226-3003 with the roboclaw 2x60A motor controller controlled by an Arduino mega.  The Ame 226-3003 motors are not powerful enough.  In addition the Ame 226-3003 has a worm gear thus the motor cannot be moved when the motor is turned off.  Our position feedback system is a gear attached to the shaft of the motor which spins a gear on a potentiometer.  The two gears have a 1:1 ratio.  </p>

<p>In order to better understand the project, please see the video: 
<a href=""https://www.youtube.com/watch?v=NL_aCwJSRiE&amp;feature=youtu.be"" rel=""nofollow"">https://www.youtube.com/watch?v=NL_aCwJSRiE&amp;feature=youtu.be</a></p>

<p>The Ame 226-3003 catalog page: 
<a href=""http://www.amequipment.com/wp-content/uploads/2013/02/801-1071-web.pdf"" rel=""nofollow"">http://www.amequipment.com/wp-content/uploads/2013/02/801-1071-web.pdf</a></p>

<p>We need a new drive system:</p>

<ul>
<li>more powerful than the Ame 226-3003 motor.  We do not have an exact torque spec but we believe any drive system that is 70-100% more powerful than the Ame 226 - 3003.  </li>
<li>We like the rpm range of the Ame 226-3003.</li>
<li>The drive system must be able to spin freely when the motor is not in use.  </li>
<li>We need a way to get position feedback, the potentiometer system we are using seems to work, however it adds to much extra hardware(more stuff to break), (ie) the gear on the potentiometer and the gear on the shaft have to mesh constantly and we have to zero the potentiometer every time we put the leg together so the potentiometer doesn't over spin.  * We would prefer to have an optical encoder inside the motor.</li>
<li>We need to have the drive system be at a right angle.</li>
</ul>

<p>I need help designing a drive system that will meet the requirements.  </p>

<p>I think I might have found a motor that will work:</p>

<p>The amp flow G43-500</p>

<p><a href=""http://www.ampflow.com/standard_motors.htm"" rel=""nofollow"">http://www.ampflow.com/standard_motors.htm</a></p>

<p>I like the G43-500 because it can run at 24 v, thus it will take less amps than 12v. Will that motor get the job done?</p>

<p>I need to gear this down to around 80rpm.  What type of gear box would work best?</p>
","motor control microcontroller motion-planning encoding"
"4326","Is there a rule of thumb for actuator torque overhead?","<p>When installing a servo or other actuator, I measure the force needed to perform whatever action is needed and find an actuator that can generate <em>more</em> than the necessary force. However, I recently wondered if there's a rule of thumb or guideline for <em>how much</em> overhead is useful, before it becomes a waste.</p>

<p>For a (perhaps oversimplified) example, say I have a lever to lift something, and the force needed is 100 Newtons. An actuator that can manage 100 N maximum will have problems with this and stall, with any sort of friction or other imperfections. I would use an actuator that can produce 150 or 200 N - whatever is available and fits the design and budget. After testing, it may become apparent that 200 is overkill, 120 is sluggish, but 150 is good. Other than trial and error, is there a way to measure this, or a rule of approximation?</p>

<p>I realize that variables in the mechanics and construction can significantly alter what force might is needed to be considered ideal, but is there a commonly accepted value for simple applications? Something like ""If you need x force, install an actuator with x + 20% force.""</p>
","actuator force"
"4327","Best technique to built an ejectable drawer?","<p>I want to build a closet with ejectable drawers. On the top should be 4 buttons, each eject opening one of the four drawers of the closet. </p>

<p>I am looking for ideas on how to accomplish this. What kind of springs, slider mechanisms perhaps, and other materials to use? </p>

<p>Any examples?</p>
","motion"
"4329","How to machine aluminium on a low budget?","<p>For my robotic projects I need some aluminium parts. Currently I am looking for a way to build a chassis including simple gear box. So I need relatively high precision. Which options do I have to machine aluminium without investing in expensive tools?</p>

<p>This is what I could think of so far.</p>

<ul>
<li>Design parts in CAD and send them to a third party company for fabrication. The problem with this is that hobby projects almost never need large quantities and piece production can be still expensive.</li>
<li>Buy cheap tools to work aluminium by hand. I don't know which tools would fit this task best. Moreover, the results might be inaccurate, which is a problem for designs with moving parts.</li>
<li>Find someone with a CNC who let's me machine my parts. This would most likely result in very slow prototyping cycles though.</li>
</ul>

<p>A method that I can do at home with not too expensive tools would be perfect, but I'm looking forward to every solution.</p>
","cnc chassis"
"4336","Position Control of an Omni Wheel Drive Robot","<p>I want to create a robot that will navigate on a desired path!
That path can be a straight line or a circular path with a given radius.</p>

<p>I will use 3 or 4 omni wheel drive platform and for positioning,
I am using this research paper which perform dead-reckoning using mouse sensors.</p>

<p><a href=""https://www.google.co.in/url?sa=t&amp;rct=j&amp;q=&amp;esrc=s&amp;source=web&amp;cd=1&amp;cad=rja&amp;uact=8&amp;ved=0CBsQFjAA&amp;url=https%3A%2F%2Fcourses.cit.cornell.edu%2Fee476%2FFinalProjects%2Fs2009%2Fncr6_wjw27%2Fncr6_wjw27%2Fdocs%2Fdead_reckoning_with_mouse_sensors.pdf&amp;ei=5nvcU42dG4mdugSIooBY&amp;usg=AFQjCNGUXJ_pM78JFv-N1wEq9OjSiUmQtA&amp;sig2=aX-6u7jQK3wwtEeLbKbIaQ&amp;bvm=bv.72197243,d.c2E"" rel=""nofollow"">Dead-Reckoning using Mouse Sensors</a></p>

<p>I've understood that I will get x, y and θ positions, which are actual positions of robot.
These can be used to calculate the error and then using PID to compensate the error.</p>

<p>But, to find the error, I must have the desired position of the robot at that moment!</p>

<p>For example, the Robot is at (0,0) and it needs to move in a circular path of equation</p>

<p>$$ x^2 + y^2 - 10y = 0 $$</p>

<p>Now, I want to calculate the position at t = 2 sec, how to do that?</p>

<p>If someone has already done similar stuff, please post the link. I am not able to find any resource on web!</p>
","mobile-robot pid kinematics wheeled-robot motion"
"4340","Matlab for moving a robot towards the detected block","<p>the matlab code is used to detect red colored object, but i want to control a bot to move towards the detected object. just need a simple algorithm or idea, controlling the servo i will be able to do it.    </p>

<p><img src=""http://i.stack.imgur.com/x0Jvm.png"" alt=""Detailed Diagram""></p>

<pre><code>%get snapshot
data = imread('image.jpg');
% Now to track red objects in real time
% we have to subtract the red component 
% from the grayscale image to extract the red components in the image.
diff_im = imsubtract(data(:,:,1), rgb2gray(data));
%Use a median filter to filter out noise
diff_im = medfilt2(diff_im, [3 3]);
% Convert the resulting grayscale image into a binary image.
diff_im = im2bw(diff_im,0.18);

% Remove all those pixels less than 300px
diff_im = bwareaopen(diff_im,300);

% Label all the connected components in the image.
bw = bwlabel(diff_im, 8);

% Here we do the image blob analysis.
% We get a set of properties for each labeled region.
stats = regionprops(bw, 'BoundingBox', 'Centroid');

% Display the image
imshow(data)

hold on

%This is a loop to bound the red objects in a rectangular box.
for object = 1:length(stats)
    bb = stats(object).BoundingBox;
    bc = stats(object).Centroid;
    rectangle('Position',bb,'EdgeColor','r','LineWidth',2)
    plot(bc(1),bc(2), '-m+')
    a=text(bc(1)+15,bc(2), strcat('X: ', num2str(round(bc(1))), '    Y: ', num2str(round(bc(2)))));
    set(a, 'FontName', 'Arial', 'FontWeight', 'bold', 'FontSize', 12, 'Color', 'yellow');
end



hold off
</code></pre>
","mobile-robot microcontroller wheeled-robot robotc"
"4346","how to plot a line between two centroids in matlab","<p><img src=""http://i.stack.imgur.com/J61he.jpg"" alt=""Curve""></p>

<p>I am able to locate centroids of each blocks, but i am unable to join two blocks with a line segment by avoiding the obstacle as shown in the figure. Please need help how do i achieve this using matlab.</p>
","computer-vision motion-planning navigation matlab"
"4348","BeagleBone uart/spi/i2c latency","<p>There seems to be consensus here that the BeagleBone Black has 1ms+ latency while toggling gpio pins due to the fact that gpio is handled outside of the cpu. Are the uart/i2c/spi lines equaly slow, or are they significantly faster?</p>

<p>I've seen references to people talking to the gpio <a href=""http://stackoverflow.com/questions/13124271/driving-beaglebone-gpio-through-dev-mem"">more directly</a>. Could this decrease uart/i2c/spi latencies as well?</p>
","i2c beagle-bone"
"4357","Device to roll a mat","<p>I'm looking for some direction on how to create a device that does the following:
Imagine you have a yoga mat, I want a device that can roll it up and unroll it without a human intervening in the rolling process.
I reliable this is a robotics forums but there doesn't appear to be a section for mechanical engineering so I'm posting my question here.</p>
","motor mechanism"
"4359","DIY Laser cutter for small Acrylic robot baseplate","<p>Need help to choose <strong>desk top, low cost, DIY/High school grade laser cutter</strong> for making base plate for DIY robots, about maximum A4 paper size, as this photo.  Idea, comment, advises, even if only partially cover some and not all questions, are welcome.</p>

<p><img src=""http://i.stack.imgur.com/wBfiY.jpg"" alt=""enter image description here""></p>

<ol>
<li><p>What power is needed to cut Acrylic 3 to 5mm thick. Many sellers at 40 to 60 watts range. What can these do?</p></li>
<li><p>How cut thickness depends on cut speed? To what extend can I choose slow speed to cut thicker sheet.</p></li>
<li><p>Does cut thickness depends on color/clear acrylic? It is CO2 laser. </p></li>
<li><p>Some units have options, like air blower and honeycomb bottom plate. What are their functions? what options are useful for this case.</p></li>
<li><p>Which CAD 2D drawing software is best supported by these range of products?</p></li>
<li><p>Apart from main function of cutting flat acrylic plate, some has additional Z axis motor to rise/lower work piece for engrave/photo/line_letter marking on 3D objects. What software is needed to support these 3D operation.</p></li>
</ol>
","laser"
"4360","Natural frequency computation (for PID gains computations)","<p>I am currently trying to parametrize the low-level gains of a robotic arm. This arm uses a classical PID for each joint.</p>

<p>I am trying to use a method based on computation rather than a trial-and-error/tweaking approach. The method I use considers each joint independently and assumes the system driven by the PID is linear. Hence I infer a transfer function, a characteristic polynomial, poles and this gives me gains $K_p$, $K_i$, and $K_d$ for each joint.</p>

<p>Now, computed as I did, these gains depend on the natural angular-frequency. For example:
$$
K_p = 3 a w^2
$$
where $a$ is the inertia and $w$ is the natural angular-frequency.</p>

<p>Hence my question: <strong>how shall I compute $w$, the natural angular-frequency for my system</strong>? Is this an enormous computation involving the geometry and other complex characteristics of the robot, and that only a computer can do or are there simple assumptions to be made which can already give a rough result for $w$?</p>

<p>I guess this is a complex computation and this is one of the reasons why PID gains are most often found by trial-and-error rather than from computation. Though I am looking for some more details on the subject to help me understand what is possible and what is not.</p>

<p>Kind regards,</p>

<p>Antoine</p>
","control pid robotic-arm"
"4362","Mechanical or electrical engineering for robotic and automation?","<p>I have decided to pursue a career in automation and robotic. At the moment, I am being torn between Mechanical and Electrical Engineering. I know that both of them relate to my choices of career, and at the moment, I think that I like them equally. I hope you guys can help me solve my dilemma by using your insights/experiences to assist me with the following questions:   </p>

<p>1/ From your experiences and opinions, which of the two engineering fields is generally more crucial and challenging, especially in an automation/robotics project?<br>
2/ Which will see an increase in demand and importance in the near future? Which of them might become outdated/obsolete or at least develop at a slower rate compare to the other?(I have a feeling that EE has a slight edge over this matter; however, I am not so sure)
3/ Which of the fields is more versatile? Which is more physical demanding (I am actually quite frail)<br>
4/ Which is generally easier to self-study? Robotics is obviously an incredibly broad and complex field and I have prepared to step outside of my comfort zone and do lots of studying by myself to achieve my goals and passion.  </p>

<p>I could probably come up with a few more questions; however, I am sure that you guys got the gist of my puzzle. Thank you very much and I apologize if there is any grammatical error.</p>
","beginner automatic"
"4364","Denavit-Hartenberg parameters for SCARA manipulator","<p>I'm going through the textbook Robot Modeling and Control, learning about the DH convention and working through some examples. </p>

<p>I have an issue with the following example. Given below is an image of the problem, and the link parameter table which I tried filling out myself. I got the same answers, except I believe there should be a parameter d1 representing the link offset between frames 1 and 2. This would be analogous to the d4 parameter. </p>

<p><img src=""http://i.stack.imgur.com/jc3Ax.png"" alt=""enter image description here""></p>

<p>If anyone could explain why I might be wrong, or confirm that I have it right, that would be great. I hate it when it's me against the textbook lol. </p>

<p>Cheers.</p>
","forward-kinematics dh-parameters"
"4366","PID in a system with pole at origin","<p>I've seen in a lot of places some methods of tuning a PID controller. Most of them will say that one should apply a step input to the system and based on that response you can tune the PID parameters following some rule of thumb. But what about a system which one of its pole is at origin? In other words, a step response on a system like that will have an infinitely increasing ramp in the output (theoretically).</p>

<p>An example:</p>

<p>let's say we have a spinning wheel (fixed at center) and all we can control is the amount of torque applied to make it spin. If we can read its position (angle) and we want to design a PID controller to set its position (more or less like a step-motor). How can that be done? Note that a step input in this case will be a constant torque and this will make the wheel spin faster and faster. How should one proceed?</p>
","pid"
"4369","Artificial Potential Field navigation","<p>I've been working on my two-wheeled mobile robot I've been trying to perfect my obstacle avoidance algorithm which is Artificial Potential Field method . Also i use Arduino Uno kit . The basic concept of the potential field approach is to compute a artificial potential field in which the robot is attracted to the target and repulsed from the obstacles. The artificial potential field is used due to its computational simplicity. the mobile robot applies a force generated by the artificial potential field as the control input to its driving system . the Artificial Potential Field method in its computations depends on the distance between robot and goal or target and the distance between robot and obstacles that effected the robot (which could easily get for ultrasonic senors)</p>

<p>I applied the Artificial potential field method in Matlab environment / simulation and it is done successfully , really what I need in the simulation is to get the current position of mobile robot and position of goal as x, y coordinates (to compute the distance between robot and goal) and the obstacles positions.</p>

<p>The output of the Artificial potential field is the desired angle to avoid obstacle and reach to the goal , the method give the robot the angle the pointed to the goal then the robot goes toward that angle and if the robot face an obstacle in his way (got from sensor reading) the Artificial potential field will update the angle to avoid the obstacle and then re-give the robot the angle that pointed to the goal and so on.</p>

<p>The question is how could I apply the Artificial potential field method in real would? what should I get? is it easy to do that or it is impossible? </p>

<p>I had Rover 5 with two normal DC motors and two encoders (incremental rotary encoder) per each wheel.</p>

<p>Any Help or suggestion on the topic will be highly appreciated please.</p>

<hr>

<p><strong>Edit:</strong> Based on the response from Shahbaz.</p>

<p>The case is very simple, but first, there is something to know that I constrained with some limitations that I couldn't overstep them, one of them is that the real world should be exactly as simulation for example in the simulation I consisted that robot started with (0,0) on coordinates  x, y axis and I should put the goal point for example (10,20)  and feed this point in the Artificial potential field method and then compute distance between  robot and goal (so I don't need any technique to determine the position of goal) and I don't know if I could applied that.</p>

<p>The second constraint is that I should use the encoders of wheels to determine the current position of mobile robot and its orientation depending on a calculation formula (something like this <a href=""http://rossum.sourceforge.net/papers/DiffSteer/"" rel=""nofollow"">here</a>) even if that will be inaccurate.</p>

<p>I had a Rover 5 with two normal DC motors and two encoders (incremental rotary encoder) per each wheel, each encoder has four wires I don't know how to deal with them yet, and how could I translate the pulses of encoders or how to work out the x.y position of your robot based on the shaft encoder data.</p>

<p>I am still searching for ….</p>
","mobile-robot"
"4370","rock/syskit: How to add multiple instances of same component into a network","<p>I've been going through Syskit tutorials at rock-robotics.org. In the tutorials e.g. <a href=""http://rock-robotics.org/stable/documentation/system_management_tutorials/200_first_composition.htm"" rel=""nofollow"">First composition</a>, there are two different components declared with:</p>

<pre><code> add Controldev::JoystickTask, :as =&gt; ""cmd""
 add RockTutorial::RockTutorialControl, :as =&gt; ""rock""
</code></pre>

<p>I was wondering how could I add an additional <code>RockTutorialControl</code> into the composition, so that the instantiation would then create two separate instances of the same component?</p>

<p>I've tried something like</p>

<pre><code>add RockTutorial::RockTutorialControl, :as =&gt; ""foo""
</code></pre>

<p>but this apparently isn't the way to go. <code>syskit instanciate</code> command shows only one instance of RockTutorialControl, but gives two roles to it (rock and foo). What is the meaning of ""role"" in this context?</p>

<p>I've noticed that the tutorial explains how to make multiple instances of the same component when we're declaring our components as <a href=""http://rock-robotics.org/stable/documentation/system_management_tutorials/800_devices.html"" rel=""nofollow"">Devices</a>. But how to do this with components that should not be concerned as devices?</p>

<p>BR,
Mathias</p>

<p><strong>EDIT:</strong></p>

<p>This was my first question to StackExchange, and I don't know what's the policy for adding additional information to the original question, but here we go:</p>

<p>It seems that both the deployment and configuration need to be different when there are two instances of the same component. I did a small scale testing with two components:</p>

<pre><code>using_task_library 'foobar_user'
using_task_library 'foobar_proxy'

module FooModule
  class FooControl &lt; Syskit::Composition

  add FoobarUser::Task, :as =&gt; ""producer"" 
  add FoobarProxy::Task, :as =&gt; ""proxy""
  add FoobarUser::Task, :as =&gt; ""consumer"" 

  producer_child.connect_to proxy_child
  proxy_child.connect_to consumer_child
  end
end
</code></pre>

<p>where <code>FoobarUser::Task</code> has an input &amp; output port of /std/string. <code>FoobarProxy::Task</code> has corresponding i&amp;o ports. <code>FoobarUser::Task</code> has also two configurations called 'default' and 'other'. It also has two deployments 'foo_depl' and 'bar_depl'.</p>

<p>In order to create a ""pipeline"" where data flows producer ==> proxy ==> consumer, I made define line:</p>

<pre><code>define 'conf_and_depl', FooModule::FooControl.use('producer' =&gt; FoobarUser::Task.use_conf('other').prefer_deployed_tasks(/foo_depl/), 'consumer' =&gt; FoobarUser::Task.use_conf('default').prefer_deployed_tasks(/bar_depl/))
</code></pre>

<p>and then instanciated the network with </p>

<pre><code>syskit instanciate scripts/03-nwtest.rb conf_and_depl_def!
</code></pre>

<p>The component instanciation failed if either <code>use_conf</code> or <code>prefer_deployed_tasks</code> clause was left out. In both cases the produced error was ""cannot deploy the following tasks... ...multiple possible deployments, choose one with #prefer_deployed_tasks"". </p>
","rock syskit"
"4371","Dynamic tracking precision of UR5/10","<p>I am willing to use a universal robot arm (UR10) in a path following mode. i.e. I have a desired trajectory for the robot's effector and I would like the effector to follow it as close as possible.</p>

<p><a href=""http://media1.limitless.dk/UR_Tech_Spec/UR10_GB.pdf"" rel=""nofollow"">The specs here</a> give a repeatability of +-0.1mm. This is not written but <em>I guess</em> this is the <strong>static precision</strong> (after the robot had enough time to converge to the position). Now what about the <strong>dynamic precision</strong> (i.e. max position error while performing the desired trajectory)?</p>

<p>Does anyone know more than me on this matter?</p>

<p>Kind regards,</p>

<p>Antoine.</p>
","dynamics errors"
"4379","Torque control and monitoring of servo","<p>I am trying to control the servo motor operation by torque control by interfacing the sensor to an avr , which will continuously monitor the torque value from the sensor and control the torque according to the given set point .Is it possible to make such a setup? If yes how?</p>

<p>Thanks.</p>
","torque servomotor avr"
"4380","Mechanism to oscillate a needle like object in vertical motion","<p>I need to pop out a needle like object(toothpick,matchstick,etc) from a hole in a
surface and push it back in automatically.I need to make a array of such needles in which each needle's position can be controlled individually.The objects aren't supposed to be oscillated continuously, instead they are to be locked in one of the two positions-either above the surface or inside it.
I am trying to search a mechanism to achieve this.This can be easily done with a simple DC servo motor, but the problem is I have to do this in very limited space-about 6 such objects in base area of 3 cm x 3 cm.Moreover the power source would be DC +5 V</p>

<p>So far I have thought of creating small electromagnets with springs,but still not sure about it.Any inputs will be appreciated.</p>

<p><img src=""http://i.stack.imgur.com/ibpBC.png"" alt=""just a image to depict what I am trying to achieve""></p>
","mechanism"
"4384","translation the shaft encoder data","<p>I am designing a robot in real world and i want to plot everything in X,Y (Cartesian) coordinates </p>

<p>I just want to use the encoders of wheels to determine the current position of mobile robot and its orientation depending on a specific calculation formula (like this <a href=""http://rossum.sourceforge.net/papers/DiffSteer/"" rel=""nofollow"">http://rossum.sourceforge.net/papers/DiffSteer/</a> ) even if that will lead to inaccurate calculations .</p>

<p>Actually , I found out this formula below to compute x, y coordinates from encoder data but I still confused in some sides of this formula </p>

<p><img src=""http://i.stack.imgur.com/QbN00.jpg"" alt=""enter image description here"">
<img src=""http://i.stack.imgur.com/Pl42O.jpg"" alt=""enter image description here""></p>

<p>I had a Rover 5 chassis form Dagu with two normal DC motors and two encoders (incremental rotary encoder) per each wheel, how could I translate the pulses of encoders or how to work out the x.y position of the robot based on the shaft encoder data.</p>

<p>I deduced some of values from Rover 5 chassis :</p>

<p>cm = conversion factor that translates encoder pulses into linear wheel displacement</p>

<p>Dn = nominal wheel diameter (in mm)
:
<strong><em>about 20 Cm</em></strong></p>

<p>Ce = encoder resolution (in pulses per revolution)
:
<strong><em>Encoder resolution: 1000 state changes per 3 wheel rotations</em></strong></p>

<p>n = gear ratio of the reduction gear between the motor (where the encoder is attached) and the
drive wheel.
:
<strong><em>Gearbox ratio: 86.8:1</em></strong></p>

<p>In Rover 5 chassis there are 4 small wires with female headers. RED is +5V for the encoder , BLACK is 0V (ground) , WHITE is signal A , YELLOW is signal B . The important wires in each encoder are signal A and signal B ,so </p>

<p>How to get values of NL , NR  in the formula above from signal A &amp; signal B ?</p>

<p>Is the value of NL is the direct value from  wire signal A or signal B ? the same question for NR . </p>

<p>Thanks a lot</p>
","wheeled-robot quadrature-encoder"
"4385","it's worth to make a line follower using a raspberry pi and a web cam?","<p>I wonder if this would be a competitive robot compared with one made with a traditional approach using a microcontroller and infrared sensors. I suppose that raspberry can perform an edge detection to tell the dynamic of the line far away, much more that the infrared sensor, but how fast can the raspberry do this process? should be a relative simple process in terms of computational requirements , an edge detection in a high contrast arena. Probably the bigger issue would be get the relative position of the robot respect to the line, may be a combination of the camera with some infrared sensors would work better, and what about the size? the robot will be significantly bigger when is used a camera and a raspberry for the control.</p>
","raspberry-pi line-following"
"4392","EKF SLAM and Mahalanobis distance?","<p>So far I have done EKF Localization (known and unknown correspondences) and EKF SLAM for only known correspondences that are stated in <a href=""http://www.probabilistic-robotics.org"" rel=""nofollow"">Probabilistic Robotics</a>. Now I moved to EKF SLAM with unknown correspondences. In the algorithm in page 322, </p>

<blockquote>
  <p>16. &nbsp; &nbsp; $\Psi_{k} = H^{k} \bar{\Sigma}[H^{k}]^{T} + Q$</p>
  
  <p>17. &nbsp; &nbsp; $\pi_{k} = (z^{i} - \hat{z}^{k})^{T} \Psi^{-1}_{k}(z^{i} - \hat{z}^{k})$</p>
  
  <p>18. &nbsp; &nbsp; $endfor$</p>
  
  <p>19. &nbsp; &nbsp; $\pi_{N_{t+1}} = \alpha$</p>
  
  <p>20. &nbsp; &nbsp; $j(i) = \underset{k}{argmin} \ \ \pi_{k}$</p>
  
  <p>21. &nbsp; &nbsp; $N_{t} = max\{N_{t}, j(i)\}$</p>
</blockquote>

<p>I don't understand the line 19. In the book page 323, The authors state </p>

<blockquote>
  <p>Line 19 sets the threshold for the creation of a new landmark: A new landmark is created if the Mahalanobis distance to all existing landmarks in the map exceeds the value $\alpha$. The ML correspondence is then selected in line 20. </p>
</blockquote>

<p>what is $\alpha$ in line 19 and how is it computed? Also, what is the Mahalanobis distance? I did research about Mahalanobis distance but still I can't understand its role in EKF SLAM. </p>

<hr>

<p>Edit: 
I found another book in my university's library <a href=""http://rads.stackoverflow.com/amzn/click/B00BEZNHGY"" rel=""nofollow"">Robotic Navigation and Mapping with Radar</a> The authors state </p>

<blockquote>
  <p>The Mahalanobis distance measure in SLAM is define as
  $d^{2}_{M}(z^{j}_{k}, \hat{z}^{i}_{k})$, which provides a measure on
  the spatial difference between measurement $z^{j}_{k}$ and predicted
  feature measurement $\hat{z}^{i}_{k}$, given by  $$ d^{2}_{M}(z^{j}_{k}, \hat{z}^{i}_{k}) = (z^{j}_{k} - \hat{z}^{i}_{k})^{T} S^{-1}_{k}(z^{j}_{k}, \hat{z}^{i}_{k}) $$ This
  value has to be calculated for all possible $(z^{j}_{k}, \hat{z}^{i}_{k})$ combinations, for which  $$ d_{M}(z^{j}_{k},\hat{z}^{i}_{k}) \leq \alpha $$ Often referred to as a validation gate.</p>
</blockquote>

<p>Leave me to the same question what is $\alpha$?</p>
","slam ekf mapping"
"4393","Is a Genetic alogorithm suitable for mobile robot path planning?","<p>Regarding my project work, I have to write an algorithm for mobile robot planning. For that, I have chosen Genetic algorithm. Is it good for mobile robot path planning? If it is, then where can I start from and get some guidelines?</p>
","mobile-robot navigation algorithm"
"4394","Single-shaft vs Double-shaft motors","<p>I am trying to make a line follower robot and I need help regarding the type of dc motor to use.
So we have a single shaft BO Motor and a double shaft BO Motor. Can anyone help me understand what is the difference between the two?
Here's the link for 
Single Shaft BO Motor:
<a href=""http://www.evelta.com/industrial-control/motors-and-accessories/100-rpm-l-type-single-shaft-bo-motor"" rel=""nofollow"">http://www.evelta.com/industrial-control/motors-and-accessories/100-rpm-l-type-single-shaft-bo-motor</a></p>

<p>Double Shaft BO Motor:
<a href=""http://www.evelta.com/industrial-control/motors-and-accessories/100-rpm-l-type-double-shaft-bo-motor"" rel=""nofollow"">http://www.evelta.com/industrial-control/motors-and-accessories/100-rpm-l-type-double-shaft-bo-motor</a></p>
","motor line-following"
"4395","Counts of Quadrature Encoder","<p>Simply, I had Rover 5 with 2 DC motors and 2 quadrature encoders, I just want to use encoders to measure the distance of travelling for each wheel.</p>

<p>To start with, I just want to determine the total counts per revolution. I read the article about quadratic encoder from <a href=""http://letsmakerobots.com/node/24031"" rel=""nofollow"">this broken link</a>.</p>

<p>In Rover 5, each encoder has four wires: red (5V or 3.3V), black(Ground), yellow (Signal 1) and white (Signal 2). I connected each wire in its right place on Arduino Uno board, using the circuit:</p>

<ul>
<li>rotary encoder ChannelA attached to pin 2 </li>
<li>rotary encoder ChannelB attached to pin 3</li>
<li>rotary encoder 5V attached to 5V</li>
<li>rotary encoder ground attached to ground </li>
</ul>

<p>For one encoder, I test the code below to determine the total counts or ticks per revolution, the first program by using loop and second by using an interrupt.</p>

<p>Unfortunately while I run each program separately, rotating the wheel 360 degree by hand, the outputs of these two programs was just ""gibberish"" and I don't know where is the problem . Could anyone help?</p>

<p>Arduino programs posted below.</p>

<p>First program:</p>

<pre><code>// Constants
const int  ChanAPin = 2;    // pin for encoder ChannelA
const int  ChanBPin = 3;    // pin for encoder ChannelB

// Variables
int encoderCounter = -1;   // counter for the number of state changes
int ChanAState = 0;         // current state of ChanA
int ChanBState = 0;        // current state of ChanB
int lastChanAState = 0;     // previous state of ChanA
int lastChanBState = 0;    // previous state of ChanB

void setup() {
  // initialize the encoder pins as inputs:
  pinMode(ChanAPin, INPUT);
  pinMode(ChanBPin, INPUT);
  // Set the pullup resistors
  digitalWrite(ChanAPin, HIGH);
  digitalWrite(ChanBPin, HIGH);

  // initialize serial communication:
  Serial.begin(19200);
  Serial.println(""Rotary Encoder Counter"");
}

void loop() {
  // read the encoder input pins:
  ChanAState = digitalRead(ChanAPin);
  ChanBState = digitalRead(ChanBPin); 
  // compare the both channel states to previous states
  if (ChanAState != lastChanAState || ChanBState != lastChanBState) {
    // if the state has changed, increment the counter
      encoderCounter++;
      Serial.print(""Channel A State = "");
      Serial.println(ChanAState);
      Serial.print(""Channel B State = "");
      Serial.println(ChanBState);      
      Serial.print(""State Changes = "");
      Serial.println(encoderCounter, DEC);
    // save the current state as the last state, 
    //for next time through the loop
    lastChanAState = ChanAState;
    lastChanBState = ChanBState;    
  }
}
</code></pre>

<p>The second program (with interrupt)</p>

<pre><code>static long s1_counter=0;
static long s2_counter=0;

void setup()
{
  Serial.begin(115200);

  attachInterrupt(0, write_s1, CHANGE); /* attach interrupt to pin 2*/
  attachInterrupt(1, write_s2, CHANGE); /* attach interrupt to pin 3*/
  Serial.println(""Begin test"");
}

void loop()
{
}

void write_s1()
{
  s1_counter++;
  Serial.print(""S1 change:"");
  Serial.println(s1_counter);
}

void write_s2()
{
  s2_counter++;
  Serial.print(""S2 change:"");
  Serial.println(s2_counter);
}
</code></pre>
","mobile-robot quadrature-encoder"
"4396","ROS NavStack with Skid Steering robots","<p>I am migrating from a differential drive design to a skid steering design for my robot, and I want to know how easy would it be to use the NavStack with skid steering. Would there be any problems in terms of localization and things like that?</p>

<p>If I let two wheels on the same side of my robot (two on left side and two on the right side) maintain same velocity and acceleration, would the unicycle model of a differential drive robot still apply for skid steering?</p>
","ros navigation differential-drive driver"
"4398","Machine Vision vs Computer Vision?","<p>I'm trying to understand the core differences between the two topics.  Is one simply a newer term?  Connotations of automobile vs automation?  Something with a screen vs without?</p>

<p>I've only ever heard the term <code>computer vision</code> (tagged).</p>
","computer-vision"
"4399","What laser power for cutting and engraving wood and Acrylic robot baseplates?","<p>Need to buy a DIY/High School grade laser cutter/engraver  </p>

<p><strong>How much laser power</strong> is needed for <strong>wood, acrylic (3 to 6mm thick), cutting and decorative engraving?</strong> </p>

<p>What parameters I need to take care in selecting suitable machines?</p>

<p><img src=""http://i.stack.imgur.com/vWbbY.jpg"" alt=""enter image description here""></p>

<p><img src=""http://i.stack.imgur.com/l8otO.jpg"" alt=""enter image description here""></p>
","laser"
"4405","Expression used with Rotary Encoders","<p>While I am reading and collecting information about rotary encoders , I faced some troubles about the meaning of some expressions concerned with encoder ,which make me to be confused and stray,  these expressions or words are :</p>

<p>-Count per revolution (rotation) </p>

<p>-Pulse per revolution</p>

<p>-Tick per revolution</p>

<p>-Transitions per revolutions </p>

<p>-Number of transitions</p>

<p>-Number of state changes</p>

<p>I thought the transition is same as state changes which means change from high to low or low to high , but what about the others what is the diffenece among them (count , tick ,pulse ,transition .... etc)? and what the relationship between transitions and pulse ? Could anyone clarify that , please</p>
","mobile-robot"
"4409","Quadcopter: Stabilization along the z-axis (for holding altitude)","<p>I recently spent some work on my quadcopter firmware.
The model is stabilizing its attitude relatively well now.
However I noticed, that it is changing its altitude sometimes (maybe pressure changes, wind or turbulence).
Now I want to get rid of these altitude drops and found not much literature.
My approach is using the accelerometer:</p>

<ul>
<li>Calculates the current g-force of the z-axis</li>
<li>if the g-force is > 0.25 g and longer than 25 ms, then I feed the accelerometer term (cm per s²) into the pid</li>
<li>the output is sent to the motors</li>
</ul>

<p>The model now reacts when it is falling down with an up-regulation of the motors.
However, I am not sure, whether it is smart to feed the current acceleration into the regulator and I currently wonder, whether there is a smarter method to deal with sudden and smaller changes in altitude.</p>

<p>Current code: </p>



<pre><code># define HLD_ALTITUDE_ZGBIAS 0.25f
# define HLD_ALTITUDE_ZTBIAS 25

const float fScaleF_g2cmss = 100.f * INERT_G_CONST;
int_fast16_t iAccZOutput = 0; // Accelerometer

// Calc current g-force
bool bOK_G;
float fAccel_g = Device::get_accel_z_g(m_pHalBoard, bOK_G); // Get the acceleration in g

// Small &amp; fast stabilization using the accelerometer
static short iLAccSign = 0; 
if(fabs(fAccel_g) &gt;= HLD_ALTITUDE_ZGBIAS) {
  if(iLAccSign == 0) {
    iLAccSign = sign_f(fAccel_g);
  }

  // The g-force must act for a minimum time interval before the PID can be used
  uint_fast32_t iAccZTime = m_pHalBoard-&gt;m_pHAL-&gt;scheduler-&gt;millis() - m_iAccZTimer;
  if(iAccZTime &lt; HLD_ALTITUDE_ZTBIAS) {
     return; 
  }

  // Check whether the direction of acceleration changed suddenly
  // If so: reset the timer
  short iCAccSign = sign_f(fAccel_g);
  if(iCAccSign != iLAccSign) {
    // Reset the switch if acceleration becomes normal again
    m_iAccZTimer = m_pHalBoard-&gt;m_pHAL-&gt;scheduler-&gt;millis();
    // Reset the PID integrator
    m_pHalBoard-&gt;get_pid(PID_ACC_RATE).reset_I();
    // Save last sign
    iLAccSign = iCAccSign;
    return;
  }

  // Feed the current acceleration into the PID regulator
  float fAccZ_cmss = sign_f(fAccel_g) * (fabs(fAccel_g) - HLD_ALTITUDE_ZGBIAS) * fScaleF_g2cmss;
  iAccZOutput = static_cast&lt;int_fast16_t&gt;(constrain_float(m_pHalBoard-&gt;get_pid(PID_ACC_RATE).get_pid(-fAccZ_cmss, 1), -250, 250) );
} else {
  // Reset the switch if acceleration becomes normal again
  m_iAccZTimer = m_pHalBoard-&gt;m_pHAL-&gt;scheduler-&gt;millis();
  // Reset the PID integrator
  m_pHalBoard-&gt;get_pid(PID_ACC_RATE).reset_I();
}
</code></pre>
","quadcopter multi-rotor"
"4410","Programming the Odometry of Rover 5","<p>I have started in the programming stage of my project , and my first step is to made and test the odometry of my Rover 5 robot on Arduino Uno by using encoders to determine position and orientation .</p>

<p>I wrote this code and I don’t know if that code right or there are some mistakes,  because I am novice to Arduino and Robotic field so I need for some suggestions and corrections if  there were . </p>

<p>thanks a lot</p>

<p>Arduino codes posted below.</p>



<pre><code>#define encoder1A  0       //signal A of left encoder  (white wire)
#define encoder1B  1      //signal B of left encoder  (yellow wire)
#define encoder2A  2      //signal A of right encoder  (white wire)
#define encoder2B  3      //signal B of right encoder  (yellow wire)

volatile int encoderLeftPosition = 0;      // counts of left encoder
volatile int encoderRightPosition = 0;     // counts of right encoder

float  DIAMETER  = 61  ;         // wheel diameter (in mm)
float distanceLeftWheel, distanceRightWheel, Dc, Orientation_change;
float ENCODER_RESOLUTION = 333.3;      //encoder resolution (in pulses per revolution)  where in Rover 5,  1000 state changes per 3 wheel rotations

int x = 0;           // x initial coordinate of mobile robot
int y = 0;           // y initial coordinate of mobile robot
float Orientation  = 0;       // The initial orientation of mobile robot
float WHEELBASE=183  ;       //  the wheelbase of the mobile robot in mm
float CIRCUMSTANCE =PI * DIAMETER  ;

void setup()
{
  pinMode(encoder1A, INPUT);
  digitalWrite(encoder1A, HIGH);       // turn on pullup resistor
  pinMode(encoder1B, INPUT);
  digitalWrite(encoder1B, HIGH);       // turn on pullup resistor
  pinMode(encoder2A, INPUT);
  digitalWrite(encoder2A, HIGH);       // turn on pullup resistor
  pinMode(encoder2B, INPUT);
  digitalWrite(encoder2B, HIGH);       // turn on pullup resistor
  attachInterrupt(0, doEncoder, CHANGE);       // encoder pin on interrupt 0 - pin 3
  Serial.begin (9600);
}

void loop()
{
  distanceLeftWheel = CIRCUMSTANCE * (encoderLeftPosition / ENCODER_RESOLUTION);       //  travel distance for the left and right wheel respectively
  distanceRightWheel = CIRCUMSTANCE * (encoderRightPosition / ENCODER_RESOLUTION);     // which equal to pi * diameter of wheel * (encoder counts / encoder resolution )
  Dc=(distanceLeftWheel + distanceRightWheel) /2 ;            // incremental linear displacement of the robot's centerpoint C
  Orientation_change =(distanceRightWheel - distanceLeftWheel)/WHEELBASE;    // the robot's incremental change of orientation , where b is the wheelbase of the mobile robot ,
  Orientation = Orientation + Orientation_change ;          //  The robot's new relative orientation
  x = x + Dc * cos(Orientation);                            // the relative position of the centerpoint for mobile robot
  y = y + Dc * sin(Orientation);
}

void doEncoder(){
  //  ---------- For Encoder 1 (Left)  -----------
  if (digitalRead(encoder1A) == HIGH) {   // found a low-to-high on channel A
    if (digitalRead(encoder1B) == LOW) {  // check channel B to see which way
                                             // encoder is turning
      encoderLeftPosition = encoderLeftPosition - 1;         // CCW
    }
    else {
      encoderLeftPosition = encoderLeftPosition + 1;         // CW
    }
  }
  else                                        // found a high-to-low on channel A
  {
    if (digitalRead(encoder1B) == LOW) {   // check channel B to see which way
                                              // encoder is turning
     encoderLeftPosition = encoderLeftPosition + 1;          // CW
    }
    else {
      encoderLeftPosition = encoderLeftPosition - 1;          // CCW
    }
  }
  //  ------------ For Encoder 2 (Right)-------------
  if (digitalRead(encoder2A) == HIGH) {   // found a low-to-high on channel A
    if (digitalRead(encoder2B) == LOW) {  // check channel B to see which way  encoder is turning
      encoderRightPosition = encoderRightPosition - 1;         // CCW
    }
    else {
      encoderRightPosition = encoderRightPosition + 1;         // CW
    }
  }
  else                                        // found a high-to-low on channel A
  {
    if (digitalRead(encoder2B) == LOW) {   // check channel B to see which way  encoder is turning
     encoderRightPosition = encoderRightPosition + 1;          // CW
    }
    else {
     encoderRightPosition = encoderRightPosition - 1;          // CCW
    }
  }
}
</code></pre>
","mobile-robot"
"4413","Arduino Power Supply","<p>I want to power my Arduino Uno and I know I can do that either by connecting it with USB to PC or with DC power supply.But I want to connect it with a battery source(kindly see the image below) and I know its a silly question but how do I do it? The battery connector is not the regular DC jack but the one that's found in RC toys. So how do I power my Arduino with that battery? 
And also how do I connect it with a DC power supply adapter to charge it once its discharged? Please also mention the specifications of the DC power supply adapter that is to be used while charging this battery.
<img src=""http://i.stack.imgur.com/IFA2O.jpg"" alt=""Battery Image""></p>
","arduino battery"
"4414","Motor control using arduino/raspberry pi","<p>I'm new to robotics. I would like to know if 56 output lines can be taken from an arduino or raspberry pi?</p>
","arduino raspberry-pi"
"4415","Virtual Testing Environment for Drones","<p>Does anyone know of a robotics developer environment ideal for testing AI programs for drones (e.g. quadrocopters, planes, helicopters, etc.)? I would like something like Microsoft Robotics Developer Studio that includes a virtual environment (such as an outdoor environment with gravity, wind, etc.) to test out flight dynamics. I would like the options to add sensors to the virtual drone, such as gps, altimeter, gyros, etc. that the AI program can then use to steer the drone.</p>
","quadcopter artificial-intelligence machine-learning"
"4420","Why models are not perfect to represent robotic environments?","<p>Sebastian Thrun says in his paper on Particle Filters that - no model however detailed fails to represent the complexity of even the simplest of robotic environment. What does he means by this? Can someone please elaborate?</p>
","localization theory"
"4424","Comprehensive comparison of SLAM algorithms","<p>I'm looking for a research paper or series of papers that compare the performance of various simultaneous localization and mapping algorithms for rovers in a variety of real world environments.  In particular, i'm looking for computational speed, accuracy (compared to the real world environment) and memory &amp; power efficiency metrics.  Is there a journal that regularly publishes experimental performance comparisons?</p>
","slam reference-request"
"4426","How to filter vibration programatically?","<p>I'm working on a quadcopter. I'm reading the accelerometer and gyro data out from the MPU6050 and using complementary filter to calculate the roll and pitch values. When the quad is on the floor, and the motors are turned on the roll values are: </p>

<pre><code>-4.88675227698
-5.07656137566
 7.57363774442
-3.53006785613
 4.44833961261
-2.64380479638
-3.70460025582
</code></pre>

<p>It is very messy. After minus five there is plus seven. I would like to filter out this too high/low values programmatically but I have no idea how to do it.</p>

<p>EDIT:
At this moment I think the solution is the Low-pass filter. I'll let you know if it is successful or not.</p>
","quadcopter accelerometer gyroscope"
"4427","Free of charge Robot Magazine, Journal, Newsletter or similar","<p>What free of charge Robot Magazine, Journal, Newsletter or similar publication are available?  </p>

<p>Either geared toward technical professionals or the general public.</p>
","untagged"
"4430","What would be a good heuristic to solving this?","<p>The aim is to guide a bot from Source <code>S</code> to Goal <code>G</code> while passing through all the checkpoints <code>@</code> (in any order).  </p>

<pre><code>########
#@....G#
##.##@##
#..@..S#
#@.....#
########
</code></pre>

<p>One way to solve it would be to select one checkpoint as goal from current state and then guide the bot to it. Then select the next checkpoint as goal and current checkpoint as source and guide the bot to its new goal. Eventually guide it to the state <code>G</code> from the last checkpoint.<br/><br/>But this technique relies heavily on the order of checkpoints traversed.<br/><br/> I would like to know if a good heuristic can be found to decide which checkpoint to go to next?</p>
","artificial-intelligence"
"4435","computer vision using the FREAK local features descriptor - why overlapping fields?","<p>I am currently studying the <a href=""http://infoscience.epfl.ch/record/175537/files/2069.pdf"">FREAK</a> descriptor and I read the article published by its designers. It states that the aim was to mimic the retinal topology, and one of the advantages that could be gained is the fact that retinal receptive fields overlap, which increases the performance. </p>

<p>I thought about it a lot, and the only explanation I was able to come up with is the fact that, looking at this problem from an implementation point of view, a receptive field is the ensemble of an image patch centred around a pixel, plus the standard deviation of the Gaussian filter applied to this patch. The size of the receptive field represents the value of the standard variation. The bigger the size is, the more pixels will be taken into consideration when Gaussian filtering, and so we ""mix"" more information in a single value. </p>

<p>But  this guess of mine is very amateurish, I would appreciate it if someone could give an explanation from what goes on in the field of image processing-computer vision-neuroscience.</p>
","computer-vision"
"4439","Raspberry Pi finer servo control","<p>I'm usig RPI and Servoblaster to control servos. I've set the <code>--step-size to 2 us</code>, but I'd like to decrease it to 1us. I've tried to set the <code>step-size to 1us</code>, but the Servoblaster displays: Invalid step-size specified. </p>

<p>I've also tried to set the pulse width in micoseconds like <code>echo 1=1140us &gt; /dev/servolaster</code>. It works, but it's unpredictabe (step size is set to 2us):</p>

<pre><code>echo 1=1140us &gt; /dev/servoblaster - motor starts spinnig
echo 1=1142us &gt; /dev/servoblaster - motor **smoothly** speeds up
echo 1=1144us &gt; /dev/servoblaster - motor's speed has not changed
echo 1=1146us &gt; /dev/servoblaster - motor smoothly speeds up (OK, assume that it can be changed by +/- 4)
BUT:
echo 1=1150us &gt; /dev/servoblaster - motor's speed has not changed - why??
echo 1=1152us &gt; /dev/servoblaster - motor speeds up, but **fastly**
echo 1=1156us &gt; /dev/servoblaster - motor **smoothly** speeds up
</code></pre>

<p>Motor: Turnigy aerodrive 2830-11, ESC: Turnigy Multistar 30A</p>

<p>Any idea?</p>
","raspberry-pi servos"
"4440","Finding changes in environment using 2d laser","<p>I have known map of the environment (2d occupancy grid map). I am trying to find if anything changed in environment using 2d laser while navigating by using maximum likelihood of laser with known map. </p>

<p>My question is how to know <em>which measurements are corresponding to changes</em>. My environment is not static and has some changes which is differs from known map. Now i am trying to find which objects newly came into the environment or moved out of the environment using laser.</p>
","slam mapping laser occupancygrid"
"4442","What is the technical name when robot wheels are aligned to perform spot turn?","<p>I have a robotic simulator that enables a 6 wheel rover to perform spot turn.
To prepare the rover to spot turn, I have to arrange/align the wheels in such a fashion:</p>

<pre><code>&lt;front side&gt;
//    \\
||    ||
\\    //
&lt;rear side&gt;
</code></pre>

<p>What is the technical name of it? Circular wheel arrangement? Circular alignment? </p>
","mobile-robot wheeled-robot wheel"
"4447","Is there any advantage to velocity motion models over odometry motion models for SLAM?","<p>I've seen several examples of SLAM algorithms (<a href=""http://en.wikipedia.org/wiki/EKF_SLAM"" rel=""nofollow"">EKF SLAM</a>, <a href=""http://en.wikipedia.org/wiki/GraphSLAM"" rel=""nofollow"">Graph SLAM</a>, <a href=""http://en.wikipedia.org/wiki/SEIF_SLAM"" rel=""nofollow"">SEIF SLAM</a>) written in terms of the <a href=""http://robotics.stackexchange.com/questions/3101/velocity-model-motion-in-matlab-probabilistic-robotics"">velocity motion model</a>.  I have yet to see an example of any SLAM algorithm utilizing the <a href=""https://www.cs.princeton.edu/courses/archive/fall11/cos495/COS495-Lecture5-Odometry.pdf"" rel=""nofollow"">odometry motion model</a>.  I wonder if there is an inherent advantage to using the velocity motion model over the odometry model for this problem.  Does it have something to do with the fact that odometry sensor information comes <strong>after</strong> the motion has already taken place, whereas velocity control commands are executed <strong>before</strong> motion?</p>
","slam motion"
"4449","Assumptions about the nature of landmarks in SLAM algorithms","<p>I'm trying to understand the role of landmarks in SLAM algorithms.  I've glanced over a few books concerning landmark based SLAM algorithms and I've come up with a rudimentary understanding which I believe is <strong>flawed</strong>.</p>

<p><strong><em>How I think SLAM works:</em></strong></p>

<p>As I understand it, landmarks are a set of points in a map whose locations are known <strong>a priori</strong>.   Furthermore, the number of landmarks in a map is fixed.  The number of landmarks detected at any one time may change, but the number of landmarks that exist in the map remains static at all times.  </p>

<p>My understanding is that SLAM algorithms exploit the fact that these points are both <strong>uniquely identifiable</strong> and <strong>known a priori</strong>.   That is, when a robot senses a landmark, it knows exactly which landmark it detected and thus knows the exact location of that landmark.  Thus, a slam algorithm uses the (noisy) distance to the detected landmarks (with known location) to estimate its position and map.  </p>

<p><strong><em>Why I think I'm wrong</em></strong>  </p>

<p>In my <em>naive</em> understanding, the usefulness of SLAM would be limited to controlled environments (i.e. with known landmarkds) and completely useless in unknown environments with no a priori known landmarks.  I would presume that some sort of feature detection algorithm would have to dynamically add landmarks as they were detected.  However, this fundamentally changes the assumption that the number of given landmarks must be static at all times.  </p>

<p>I know I'm wrong in my understanding of feature based SLAM, but I'm not sure which of my assumptions is wrong:  </p>

<p>Do feature based SLAM algorithms assume a static number of landmarks?  </p>

<p>Do the landmarks need to be known a priori?  Can they be detected dynamically?  And if so, does this fundamentally change the algorithm itself?</p>

<p>Are there special kinds of SLAM algorithms to deal with unknown environments with an unknown total number of landmarks in it?</p>
","slam"
"4456","Pulse Position Modulation as used in RC controls","<p>How are several channels multiplexed down to a single physical wire? If two channels are transmitting the same value in the same frame, wont there be an overlap of the pulses?</p>
","rcservo pwm"
"4463","Choosing motor for a tricopter","<p>I'm a newbie in RC field..I am planning to construct my first Tricopter ever.  Can anyone help me to find the power rating to select the motor for a tricopter?</p>

<p>I am at the beginning stage of construction.  Arm length of frame: 50cm each.  I need a thrust of about 2Kg -- nearly 666 gms for each motor.</p>
","motor"
"4465","What's the difference between Yaw and Attitude in Quad Rotor","<p>I have a big miss conception between Yaw and attitude ? </p>

<p>Isn't both represent ""how far is the quad from earth ?"" </p>

<p>Also if you could post how to calculate them from IMU (gyro +accele + magent ) </p>
","imu"
"4467","Need to clear some concepts: AHRS - Attitude - Yaw,Pitch and Roll - MARG sensors -INS","<p>it's been while since I started reading about INS, orientation and so for quadrotors . </p>

<p>I faced the following terms : AHRS - Attitude - Yaw,Pitch and Roll - MARG sensors</p>

<p>I know for example how to calculate Yaw,Pitch and Roll , but does it related to Attitude ? </p>

<p>What's Attitude  any way and how it get calculated ?</p>

<p>AHRS ""Attitude and heading reference system"" does it formed from Yaw,Pitch and Roll ? </p>

<p>MARG(Magnetic, Angular Rate, and Gravity) ? how it's related to other terms ? </p>

<p>What about INS ( Inertial Navigation Systems ) ? </p>

<p>My questions here are about these concepts, and there meaning , how they cooperate with each other , how they got calculated and which sensors suits for what ?</p>
","navigation"
"4469","image size vs image resolution","<p>I read somewhere that in the case of photoshop for example, the size refers to the number of pixels an image contains, but resolution involves the pixel's size, I don't know whether this definition goes for all the other fields. In computer vision, what's the difference between image size and image resolution?</p>
","computer-vision"
"4472","how to make Mac detect AVR board using USBasp and burn program to it?","<p>I am new to Embedded, starting with <strong>AVR programming using C</strong>. I am working on <strong>Mac OS 10.9.4</strong>, so far I am using <strong>avrdude</strong> and <strong>xCode</strong> as IDE. It works very well, for now I am testing my code using <strong>Proteus</strong>.</p>

<p>But now I want to burn my <strong>.hex</strong> to <strong>AVR ATMega16</strong> board. I have <strong>USBasp</strong>, which I am able to connect and it lights up the board. Now after searching on the internet, I think Mac is not detecting my board. I have checked <strong>/dev</strong> directory, but no usb device found. </p>

<p>So I am not sure what to next, how to make Mac detect my board and burn my .hex on it. I've found this: <a href=""http://www.fischl.de/usbasp/"" rel=""nofollow"">http://www.fischl.de/usbasp/</a> but no idea how to use this or its required or not.</p>

<p>So question stand is: <strong>how to make Mac detect AVR board using USBasp and burn program to it?</strong></p>

<p>FYI: I've installed <strong>CrossPack</strong> on Mac. </p>
","usb embedded-systems avr"
"4474","Servo controlled valve","<p>I am trying to build a servo-controlled water valve.  Max pressure 150 psi , valve size 1/2"".</p>

<p>Can anyone recommend a suitable 1/4-turn valve, either ceramic, ball valve, or anything else that is <em>easy</em> to turn, even under pressure? It must require very little torque to turn, so a standard servo can rotate it with a small lever attached.</p>
","servos valve"
"4476","Quadcopter one beep and blink problem","<p>I have just built my first quadcopter, and have run into a bit of a snag. When I plug in the power, I only get one beep and a red blink from the <a href=""http://www.hobbyking.com/hobbyking/store/__21977__hobbyking_multi_rotor_control_board_v3_0_atmega328_pa_.html"" rel=""nofollow"">flight control board</a>, and nothing else happens. When I turn on the controller, however, a red light turns on on the reciever. Otherwise, nothing else happens. From what I can tell, I have plugged in everything correctly, and am not sure how to proceed.</p>

<ul>
<li><a href=""http://www.hobbyking.com/hobbyking/store/__21977__hobbyking_multi_rotor_control_board_v3_0_atmega328_pa_.html"" rel=""nofollow"">flight control board</a></li>
<li><a href=""http://hobbyking.com/hobbyking/store/uploads/116867307X7478X23.pdf"" rel=""nofollow"">Flight Control Board manual (PDF)</a></li>
<li><a href=""https://www.hobbyking.com/hobbyking/store/__25365__Turnigy_Multistar_30_Amp_Multi_rotor_Brushless_ESC_2_4S.html"" rel=""nofollow"">ESC's</a></li>
</ul>

<p>I do not have a connection from the power distribution board to the flight control board, because I am assuming that it gets its power from the ESC's.</p>

<p>Here is <a href=""http://www.youtube.com/watch?v=csnYH_eq-Xk"" rel=""nofollow"">the video I used to figure out how to build a quad</a>.  (side- note about the video: I have not cut the ESC's cords as done in the guide, seemed like a silly step, also I have seen other applications where they were not cut)</p>

<p>I have not updated the firmware on the board, i have put it in out of the box</p>

<p><a href=""http://www.hobbyking.com/hobbyking/store/uploads/116867307X7478X23.pdf"" rel=""nofollow"">here</a> is the board's user manual (PDF)</p>
","control quadcopter"
"4477","How to calculate Altitude from IMU?","<p>How to calculate attitude from IMU ? </p>

<p>For example, mathematical equations </p>
","imu"
"4486","Ackerman steering model","<p>I am trying to create a simulation of a robot with <a href=""https://en.wikipedia.org/wiki/Ackermann_steering_geometry"">Ackerman steering</a> (the same as a car). For now I'm assuming that it's actually a 3-wheeled robot, with two wheels at the back, and one steering wheel at the front:</p>

<p><img src=""http://i.stack.imgur.com/DqbhM.png"" alt=""Steering robot""></p>

<p>Knowing the wheel velocity, and the steering angle <strong>a</strong>, I need to be able to update the robot's current position and velocity with the new values at time t+1.</p>

<p>The obvious way to do this would be to calculate the position of the centre of rotation, where the axles of the wheels would meet, however, this leads to an undefined centre of rotation when <strong>a</strong> = 0. This means that the model doesn't work for the normal case of the robot just driving in a straight line.</p>

<p>Is there some other model of Ackerman steering which works over a reasonable range of <strong>a</strong>?</p>
","mobile-robot simulation"
"4487","Relative Navigation Systems","<p>Im trying to develop a system that autonomously navigates a large outside space, where accuracy is vital (GPS is too inaccurate). There are a number of options but have largely been used inside, has anyone tried these or used anything else?</p>

<p>WiFi triangulation,
Dead reckoning,
RFID landmarks</p>
","navigation"
"4488","What's the most adapted programming language for Robotic and principally AI?","<p>I'm currency a Web programmer and I'm very passionate by robotics and specialty for Artificial Intelligence. </p>

<p>I have already make some C++ program for Microship and Arduino for little robots and other Lisp codes (example for labyrinth path search) but I think it's not really applicable for projects further. </p>

<p>I have read a lots for artificial neural network to create artificial mind, but it's very theoretical and I have no idea to reproduce that on code.</p>

<p>Someone have a idea to help me, a specific language, or just a C++ library ? 
If you have some links, articles, or other tutorials I take it.</p>

<p>Thank a lots !</p>
","artificial-intelligence programming-languages"
"4489","How to calibrate an IMU unit?","<p>Simply , how can I calibrate IMU unit ? </p>

<p>I read some papers about this topic and was wondering if there are any standard methods.</p>
","imu"
"4492","DC motor control - speed-torque curve","<p>I am having some trouble understanding how to practically use the speed-torque curve of a DC motor.</p>

<p>I understand that the gradient of the speed-torque curve is defined by the design of the motor, the exact position of the curve depending on the voltage applied. So if the voltage is changed the speed-torque curve is also changed but remains parallel to the initial curve before the voltage was changed. See figure below.</p>

<p><img src=""http://i.stack.imgur.com/eIjJF.jpg"" alt=""enter image description here""></p>

<p>So my intuitive guess is that when using the motor at a given desired operation point (desired speed and desired torque), the corresponding speed-torque curve <em>Cd</em> has a gradient specified in the data sheet of the motor and passes through the operation point. This curve <em>Cd</em> is obtained at a corresponding voltage <em>Vd</em>. See diagram below.</p>

<p><img src=""http://i.stack.imgur.com/wlmxm.jpg"" alt=""enter image description here""></p>

<p>So my next guess is that in order to have the motor operate at this desired operation point, you have to set the voltage applied to the motor to <em>Vd</em>, and apply a current <em>Id</em> (computed using the torque and the torque constant).</p>

<p>Now from what I read this is not what is done in DC motor controllers. These seem to only drive the motor using current and some sort of PWM magic as is shown in the following diagram by maxon.</p>

<p><img src=""http://i.stack.imgur.com/rRxj2.jpg"" alt=""enter image description here""></p>

<p>Anyone knows why voltage is not used in DC motor control and only current is? I do not understand how you can set the speed if you do not modify the voltage? And what is PWM useful for?</p>

<p>I have looked for hours over the internet and could not find anything relevant.</p>

<p>Thanks,</p>

<p>Antoine.</p>
","motor control"
"4496","How to get pure end-effector translation through Jacobian?","<p>I have a 7 DOF arm that I am controlling with joint velocities computed from the Jacobian in the standard way.  For example:
$$
{\Large J} = \begin{bmatrix} J_P \\J_O
\end{bmatrix}
$$
$$
J^{\dagger} = J^T(JJ^T)^{-1}
$$
$$
\dot{q}_{trans} = J^{\dagger}_P v_{e_{trans}}
$$
$$
\dot{q}_{rot} = J^{\dagger}_O v_{e_{rot}}
$$
$$
\dot{q} = \dot{q}_{trans} + \dot{q}_{rot}
$$</p>

<p>However, when specifying only translational velocities, the end-end effector also rotates.  I realized that I might be able to compute how much the end-effector would rotate from the instantaneous $\dot{q}$, then put this through the Jacobian and subtract out its joint velocities.  </p>

<p>So I would do this instead of using the passed in $v_{e_{rot}}$:</p>

<p>$$
v_{e_{rot}} =  R(q) - R(q+\dot{q}_{trans})
$$</p>

<p>Where $R(q)$ computes the end-effector rotation for those joint angles.  </p>

<p>Is this OK to do, or am I way off base?  Is there a simpler way?  </p>

<p>I am aware that I could also just compute the IK for a point a small distance from the end-effector with no rotation, then pull the joint velocities from the delta joint angles.  And that this will be more exact.  However, I wanted to go the Jacobian route for now because I think it will fail more gracefully.</p>

<p>A side question, how do I compute $R(q) - R(q+\dot{q}_{trans})$ to get global end-effector angular velocity?  My attempts at converting a delta rotation matrix to Euler angles yield wrong results.  I did some quick tests and implemented the above procedure to achieve pure end-effector rotation while maintaining global position.  (This is easier because $T(q) - T(q+\dot{q}_{rot})$ is vector subtraction.)  And it did kind of work.</p>
","kinematics robotic-arm jacobian"
"4498","Robot Loud Alarm","<p>We are working on a project where we want to sound an alarm if somebody is messing around with our Robot (e.g., the Robot is being shaken abruptly or the cameras/LIDARs are blocked).
I am using ""loud speakers"" (4.1 x 3 inch 10 Watts 8 ohm speakers), but they are not loud enough.</p>

<p>Are there any small speakers or alarm systems small enough, but loud enough (closed to a car alarm) that you would recommend? 
Ideally something that I can just plug into the robots computer, or interface with through a microcontroller. Either one would be fine.</p>
","microcontroller ros navigation"
"4504","Quadcopter cannot balance!","<p>I am bulding a quadcopter using these compenents:</p>

<ul>
<li>Microcontroller: Tiva C Lanchpad (ARM Cortex M4 - 80 MHz), but
running at 40MHz in my code</li>
<li>MPU 9150 - Sensorhub TI</li>
<li>ESC - Hobbywing Skywalker 40A</li>
</ul>

<p>I use the sample project comp_dcm from Tivaware and use that angles for my PID which running at 100Hz</p>

<p>I test PID Control on 2 motors, but the motors oscillate as in the video i found on youtube from one guy!</p>

<p><a href=""https://www.youtube.com/watch?v=k1Ks1e67bwI"" rel=""nofollow"">Quadcopter  Unbalance</a></p>
","quadcopter"
"4510","which type of camera should be used for detecting road lanes for good processing in matlab","<p>What are the parameters which should be selected to choose camera for lane detection system.What parameters should be kept in mind (like picture quality,frame rate,cost e.t.c). Which camera will suit best to my application.</p>
","cameras"
"4511","Euler Angles from 9DOF IMU","<p>Using the Adafruit 9DoF module I Need to convert the Accel + Magneto + Gyro into Euler Angles for a motion capture application. Any hints on where to start?</p>

<p>Managed to get X,Y,Z when the IMU is facing upward but when that orientation changes the axes dont behave normally that is because i am not using Euler angles. So Any hints to any reference where to start?</p>

<p>The Euler Compass App is an example of what I am trying to get to.
Get Pitch,Yaw, Roll for the IMU module irrespective to how its kept.</p>
","imu"
"4513","Whats the logic to implement a particle filter for a robot with range sensor?","<p>I am trying to implement a particle filter for a robot in Java. This robot is having a range sensor. The world has 6 obstacles - 3 in the top and 3 in bottom. I am calculating the distance of the robot from each obstacle's center and then performing the same activity for each particle. Then, i calculate the difference between the robot and particles. The particles for which the difference with robot measured distance are small, i give them higher probability in resampling.</p>

<p>But, the problem with this approach as told by my friend is that I am assuming that I already know the locations of the obstacles, which make this all process useless. How should I approach it rather in a sense that I don't know the obstacles. How can the particle filter be implemented then? How will the particle filter work in case i don't know the obstacles location? An example of process would be great help. Thanks</p>

<p><img src=""http://i.stack.imgur.com/VQ8tz.jpg"" alt=""Screenshot of my application""></p>
","localization particle-filter"
"4514","Pledge algorithm for maze solving robots","<p>i saw this maze, and tried to apply pledge algorithm on it. But i am not able to solve this maze using this algorithm. what am i missing?
i want to ask, what i am doing wrong?</p>

<p>PLEDGE ALGORITHM:
in both cases we don't get to exit.
<img src=""http://i.stack.imgur.com/rvcN1.png"" alt=""enter image description here""></p>

<p>you can read about these algorithms at:</p>

<p><a href=""http://en.wikipedia.org/wiki/Maze_solving_algorithm"" rel=""nofollow"">http://en.wikipedia.org/wiki/Maze_solving_algorithm</a></p>

<p><a href=""http://www.astrolog.org/labyrnth/algrithm.htm"" rel=""nofollow"">http://www.astrolog.org/labyrnth/algrithm.htm</a></p>
","mobile-robot"
"4516","How can one create a robot which respondes to input following a flowchart?","<p>What are the most basic skills and components needed for creating a robot which gets two ""yes"" or ""no"" inputs with two push buttons, and goes down the defined flowchart and plays the relevant audio file each time it gets an input.</p>

<p>a flowchart like this:</p>

<pre><code>____question 1_____________
|                         |
Yes                       No
|                         |
question 2___             question 3______
|           |             |              |
Yes         No            Yes            No
|           |             |              |
question 4  question 5    question 6     question 7
...
</code></pre>
","microcontroller"
"4520","Correcting GPS track with visual odometry (sensor fusion)","<p>I am trying to build low cost and precise outdoor positioning. I explored <a href=""http://navspark.mybigcommerce.com/ns-raw-carrier-phase-raw-measurement-output-gps-receiver/"" rel=""nofollow"">NS-RAW</a> with RTKLIB - this would be doable but probably will need either a base station to get the correction data for rover or external correction data which may be a hassle. The action radius with own base station is quite limited too. This solution is not really straightforward while you have to deal with either in-house or streamed correction data.</p>

<p>I am wondering whether one would be able to substantially improve the accuracy of an ordinary (uncorrected) GPS+GLONASS device (maybe one found in a common smartphone) with stereo visual odometry. Today's consumer GNSS chips seem to have reasonably stable accuracy in the 5m range. The <a href=""http://www.cvlibs.net/software/libviso/"" rel=""nofollow"">VISO 2</a> library has a <a href=""http://www.cvlibs.net/datasets/kitti/eval_odometry_detail.php?&amp;result=3efc7160bd72636a4b031854f4aef473121b868f"" rel=""nofollow"">translation error of about 3% on 500m distance</a>. The idea is to use the visual odometry for ""smoothing"" the rough GPS track. </p>

<p>The question is how this can be technically done in terms of SW. The input would be two tracks - one from GPS device and the other VISO 2 library. I think I need a kind of filter that will fuse the sensor data to get greater precision.</p>
","computer-vision gps sensor-fusion odometry"
"4523","Ensemble Kalman Filter SLAM","<p>I know that there is an extended kalman filter approach to simultaneous localization and mapping.  I'm curious if there is a SLAM algorithm that exploits the <strong><em>ensemble</em></strong> kalman filter.  A citation would be great, if at all possible.</p>
","kalman-filter slam reference-request"
"4526","PID Tuning Quadcopter Problem","<p>I am tuning PID for quadcopter, the problem i have is that with different base Throttle, i seems that i have to adjust different PID gains in order for the quadcopter to balance! </p>
","pid"
"4531","Purpose of programming an ESC","<p>I am planning to buy an ESC for my tricopter setup. What is the purpose of programming an ESC? I am cost effective and is it really necessary that I should necessarily buy a programming card to program my ESC for my model?</p>
","esc"
"4533","12V Arduino Dual Bridge to supply at least 5A","<p>I am looking for a 12V Dual Motor Controller that can supply at least 5A per channel for two 12V motors, and that can be used with an arduino. Do you know of any product with those specs?</p>

<p>Thanks</p>
","arduino motor ros h-bridge"
"4536","Yaw angle calculation for drone PID from two distance sensors","<p>I'm building a control system with a Parrot AR 2.0 drone where I have access to thrust controls for up/down (z), left/right (y), forward/backwards (x), turn left and turn right (yaw) through a Ruby library on my computer.</p>

<p>The goal of the system is to keep the drone a particular distance from and parallel to a wall while moving in the up/down and left/right directions. We have added two sonar distance sensors to the left and right forward props. </p>

<p>The main problem I am having is figuring out how the two distance sensors equal a yaw reading (ψ) so I can feed that into the PID and then take action on thrust to the turn left or right for correction.</p>

<p>Maybe just getting some help with the conversion from two distances to the yaw angle would be a big help, but any thoughts on the PID are greatly appreciated too since it is my first time working with it.</p>
","sensors quadcopter pid"
"4540","Can we use this line sensor as a proximity sensor?","<p>I have <a href=""http://www.allsensor.in/ProductDetails.aspx?CID=qcj0eWfX11k%3d&amp;Counter=0&amp;SEOType=BSuwhmB9Ll4%3d&amp;PID=OlyqUUQ3npg%3d&amp;CType=ZgDIC937E8s%3d"" rel=""nofollow"">an RSL Line Sensor</a> which is designed to distinguish black and white lines. It detects white surface and gives me digital <em>1</em> as output, with <em>0</em> in case of black, but the surface needs to be close to it.</p>

<p>As it uses infra-red-sensors, I wanted to use this sensor as a proximity sensor, to tell me if there is a white surface <em>near</em> it. Is it possible to do this?</p>

<p>I think the only problem here is that we need to increase it's range of giving <em>1</em>. Currently, it gives <em>1</em> only when white surface is too close to the sensors. I want 1 even if the white surface is there at a bit more distance.</p>

<p>Also there is an adjustable screw there to adjust something, under which POT is written. I am working with an Arduino.</p>
","arduino mobile-robot sensors"
"4542","Velocity derivatives using Quaternions","<p>How to compute the angular and linear velocities Quaternions? I am new to this area and although I have studied the algebra I am unable to understand how to compute the velocities.</p>
","kinematics"
"4545","How to control an Arduino Board with wireless PS3 controler?","<p>I'm currently building a hexapod bot, composed with an <a href=""http://arduino.cc/en/Main/arduinoBoardMega"" rel=""nofollow"">Arduino Mega</a> board and an <a href=""http://www.lynxmotion.com/p-395-ssc-32-servo-controller.aspx"" rel=""nofollow"">USB SSC-32</a> (from <a href=""http://www.lynxmotion.com/"" rel=""nofollow"">Lynxmotion</a>).
But now I want add a PS3 wireless controller to move my hexapod, I have made some search but nothing realy interesting. Maybe the <a href=""http://www.servoshock.com/"" rel=""nofollow"">Servoshock module</a> but it seems works only with the ServoshockShield, a kind of Arduino card with Servo output. </p>

<ul>
<li>Can I use the ServoShock module alone ? </li>
<li>Can I connect it with Rx/Tx port of the Arduino Mega board ?</li>
<li>Do you have other solution for me ? Board with documentation and sources codes ?</li>
</ul>

<p>Thank you all</p>
","arduino wireless"
"4546","What are the calculations for a badminton robot and mechanisms?","<p>I am designing a badminton robot but i am very confused about mechanisms needed for a badminton robot and various calculations needed for millisecond response.I am also confused about calculations needed about the forces needed and efficient angles needed for hitting the shuttlecock.Please suggest me some ideas or suggestions needed for construction of badminton robot.</p>
","sensors design mechanism actuator servomotor"
"4554","Using kinect for medical application but without computer. Is it possible?","<p>I have to use kinect for an application. However,  the final work must be mobile: it means no computer. Consequently, I thought using a microcontroller to handle data from kinect. But is it possible? My job is mesuring some points of a body (axis X, Y, Z) and get back these coordinates. I don't know if I'm enough accurate. </p>
","microcontroller kinect"
"4555","Reverse lift mechanism","<p>I have made a RC robot from a wheelchair and I'm planning to attach a snow plow. I'm wondering if there is any mechanism that would be able to lift the plow when reversing. 
I have only 2 channel transmitter so I can't control the plow's movement through it so I was thinking of some mechanical lift that triggers when reversing.</p>

<p>Do you guys know about something I could use for it?
Thanks.</p>
","wheeled-robot mechanism"
"4558","PD Algorithm for a Quadrotor [Simulation]","<p>I have a big problem trying to stabilize a quadrotor with a PD controller.
The model and the program has been written in C++ and the model dynamic has been taken from this source in internet: </p>

<p>Well, in my code I wrote the model like in the eq. system ( see eq. 3.30 on page 21):</p>

<pre><code>/* Calculate the acceleration about all 6 axis */
body_pos_current_.x_dot_2 =  ( thrust_.total / masse_ ) * ( sin( body_ang_current_.theta ) * cos( body_ang_current_.phi ) * cos( body_ang_current_.psi ) + sin( body_ang_current_.psi ) * cos( body_ang_current_.phi ) );
body_pos_current_.y_dot_2 =  ( thrust_.total / masse_ ) * ( sin( body_ang_current_.theta ) * sin( body_ang_current_.psi ) * cos( body_ang_current_.phi ) - cos( body_ang_current_.psi ) * sin( body_ang_current_.phi ) * cos( body_ang_current_.psi ) );
body_pos_current_.z_dot_2 =  ( thrust_.total / masse_ ) * ( cos( body_ang_current_.theta ) * cos( body_ang_current_.phi ) ) - 9.81;

body_ang_current_.phi_dot_2   = ( torque_.phi   / Jxx_ );
body_ang_current_.theta_dot_2 = ( torque_.theta / Jyy_ );
body_ang_current_.psi_dot_2   = ( torque_.psi   / Jzz_ );
</code></pre>

<p>where <code>body_ang_current.&lt;angle&gt;</code> and <code>body_pos_current_.&lt;position&gt;</code> are structures defined in a class to store position, velocities and accelerations of the model given the 4 motor velocities about all 3 axis.</p>

<p>$$
\large \cases{ 
 \ddot X = ( \sin{\psi} \sin{\phi} + \cos{\psi} \sin{\theta} \cos{\phi}) \frac{U_1}{m}  \cr
 \ddot Y = (-\cos{\psi} \sin{\phi} + \sin{\psi} \sin{\theta} \cos{\phi}) \frac{U_1}{m} \cr
 \ddot Z = (-g + (\cos{\theta} \cos{\phi}) \frac{U_1}{m} \cr
 \dot p  = \frac{I_{YY} - I_{ZZ}}{I_{XX}}qr - \frac{J_{TP}}{I_{XX}} q \Omega + \frac{U_2}{I_{XX}} \cr
 \dot q  = \frac{I_{ZZ} - I_{XX}}{I_{YY}}pr - \frac{J_{TP}}{I_{YY}} p \Omega + \frac{U_3}{I_{YY}} \cr
 \dot r  = \frac{I_{XX} - I_{YY}}{I_{ZZ}}pq - \frac{U_4}{I_{ZZ}}
}
$$</p>

<p>Once I get the accelerations above I m going to integrate them to get velocities and positions as well:</p>

<pre><code>/* Get position and velocities from accelerations */
body_pos_current_.x_dot = body_pos_current_.x_dot_2 * real_duration + body_pos_previous_.x_dot;
body_pos_current_.y_dot = body_pos_current_.y_dot_2 * real_duration + body_pos_previous_.y_dot;
body_pos_current_.z_dot = body_pos_current_.z_dot_2 * real_duration + body_pos_previous_.z_dot;

body_ang_current_.phi_dot   = body_ang_current_.phi_dot_2   * real_duration + body_ang_previous_.phi_dot;
body_ang_current_.theta_dot = body_ang_current_.theta_dot_2 * real_duration + body_ang_previous_.theta_dot;
body_ang_current_.psi_dot   = body_ang_current_.psi_dot_2   * real_duration + body_ang_previous_.psi_dot;

body_pos_current_.x = 0.5 * body_pos_current_.x_dot_2 * pow( real_duration, 2 ) + ( body_pos_previous_.x_dot * real_duration ) + body_pos_previous_.x;
body_pos_current_.y = 0.5 * body_pos_current_.y_dot_2 * pow( real_duration, 2 ) + ( body_pos_previous_.y_dot * real_duration ) + body_pos_previous_.y;
body_pos_current_.z = 0.5 * body_pos_current_.z_dot_2 * pow( real_duration, 2 ) + ( body_pos_previous_.z_dot * real_duration ) + body_pos_previous_.z;

body_ang_current_.phi   = 0.5 * body_ang_current_.phi_dot_2   * pow( real_duration, 2 ) + ( body_ang_previous_.phi_dot   * real_duration ) + body_ang_previous_.phi;
body_ang_current_.theta = 0.5 * body_ang_current_.theta_dot_2 * pow( real_duration, 2 ) + ( body_ang_previous_.theta_dot * real_duration ) + body_ang_previous_.theta;
body_ang_current_.psi   = 0.5 * body_ang_current_.psi_dot_2   * pow( real_duration, 2 ) + ( body_ang_previous_.psi_dot   * real_duration ) + body_ang_previous_.psi;

/* Copy the new value into the previous one (for the next loop) */
body_pos_previous_.x = body_pos_current_.x;
body_pos_previous_.y = body_pos_current_.y;
body_pos_previous_.z = body_pos_current_.z;

body_pos_previous_.x_dot = body_pos_current_.x_dot;
body_pos_previous_.y_dot = body_pos_current_.y_dot;
body_pos_previous_.z_dot = body_pos_current_.z_dot;

body_ang_previous_.phi   = body_ang_current_.phi;
body_ang_previous_.theta = body_ang_current_.theta;
body_ang_previous_.psi   = body_ang_current_.psi;

body_ang_previous_.phi_dot   = body_ang_current_.phi_dot;
body_ang_previous_.theta_dot = body_ang_current_.theta_dot;
body_ang_previous_.psi_dot   = body_ang_current_.psi_dot;
</code></pre>

<p>The model seems to work well but, as like reported in many papers, is very unstable and needs some controls.</p>

<p>The first approach for me was to create a controller (PD) to keep the height constant without moving the quadcopter, but just putting a value (for example 3 meter) and see how it reacts.</p>

<p>Here the small code I tried:</p>

<pre><code>/* PD Controller */
double e = ( 3.0 - body_pos_current_.z );  // 3.0 is just a try value!!!
thrust_.esum = thrust_.esum + e;
thrust_.total = 1.3 * e + 0.2 * real_duration * thrust_.esum;
</code></pre>

<p>The problem, as you can see here in this video, is that the copter starts falling down into the ground and not reaching the desired altitude (3.0 meters).
Then it comes back again again like a spring, which is not damped.
I tried already many different value for the PD controller but it seems that it doesn't affect the dynamic of the model.</p>

<p>Another strange thing is that it goes <em>always</em> to a negative point under the ground, even if I change the desired height (negative or positive).</p>

<p>What s wrong in my code? 
Could you me please point to some documents or code which is understandable and well documented to start?</p>

<p>Thanks</p>

<p><strong>EDIT:</strong>
Many thanks to your suggestion.
Hi was really surprise to know, that my code had lots of potential problems and was not very efficient. So I elaborate the code as your explanation and I implementers a RK4 for the integration. After I ve read those articles: <a href=""http://gafferongames.com/game-physics/integration-basics/"" rel=""nofollow"">here</a> and <a href=""http://buttersblog.com/runge-kutta/"" rel=""nofollow"">here</a> I got an idea about RK and its vantage to use it in simulations and graphics PC.
As an example I rewrote again the whole code:</p>

<pre><code>/* Calculate the acceleration about all 6 axis */
pos_.dVel.x =  ( ( thrust_.total / masse_ ) * ( -sin( body_position_.angle.theta ) * cos( body_position_.angle.phi ) * cos( body_position_.angle.psi ) - sin( body_position_.angle.phi ) * sin( body_position_.angle.psi ) ) );
pos_.dVel.y =  ( ( thrust_.total / masse_ ) * ( sin( body_position_.angle.phi ) * cos( body_position_.angle.psi ) - cos( body_position_.angle.phi ) * sin( body_position_.angle.theta ) * sin( body_position_.angle.psi ) ) );
pos_.dVel.z =  ( ( thrust_.total / masse_ ) * ( -cos( body_position_.angle.phi ) * cos( body_position_.angle.theta ) ) - 9.81 );

pos_.dOmega.phi   = ( torque_.phi   / Jxx_ );
pos_.dOmega.theta = ( torque_.theta / Jyy_ );
pos_.dOmega.psi   = ( torque_.psi   / Jzz_ );

/* Get position and velocities from accelerations */
body_position_ = RKIntegrate( body_position_, real_duration );
</code></pre>

<p>which is much more clear and easy to debug. Here some useful functions I implemented:</p>

<pre><code>QuadrotorController::State QuadrotorController::evaluate( const State &amp;initial, const Derivative &amp;d, double dt ) {

State output;
output.position.x = initial.position.x + d.dPos.x * dt;
output.position.y = initial.position.y + d.dPos.y * dt;
output.position.z = initial.position.z + d.dPos.z * dt;

output.velocity.x = initial.velocity.x + d.dVel.x * dt;
output.velocity.y = initial.velocity.y + d.dVel.y * dt;
output.velocity.z = initial.velocity.z + d.dVel.z * dt;

output.angle.phi   = initial.angle.phi   + d.dAngle.phi   * dt;
output.angle.theta = initial.angle.theta + d.dAngle.theta * dt;
output.angle.psi   = initial.angle.psi   + d.dAngle.psi   * dt;

output.omega.phi   = initial.omega.phi   + d.dOmega.phi   * dt;
output.omega.theta = initial.omega.theta + d.dOmega.theta * dt;
output.omega.psi   = initial.omega.psi   + d.dOmega.psi   * dt;

return output;

};

QuadrotorController::Derivative QuadrotorController::sampleDerivative( double dt, const State &amp;sampleState ) {

Derivative output;

output.dPos = sampleState.velocity;
output.dVel.x = pos_.dVel.x;
output.dVel.y = pos_.dVel.y;
output.dVel.z = pos_.dVel.z;

output.dAngle = sampleState.omega;
output.dOmega.phi   = pos_.dOmega.phi;
output.dOmega.theta = pos_.dOmega.theta;
output.dOmega.psi   = pos_.dOmega.psi;

return output;

};

QuadrotorController::State QuadrotorController::RKIntegrate( const State &amp;state, double dt ) {

const double C1 = 0.0f;
const double C2 = 0.5f, A21 = 0.5f;
const double C3 = 0.5f, A31 = 0.0f, A32 = 0.5f;
const double C4 = 1.0f, A41 = 0.0f, A42 = 0.0f, A43 = 1.0f;

const double B1 = 1.0f/6.0f, B2 = 1.0f/3.0f, B3 = 1.0f/3.0f, B4 = 1.0f/6.0f;

Derivative k1 = sampleDerivative( 0.0f, state );
Derivative k2 = sampleDerivative( C2 * dt, evaluate( state, k1 * A21, dt ) );
Derivative k3 = sampleDerivative( C3 * dt, evaluate( state, k1 * A31 + k2 * A32, dt ) );
Derivative k4 = sampleDerivative( C4 * dt, evaluate( state, k1 * A41 + k2 * A42 + k3 * A43, dt ) );

const Derivative derivativeSum = k1 * B1 + k2 * B2 + k3 * B3 + k4 * B4;

return evaluate( state, derivativeSum, dt );
}
</code></pre>

<p>Now I m really lost because...because the simulated qudrotor has the same behavior as before. Nevertheless I ve implemented the same PD algorithm as discussed in the paper, it stabilize on Z (height) but it get really crazy due to unstable behavior. 
So... I dunno what is wrong in my code and my implementation. And above all I cannot find any source in internet with a good self explaned dynamic model for a quadrotor.</p>

<p>Regards</p>
","control quadcopter"
"4562","Non-linear complementary filter on so3: Corrected equations?","<p>While reading the paper <a href=""http://ieeexplore.ieee.org/xpl/freeabs_all.jsp?arnumber=6289431&amp;abstractAccess=no&amp;userType=inst"" rel=""nofollow"">""Multirotor Aerial Vehicles: Modeling, Estimation, and Control of Quadrotor"" by Mahony, Kumar and Corke</a>, I stumbled across the following equations for a non-linear attitude observer, which I would like to implement, but I believe there is something wrong.</p>

<p>$\dot{\hat{R}} := \hat{R} \left( \Omega_{IMU} - \hat{b} \right)_\times - \alpha \\
\dot{\hat{b}} := k_b \alpha \\
\alpha := \left( \frac{k_a}{g^2}((\hat{R}^T \vec z) \times a_{IMU}) + \frac{k_m}{|^Am|^2} ((\hat{R}^T {^Am}) \times m_{IMU}) \right)_\times + k_E \mathbb{P}_{so(3)} (\hat{R} R_E^T)$</p>

<p>Where $\hat{R}$ and $\hat{b}$ are etimates of orientation and gyroscope bias, $\Omega_{IMU}, a_{IMU}, m_{IMU}, R_E^T$ are measurements and $k_X$ are scalar gains, which may be set to 0 for measurements that are not evailable.</p>

<p>Now $\dot{\hat{R}}$ and $\alpha$ need to be matrices $\in \mathbb{R}^{3\times 3}$ due to their definitions. $\hat{b}$ and thus $\dot{\hat{b}}$ need to be vectors $\in \mathbb{R}^3$. But then what is the correct version of the second equation $\dot{\hat{b}} := k_b \alpha$?</p>
","control quadcopter"
"4568","What is a PID as is related to quadcopters","<p>I'm trying to make a Quadcopter from scratch, I have a fair amount of experience with adruinos, and I'm trying to understand how to necessary systems work, and I can't seem to figure out what PID means, is it a method of regulating pitch and roll? like a stabilizer? I think from what I've read that its a system that detects orientation of the craft and tries to correct it</p>
","arduino quadcopter microcontroller pid beginner"
"4569","Are consumer grade CNC machines capable of cutting tile?","<p>I'd like to slice and dice floor tile into pieces so I can arrange it in geometric patterns. I have CAD designs for the parts. Would any consumer grade CNC machine be capable of doing the job?</p>
","cnc"
"4573","Laser Beam based model probability in case of single particle","<p>I am trying to calculate likelihood of laser scan($Z$) at give pose($x$) with known map ($m$) using beam based model</p>

<p>$P\left(z_t|x_t,m \right)=\prod_{i=1}^{n}P'\left(z_i|x_t,m \right)$    </p>

<p>My scan has 360 rays i.e $n=360$, When i calculate $P\left(z_t|x_t,m \right)$ it becomes zero as multiplication all propabilities $&lt;1.$ </p>

<p>In <a href=""https://github.com/ros-planning/navigation/blob/b384cac812381a9313d7cdc9be9eb272c4fccb38/amcl/src/amcl/sensors/amcl_laser.cpp#L168"" rel=""nofollow"">ROS amcl</a> they are using ad-hoc which works better like</p>

<p>$P\left(z_t|x_t,m \right)+=\sum_{i=1}^{n}P'\left(z_i|x_t,m \right)*P'\left(z_i|x_t,m \right)*P'\left(z_i|x_t,m \right)$</p>

<p>later they normalise it with number of particle to get weight of each particle.</p>

<p>My query is how to get probability normalised and not zero with <em>single calculation</em> <em>(i.e image in case of single particle)</em></p>

<p>Thanks.</p>
","mobile-robot slam laser probability"
"4575","Use linear quadratic regulator to minimize output error","<p>I would like to create an Infinite-horizon, continuous-time LQR with a cost functional defined as</p>

<p>$$J = \int_{0}^\infty \left( e^T Q e + u^T R u \right) dt$$</p>

<p>where e is the states' error $x-x_d$, but I have trouble concluding to the appropriate Ricatti equation since $x_d$ is a function of time therefore leading to a term of $\dot x_d$ . Is this problem solvable? Any ideas?</p>
","control"
"4577","Can't Read current on Pololu Dual MC33926 Motor Driver Shield for Arduino","<p>I purchased a <a href=""http://www.pololu.com/product/2503"" rel=""nofollow"">Pololu Dual MC33926 Motor Driver Shield for Arduino</a>, and for some reason I cannot read current from the motor controller. On the Serial.println() it just prints weird data (garbage), and when I use ROS (Robot Operating System) I only see -0.0 (minus zero) value for both motors.</p>

<p>All I've done is plug the shield on my Arduino UNO R3 model, and run the demo that comes with the sample library --
<a href=""http://github.com/pololu/dual-mc33926-motor-shield"" rel=""nofollow"">http://github.com/pololu/dual-mc33926-motor-shield</a> .</p>

<p>How can I fix this issue?</p>
","arduino ros actuator stepper-driver current"
"4578","kinect - development kit to aid obstacle-avoiding robot?","<p>Some friends and I are interested in working on a robot. I know little to nothing about robotics, but do have a lot of experience with programming. Before we start, I am hoping that I can find some development kits or libraries that will help aid the goals of the robot. Which are:</p>

<ul>
<li>Robot needs to move from point A to point B. While moving, it needs to detect rocks (approx. 1 foot diameter) on ground. It needs to detect rocks that are big enough to stop it, turn away from them, and proceed. </li>
</ul>

<p>In theory, we will want to detect the kinect's angle via the accelerometers, and use that data to obtain Cartesian coordinates of the ground from the kinect's sensors. Later, we will want a way to assemble a 'map' in the robot's memory so that it can find better paths from A to B.</p>

<p>Right now we aren't concerned with the motors on the robot - only the vision element. Ie, I am not really interested in software that interfaces with the motors of the robot, only only something that interfaces with the kinect. </p>
","computer-vision kinect"
"4579","Quadrature Encoder Counts","<p>Actually , I have been since two weeks looking for convinced and final solution for my problem , actually I am completely lost , I am working on mobile robot (Rover 5) with 2 motors , 2 encoders . the controller that designed to the robot needs to know the odometery of mobile robot (X ,Y, Heading Angle ) , actually I am trying to function the encoders for this purpose , getting X ,Y, Heading Angle by measuring the traveled distance by each wheel , so to get the X ,Y, Heading Angle values , I should compute a accurate readings without missing any counts or ticks as could as possible .</p>

<p>The problem now is :</p>

<p>In the code in the attachment , while I am testing the encoders counts , I noticed that there is a difference between counts of encoders even when they spin in the same constant speed (PMW) , the difference increases as the two motors continue . so I thought that is the main cause of inaccurate odometery results .</p>

<p>In the output of the code (in the attachment also) the first two columns are right and left motors speed , the third &amp; forth columns are right and left encoder counts , the fifth column is the difference between two encoders count , as you could see ,that even when the speed of two motors are approximately the same (each motor feed up with 100 PWM) there is a difference in the encoder counts and as you could see that the difference become big and big as the motors continuing spin .</p>

<p>One thing I thought that sending the same PWM value to two different motors will almost never produce the exact same speed , so I think that I should detect the absolute motion of the motors and adjust the power to get the speed/distance , but when I test the speed of motors after feed them with 100 PWM at same time , the two speeds were almost identical , but I noticed that there is a difference between counts of two encoders even when the motors spin in the same constant speed .</p>

<p>Actually , I don't know where is the problem , Is it in the code ? Is it in the hardware ? or what ? I am completely lost , I need for patient someone to help.</p>

<pre><code>/* Encoder-ino.ino
*/
#define encoder0PinA 2
#define encoder0PinB 4
#define encoder1PinA 3
#define encoder1PinB 5

volatile int encoder0Pos = 0;
volatile int encoder1Pos = 0;
int WR=100;  // angular velocity of right wheel  
int WL=100;  // angular velocity of right wheel                       



long newposition;
long oldposition = 0;
unsigned long newtime;
unsigned long oldtime = 0;
long vel;


long newposition1;
long oldposition1 = 0;
unsigned long newtime1;
unsigned long oldtime1 = 0;
long vel1;



int ENA=8;    // SpeedPinA connected to Arduino's port 8  
int ENB=9;    // SpeedPinB connected to Arduino's port 9 

int IN1=48;    // RightMotorWire1 connected to Arduino's port 48
int IN2=49;    // RightMotorWire2 connected to Arduino's port 49

int IN3=50;    // RightMotorWire1 connected to Arduino's port 48
int IN4=51;    // RightMotorWire2 connected to Arduino's port 49


void setup() {
 pinMode(ENA,OUTPUT);
 pinMode(ENB,OUTPUT);
 pinMode(IN1,OUTPUT);
 pinMode(IN2,OUTPUT);
 pinMode(IN3,OUTPUT);
 pinMode(IN4,OUTPUT);

 digitalWrite(ENA,HIGH);    //enable motorA
 digitalWrite(ENB,HIGH);    //enable motorB
  pinMode(encoder0PinA, INPUT); 
  pinMode(encoder0PinB, INPUT);
  pinMode(encoder1PinA, INPUT); 
  pinMode(encoder1PinB, INPUT);  
// encoder pin on interrupt 0 (pin 2)
attachInterrupt(0, doEncoderA, CHANGE);  

// encoder pin on interrupt 1 (pin 3)

attachInterrupt(1, doEncoderB, CHANGE);  
  Serial.begin (9600);
}

void loop(){ 

    int rightPWM;
  if (WR &gt; 0) {
    //forward
  digitalWrite(IN1,LOW);
  digitalWrite(IN2,HIGH);

  }  else if (WR &lt; 0){
    //reverse
  digitalWrite(IN1,HIGH);
  digitalWrite(IN2,LOW);
  }

  if (WR == 0) {
   rightPWM = 0;
   analogWrite(ENA, rightPWM);
  } else {
    rightPWM = map(abs(WR), 1, 100, 1, 255);
    analogWrite(ENA, rightPWM);
  }

 int leftPWM;

  if (WL &gt; 0) {
     //forward
  digitalWrite(IN3,LOW);
  digitalWrite(IN4,HIGH);
  }  else if (WL &lt; 0) {
     //reverse
  digitalWrite(IN3,HIGH);
  digitalWrite(IN4,LOW);}

  if (WL == 0) {
    leftPWM = 0;
    analogWrite(ENB, leftPWM);
  } else {
    leftPWM = map(abs(WL), 1, 100, 1, 255);
    analogWrite(ENB, leftPWM);
  }


// to determine the speed of motors by encoders 

 newposition = encoder0Pos;
 newtime = millis();
 vel = (newposition-oldposition) * 1000 /(long)(newtime-oldtime);
 oldposition = newposition;
 oldtime = newtime;

 newposition1 = encoder1Pos;
 newtime1 = millis();
 vel1 = (newposition1-oldposition1) * 1000 /(long)(newtime1-oldtime1);
 oldposition1 = newposition1;
 oldtime1 = newtime1;

Serial.print (vel);
Serial.print (""\t"");
Serial.print (vel1);
Serial.print (""\t"");
Serial.print (encoder0Pos*-1); 
Serial.print(""\t""); 
Serial.print (encoder1Pos*-1); 
Serial.print(""\t""); 
Serial.println ((encoder0Pos*-1) -( encoder1Pos*-1)); 
}



// 1 encoder counts

void doEncoderA(){
  // look for a low-to-high on channel A
  if (digitalRead(encoder0PinA) == HIGH) { 
    // check channel B to see which way encoder is turning
    if (digitalRead(encoder0PinB) == LOW) {  
      encoder0Pos = encoder0Pos + 1;         // CW
    } 
    else {
      encoder0Pos = encoder0Pos - 1;         // CCW
    }
  }
  else   // must be a high-to-low edge on channel A                                       
  { 
    // check channel B to see which way encoder is turning  
    if (digitalRead(encoder0PinB) == HIGH) {   
      encoder0Pos = encoder0Pos + 1;          // CW
    } 
    else {
      encoder0Pos = encoder0Pos - 1;          // CCW
    }
  }
}



// 2 encoder counts

void doEncoderB(){
  // look for a low-to-high on channel B
  if (digitalRead(encoder1PinB) == HIGH) {   
   // check channel A to see which way encoder is turning
    if (digitalRead(encoder1PinA) == HIGH) {  
      encoder1Pos = encoder1Pos + 1;         // CW
    } 
    else {
      encoder1Pos = encoder1Pos - 1;         // CCW
    }
  }
  // Look for a high-to-low on channel B
  else { 
    // check channel B to see which way encoder is turning  
    if (digitalRead(encoder1PinA) == LOW) {   
      encoder1Pos = encoder1Pos + 1;          // CW
    } 
    else {
      encoder1Pos = encoder1Pos - 1;          // CCW
    }
  }
}
</code></pre>

<p>the result:</p>

<pre><code>0   0   0   0   0
0   0   0   0   0
0   0   0   0   0
0   0   0   0   0
0   0   0   0   0
0   0   0   0   0
0   0   0   0   0
0   0   0   0   2
-181    -90 3   2   1
-111    -55 5   4   1
-187    -187    9   8   2
-176    -235    12  12  1
-200    -200    16  16  1
-250    -250    21  21  1
-250    -250    26  26  1
-210    -210    31  31  1
-238    -285    36  36  1
-315    -263    41  41  1
-300    -200    47  46  2
...
-227    -272    184 182 3
-285    -285    190 187 4
-260    -217    195 193 3
-238    -285    201 199 3
...
-250    -250    1474    1473    2
-250    -250    1480    1479    0
-208    -291    1485    1485    1
-304    -260    1491    1492    1
-240    -240    1498    1498    1
-260    -260    1504    1505    0
-250    -291    1510    1511    1
-280    -240    1516    1517    1
-260    -260    1523    1523    1
...
-250    -250    2953    2948    5
-250    -291    2959    2955    6
-250    -250    2965    2961    6
-291    -250    2971    2967    5
-250    -291    2978    2973    5
-304    -250    2985    2980    8
-320    -250    2992    2986    8
...
-320    -240    3085    3075    10
-291    -291    3092    3082    12
-269    -230    3099    3089    11
-250    -291    3105    3095    11
-280    -280    3112    3102    11
-269    -230    3118    3108    12
-250    -291    3125    3115    11
...
-291    -250    3607    3587    19
-115    -269    3610    3594    17
-240    -240    3617    3601    18
-375    -291    3625    3607    19
-269    -269    3632    3614    20
-291    -250    3638    3620    20
-240    -280    3645    3627    20
-280    -240    3652    3633    18
-200    -280    3657    3640    19
-269    -230    3664    3647    19
-333    -291    3674    3653    23
-400    -280    3682    3659    23
-280    -240    3688    3666    24
-240    -280    3695    3673    24
...
-230    -269    4677    4644    32
-208    -291    4681    4651    32
-280    -240    4690    4657    35
-320    -280    4696    4664    34
-240    -240    4703    4670    34
-291    -291    4710    4677    34
-269    -230    4716    4683    34
-240    -280    4723    4690    34
-280    -240    4727    4697    32
-160    -280    4736    4703    35
-416    -291    4745    4709    38
-346    -230    4753    4716    39
...
-360    -240    6240    6190    51
-375    -291    6247    6197    51
-269    -269    6253    6203    52
-291    -250    6261    6210    53
...
-192    -269    6428    6374    56
-240    -280    6436    6380    57
-291    -250    6443    6387    57
-269    -269    6449    6394    57
...
-269    -269    7763    7687    78
-240    -280    7770    7694    78
-291    -250    7776    7700    76
-192    -269    7781    7707    76
...
-269    -230    8263    8179    84
-250    -291    8269    8186    85
-240    -240    8276    8192    88
-384    -269    8286    8199    88
-250    -291    8292    8206    88
-269    -230    8299    8212    87
-291    -291    8305    8219    88
-240    -240    8310    8225    85
...
-160    -120    8359    8276    83
-125    -166    8362    8280    82
-115    -115    8365    8283    83
-80 -120    8367    8285    82
-125    -83 8370    8288    82
-83 -125    8371    8290    82
-43 -43 8373    8291    81
-83 -83 8374    8293    82
-45 -90 8375    8294    81
-43 -43 8376    8296    81
-43 -43 8377    8296    81
-43 -43 8378    8297    81
</code></pre>
","mobile-robot wheeled-robot quadrature-encoder"
"4580","Linear slider motor mount location - Pros/cons","<p>I'm currently designing a linear camera slider, that will be used to hold camera equipment weighing just about 15 Kgs including all of the lenses and monitors and everything else. </p>

<p>For those who don't know what a camera slider is, it's a linear slider on top of which a camera is mounted and then the camera is slided slowly to create some nice footage <a href=""https://www.youtube.com/watch?v=hLvP2Dr2FWQ"" rel=""nofollow"">like this</a>.</p>

<p><strong>The problem</strong></p>

<p>Now, looking at the commercially available camera sliders out there, there seems to be two ways in which the motor maybe mounted on these sliders:</p>

<ol>
<li><p>Motor mounted on the side:</p>

<p><img src=""http://i.stack.imgur.com/6TMcu.jpg"" alt=""enter image description here""></p></li>
<li><p>Motor mounted directly on the carriage:</p>

<p><img src=""http://i.stack.imgur.com/PE1PW.jpg"" alt=""enter image description here""></p></li>
</ol>

<p>I would like to know which option would be optimal - Performance-wise (this slider maybe used vertically too, to create bottom to top slide shots), efficiency-wise and 
which one of these two will be resistant to motor vibration (these motors vibrate a lot, the effects of which may sometimes leak into the produced footage).</p>

<p><strong>Additional Questions</strong></p>

<ol>
<li><p>Motor mounted on the carriage directly <em>maybe, just maybe</em> more efficient, but it also has to carry it's own weight in addition to the 15kg camera load?</p></li>
<li><p>Pulling force is greater than pushing force (I have no idea why, would be great if someone explained why, atleast in this case?), so a motor mounted in the end should be able to lift vertically with ease?</p></li>
<li><p>Does a belt setup as shown in the first figure above really dampen the motor vibrations? Will/won't the motor vibrating on the end get amplified (because, the whole setup will be attached to a single tripod in the exact center of the slider)</p></li>
<li><p>Which design will be less stressful for the motor, taking inertia into consideration for both cases?</p></li>
<li><p>Which one of these designs will be best suitable for vertical pulling of load against gravity?</p></li>
</ol>

<p>Manufacturers use both designs interchangeably, so it's hard to predict which design is better than which.</p>

<p>Any help would be much appreciated!</p>

<p>Please note, this question has been migrated from the Stackexchange Physics (and Electrical) forum by me because the mods thought it would be appropriate here.</p>
","control design stepper-motor motion"
"4583","What are some low cost alternatives for lidar?","<p>It need not be as effective as lidar or it may have some disadvantages when compared with lidar.   What are the probable alternatives?</p>

<p>Edit:
I'm intending to use it outdoors for navigation of autonomous vehicle. Is there any low cost LIDAR or is there any alternative sensor for obstacle detection?</p>
","sensors navigation cameras sonar lidar"
"4587","Two State Linear Actuator","<p>I need two state linear actuator. You can have a look at the picture to understand what I mean. </p>



<p>Don't care about the hand !</p>

<p><img src=""http://www.robaid.com/wp-content/gallery/mit/mit-inform-system-1.jpg"" alt=""mit inform project""></p>

<p>I need to electrically move the things like this squares up and down. Bidirectional linear actuators are needed.
What is the cheapest and tiniest actuator (or sth else) that I can use to move this squares up and down. There are just two states ('up','down'). Don't care how much higher a square rises, when it is up.</p>
","actuator"
"4589","Particles not behaving correctly in the implementation of particle filter","<p>I am implementing a particle filter in Java. The problem with my particle filter implementation is that the particles suddenly go away from the robot i.e the resampling process is choosing particles which are away from robot more than those which are near.It is like particles chase the robot, but always remain behind it. I am trying to find the root cause, but to no luck. Can anyone please help me where I am going wrong?</p>

<p>I am adding all the imp. code snippets and also some screenshots in consecutive order to make it more clear. </p>

<p><a href=""http://i.stack.imgur.com/Tul8Vm.jpg"" rel=""nofollow""><img src=""http://i.stack.imgur.com/Tul8Vm.jpg"" alt=""1""></a>
<a href=""http://i.stack.imgur.com/t04Wz.jpg"" rel=""nofollow""><img src=""http://i.stack.imgur.com/t04Wzm.jpg"" alt=""2""></a>
<a href=""http://i.stack.imgur.com/5EZJZ.jpg"" rel=""nofollow""><img src=""http://i.stack.imgur.com/5EZJZm.jpg"" alt=""3""></a>
<a href=""http://i.stack.imgur.com/bNqBV.jpg"" rel=""nofollow""><img src=""http://i.stack.imgur.com/bNqBVm.jpg"" alt=""4""></a>
<a href=""http://i.stack.imgur.com/tibqD.jpg"" rel=""nofollow""><img src=""http://i.stack.imgur.com/tibqDm.jpg"" alt=""5""></a></p>

<p>Details:</p>

<p>I am using a range sensor which only works in one direction i.e. its fixed and tells the distance from the obstacle in front. If there is no obstacle in its line of vision, then it tells the distance to boundary wall. </p>

<p>Code:
Calculating Range</p>

<pre><code>/*
 * This method returns the range reading from the sensor mounted on top of robot. 
 * It uses x and y as the actual position of the robot/particle and then creates Vx and Vy as virtual x and y.
 * These virtual x and y loop from the current position till some obstruction is there and tell us distance till there.
 */

private int calculateRange(double x, double y, double Vx, double Vy, int counter, int loop_counter)
{
    while(robotIsWithinBoundary(Vx, Vy))
    {
        int pace = 2;
        Vx += pace* Math.sin(Math.toRadians(robot_orientation));
        Vy += pace* Math.cos(Math.toRadians(robot_orientation));
        counter++;
        Line2D line1 = new Line2D.Double(x,y,Vx,Vy);
        if(line1.intersects(obst1))
        {
            //System.out.println(""Distance to obst1:""+counter);
            loop_counter++;
            break;
        }
        if(line1.intersects(obst2))
        {
            //System.out.println(""Distance to obst2:""+counter);
            loop_counter++;
            break;
        }

    }
    return counter;
}

/*
 * This method tells us whether the robot/particle is within boundary or not.
 */
private boolean robotIsWithinBoundary(double x, double y)
{
    boolean verdict = true;
    if(x&gt;680||x&lt;0)
    {
        verdict = false;
    }
    if(y&lt;0||y&gt;450)
    {
        verdict = false;
    }
    return verdict;
}   /*
 * This method returns the range reading from the sensor mounted on top of robot. 
 * It uses x and y as the actual position of the robot/particle and then creates Vx and Vy as virtual x and y.
 * These virtual x and y loop from the current position till some obstruction is there and tell us distance till there.
 */
private int calculateRange(double x, double y, double Vx, double Vy, int counter, int loop_counter)
{
    while(robotIsWithinBoundary(Vx, Vy))
    {
        int pace = 2;
        Vx += pace* Math.sin(Math.toRadians(robot_orientation));
        Vy += pace* Math.cos(Math.toRadians(robot_orientation));
        counter++;
        Line2D line1 = new Line2D.Double(x,y,Vx,Vy);
        if(line1.intersects(obst1))
        {
            //System.out.println(""Distance to obst1:""+counter);
            loop_counter++;
            break;
        }
        if(line1.intersects(obst2))
        {
            //System.out.println(""Distance to obst2:""+counter);
            loop_counter++;
            break;
        }

    }
    return counter;
}

/*
 * This method tells us whether the robot/particle is within boundary or not.
 */
private boolean robotIsWithinBoundary(double x, double y)
{
    boolean verdict = true;
    if(x&gt;680||x&lt;0)
    {
        verdict = false;
    }
    if(y&lt;0||y&gt;450)
    {
        verdict = false;
    }
    return verdict;
}
</code></pre>

<p>Calculating Weights</p>

<pre><code>/*
 * This method calculates the importance weights for the particles based on the robot_range which is 
 * the reading of the range sensor for the robot.
 */
private double measurementProbability(int index)
{
    double probability=1;
    double particle_x_position=particleListX.get(index);
    double particle_y_position=particleListY.get(index);
    double particle_Vx=particle_x_position;
    double particle_Vy=particle_y_position;
    int range_counter=0;
    int loop_counter=0;

    int distance = calculateRange(particle_x_position, particle_x_position, particle_Vx, particle_Vy ,range_counter, loop_counter); 
    probability *= calculateGaussianDistance(distance, senseNoise, robot_range);
    //System.out.println(probability);          
    return probability;
}

private double calculateGaussianDistance(double mu, double sigma, double x )
{
    double gDistance=Math.exp(-(((Math.pow((mu - x),2))/(Math.pow(sigma,2)) / 2.0) / (Math.sqrt(2.0 * Math.PI * (Math.pow(sigma,2))))));
    return gDistance;
}
</code></pre>

<p>Resampling</p>

<pre><code>/*
 * This method provides a resampled particle back to the list. It chooses a particle randomly 
 * from the list based on the weights with replacement.
 */
private int giveResampledParticle()
{
    int index = randomInteger(0, n-1);
    double sample =0.0;
    double maxWeight = maximumWeight();

    sample += randomDouble(0, maxWeight);
    while(sample &gt; particleListProbability.get(index))
    {
        sample -= particleListProbability.get(index);
        index = (index +1) % n;
    }
    return index;
}
</code></pre>
","localization particle-filter"
"4591","Robotc color sensor error","<p>I am trying to write a simple program where the robot(Lego NXT2) will follow a blue line.</p>

<pre><code>#pragma config(Sensor, S1,     ,               sensorCOLORBLUE)
//!!Code automatically generated by 'ROBOTC' configuration wizard               !!//

task main()
{
    while(true)
    {
        if(SensorValue[S1] == sensorCOLORBLUE)
        {
            motor[motorB] = 0;
            motor[motorC] = -50;
        }
        else
        {
            motor[motorB] = -50;
            motor[motorC] = 0;
        }
    }
    wait1Msec(1);
}
</code></pre>

<p>I am using an nxt color sensor and the problem is that only 1 motor is moving. I know that none of the motors are broken either because I tested them out.
Can somebody help me diagnose my problem?</p>
","sensors robotc"
"4592","For robot wheel control : Brushless DC motor or Servo Motor?","<p>Simply, when to use brushless dc motor and when to use Servo Motor ? </p>

<p>what are the differences , specially when adding an encoder to the dc motor you can have the position and it will be similar to Servo Motor ? </p>
","motor brushless-motor servomotor"
"4595","Do simple, non-sonic, omni-directional rangefinding beacons exist?","<p>I am on a robotics team that plans to compete in a competition where one of the rules is that no sort of sonic sensor is allowed to be used. I guess that limits it to some sort of EM frequency right?</p>

<p>Ideally, my team is looking for a simple beacon system, where beacon A  would be attached to the robot, while beacon B would be attached to a known point on the competition space. Then, beacon A can give information about how far away B is. After some searching, I could only turn up laser rangefinders that required pointing at the target. I am a CS student, so I'm not familiar with the terminology to aid searches.</p>

<p>Another nice property would be if the beacons also gave the angle of beacon A in beacon B's field of view, although this is not necessary, since multiple beacons could be used to obtain this information.</p>

<p>We have an Xbox 360 Kinect working, and able to track things and give distances, but it looses accuracy over distance quickly (the arena is about 6 meters long), and this beacon should be as simple as possible. We ONLY need it for a relative position of our robot.</p>

<p><strong><em>Alternate Solution:</em></strong>
Another way to solve this would be for an omni-directional beacon to only give angle information, two of these could be used to triangulate, and do the job just as well.</p>
","localization electronics laser rangefinder"
"4599","the uncertainty of initializing new landmark in EKF-SLAM","<p>In EKF-SLAM (based-feature map) once the robot senses a new landmark, it is augmented to state vector. As a result, the size of the state vector and the covariance matrix are expanded. My question is about the uncertainty of the new landmark and its correlation with other pairs of the covariance matrix. How should I assign them? When I assign them to be zero, the error of the estimation this landmark won't change as time goes. If I assign them with very large value, the estimation is getting better every time the robot reobserves this landmark however, the error approaches to fixed value not to zero. I assume the problem id with assigning the uncertainty. Any suggestions?</p>
","slam ekf errors mapping"
"4607","Quadcopter Roll, Pitch Fluctuation","<p>For my quadcopter, i turn on the quadcopter while letting it stable on the ground. But i see that the Roll, Pitch fluctuate with the max difference being 15 degree. When i protect the sensor with soft material, then i observe the max difference is around 6 degree. Is this fluctuation for the quadcopter? By the way, i use complementary filter and DCM with scaling factor being 0.8 gyro and 0.2 accel
Thanks in advance!</p>
","quadcopter"
"4608","Deducing single wing plane transfer function Aka Transfer function estimation through set of points","<p>I'm trying to control a plane via roll  using PID controller , </p>

<p>I had a problem finding the transfer function thus I used the following method :- </p>

<blockquote>
  <p>Fix the plane in an air tunnel </p>
  
  <p>change the motor that controls the roll in fixed steps and check the
  roll </p>
  
  <p>thus I will have a table of roll/motor degree </p>
  
  <p>next is to deduce the nonlinear function using wolfram alpha or
  approximation neural network  .</p>
</blockquote>

<p>Is this a correct method or should I try another method ? </p>
","pid"
"4609","Is increasing gyro , accelerometer sensor range is good or bad ? how does it affect the accuracy","<p>I've been using mpu6050 IMU unit ( gyro + accelerometer  ) </p>

<p>I found that I can set acc range to +/- 2g or 4g till 16 g </p>

<p>and same for gyro +/- 250 deg/sec , 500 deg/sec and so </p>

<p>I know that they are low cost and full noise , so which settings to the range are best to ensure higher accuracy ? </p>
","sensors imu accelerometer gyroscope"
"4610","Forward Kinematics/D-H parameters for perpendicular joint axes","<p>I am trying to compute forward kinematics of the Kuka youBot using DH convention: </p>

<p><a href=""http://www.youbot-store.com/youbot-developers/software/simulation/kuka-youbot-kinematics-dynamics-and-3d-model"">http://www.youbot-store.com/youbot-developers/software/simulation/kuka-youbot-kinematics-dynamics-and-3d-model</a></p>

<p>The arm joint 1 and arm joint 5 are revolute and rotate about the world z-axis (pointing to the sky)</p>

<p>But the other 3 joints are all revolute and rotate about x-axis, let's say (points horizontally)</p>

<p>DH convention says the ""joint distance"" is along the ""common normal"". But unless I am mistaken, the only common normal is the y-axis, and that is also horizontal, meaning there is no joint distance.</p>

<p>I was thinking I would use link offset for joint1 - joint2, but then I ran into a problem with joint4 - joint5. Link offset is supposed to be along the previous z-axis, and in that case it would point horizontally out to nowhere. But link distance STILL doesn't work either, because that is the common normal distance, and as established the common normal is x-axis, also horizontal. So now I feel very screwed. I am sure there is a simple solution but I can't see it. </p>

<p>So I guess the question is, how do I use the DH convention for the links between 1-2 and 4-5, when the joint rotational axes are perpendicular?</p>
","kinematics forward-kinematics dh-parameters"
"4612","Controlling multiple Arduinos wirelessly","<p>I am designing a experiment of controlling 6 small wind turbines wirelessly. For each wind turbine, I need to measure power time series (or voltage or current time series) from the generator, and control blade pitch angle, yaw angle, and generator load (using variable resistance). The control input will be all PWM signal.</p>

<p>I am planning to put an Arduino UNO with a ZigBee wireless module to each wind turbine, making it measure the power time series and transmit to the central node, as well as receive the control input from the central node and command the control input to servo motors. The central node will be additional Arduino UNO.</p>

<p>Here are my questions:</p>

<p>Is it possible for each Arduino to send time series signal to central node wirelessly without interference with other Arduino? (6 wind turbines transmitting time series to a central server). If it is possible, How can I implement such network ? recommending a source for learning would be also greatly helpful.</p>

<p>Interface between the central node and the computer software: The algorithm in the computer need to process the received power time series and determine the optimum control input for 6 wind turbines. Then these control input should be transmitted to wirelessly to 6 wind turbines. In such case, what is the good option to interface the algorithm and the Arduino connected to the computer? Currently the algorithm is written in Matlab. I heard there is the sketch interfacing Arduino and Matlab, is it efficient enough for such project?</p>
","arduino"
"4625","Seeking dirt cheap, wheeled, programmable robot","<p>I was playing the old ""confuse the cat with a flash-light"" game, when I thought that I might like to program a confuse-a-cat robot.</p>

<p>Something, probably with tracks, which can right itself if he flips it over, and which I can program to move randomly around a room, turning at walls, making an occasional sound or flashing a light.</p>

<p>Since I am on a <strong><em>very</em></strong> tight budget, I wondered if there is some cheap kit which I can program ...</p>

<p>Arduino, Raspberry Pi, any platform, so long as it is programmable.</p>

<p>Thanks in advance for your help</p>
","mobile-robot"
"4627","Control a 2.4 Ghz AR Drone from the computer","<p>I had a Doyusha Nano Spider R/C mini-copter, it's controlled by a 4ch joystick 2.4 Ghz.</p>

<p>I look for a low cost method to control it from the computer. The software is not a problem, but how can I transform the WIFI or the Bluetooth signal of the computer to an R/C signal compatible with the mini-copter receptor?</p>

<p>Or is there another solution that is low cost?</p>
","control quadcopter wireless"
"4632","What mechanical parts can be attached to DYJ48 stepper motor?","<p>Sorry I am asking a mechanical question here, but, after all, where else people have experience with using motors? If there is a better forum for this, please do guide me.</p>

<p>Everywhere I've seen online, the stepper motor DYJ48 is used in tutorials, to rotate on its own, or, at most, to spin a clothes pin attached to it. I am trying to get Arduino to work for my 10 year old kid. He's got the motor rotating, now what? How does he attach anything to it?</p>

<p>Don't laugh, I made him a wheel out of a raw potato. He is happy with it now. Where can I find any guidance as to what to do next?</p>
","arduino motor"
"4635","12V compressor and air pressure control","<p>I am trying to make a simple robot with few functionality for someone, one of these functionality is inflating a balloon inside the robot, I know how to control a compressor using Arduino but the problem is that the requested task is bit different here:
There must be an air exit and it must be controllable through arduino, so he can inflate the balloon to a certain pressure, and depress the air from another exit if needed (I don't know if it is possible to have a depression through the same pressure-in valvle.</p>

<p>I think that it can be done somehow using a solenoid 3/2 valve or something but I am bit unfocused these days and I need some hints.</p>

<p>Any thoughts?</p>
","arduino wheeled-robot mechanism industrial-robot valve"
"4639","Where do roboticists look for used sensors/hardware?","<p>I recently built a self-driving vehicle-type robot for a competition, and am looking to sell sensors (GPS, INS, etc.) used in order to have money for the next project. Is ebay where people tend to go looking for used sensors and hardware?</p>
","sensors servos"
"4642","Finding a light load high precision servo motor","<p>I have a project that requires me to be able to accurately and repeatedly rotate an object 120 degrees.</p>

<p>The object is small and lightweight (let's say several grams). The axis does not necessarily have to always spin the same direction. It simply needs to be able to stop reliably at 0, +/- 120, and +/-240 degrees from the origin.</p>

<p>I have VERY limited experience with motors and robotics, but my understanding is that a servo motor will be my best bet for accuracy (if that incorrect, please let me know).</p>

<p>Since I know next to nothing about these motors, the spec sheets list a lot of specs which don't mean all that much to me. I'm hoping to learn, but in the mean time, what specifications do I need to be focusing on for these requirements?</p>

<p>It doesn't need to be high speed. When I say accurate, it doesn't have to be absolutely perfect to the micrometer, but I would like it to be able to run through a loop of stopping at 0, 120, and 240 hundreds of times without visually noticeable variance - the more precise the better though.</p>

<p>To be more specific about the accuracy. Let's say the object being rotated will have a flat surface on the top at each of those 3 stopping points.  Upon inspection the surface needs to appear level each and every time through hundreds of cycles.</p>

<p>Could these requirements be met by a servo that might be used in building a quadricopter, or am I going to be looking for something higher grade than that?</p>
","servomotor"
"4648","Controlling Dynamixel servo wirelessly using Arduino Mega","<p>I am planning to control multiple Dynamixel servos (MX28T or MX-64T) wirelessly using Arduino Mega. Since this servo uses serial communication, I need an additional serial port to interface with Xbee module. Although it seems to be very common application controlling these servos wirelessly based on Arduino, I could't find any of them in web. I found the two very well constructed libraries.</p>

<p><a href=""https://code.google.com/p/slide-33/downloads/list"" rel=""nofollow"">https://code.google.com/p/slide-33/downloads/list</a>. This library is for MX28T servo, which is the same servo I am trying to use, but it uses UNO;therefore, I cannot interface with Xbee.</p>

<p><a href=""http://www.pablogindel.com/informacion/the-arduinodynamixel-resource-page/"" rel=""nofollow"">http://www.pablogindel.com/informacion/the-arduinodynamixel-resource-page/</a>. This library use UART1 (serial1) to interface with servo (AX-12) motors. Therefore, I can connect Xbee module to UART0. But, the problem is that this library is outdated and not compatible with MX64-T servo anymore.</p>

<p>So my question is here:</p>

<p>Is there any one who has experience in controlling Dynamixel MX24T, MX64T servo series using Xbee module simultaneously? If you have experience, please share with me.</p>

<p>Is it possible for Arduino Mega can interface with Xbee module using Serial1 (i.e., RX18 TX19)? If it can, I might be able to use the library1 without any modification.</p>
","serial"
"4650","SLAM without landmarks using sonar","<p>I'm currently programming an app for a robot and I'd like to make him map a zone and then make him move autonomously from one point to another.</p>

<p>I have to solve a SLAM problem, but the biggest matter is that I can't use landmarks to find myself in the environment. The robot just has the abilities to move, and to make distance measurements over -120/+120 degrees using a sonar.</p>

<p>I can't find any simply explained algorithm that permits me to solve this SLAM problem with the no-landmark limitation.</p>

<p>Have you any idea ?</p>
","slam sonar"
"4656","Over-voltage on a brushed electronic speed controller","<p>This is for a battle robot in the hobby-weight class (5.44 Kg max)</p>

<p>I want to drive the robot using  2 cordless drill motors rated at 14.4 volts. I have 4S LIPOs which means I have 4 x 3.7 volts or 14.8 volts. So far so good. </p>

<p>The problem is that I bought 2 ESCs and only afterwards noticed that they are rated for 2-3S (or max of 11.1 volts).</p>

<p>So my question is am I likely to damage the ESC if I use my 4S LIPOs instead of 3S LIPOs?</p>

<p>Or should I just buy 3S LIPOs and live with the reduced performance?</p>
","motor esc"
"4657","Adding reverse function to a brushed motor electronic speed controller","<p>This is for a Hobby-weight (5.44 Kg) battle robot.</p>

<p>I bought two ESCs for my drive motors but the ESCs do not have a reverse function (or brake for that matter).</p>

<p>Is there any simple way I can achieve this through maybe either: </p>

<ol>
<li><p>The R/C settings (setting middle position of joystick as stopped, top-wards as forward and bottom-wards as reverse?) </p></li>
<li><p>Or could I maybe achieve this using Arduino? I have a card with relay switches that I can use with the Arduino so am not worried about high voltage or current but I am worrying it could get messy..</p></li>
</ol>

<p>I could just buy two new ESCs with the above features but they cost quite a bit more than the ones I already have so I would prefer to try a few tricks first - if there are any!</p>
","motor esc"
"4658","Can a 5S LIPO battery be changed to a 3S and a 2S?","<p>Newbie to robotics here! </p>

<p>I bought a 5S LIPO but now realise that it is overkill. And these things are expensive!</p>

<p>So, given that (as far as I know) the pack is apparently made up of individual cells of 3.7 volts each, is there any way in which I could somehow (safely) separate out the cells to get a 3S and a 2S or even single 1S cells?</p>
","battery"
"4663","How to periodically estimate states of a LTI if the output is measured irregularly?","<p>How can I periodically estimate the states of a discrete <a href=""http://en.wikipedia.org/wiki/LTI_system_theory"" rel=""nofollow"">linear time-invariant system</a> in the form $$\dot{\vec{x}}=\textbf{A}\vec{x}+\textbf{B}\vec{u}$$ 
$$\vec{y}=\textbf{C}\vec{x}+\textbf{D}\vec{u} $$if the measurements of its output $y$ are performed in irregular intervals? (suppose the input can always be measured).</p>

<hr>

<p>My initial approach was to design a <a href=""http://en.wikipedia.org/wiki/State_observer"" rel=""nofollow"">Luenberger observer</a> using estimates $\hat{\textbf{A}}$, $\hat{\textbf{B}}$, $\hat{\textbf{C}}$ and $\hat{\textbf{D}}$ of the abovementioned matrices, and then update it periodically every $T_s$ seconds according the following rule:</p>

<blockquote>
  <p>If there has been a measurement of $y$ since the last update: $$\dot{\hat{x}}=\hat{\textbf{A}}\hat{x}+\hat{\textbf{B}}\hat{u}+\textbf{L}(y_{measured}-\hat{\textbf{C}}\hat{x})$$
  If not:
  $$\dot{x}=\hat{\textbf{A}}\hat{x}+\hat{\textbf{B}}\hat{u}$$</p>
</blockquote>

<p>(I have omitted the superscript arrows for clarity)</p>

<p>I believe that there may be a better way to do this, since I'm updating the observer using an outdated measurement of $y$ (which is outdated by $T_s$ seconds in the worst case).</p>
","control sensor-fusion"
"4672","Shortest path using wave planner?","<p>How could I compute the shortest path between point a and b using wave planner?</p>

<p>I don't see how using the wave planner would give me the shortest; it would just give me a path! As far as I can tell, I would only be able to give a random path to the destination, but nothing else than that.</p>
","theory mapping"
"4674","What should I be looking for in a DC motor that will be used in a UROV thruster?","<p>I know that some DC motors produce a lot of torque but only actually move at a very slow rate, while others do the exact opposite. I know that I need some sort of balance between torque and the RPM's of the motor for use in a underwater thruster, but I am not sure what I should favor more, torque or RPM's? Also, it would be great if someone could suggest a motor at or below the $300 range for a UROV.</p>
","motor torque"
"4675","Position Controller for a Quadrotor","<p>I have a question regarding the implementation of a quadrotor's position controller. 
In my Matlab model the quadrotor takes 4 inputs: a desired altitude ($Z_{des}$) and desired attitude angles($\Phi_{des}$, $\Theta_{des}$, $\Psi_{des}$) which reflects the motion described by the differential equations of the model (see last picture). </p>

<p><a href=""http://i.stack.imgur.com/fzk2D.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/fzk2D.png"" alt=""theoretical loop controller for a quadrotor""></a></p>

<p>Here an insight into the implemented Matlab dynamic model. As you can see it has a structure like an inner loop controler:</p>

<p><a href=""http://i.stack.imgur.com/IGZaA.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/IGZaA.png"" alt=""enter image description here""></a></p>

<p>Anyway...it ""hovers"" perfectly on the starting point. (perfect graphs :) )
Now I just need to go over and implement a sort of position controller to let the quadrotor to get from a start to a goal point, defined as usual through 3 coordinates $[X_d, Y_d, Z_d]$. </p>

<p>That's tricky because I don't have the same space state variables as input and output of the system. So the controller must take a vector of three coordinates and be able to output 3 different angles to get there. The only exception is the height because it will be simply bypassed by the controller and doesn't need another calculation loop. A different story is for the three angles... </p>

<p>My first idea was to simply create a feedback between the position given at the output of the simulated system and the desired position as in the figure above.
But that rises another question: my quadrotor model solves the following equation system:</p>

<p>$$
\large \cases{ 
 \ddot X = ( \sin{\psi} \sin{\phi} + \cos{\psi} \sin{\theta} \cos{\phi}) \frac{U_1}{m}  \cr
 \ddot Y = (-\cos{\psi} \sin{\phi} + \sin{\psi} \sin{\theta} \cos{\phi}) \frac{U_1}{m} \cr
 \ddot Z = (-g + (\cos{\theta} \cos{\phi}) \frac{U_1}{m} \cr
 \dot p  = \frac{I_{YY} - I_{ZZ}}{I_{XX}}qr - \frac{J_{TP}}{I_{XX}} q \Omega + \frac{U_2}{I_{XX}} \cr
 \dot q  = \frac{I_{ZZ} - I_{XX}}{I_{YY}}pr - \frac{J_{TP}}{I_{YY}} p \Omega + \frac{U_3}{I_{YY}} \cr
 \dot r  = \frac{I_{XX} - I_{YY}}{I_{ZZ}}pq - \frac{U_4}{I_{ZZ}}
}
$$</p>

<p>that means that they expect (as in the matlab model above) the desired angles and height. 
But now I need right the inverse: given a desired position calculate the right angles!!! 
For the direction is the solution really simple, since I can write something like:</p>

<pre><code>Psi = atan2( (yd - yactual), (xd - xactual) );
</code></pre>

<p>where y and x lies on the horizontal plane. This is not so simple for the other two angles. So what can I do at this point? Just ""invert"" the given equations to get the desired angles?</p>

<p>Another idea could be to implement a simple PD or PID controller. This is much more easier given the fact that I can experiment very quickly using Simulink and get very good results. But the problem is here again: how get I the desired angles from a desired position?</p>
","pid quadcopter"
"4677","How to estimate yaw angle from tri-axis accelerometer and gyroscope","<p>I would like to estimate the <strong>yaw</strong> angle from <strong>accelerometer</strong> and <strong>gyroscope</strong> data. For <em>roll</em> and <em>pitch</em> estimate I've used the following trigonometric equations:</p>

<pre><code>roll  = atan2(Ax,Az)  * RAD_TO_DEG
pitch = atan2(Ay,Az)  * RAD_TO_DEG
</code></pre>

<p>and a simpified version of the <strong>Kalman Filter</strong> to consider also angular rates. The roll and pitch estimates are <em>accurate</em> (accelerometer values need to be filtered in presence of chassis vibrations).</p>

<p><img src=""http://i.stack.imgur.com/5XwmO.png"" alt=""enter image description here""></p>

<p>In order to get the <strong>Yaw</strong> angle I'm using the following equation:</p>

<pre><code>yaw = atan2(Ax,Ay)  * RAD_TO_DEG;
</code></pre>

<p>but the it doesn't work. Do you have any advice?</p>
","sensors quadcopter kalman-filter imu accelerometer"
"4679","MPU9150 - Yaw angle Drift","<p>I use MPU9150, also use DCM and Complimentary filter to compute roll, pitch and yaw. However, my Yaw is not so smooth. How can I solve that problem?</p>

<p>I looked at the datasheet of MPU9150, but I didn't see anything related to sampling frequency of magnetometer like gyro and accel.</p>
","quadcopter"
"4682","Is there an advantage to multiple magnetometers for heading computation","<p>I'm building an autonomous sail boat (ripped out the guts of an RC sail boat and replaced with my own mainboard etc.)</p>

<p>The controller board I have can accommodate both an MPU9150 and an HMC5883. Is there any advantage is using both magnetometers for a tilt-compensated heading? I'm thinking that I could compute the unit vector with soft/hard iron offsets removed for both, and then average  the two vectors to get one slightly better one?</p>

<p>Not sure if it would yield a better result though.</p>
","sensors sensor-fusion"
"4687","tricopter simulation to test control algorithms","<p>I am looking to write and test my own control algorithms for tricopter flight. I am looking for a simulator that can simulate a tricopter but at the level of receiving simulated PWM and returning simulated gyro, compass and other sensor readings. Ideally it would also have graphics for visualization (need not be fancy). Ultimately, I want to port this to a real tricopter but at the moment I would just like to simulate it. Any suggestions for free simulators that are low level as I described? </p>
","control quadcopter simulator"
"4688","Acceleration formula for a differential steering robot","<p>I have the formulas to derive the RPM's of each wheel from the robot's linear velocity.
Now, I am trying to do the same thing for the acceleration (mainly angular acceleration).
For linear acceleration I am always assuming that the linear velocity of the wheels is the same as the robots when the robot is moving on a straight line...according to physics. Am I right?</p>

<p>But angular acceleration seems more complicated, specially when the robot is following a curved path (not necessarily turning in place).</p>

<p>Any readings or ROS packages that deal with this acceleration issue?</p>

<p>Thanks</p>
","ros wheeled-robot differential-drive"
"4689","Short distance ball transport","<p>I'm looking for a way to transport balls (diameter 50mm) 220 mm up with over a slope with a length of 120 mm. Currently I'm considering the usage of a belt system but I cannot seem to find a good belt system.</p>

<p>Because of space constraints within my robot, normally I would probably take a nylon belt and jam nails trough it to make little slots and then use that. However this would result in considerable reduction in available space as it means that I have to also take into account the extra space required for the nails on the way back. This means that ideally there would be a way to reduce the space used by the nails on the way back.</p>

<p>Does anybody have a good solution for this?</p>
","kinematics manipulator"
"4693","is it possible to track movement on a tennis court?","<p>I'd like to track my run in an indoor tennis court. GPS won't be available so I was thinking researching for other solutions:</p>

<p>Accelerometer: I concluded it's a no go because while playing tennis the player makes a lot of movements that include spinning his body that can alter the data.</p>

<p>Then I thought that a 3/4 point IR system might help but again from what I've understood it's hard for the IR system to track the movement since they won't be able to focus on the player.</p>

<p>So my final thought went to radio systems but I couldn't find any info and it's also hard for me to see a theoretical solution at least on how I can mesure the movement/speed of the player.</p>

<p>So here is my question: Is there any existing system that is able to track random movement of an object (athlete) and give info like speed and distance? is there anywhere resources about how such a system might be achieved or at least the exact technology used for it?</p>

<p>Any suggestions and ideas are greatly appreaciated.</p>
","arduino kinematics movement"
"4698","Interface multiple I2C slaves to microcontroller","<p>I want to communicate the Tiva C ARM Cortex M4 to sensorhub from TI which has multiple sensors with different I2C addresses such as MPU9150, BMP180, Temperature Sensors...
With a single I2C slave, i can communicate to it successfully, but if my project involves interface Microcontroller with both MPU9150 and BMP180, then i get stuck.
Anybody suggest me the process of commnunication in this case?</p>
","i2c"
"4699","Robot docking for self-recharging","<p>I want to build a simple obstacle avoider robot, but this time I want it to be self-recharging so I am building a dock for this purpose, so I want it to be able to locate the dock and go for it when battery voltage is lower than a fixed value.</p>

<p>I am having trouble to chose the right components for locating the dock, I think I am going to use an IR emitter on the dock so the robot can head toward it when battery is low (let's forget about the orientation problem for the moment, but if you have any thoughts about it that will be helpful) but I am not sure if the robot is able to detect the IR LED (or whatever) from a long distance (over 10 meter)</p>

<p>Is it possible to use this solution for this distance? If not, what do you suggest?</p>

<p>(If there is a simple ready solution to buy that's ok, let's say I have no budget limit)</p>
","sensors localization wheeled-robot battery wireless"
"4701","A general question about PID Controller","<p>I have a basic question because I'm trying to understand right now a concept that I thought it was obvious.
Looking at this <a href=""http://youtu.be/JJSgUNfZqgU?t=3m10s"" rel=""nofollow"">video</a> he is going to feedback the variable state <strong><em>x</em></strong> with the input of the system, which is a force <strong><em>f</em></strong>. </p>

<p>Now, if I'm correct it is only possibile to feedback variables which share the same units, so I expect to drive a meter through an input variable which is a meter and the difference will be then feed into the PID. Is the example in the video just to show up how to use simulink?
Or I m wrong? </p>
","pid"
"4702","Advice on mounting a servo for a nerf sentry gun","<p>I am trying to make a nerf sentry gun to shoot my co workers. I am building it more or less from scratch and have come to the part where I need to come up with plans to assemble it. I am looking for advice on how to mount the mg995 servos to allow them to tilt and pan. I originally thought about having a base with a metal rod through the middle and use a gear to control the pan functionality. The idea is it would mimic a skateboard truck with a gear that would turn the rod through the middle and pivot the shooting mechanism. Another idea was to have the metal plate sit on top of the servo and use one of the attachments to attach it to the top plate. The problems I see behind this is the attachment is just a small piece of plastic and over a short period of time I could see this wearing out especially if the shooting mechanism is not centered perfectly. I also need to come up with a solution to make it tilt but I think I have an idea for this to simply use a rod with a gear to turn the pvc pipe barrel. </p>

<p><a href=""http://rads.stackoverflow.com/amzn/click/B00D3ZXU2O"" rel=""nofollow"">Here are the servo's I am using</a></p>

<p>Sorry if this is the wrong forum for the question but I was unsure where else to look for some expert advice. </p>

<p><strong>EDIT 1</strong></p>

<p>For anyone intersted I found an example of someone doing almost exactly the same thing with blueprints. I am going a slightly simpler / cheaper route and mounting the servo to the bottom of the spinning plate between the lazy susan plate I ordered. This way I don't have to buy the gears which are rather expensive and without the gears it may reduce some of the torque.</p>

<p><a href=""http://projectsentrygun.freeforums.org/build-progress-gladiator-ii-paintball-sentry-t130.html"" rel=""nofollow"">http://projectsentrygun.freeforums.org/build-progress-gladiator-ii-paintball-sentry-t130.html</a></p>
","mechanism servos servomotor"
"4704","Self Powered Quadcoptor","<p>I have this idea or a very curious question in my mind. I am no where near professional though, but i would like it to be answered. </p>

<p>We all know how wind turbines can be used to generate electricity. So is it possible to create a quadcoptor that will start with some minimal power by small battery but in time will sustain and keep its system on by self generating electricity and keep on rotating its rotors on its own without other external supply? </p>
","quadcopter"
"4705","Amperage on brushed motors","<p>I am currently building a hobby-weight (5.44kg) robot and will be using 2 x 14.4 cordless driller motors for my wheels. </p>

<p>The thing is I keep reading about high amperages when working with r/c models such as quadcopters BUT when I connect my cordless driller motor to my bench power supply and monitor current draw it never rises above 3.2 Amps even when I try to stop the motor by hand. </p>

<p>Of course in the arena in the event of a stand off I have plastic wheels which will slip so I am not too concerned about stall currents. </p>

<p>I am now left wondering whether I have mis-calculated or whether people make a lot of fuss about high currents for nothing. or do these currents only perhaps really apply to brush-less motors?</p>
","motor"
"4706","Diode or capacitor across terminals of brushed motor","<p>I am currently building a hobby-weight robot (5.44kg) and will be using 2 x 14.4v cordless drill brushed motors to drive my wheels. </p>

<p>I have read somewhere that due to ""induced currents"" when I turn the motor off (or reverse it presumably?) I should protect it by using a diode or a capacitor across the terminals. </p>

<p>Which should I use (capacitor or diode) and what are the parameters I need to consider for these components (voltage or current)? </p>

<p>Some answers to a similar question discussed capacitors but not diodes. Are diodes relevant?</p>

<p>Would I seriously damage the cordless drill (presumably quite tough) motor if I did nothing?</p>

<p>And don't motor controllers have any form of inbuilt protection for the motors anyway?</p>
","motor"
"4707","Spinning disk Weapon","<p>I am building a Hobby-weight robot and my weapon of choice is a spinning disk at the front.</p>

<p>As regards the disk I was thinking of buying commercial (grinder-type) disks and change type of disk depending on the ""enemy's"" chassis construction material. So for instance I would have an aluminum cutting disk if the enemy's chassis is made of aluminum and so on.</p>

<p>First question is therefore; do such disks do the job in practise (or break, fail to cut?)</p>

<p>Secondly, should I use a brushed or brush-less motor for the disk? I actually have ESCs for both but sort of feel a brushed motor will give me more torque while a brush-less motor might give me more speed. So which is more important speed or torque? </p>

<p>I do know - from my uncle who uses metal lathes - that machines that cut metal usually spin at a slower speed (drills, cutting wheels etc)- indeed he likes to say that metal working machines are safer than wood-working ones partially for this reason. </p>

<p>But I am a newbie and really would like to have an effective weapon if possible and breaking or not-cutting disks do not make such a weapon!</p>

<p>Also is it normal practise to use one battery for everything (drive and weapon) or have two separate batteries?</p>
","motor battle-bot"
"4713","Rotate (and stop) a large disk in very tiny increments","<p>In a lab build I'm doing, I'm stuck at this problem, so I am fishing for suggestions.</p>

<p>I'm creating a turn-table type setup where I need to make readings (with a nanotube-tip probe I've already designed, similar to an AFM probe) on the very edge/circumference of a 10 cm radius disk (substrate).</p>

<p>The current hurdle is: I need to get the substrate disk to move circularly in steps of 0.1 mm displacement -- meaning, I occasionally need to STOP at certain 0.1mm-increment positions.</p>

<p>What would be a way I can achieve this, assuming an accurate feedback system (with accuracy of say ~0.1 mm, e.g., with quadrature optical encoders) is available if needed for closed-loop control?</p>

<p>Specs of commonly sold steppers don't seem to allow this kind of control. I'm at the moment trying to study how, e.g. hard disks achieve extreme accuracies (granted they don't have such large disks).</p>

<p>Certainly, direct-drive like I'm currently building (see below image) probably doesn't help!</p>

<p><img src=""http://i.stack.imgur.com/P24mX.png"" alt=""""></p>
","motor stepper-motor"
"4715","control a robot with pymorse on MORSE simulator question","<p>I am new to Morse and robotics.</p>

<p>This code control the robot by giving the linear and angular velocity.</p>

<p>This is the scene description</p>

<pre><code>from morse.builder import *
robot = ATRV()
motion = MotionVW()
motion.add_stream('socket')
robot.append(motion)

semanticL = SemanticCamera()
semanticL.translate(x=0.2, y=0.3, z=0.9)
robot.append(semanticL)

semanticR = SemanticCamera()
semanticR.translate(x=0.2, y=-0.3, z=0.9)
robot.append(semanticR)

motion.add_stream('socket')
semanticL.add_stream('socket')
semanticR.add_stream('socket')

env = Environment('land-1/trees')
env.set_camera_location([10.0, -10.0, 10.0])
env.set_camera_rotation([1.0470, 0, 0.7854])
</code></pre>

<p>and this is the control script</p>

<pre><code>import pymorse
with pymorse.Morse() as simu:
    simu.robot.motion.publish({""v"": 3, ""w"": -1})
</code></pre>

<p>The robot moves well. But when I remove the semantic cameras from the scene description the robot do not move. I am confused, they are just sensor, why the robot don't move ?</p>
","mobile-robot sensors control"
"4716","Motor for weapon for Hobbyweight","<p>I am currently building a hobby-weight (5.44kg) robot. The weapon will be a vertical spinning disk at the front. It will probably be a commercial one from the hardware store or I could maybe get one made.</p>

<p>I have 2 cordless drill motors to drive my wheels so I should be ok there, but I am still lost where it comes to what motor I should get for my weapon. I am now inclined to think it should be brush-less although I am still open to other opinions.</p>

<p>Can anyone please recommend a good motor (in-line brush-less) or brushed motor that will give me the speed and strength I need for the weapon?</p>
","motor battle-bot"
"4717","Detect road surface in a traffic scene point cloud","<p>I want to analyze a traffic scene. My source data is a point cloud like <a href=""http://jankucera.blogspot.cz/2014/09/reprojecting-stereo-images-to-3d-point.html"" rel=""nofollow"">this one</a> (see images at the bottom of that post). I want to be able to detect objects that are on the road (cars, cyclists etc.). So first of all I need know where the road surface is so that I can remove or ignore these points or simply just run a detection above the surface level.</p>

<p>What are the ways to detect such road surface? The easiest scenario is a straight and flat road - I guess I could try to registrate a simple plane to the approximate position of the surface (I quite surely know it begins just in front of the car) and because the road surface is not a perfect plane I have to allow some tolerance around the plane.</p>

<p>More difficult scenario would be a curvy and wavy (undulated?) road surface that would form some kind of a 3D curve... I will appreciate any inputs.</p>
","mobile-robot wheeled-robot computer-vision algorithm stereo-vision"
"4721","How to convert PID outputs to appropriate motor speeds for a quad copter","<p>I am building an autonomous robot using PID control algorithm. So, far I have implemented PID using online resources/references. I am testing for stabilizing an axis of the quad copter. However, I am not successful to stabilize even one axis.
Description: My input for the PID is an angle value i.e the orientation of the quad copter measured by AHRS (a Gyroscope that measures angles) and the motors take integer values as speeds. What I am doing is,</p>

<pre><code>motor_right_speed = base_speed + adjusted_value;
motor_left_speed = base_speed - adjusted_value;
adjusted_value += PID_output;
</code></pre>

<p>Where ajusted_value is a buffer that accumulates or subtracts the PID output value based on either PID output is +ve or -ve.</p>

<p>I also tried,</p>

<pre><code>motor_right_speed = base_speed + PID_output;
motor_left_speed = base_speed - PID_output;
</code></pre>

<p>Both don't seem to be working.</p>

<p>I have tested using a wide range of P gain values (from very small to very large), but the quad copter only oscillates; it does not self-correct. Your help with suggestions would be greatly appreciated. Thanks!</p>
","quadcopter"
"4722","Brush-less motor specs vs efficiency for multi-copters","<p>I am looking for some figures surrounding the specs of brushless motors and their relative efficiency (in power usage terms) for multi-copter use. </p>

<p>There are 4 basic specs for motors themselves:
 - Motor width (EG 28mm)
 - Motor height (EG 30mm)
 - ""KV"" - RPM per volt supplied (EG 800KV)
 - wattage (eg 300w)</p>

<p>This would then be a 28-30 800kv 300w motor. </p>

<p>What i am looking for is a chart containing:
 - Motor spec
 - pack voltage (eg 14.8v)
 - Amps drawn @ various % throttle (10% to 100% say)
 - static thrust from various propellers (11x5, 12x6 etc etc)</p>

<p>Does such information exist?
I know its a BIT subjective as prop and motor designs vary slightly, but a baseline would be a start.</p>
","brushless-motor"
"4723","Hokuyo URG-04LX-UG01 and Mac compatibility issues","<p>I've got my hands on this laser range scanner but seem to have some problem receiving any output from it.</p>

<p>I can't find any guide on  how to set it up on the internet, so I was wondering if it is even possible to set it up for mac or shall i do it using Linux and ROS ?</p>
","communication laser rangefinder"
"4724","biped walking using Genetic Algorithm","<p>I am working on a project but I lack advanced programming knowledge, especially about genetic algorithms. I am developing a prototype using WEBOTS 7.4.3 for the simulation. The project is to use genetic algorithms to evolve the gait of a biped robot. I have developed a physical model, but I am still uncertain about the motor choice. For the algorithm part, I find it hard to understand how to set the algorithm parameters and how to determine the fitness function. Could you please suggest a fitness function?</p>

<p>Thank you for your help and efforts.</p>
","algorithm machine-learning legged"
"4725","Recursive Tree Representation for Multi Agent Robots?","<p>I have been going through a code base for multi agent motion planning. And I came across a recursive tree building algorithm for the agents. I haven't been able to figure out the algorithm. Does anyone know what it is called? Or any other similar kinds of algorithms so I could read more about it?</p>

<p>Here is what I got from the code: 
The node of the tree is as follows - </p>

<pre><code>&gt;   struct AgentTreeNode {
&gt;     int begin;
&gt;     int end;
&gt;     float minX, maxX, minY, maxY;
&gt;     int left;
&gt;     int right;   };
</code></pre>

<p>Each node has a max and min value for x and y. And also a begin, end, left and right value. </p>

<p>Then the tree is split either horizontally or vertically based on which limits are longer (x or y). And an optimal value is found and agents are split. </p>

<p>Then these split agents are recursively build again.</p>

<p>Thank you very much. </p>
","multi-agent"
"4726","ros node does not seem to do anything","<p>I have the following code for the ros turtlesim: </p>

<pre><code> #include &lt;ros/ros.h&gt;
#include &lt;std_msgs/String.h&gt; 
#include ""geometry_msgs/Twist.h""
#include &lt;stdio.h&gt;     
#include &lt;stdlib.h&gt;     
#include &lt;time.h&gt;       
void disruptcb(geometry_msgs::Twist msg) {
    ros::NodeHandle pubHandle;
    ros::Publisher publisher = pubHandle.advertise&lt;geometry_msgs::Twist&gt;(""turtle1/cmd_vel"", 1000);
    ros::Rate loop_rate(2);

        double dist1=(rand()%100);
        double dist2=(rand()%100);
        std::cout&lt;&lt;dist1&lt;&lt;std::endl;
        dist1=dist1;
        dist2=dist2; 
        msg.linear.x+=dist1;
        msg.angular.z+=dist2; 
        std::cout&lt;&lt;msg&lt;&lt;std::endl;
        ROS_INFO(""hello"" );
        publisher.publish(msg);

}
int main(int argc,char** argv){
    srand(time(NULL));
    ros::init(argc,argv, ""things_going_wrong"");
    ros::NodeHandle nh;
    ros::Subscriber sub = nh.subscribe(""/ros_1/cmd_vel"",1000,&amp;disruptcb);
    ros::spin(); 
}
</code></pre>

<p>the idea behind this code is to introduce a random error to then practice error recovery in my code but this node does not appear to do anything at all. I do know that my other nodes are running but this one just doesn't appear to do anything, it doesn't exit it just hangs. Anybody know how to fix this?</p>
","ros"
"4734","Can I make an automatically-parking robot car with IR sensor?","<p>It would be easy to understand if you imagine a robotic vacuum cleaner. (For some models) It goes back to a specific place automatically to recharge. Like this, I want to make a robot which automatically goes to the place where a specific signal(like infrared ray) is emitting. 
Following is the scenario that i've imagined.</p>

<p>1.Set the IR emitter in a specific place of a room. It always emits Infrared ray.</p>

<p>2.I connect 4 IR receiver to my 4WD robot car - front, left, right, and back side.</p>

<p>3.They receive IR from the emitter. I earn the distance from the emitter to each receiver with the intensity of IR.</p>

<p>4.With these values, Arduino find out which receiver is closest from the emitter and choose the direction to go.</p>

<p>But I could't know this will be possible. Because IR is a kind of light ray, so I can't get the distance with the difference of arrival time(like Ultrasonic). I searched several kinds of IR sensors, but they were only for sensing the possibility of collision. </p>

<p>So my question is these..</p>

<ol>
<li><p>Can I get the distance and the direction from IR emitter to my arduino device with an IR receiver? </p></li>
<li><p>If I can, then how many IR receivers do I need? And if I can't, what can I use to substitute IR emitters and receivers?</p></li>
<li><p>I guess IR can be interrupted because of sunlight or other light. So I guess I need some daylight filter. Do you think it's essential?? </p></li>
</ol>
","arduino sensors wheeled-robot"
"4738","Wheeled Robot Motion Primitives: Is throttling forward and crab motion considered as one?","<p>I am simulating a wheeled robot of six-wheels and can be independently steered, like MER-Opportunity.
The wheeled robot can perform throttling forward, </p>

<pre><code>||---|| &lt;--wheel orientation
||   ||
||---||
</code></pre>

<p>crab-motion, </p>

<pre><code>//---// &lt;--wheel orientation when heading is 45
//   //
//---//
</code></pre>

<p>and turning on the spot.</p>

<pre><code>//---\\ &lt;--wheel orientation
||   ||
\\---//
</code></pre>

<p>My question is: <strong><em>Is it correct to say that I have 2 motion primitives?</em></strong> Throttling forward is basically crab-motion with heading zero.</p>
","wheeled-robot motion"
"4741","Numerical-Example for Paden-Kahan subproblems?","<p>I am writing a kinematics library in Go as part of my final year project. I am working with Product of Exponentials method and have successfully implemented the Forward Kinematics part of this. I need help with the Inverse Kinematics. </p>

<p>I understand the theoretical aspect of it. I would like a numerical example where actual numbers are used for the Paden-Kahan subproblems as the ones dealt in <a href=""http://www.cds.caltech.edu/~murray/books/MLS/pdf/mls94-complete.pdf"" rel=""nofollow"">""A Mathematical Introduction to Robotic Manipulation - Murray,Li and Sastry"" [freely-available online PDF]</a>. </p>

<p>I specifically need help with knowing what should p,q be when trying to solve the inverse kinematics. The book just says given, a point p,q around the axis of rotation of the joint. But how do you know these points in practice, like when the robot is actually moving, how do you keep track of these points? For these reasons I need a numerical example to understand it. </p>
","inverse-kinematics product-of-exponentials"
"4745","How do I get this transformation matrix?","<p>I've just started taking a robotics course and I am having a little problem.
I need to rotate the $O_i-1$ coordinate system into a position, where $X_i-1$ will be parallel with $X_i$. </p>

<p>The transformation matrix is given, but I have no idea how I can figure out this transformation matrix from the picture that can be found below.</p>

<p>Actually, I know why the last vector is [0 0 0 1] and the previous vector is [0 0 1 0], but I can't figure out why the first vector is [$\cos q_i$ $\sin q_i$ 0 0] and the second [$-\sin q_i$ $\cos q_i$ 0 0].</p>

<p><img src=""http://i.stack.imgur.com/s3BlX.png"" alt=""The picture""></p>
","robotic-arm industrial-robot"
"4749","Survey for Local Navigation","<p>I was wondering if there was a good book or paper that surveys current techniques in local navigation? The earliest one I could find was from 2005 and I was hoping to find something more recent. </p>

<p>I have worked with certain approaches such as the dynamic window approach and the velocity obstacles approach. I'm hoping for some book or paper to give me a broader perspective to the problem of local navigation which I believe has been fairly robustly solved by a number of autonomous driving companies. </p>

<p>Thank you. </p>
","navigation motion-planning"
"4750","PID Control Tuning","<p>Im currently designing a robot for my undergraduate project. One of the task of this robot is to follow the wall. For the purpose I'm using a PID control system, where the reference is given from a ultrasonic sensor. So my problem here is im having a hard time tuning the PID. I know i can find the P coefficient pretty easily by plotting the desired set point range vs desired motor output speed. Even then the robot is not so stable, so i though of adding DI part of PID. But how do find out roughly the values of these coefficients without just trying out random values (manual tuning)? Thank you so much. Much appreciated.</p>
","motor control pid"
"4754","Build a ROS robot with SLAM without laser","<p>I've build a simple wheeled robot based on two continuous servos, controlled by Raspberry Pi running ROS-groovy, with a smart phone mounted on top to provide additional sensors.  I'd like to situate the bot in a room and have it move to various points on command.  I don't have laser ranger finder but do have a good ultrasonic ranger finder and kinect sensors.</p>

<p>What are the typical ROS setup for this?</p>

<p>The idea I'm thinking is to personally (e.g. manually) map my room using kinect and use this map using only the ultrasonic range finder sensors and IMU in the lightweight robot.  Would this be possible?</p>
","localization ros slam raspberry-pi ultrasonic-sensors"
"4755","Parameter $r$ of Denavit-Hartenberg","<p>By watching this <a href=""http://youtu.be/rA9tm0gTln8"" rel=""nofollow"">video</a> which explains how to calculate the classic Denavit–Hartenberg parameters of a kinematic chain, I was left with the impression that the parameter $r_i$ (or $a_i$) will always be positive. </p>

<p>Is this true? If not, could you give examples where it could be negative? </p>
","forward-kinematics dh-parameters"
"4759","How to find the height of a rock with a rover?","<p>So, I am designing a rover that will navigate to a rock, and then calculate the height of the rock. Currently, my team's design involves using an ultrasonic rangefinder and lots of math. I was interested in what sensors you would use to solve this problem, or how you would go about it? Assume that the rover has already located the rock. </p>

<p>Additional Info: We are using an Arduino Uno to control our rover. It is completely autonomous.</p>
","arduino wheeled-robot algorithm"
"4761","Sending a smartphone's GPS wireless","<p>I'm building camera device which is able to take pictures of paragliders in mid air.</p>

<p>To let the camera know where the glider is I thought about using GPS data from the pilot's smartphone.</p>

<p>My question is: What are possible ways to transmit the GPS data to the groundstation and which can be considered a good solution?</p>

<p>I thought about sending the data to a server via some mobile network, but a direct communication solution would be preferable.</p>

<p>The pilot has mid-air pretty good mobile reception and the maximum distance between pilot and ground station is around 3km.</p>
","gps communication wireless"
"4762","Learning Materials for Beginners in Robotics and Quadrocopters","<p>I am a web developer. I am fascinated by Quadrocopters and i am trying to learn how to build one and basically i am trying to jump into robotics fields. I don't have much electric circuit and electronics knowledge so i did some research on how to build and what type of knowledge you would require to develop such flying machine. So i started learning basics of electronics from <a href=""http://www.ibiblio.org/kuphaldt/electricCircuits/"" rel=""nofollow"">Lessons In Electric Circuits by Tony R. Kuphaldt</a></p>

<p>The books are very interesting but i could not find a technique so that i can implement what i learn from the books. Basically i am just going through the stuffs, and understanding them little by little. What i want to know is that what is the right way and effective way to learn electronics and electric circuit from your experience and i should i do now so that i can increase my learning speed so that i can achieve my goal.</p>

<p>While i was researching i came across topics such as mathematical modelling and modelling the quadrocopters first and them implementing them on real. How can i gain such knowledge to model something mathematically and implement such in real life? How much math and what areas of mathematics do i need to learn and how can i learn such? </p>

<p>Now you have idea what i want to learn and achieve. Can you please suggest me a road map or steps i need to take to gain such knowledge and skill to develop myself, so that in near future i would be able to build such flying machines on my own. </p>
","quadcopter"
"4764","Which is more useful? Instantaneous rate of change of displacement or Average rate of change of displacement?","<p>Instantaneous rate of change of displacement is given by,</p>

<pre><code>v(t) = (s(t + dt) - s(t))/dt, where dt tends to 0
</code></pre>

<p>while average rate of change of displacement is given by, </p>

<pre><code>v(t) = (s(t[n]) - s(t[n-1]))/(t[n] - t[n-1])
</code></pre>

<p>The first one gives the slope or derivative of a displacement function at a particular instant of time and thus varies with time. I was wondering how is it going to help me calculate the velocity of my robot's end effector, which is the foot of the leg of the robot(bipedal). If i make a reading of the position of the robot every 1ms to keep the approximation as accurate as possible, my instantaneous velocity would be zero wouldn't it? Since my robot wouldn't have moved anywhere in 1ms time. Agreed, 't' would increment as t+dt, dt == 0.001s. Then v(t) would be v(0.001) = s(0.002) - s(0.001) which is zero, because there is no displacement in that small time frame, right? Am I doing something wrong here? Or on the other hand, do I just use average rate of change?
I have this question, since, if there is a manipulator, in my case the foot of my robot, and it's trajectory is given by a 3x3 homogenous matrix,</p>

<pre><code>[{c(t),-s(t), 0},
 {s(t), c(t), 0},
 {  0,    0,  1}], where c,s are cos(theta) and sin(theta)respectively,
</code></pre>

<p>if on paper, this is differentiated, this would give me a spatial/body velocity matrix as </p>

<pre><code>[{0, -d(theta)/dt, 0},
 {d(theta)/dt,  0, 0},
 {    0,        0, 0}]
</code></pre>

<p>So how do I compute this differentiation in code. I just need something of the sort of a pseudocode.</p>
","mobile-robot"
"4766","how to power extra usb devices for beaglebone black","<p>I'm building a robot that uses a beaglebone black, however I have several different usb devices that I want to connect to it (microphone, usb sound device and some other things). Now I have heard that the usb output of the beaglebone doesn't power more then 0.1A. So the combined draw of these usb devices is likely to exceed this by a fair margin. So I started looking for powered usb hubs to use instead. However these tend to be powered by 220V and my robot currently only has a 12V power supply and a converter to 5V for the beaglebone. Which given the size expense and inefficiency of converting power to 220 from 12V and then back again doesn't seem very good. Is there a good method for fixing this? </p>
","power usb beagle-bone"
"4771","tracking robot in a room","<p>I'm trying to track a simple robot (e.g. arduino, raspberry pi, even toys) in a room using fixed location kinect sensor(s) and cameras at different parts of the room.  How might one usually use to do this?</p>

<p>Edit 1:  More specifically, I want to know the position (and if possible, orientation) of an moving object in the room using one or more cameras or depth sensors.  I'm new to the area, but one idea might be to use blob or haar to detect the moving object and get its location from kinect depth-map, and I'm trying to find what package I can use for that end.  But for navigation to work I'd have to pre-map the room manually or with kinect.  I can put some sensors on this tracked moving object, e.g. IMU, sonar, but not a kinect.  I am allowed full PCs running ROS/opencv/kinect sdk in the environment, and I can wirelessly communicate with the tracked object (which is presently a raspberry pi running ROS groovy on wheels)</p>
","arduino computer-vision kinect"
"4775","Image processing in bright lights","<p>We are working on a project which requires us to detect and hit a ball. We are trying to accomplish the task by detecting the position of ball by processing the input from a camera. The problem is that we are required to do this in very bright lights. The bright lights are making it difficult to detect the white colored ball.</p>

<ol>
<li>Is there a way we can write the code such that it automatically reduces the intensity of lights in the image?</li>
<li>Is there an <strong>efficient</strong> way to extract only the V component from the HSV image?</li>
</ol>

<p>We have very limited experience with image processing so any alternative approach to detecting the object will also be helpful</p>
","computer-vision"
"4782","Power switch and common ground on a battle robot","<p>I am constructing a 5.44Kg Hobby-weight battle robot and one of the safety rules is that the robot must have a power switch that turns off power to the motors (and weapon).</p>

<p>The robot has three sub-systems; the drive motors (one battery), the weapon (another battery) and some lighting (a small 9 volt battery).</p>

<p>I have read that since all these will be connected to the same receiver it is important to have all the electronics sharing a common ground for everything to work properly.</p>

<p>Now I know that usually it is the ""live"" wire that is connected to the switch, but I was thinking of hitting two birds with one stone and connecting all the ground wires (rather than the live wires) to the switch. In this way I still turn off power and also have a common ground. In terms of safety (shorts) etc I am not too concerned because I am using XT 60 connectors and have been careful to use only female plugs for the power leads (so no prongs are visible).</p>

<p>It seems to me that it should work and still be safe enough especially since I am not dealing with mains voltage levels here, but on the other hand I don't want to look stupid.</p>

<p>Does this way of connecting to the switch make sense or am I violating some unwritten law? Is this normal practice? Would it effect the circuitry in any way to have the grounds connected together?</p>

<p>I was also thinking of using a switch from a PC power supply; as far as I know this is rated for reasonably high currents. In my case I will have 3 cordless motors, each of which might be drawing up to 5 amps when under load, so say 15 amps in all. Has anyone out there ever used such switches or did you buy high current ones? In that case what should I ask for?</p>

<p>Thanks.</p>
","control power battle-bot"
"4784","gazebo import robot gives error","<p>I'm trying to import the tutorial robot given at <a href=""http://gazebosim.org/tutorials?tut=build_robot&amp;cat=build_robot"" rel=""nofollow"">this link</a></p>

<p>However this gives the following error: </p>

<pre><code>Error [Param.cc:181] Unable to set value [1,0471975511965976] for key[horizontal_fov]
Error [Param.cc:181] Unable to set value [0,100000001] for key[near]
Error [parser_urdf.cc:2635] Unable to call parseURDF on robot model
Error [parser.cc:278] parse as old deprecated model file failed.
Error [parser_urdf.cc:2635] Unable to call parseURDF on robot model
Error [parser.cc:278] parse as old deprecated model file failed.


Error [parser.cc:278] parse as old deprecated model file failed.
</code></pre>

<p>This sugest something is wrong with parsing but does not actually point towards any line of my code (the example is only 103 lines long). </p>

<pre><code>    &lt;link name='chassis'&gt;
      &lt;pose&gt;0 0 .1 0 0 0&lt;/pose&gt;
      &lt;collision name='collision'&gt;
        &lt;geometry&gt;
          &lt;box&gt;
            &lt;size&gt;.4 .2 .1&lt;/size&gt;
          &lt;/box&gt;
        &lt;/geometry&gt;
      &lt;/collision&gt;

      &lt;visual name='visual'&gt;
        &lt;geometry&gt;
          &lt;box&gt;
            &lt;size&gt;.4 .2 .1&lt;/size&gt;
          &lt;/box&gt;
        &lt;/geometry&gt;
      &lt;/visual&gt;

      &lt;collision name='caster_collision'&gt;
        &lt;pose&gt;-0.15 0 -0.05 0 0 0&lt;/pose&gt;
        &lt;geometry&gt;
          &lt;sphere&gt;
          &lt;radius&gt;.05&lt;/radius&gt;
        &lt;/sphere&gt;
      &lt;/geometry&gt;

      &lt;surface&gt;
        &lt;friction&gt;
          &lt;ode&gt;
            &lt;mu&gt;0&lt;/mu&gt;
            &lt;mu2&gt;0&lt;/mu2&gt;
            &lt;slip1&gt;1.0&lt;/slip1&gt;
            &lt;slip2&gt;1.0&lt;/slip2&gt;
          &lt;/ode&gt;
        &lt;/friction&gt;
      &lt;/surface&gt;
    &lt;/collision&gt;

    &lt;visual name='caster_visual'&gt;
      &lt;pose&gt;-0.15 0 -0.05 0 0 0&lt;/pose&gt;
      &lt;geometry&gt;
        &lt;sphere&gt;
          &lt;radius&gt;.05&lt;/radius&gt;
        &lt;/sphere&gt;
      &lt;/geometry&gt;
    &lt;/visual&gt;
  &lt;/link&gt;
  &lt;link name=""left_wheel""&gt;
    &lt;pose&gt;0.1 0.13 0.1 0 1.5707 1.5707&lt;/pose&gt;
    &lt;collision name=""collision""&gt;
      &lt;geometry&gt;
        &lt;cylinder&gt;
          &lt;radius&gt;.1&lt;/radius&gt;
          &lt;length&gt;.05&lt;/length&gt;
        &lt;/cylinder&gt;
      &lt;/geometry&gt;
    &lt;/collision&gt;
    &lt;visual name=""visual""&gt;
      &lt;geometry&gt;
        &lt;cylinder&gt;
          &lt;radius&gt;.1&lt;/radius&gt;
          &lt;length&gt;.05&lt;/length&gt;
        &lt;/cylinder&gt;
      &lt;/geometry&gt;
    &lt;/visual&gt;
  &lt;/link&gt;

  &lt;link name=""right_wheel""&gt;
    &lt;pose&gt;0.1 -0.13 0.1 0 1.5707 1.5707&lt;/pose&gt;
    &lt;collision name=""collision""&gt;
      &lt;geometry&gt;
        &lt;cylinder&gt;
          &lt;radius&gt;.1&lt;/radius&gt;
          &lt;length&gt;.05&lt;/length&gt;
        &lt;/cylinder&gt;
      &lt;/geometry&gt;
    &lt;/collision&gt;
    &lt;visual name=""visual""&gt;
      &lt;geometry&gt;
        &lt;cylinder&gt;
          &lt;radius&gt;.1&lt;/radius&gt;
          &lt;length&gt;.05&lt;/length&gt;
        &lt;/cylinder&gt;
      &lt;/geometry&gt;
    &lt;/visual&gt;
  &lt;/link&gt;
  &lt;joint type=""revolute"" name=""left_wheel_hinge""&gt;
    &lt;pose&gt;0 0 -0.03 0 0 0&lt;/pose&gt;
    &lt;child&gt;left_wheel&lt;/child&gt;
    &lt;parent&gt;chassis&lt;/parent&gt;
    &lt;axis&gt;
      &lt;xyz&gt;0 1 0&lt;/xyz&gt;
    &lt;/axis&gt;
  &lt;/joint&gt;

  &lt;joint type=""revolute"" name=""right_wheel_hinge""&gt;
    &lt;pose&gt;0 0 0.03 0 0 0&lt;/pose&gt;
    &lt;child&gt;right_wheel&lt;/child&gt;
    &lt;parent&gt;chassis&lt;/parent&gt;
    &lt;axis&gt;
      &lt;xyz&gt;0 1 0&lt;/xyz&gt;
    &lt;/axis&gt;
  &lt;/joint&gt;
  &lt;/model&gt;
&lt;/sdf&gt;
</code></pre>

<p>This is on ubuntu 14.04. Is there any hint at what I'm doing wrong or what information can I provide to better come to a solution </p>
","simulator gazebo simulation"
"4787","Scan Matching finds right rotation but false translation","<p>I'm currently developing a SLAM software on a robot, and I tried the Scan Matching algorithm to solve the odometry problem.</p>

<p>I read this article :
<a href=""http://webdiis.unizar.es/~montesan/web/pdfs/minguez06TRO.pdf"" rel=""nofollow"">Metric-Based Iterative Closest Point Scan Matching
for Sensor Displacement Estimation</a></p>

<p>I found it really well explained, and I strictly followed the formulas given in the article to implement the algorithm.</p>

<p>You can see my implementation in python there :
<a href=""https://github.com/pierrolelivier/SLAMScanMatching/blob/master/ScanMatching.py"" rel=""nofollow"">ScanMatching.py</a></p>

<p>The problem I have is that, during my tests, the right rotation was found, but the translation was totally false. The values of translation are extremely high.</p>

<p>Do you have guys any idea of what can be the problem in my code ?</p>

<p>Otherwise, should I post my question on StackOverflow or on the Mathematics Stack Exchange ?</p>

<p>The ICP part should be correct, as I tested it many times, but the Least Square Minimization doesn't seem to give good results.</p>

<p>The parts that might be problematic are the function getAXX() to getBX() (starting at line 91).</p>

<p>As you noticed, I used many decimal.Decimal values, cause sometimes the max float was not big enough to contain some values.</p>
","slam"
"4789","Probabilistic Velocity Obstacles","<p>I have been working with the Velocity Obstacles concept. Recently, I came across a probabilistic extension of this and couldn't understand the inner workings. </p>

<p>Source: Recursive Probabilistic Velocity Obstacles for Reflective Navigation <a href=""http://www.morpha.de/download/publications/FAW_ASER03_Kluge.pdf"" rel=""nofollow"">http://www.morpha.de/download/publications/FAW_ASER03_Kluge.pdf</a></p>

<p><img src=""http://i.stack.imgur.com/20jll.png"" alt=""Shape Uncertainty Formulation""></p>

<p>What does the equation at the bottom and the top mean? Vij is the relative velocity of agent i to agent j. ri &amp; ci and rj &amp; cj are their respective radius and centers.</p>

<p>Update:
What does inf(ri + rj) and sup(ri + rj) mean? Does it mean that I should define a function that goes from 1 to 0 from inf to sup? And if not, then how do I calculate the value of PCC at any given point?</p>
","probability"
"4793","Proportional controller error doesn't approach zero","<p>I'm reading this <a href=""http://www-lar.deis.unibo.it/people/cmelchiorri/Files_Robotica/FIR_05_Dynamics.pdf"" rel=""nofollow"">pdf</a>. The dynamic equation of one arm is provided which is </p>

<p>$$
l \ddot{\theta} + d \dot{\theta} + mgL sin(\theta) = \tau
$$</p>

<p>where </p>

<p>$\theta$ : joint variable. </p>

<p>$\tau$ : joint torque</p>

<p>$m$ : mass</p>

<p>$L$ : distance between centre mass and joint. </p>

<p>$d$ : viscous friction coefficient</p>

<p>$l$ : inertia seen at the rotation axis. </p>

<p><img src=""http://i.stack.imgur.com/CvrSY.png"" alt=""enter image description here""></p>

<p>I would like to use P (proportional) controller for now. </p>

<p>$$
\tau = -K_{p} (\theta - \theta_{d})
$$</p>

<p>My Matlab code is </p>

<pre><code>clear all
clc

t = 0:0.1:5;

x0 = [0; 0];

[t, x] = ode45('ODESolver', t, x0);

e = x(:,1) - (pi/2); % Error theta1

plot(t, e);
title('Error of \theta');
xlabel('time');
ylabel('\theta(t)');
grid on
</code></pre>

<p>For solving the differential equation </p>

<pre><code>function dx = ODESolver(t, x)

dx = zeros(2,1);
%Parameters:

m  = 2;
d  = 0.001;
L  = 1;
I = 0.0023;
g  = 9.81;

T = x(1) - (pi/2);


dx(1) = x(2);

q2dot = 1/I*T - 1/I*d*x(2) - 1/I*m*g*L*sin(x(1));

dx(2) = q2dot;
</code></pre>

<p>The error is </p>

<p><img src=""http://i.stack.imgur.com/TIUNE.png"" alt=""enter image description here""></p>

<p>My question is why the error is not approaching zero as time goes? The problem is a regulation track, so the error must approach zero. </p>
","control dynamics manipulator"
"4794","Motor controller calibration","<p>I bought 2 brushed motor controllers from China to use with my hobby-weight battle robot  (<a href=""http://www.banggood.com/ESC-Brushed-Speed-Controller-For-RC-Car-Truck-Boat-320A-7_2V-16V-p-915276.html"" rel=""nofollow"">http://www.banggood.com/ESC-Brushed-Speed-Controller-For-RC-Car-Truck-Boat-320A-7_2V-16V-p-915276.html</a>). </p>

<p>These are intended for use with my 2 cordless drill motors which will be driving the left and right wheel respectively. The robot will therefore be steered in ""tank mode"" by varying the speed and direction of rotation of the 2 motors using the two joysticks on my Turnigy 9x transmitter.</p>

<p>My question is: I have seen videos on youtube where people calibrate brushless motor controllers (ESCs) using some system of pushing the joystick on a standard transmitter forward and listening to tones and then doing the same for reverse and so on. </p>

<p>However when I asked the suppliers about a similar procedure for these brushed controllers, all they could say is that they did not need calibration. The exact words were ""It seems that you're talking about transmitter for copters,but this ESC is for RC car or boat. You pull the trigger, it goes forward, you push the trigger, it reverse. And you don't need to calibrate it, just plug it, then it can work.""  </p>

<p>My transmitter is not one of those gun shaped ones used for cars. So am I in trouble with these controllers or should they work correctly out of the box as the supplier seems to be implying?  </p>

<p>You may fairly ask why have I not just tried this out and the simple answer is that my LIPO charger has not yet arrived and I therefore cannot power anything up as yet. </p>
","motor"
"4796","Changing tank drive (differential) mode to single joystick drive mode","<p>I bought 2 brushed motor controllers from China to use within my hobby-weight battle robot (<a href=""http://www.banggood.com/ESC-Brushed-Speed-Controller-For-RC-Car-Truck-Boat-320A-7_2V-16V-p-915276.html"" rel=""nofollow"">http://www.banggood.com/ESC-Brushed-Speed-Controller-For-RC-Car-Truck-Boat-320A-7_2V-16V-p-915276.html</a>). </p>

<p>These are intended for use with my 2 cordless drill motors which will be driving 
the left and right wheel respectively. The robot will therefore be steered in 
""tank mode"" by varying the speed and direction of rotation of the 2 motors using 
the two joysticks on my Turnigy 9x transmitter.</p>

<p>I am seeking to refine the model and make it easier to operate so does anyone know of a way in which I can somehow synchronize the motors to get a single joystick steering system?  My transmitter has 9 available channels so if this is part of a solution then I am fine with it. I also have an Arduino available if needs be. </p>
","motor differential-drive"
"4797","Risk of overloading motor controller","<p>I bought 2 brushed motor controllers from China for my hobby-weight battle robot (<a href=""http://www.banggood.com/ESC-Brushed-Speed-Controller-For-RC-Car-Truck-Boat-320A-7_2V-16V-p-915276.html"" rel=""nofollow"">http://www.banggood.com/ESC-Brushed-Speed-Controller-For-RC-Car-Truck-Boat-320A-7_2V-16V-p-915276.html</a>). </p>

<p>These are intended for use with my 2 cordless drill 14.4v motors which will be driving the left and right wheel respectively.</p>

<p>I will be using 4S LIPOs which (when fully charged) have a voltage of 16.8V. Can someone put my mind at rest that the .8 volt excess is unlikely to damage the controller (which is rated for 7.2v - 16v)?</p>

<p>Also is the fact that the motor controllers are rated for 320Amp likely to damage my motors? </p>

<p>I am to be honest not very clear on current and how this is drawn from a LIPO battery. For instance would connecting a LIPO directly to my motor result in a massive discharge or does the motor just ""take what it needs"" in terms of current? Can someone maybe kindly point me to an article which casts some light on the subject or even more kindly explain it to me here?</p>
","motor battery"
"4798","RNN instead of a PID controller","<p>I am building a drone using the raspberry pi and I am using 6*PID controllers to control the speed and the value for each angle, can I use a recurrent neural network (RNN) or other neural network to stabilize the angles. If so what can the training data be? What type of neural network (NN) is best suited for this kind of application?</p>
","pid raspberry-pi artificial-intelligence"
"4804","AHRS Algorithm Question","<p>As far as i understand, AHRS use orientation reference vectors to detect orientation error. And we can use magnetometer to correct yaw drift. But i see from my ak895 magnetometer that the data is not so stable, it kind of fluctuates continuously.</p>

<p>How can we use this data for AHRS algorithm?</p>
","quadcopter"
"4808","Is the input of ESC really limited at 50 Hz and will the PID controllers work properly?","<p>Based on the wiki page of ESC, the ESC generally accepts a nominal 50 Hz PWM servo input signal whose pulse width varies from 1 ms to 2 ms</p>

<p><a href=""http://en.wikipedia.org/wiki/Electronic_speed_control"" rel=""nofollow"">http://en.wikipedia.org/wiki/Electronic_speed_control</a></p>

<p>For our project, we integrate a flight controller for our UAV, Naza m-lite and we want to implement position control. We already have localization and we can control the quadrotor by applying servo width to roll, pitch, yaw and thrust throttle. Since the ESC only accepts 50 Hz, will the PID controller work at only 50 Hz?</p>
","pid"
"4815","Using i2c's SCL and GPIO pins as SDA","<p>The beaglebone black which I work on has only 2 i2c busses. Say for some crazy reason, i need to use 8 i2c busses. I dont want to chain any of the devices. The idea is to have every device's SDA line separated and use a shared SCL line so i can use the same clock and have as many SDA lines as i want. Since the SCL clock is hardware controlled there wont be any major issues here. The GPIO can do around 2.5mhz of switching so I am happy with that. </p>

<p>If that works out, i can spawn 8 threads to talk on 8 i2c Lines making my solution faster!</p>

<p>Do you think its doable? I would like to hear from you guys as this idea of using 1SCL and GPIO as SDA just popped in my head and i thought of sharing it with you guys. </p>

<p>Cheers!</p>
","i2c"
"4820","Choosing motor characteristics","<p>I have just sized the DC motors I want to use (corresponding to my robot and its intended applications - my figures include a 50% uncertainty factor to account for friction in reducers and other losses). Now I need to actually choose the exact motors I want to buy from the manufacturer (I am targeting maxon motors as I am not an expert and want no problem). I have a few down to earth questions about linking the mechanical needs to the electrical characteristics:</p>

<ul>
<li>Maxon states a ""nominal voltage"" in the characteristic sheets. Is that the voltage you should apply to the motor? This may be a dumb question but I have followed the full maxon e-learning course and read about other tutorials on the web and I could not find this information anywhere. Can anyone who knows about motors confirm?</li>
<li>As far as I understand, the nominal torque corresponds to the maximum torque the motor can sustain continuously. So I guess, as a rule of thumb, I should find a motor with a nominal torque = my max torque (after reduction), or around. Right?</li>
<li>Also I chose a motor reference (310005 <a href=""http://www.maxonmotor.com/maxon/view/catalog/"" rel=""nofollow"">found here</a>) which has a stated power of 60W, as the nominal voltage is 12V, I was expecting to have a nominal current of 5A, but it states 4A. Where am I wrong?</li>
<li>The motor I chose has nominal speed = 7630rpm - nominal torque = 51.6mNm. My needs are max speed = 50.42rpm / max torque = 10620 mNm. This means a reduction factor of 151 for speed and 206 for torque. Should I choose a gear closer to 151 or 206?</li>
<li>What is the ""rated torque"" mentioned when choosing a gear? I know my input torque (torque on the motor side) and my output torque (torque on the system side), does that correspond to any of these two?</li>
</ul>

<p>I have followed some theoretical and practical courses on the web but I find it hard to find answers to my down to earth question...</p>

<p>Thanks,</p>

<p>Antoine.</p>
","motor"
"4824","Implementing a position control for UAV through a flight controller. Plant model is unknown","<p>We are using Naza-M-Lite for our flight controller without GPS. The localization is obtained through our RGB-D camera sensor. We are able to teleoperate and even implement PID controllers for Roll, Pitch, Yaw and Throttle channels for our quadrotor. However, we do not know the plant model because what we are inputting from Arduino to the Naza-M-Lite are servo PWM ranging from 1000 to 2000. </p>

<blockquote>
  <p>For throttle:   1500 altitude hold, 2000 maximum throttle, 1000
  minimum throttle</p>
  
  <p>For Pitch, Roll, Yaw: 1500 maintain 0 angle, 2000 and 1000 moves the
  quadrotor towards its respective axes.</p>
</blockquote>

<p>However, even at 1500 on every channel, the quadrotor drifts, maybe due to flying indoors and the wind pushes the quadrotor. Once it gains momentum, it drifts. We are having trouble tuning this because we do not know the relationship of the output is to the position. If the output were velocity, it would have been easier. But as in our case, it is not. Is there a way to find the plant model of the Naza-M-Lite and how can we tune this?</p>
","localization pid quadcopter uav"
"4827","NAO motor model identification","<p>I am trying to create a model for the <a href=""http://en.wikipedia.org/wiki/Nao_(robot)"" rel=""nofollow"">NAO [robot]</a>'s motors. The figure below shows the step response for the knee motor. Afaik the NAO internally uses a pid controller to control the motor. I have no control over the pid or it's parameters. Thus I would like to treat the motor including pid as a black box. Theoretically it should be possible to model <em>pid+motor</em> as a $pt_2$ system, i.e. a second order lti system.
A $pt_2$ system is defined by the following differential equation:</p>

<p>$$T^2\ddot{y}(t) + 2dT\dot{y}(t)+y(t) = Ku(t)$$.</p>

<p>I tried fitting a $pt_2$ model but was unable to find good parameters.</p>

<p>Any idea what model to use for this kind of step response?</p>

<p><strong>edit</strong>:
I tried modifying the equation to add a maximum joint velocity like this:</p>

<p>$$T^2\ddot{y}(t) + (\frac{2dT\dot{y}(t) + m - |2dT\dot{y}(t) - m|}{2})+y(t) = Ku(t)$$ 
where $m$ is the maximum velocity. The fraction should be equivalent to $min(2dT\dot{y}(t), m)$.</p>

<p>However I am not sure if this is the correct way to introduce a maximum joint velocity. The optimizer is unable to find good parameters for the limited velocity formula. I am guessing that is because the min() introduces an area where parameter changes do not cause any optimization error changes.</p>

<p><img src=""http://i.stack.imgur.com/V6QWu.png"" alt=""Step response image""></p>
","motor"
"4828","Is the nominal voltage of a motor the voltage to apply to the motor?","<p>I have just sized the DC motors I want to use (corresponding to my robot and its intended applications - my figures include a 50% uncertainty factor to account for friction in reducers and other losses). Now I need to actually choose the exact motors I want to buy from the manufacturer (I am targeting maxon motors as I am not an expert and want no problem). I have a few down to earth questions about linking the mechanical needs to the electrical characteristics, among them:</p>

<p><strong>Question #1:</strong></p>

<p>Maxon (or the other manufacturers) states a ""nominal voltage"" in the characteristic sheets. Is that the voltage you should apply to the motor? This may be a dumb question but I have followed the full maxon e-learning course and read about other tutorials on the web and I could not find this information anywhere. Can anyone who knows about motors confirm?</p>

<p>I have followed some theoretical and practical courses on the web but I find it hard to find answers to my down to earth question...</p>
","motor brushless-motor"
"4829","Choosing DC motor: max needed torque vs nominal torque","<p>I have just sized the DC motors I want to use (corresponding to my robot and its intended applications - my figures include a 50% uncertainty factor to account for friction in reducers and other losses). Now I need to actually choose the exact motors I want to buy from the manufacturer (I am targeting maxon motors as I am not an expert and want no problem). I have a few down to earth questions about linking the mechanical needs to the electrical characteristics, among them:</p>

<p><strong>Question #2:</strong></p>

<p>As far as I understand, the nominal torque corresponds to the maximum torque the motor can sustain continuously. So I guess, as a rule of thumb, I should find a motor with a nominal torque = my max needed torque (after reduction), or around. Right?</p>
","motor brushless-motor servos servomotor"
"4830","Stated power for a motor does not equal nominal voltage x current?","<p>I have just sized the DC motors I want to use (corresponding to my robot and its intended applications - my figures include a 50% uncertainty factor to account for friction in reducers and other losses). Now I need to actually choose the exact motors I want to buy from the manufacturer (I am targeting maxon motors as I am not an expert and want no problem). I have a few down to earth questions about linking the mechanical needs to the electrical characteristics, among them:</p>

<p><strong>Question #3:</strong></p>

<p>I chose a motor reference (310005 maxon reference <a href=""http://www.maxonmotor.com/maxon/view/catalog/"" rel=""nofollow"">found here</a>) which has a stated power of 60W, as the nominal voltage is 12V, I was expecting to have a nominal current of 5A, but it states 4A. Where am I wrong? </p>
","motor servos servomotor"
"4831","Selecting a gear reduction: torque vs speed","<p>I have just sized the DC motors I want to use (corresponding to my robot and its intended applications - my figures include a 50% uncertainty factor to account for friction in reducers and other losses). Now I need to actually choose the exact motors I want to buy from the manufacturer (I am targeting maxon motors as I am not an expert and want no problem). I have a few down to earth questions about linking the mechanical needs to the electrical characteristics, among them:</p>

<p><strong>Question #4:</strong></p>

<p>The motor I chose (maxon brushed DC: 310005 <a href=""http://www.maxonmotor.com/maxon/view/catalog/"" rel=""nofollow"">found here</a>) has nominal speed = 7630rpm - nominal torque = 51.6mNm. My needs are max speed = 50.42rpm / max torque = 10620 mNm. This means a reduction factor of 151 for speed and 206 for torque. Should I choose a gear closer to 151 or 206?</p>
","motor brushless-motor servos servomotor"
"4832","How does the ""rated torque"" for a gear relate to the maximum torque?","<p>I have just sized the DC motors I want to use (corresponding to my robot and its intended applications - my figures include a 50% uncertainty factor to account for friction in reducers and other losses). Now I need to actually choose the exact motors I want to buy from the manufacturer (I am targeting maxon motors as I am not an expert and want no problem). I have a few down to earth questions about linking the mechanical needs to the electrical characteristics, among them: </p>

<p><strong>Question #5:</strong></p>

<p>What is the ""rated torque"" mentioned when choosing a gear? I guess it is related to the maximum torque the gear can support... But now, I know my input torque (torque on the motor side) and my output torque (torque on the system side), does that correspond to any of these two?</p>
","motor brushless-motor servos servomotor"
"4836","How to control PID Yaw","<p>My yaw angle varies from -180 degree to 180 degree. </p>

<pre><code>                                        -170 170
                                   -135            135
                                 -90                  90
                                      45           45
                                          10 -10   
</code></pre>

<p>If my current heading is about 170 degree, then the wind makes it rotate to the left at about -170 degree, then how can PID control it to make it rotate back to the right at 170 degree.</p>

<p>Since, for PID   <code>ERROR = SETPOINT - INPUT</code>
In my case, <code>SETPOINT = 170</code>, and <code>INPUT = -170</code>, the the <code>ERROR = 170 - (-170) = 340</code>.
So instead of moving to the right and apply <code>PWM = 20</code>, it rotate to the left and apply <code>PWM = 340</code> and come back to the desired position, which is 170 degree?</p>
","pid"
"4840","A robotics computer with graphics card, lots of computation power, battery, no screen, no keyboard?","<p>I'm working on a robotics platform and we need an on-board Ubuntu machine to run ROS image recognition.</p>

<p>Does anyone know of a good set of computer hardware that has</p>

<ul>
<li>NO screen</li>
<li>NO keyboard</li>
<li>Built-in battery (for charging separate from the robot)</li>
<li>Quite a bit of compute power (i5+, 4+ GB ram)</li>
</ul>

<p>I thought about using a laptop, but the keyboard and screen are a lot of extra weight/volume I don't want to carry around. Something like an Intel NUC is appealing, but has no battery.</p>
","driver"
"4843","How to change the orientation of an object w.r.t a scene?","<p>I am trying to localize an object in a point cloud using ROS, PCL. For that I capture the scene and model using Asus xtion pro sensor. I use <a href=""http://felixendres.github.io/rgbdslam_v2/"" rel=""nofollow"">RGBDSLAMv2</a> for capturing the model. </p>

<p>Then I use <a href=""http://docs.pointclouds.org/1.0.1/classpcl_1_1_iterative_closest_point_non_linear.html"" rel=""nofollow"">ICP (nonlinear version)</a> to find the transform from the model to each cluster of the cloud. The cluster with the lowest score is chosen as the best matching cluster. </p>

<p>Pseudocode:</p>

<pre><code>Segment the point cloud into different clusters. ([Using euclidean clustering][3])
for each cluster i
     Source: 3dmodel. Target: current cluster
     Perform [ICP (nonlinear version)][2].
     score[i] = icp.getFitnessScore()
     T[i] = icp.getFinalTransformation()
end for
matchingCluster = cluster with minimum score
finalT = T[matchingCluster]
</code></pre>

<p>However, I am not able to find the correct transformation.</p>

<p>Here are the screenshots of the results I got:</p>

<p><img src=""http://i.stack.imgur.com/RJGZB.png"" alt=""Scene of objects"">
<img src=""http://i.stack.imgur.com/sjwz5.png"" alt=""3d model captured using RGBDSLAM"">
<img src=""http://i.stack.imgur.com/UWwys.png"" alt=""overlay of scene and model""> </p>

<p>The red colored object is the transformed model overlayed onto the scene. The yellow object represents the original model in the coordinate system of the scene.</p>

<p>Now, my concern is why there is no proper transformation? Am I missing something? </p>

<p>Second, I see that the object model and scene are in different coordinate system. So the model appears inverted when presented in the scene's coordinate system. Is there a way in which I can transform the model upright before running ICP?</p>

<p>Thanks :)</p>
","computer-vision kinect"
"4847","Understanding a sliding mode controller for quadrotors","<p>I'm really willing to understand and implement such a controller (sliding mode) for a quadrotor.
I've found <a href=""http://waset.org/Publication/modeling-and-control-of-a-quadrotor-uav-with-aerodynamic-concepts/7686"" rel=""nofollow"">this interesting document explaining that topic</a>.
If you scroll down until page 381 (don't be scared, the document is just 6-7 pages) you can find the following height control law (equation .19):</p>

<p>$$
U_1 = \frac{m}{\cos{\phi}\cos{\theta}}[c_1(\dot z_r - \dot z) + \ddot z_r + \epsilon_1  sgn(s_1) + k_1 s_1 + g]
$$</p>

<p>The explanation of most of the term should be quite easy, but let's focus on the variable <em>z</em>, the height (or altitude if absolute) of the quadrotor. Anyway the control law ""pretends"" not only the goal height <em>z</em> (through $s_{1}$) but even the vertical speed $\dot z_{r}$ and vertical acceleration $\ddot z_{r}$ (<em>r</em> means here <em>reference</em>).</p>

<p>Now...to me is not clear whether those variables the setpoints are, that must be reached once the quadrotor reaches its predefined height or they just symbolize an abstract mathematical formalism but are going to be most of the time Zero (because I want to reach the target height with $z = z_{r}$ but $\dot z_{r} = \ddot z_{r} = 0$)</p>

<p>?!?!?</p>

<p>I hope my question is clear. Even if this I put in the title ""sliding control"" I think it may be helpful for other type of controllers.
Regards</p>
","quadcopter"
"4853","Controlling a 400W motor with 24V 16A batteries with an arduino board","<p>I want to build a robotic vacuum. I have a 400W 24V vacuum motor that I want to switch on automatically at a set time every night. The batteries I will be using will be 2x12V 80aH deep cycle gel batteries connected in series. I want the Arduino to switch the motor on and off. So my first real question I guess is will the 5V supplied from the Arduino be able to switch on a motor that big? The second question is a mosfet the answer? My apologies I'm pretty new to all this but love it..</p>

<p>Can I control a 400W motor with 24V 16A batteries with an Arduino board and a mosfet? What type of mosfet would I use?</p>
","arduino"
"4855","Is a simple range sensor described below sufficient to implement particle filter localization?","<p>I am trying to implement a monte carlo localization/particle filter localization with a simple range sensor. The range sensor only sees in the direction the robot is heading and returns back any obstacle in its line of sight. If there is no obstacle, then the sensor returns back the distance to the boundary wall i.e. there is no maximum range for sensor.</p>

<p>But, the problem is that i am not able to locate the robot's position. Now, I am feeling is it cause the sensor is not powerful enough. Is it feasible to do localization with such a sensor or should I change the sensor type?</p>

<p>Please tell me what you guys think?</p>
","localization particle-filter"
"4857","Continous rotation with cables","<p>A motor needs to spin n*360 degrees. On top of the motor there are distance sensor which scan the room. I.e. lidar. What options do I have for implementing a continous rotation while having cables which are in the way?</p>
","motor chassis"
"4863","Real time operating system for robotics vision","<p>I have robot vision system which consists of conveyor with encoder, two cameras (gigabit eth and usb) and simple illuminator.</p>

<p>I need to trigger cameras and illuminator when encoder reaches position interval.</p>

<p>I'm considering using real time operating system for this task:
Encoder, illuminator and cameras connected to PC and vision system application runing on it.</p>

<p><strong>Which real-time solution you can reccomend for this problem?</strong> </p>

<p>I'm considering using Beckhoff TwinCAT software which turns normal operating system into RT. </p>
","computer-vision real-time"
"4866","What happens when a brush-less motor stalls?","<p>Ok apologies for those who think my questions are not direct enough as I got warned about this. I am really new to this and I will try to keep this one direct enough for the forum. </p>

<p>For obvious reasons I cannot test this out without damaging something so would prefer to learn from the experience of others.</p>

<p>I have a ""Turnigy Trackstar 1/10 17.0T 2400KV Brushless"" motor which I will be using for my weapon (spinning disk). </p>

<p>Relevant specs of the motor are:
Kv: 2400
Max Voltage: 21v
Max current:24amps
Watts: 550
Resistance: 0.0442Ohms
Max RPM: 50000</p>

<p>I will use this with an ESC with the following specs:</p>

<p>Constant Current: 30A
Burst Current: 40A
Battery: 2-4S Lipoly / 5-12s NiXX
BEC: 5v / 3A
Motor Type: Sensorless Brushless
Size: 54 x 26 11mm
Weight: 32g
Programming Functions:
Battery Type: Lipo /NiXX
Brake: On / Off
Voltage Protection: Low / Mid / High
Protection mode: Reduce power / Cut off power
Timing: Auto / High / Low
Startup: Fast / Normal / Soft
PWM Frequency: 8k / 16k
Helicopter mode: Off / 5sec / 15sec (Start up delay)</p>

<p>If the motor stalls, I know the current draw will increase drastically. So my questions are:</p>

<ol>
<li><p>In the case that the motor stalls (my disk gets stuck in the opponent etc), then what gets damaged? The motor, the ESC, both? And how long before this happens? </p></li>
<li><p>Would I have time to turn the r/c switch off before irrevocable damage occurs (once I am obviously observing the action?). Notes. I will be using an on/off switch on the r/c to just turn the motor on and off (so no proportional speed increase etc), plus I will be using an 11.1 volt battery even though the motor is rated for a 21 volt maximum.</p></li>
</ol>

<p>Thanks.</p>
","brushless-motor esc"
"4872","Roll, Pitch Calculation Problem!","<p>My problem is that when i hold my sensors (MPU9150) so that +y axis is downward, and y axis is on the horizontal plane, i expect that pitch = 90 degree,  and roll = 0 degree, but actually pitch = 90 degree, and roll = 160 degree. However, when roll = 90 degree and pitch = 0 degree (That is what i expect). Do you know what cause my problem?
Thanks</p>
","quadcopter"
"4873","Choosing suitable simulator for a swarm of AUVs","<p>Which of the following simulators is the best choice for simulating a swarm of AUVs working together to perform a mission? Please clarify your reason and if you know any better choice, I would greatly appreciate it if you kindly help me. Please consider the need for doing Hardware-In-The-Loop(HIL) simulation. </p>

<ol>
<li>Webots</li>
<li>V-REP</li>
<li><a href=""https://savage.nps.edu/AuvWorkbench/"" rel=""nofollow"">AUV Workbench</a></li>
<li>Gazebo</li>
<li>UWSim</li>
<li><a href=""http://homepages.laas.fr/afranchi/robotics/?q=node/120"" rel=""nofollow"">SwarmSimX</a> </li>
</ol>

<p>In addition, notice that capability to connect to the middle-wares like ROS is really important. </p>

<p>The other option is using a game engine like Blender but I think it needs a lot of developing effort and is time-consuming! Would you recommend this approach be used? If not, why not? And what would you recommend instead?</p>
","mobile-robot ros simulator auv"
"4874","What middle-ware do you recommend for a swarm of AUVs?","<p>Working with a swarm of robots, collaboration between the nodes is really important(either for the goal of simulation or real-word operation). Middle-wares are the frameworks for this special purpose. I know some of the relevant middle-wares like ROS(general-purpose but popular) or uMVS(that is basically design for AUVs). Now, I have two questions:</p>

<ol>
<li>Do you know any other choice for the above mentioned purpose? </li>
<li>What criteria should I consider for choosing a middle-ware suitable for my purpose?</li>
</ol>

<p>Thank you.</p>
","mobile-robot ros"
"4875","Arduino Lightsensor Blocking Code","<p>I have a <a href=""http://smart-prototyping.com/GY-31-TCS230-TCS3200-Color-Sensor-Recognition-Module-for-Arduino.html"" rel=""nofollow"">GY-31 Light sensor</a> and I am trying to make a skittle sorting machine. There is a little wheel skittles can drop into then they are turn to 90 degrees read by the light sensor then dropped out the bottom. The problem is the code seems to get stuck after reading the light sensor (it still flows through but won't execute the moving of the servo). As soon as I take out the ReadSensor line the servo works like normal. Do I have to dispose of the Color Sensor some how?</p>

<pre><code>#include &lt;Servo.h&gt;
#include &lt;MD_TCS230.h&gt;
#include &lt;FreqCount.h&gt;

#define  S2_OUT  2
#define  S3_OUT  3
MD_TCS230  CS(S2_OUT, S3_OUT);

Servo myservo;  // create servo object to control a servo 
                // a maximum of eight servo objects can be created 

void setup() 
{ 
  myservo.attach(11);  // attaches the servo on pin 9 to the servo object 
  Serial.begin(57600);
  Serial.println(""[TCS230 Simple BLOCKING Example]"");
  Serial.println(""Move the sensor to different color to see the value"");
  Serial.println(""Note: These values are being read in without sensor calibration"");
  Serial.println(""and are likely to be far from reality"");

  CS.begin();
  CS.setFilter(TCS230_RGB_X);
  CS.setFrequency(TCS230_FREQ_HI);
  CS.setSampling(100); 
} 

 void readSensor()
{
  uint32_t v;
  v = CS.readSingle();
  Serial.print(""Simple value = "");
  Serial.println(CS.readSingle());

}

void loop() 
{ 
  Serial.println(""Start"");
  myservo.write(0);
  delay(1000);
  myservo.write(90);
  delay(1000);
 readSensor();
  myservo.write(180);
  delay(1000);
} 
</code></pre>
","arduino sensors rcservo"
"4880","Programming a G-code Interpreter","<p>I want programme my very own G-code generator for my final year electrical engineering project. I know that there are many open source G-code generators out there, but I need a G-code generator which generates a G-codes for custom circuit designs drawn by the user and pass the G-code serially to my 2 axis CNC machine. So currently I'm working on a Qt based GUI where I draw .dxf format circuit diagrams and electrical components (like resistors, capacitors) and when I press a ""Generate G-code"" push button I should generate a text file with nice set of G-codes for my designed diagram.</p>

<p>So the problem here is, how do I generate the G-code? Is there any specific algorithm to follow or adapt? I tried googling for G-code generator algorithm but I couldn't find any helpful stuff.</p>
","cnc circuit"
"4885","Developing a Quadrotor using ROS","<p>I
 suppose who know <a href=""http://wiki.ros.org/ROS/Tutorials"" rel=""nofollow"">ROS</a> and how it works (at least most of you)
I have some question regarding the implementation of a quadrotor in that
 framework.</p>

<ol>
<li><p><strong>3D movements:</strong> A quadrotor has 6DOF and moves in a 3D 
environment. Looking at the various ROS packages I could not find any 
package that allows to drive a ""robot"" in the 3D space. The package 
<strong>/move_base</strong> for instance allows only 2D. Make sense to use this 
package for such a project? I thought to use 2D navigation projecting 
the ""shadow"" of a quadrotor on the ground...</p></li>
<li><p><strong>MoveIt:</strong> it seems a real interesting and promising package, but I
read that it is for robotic arms and not expressly indicate for 
quadrotor. Maybe one can use the possibility to create a virtual 
floating joints in <strong>MoveIt</strong> to let the quadrotor any movement in a 3D 
environment...that's ok, but I cannot understand whether is ""too much"" 
and not useful for a flying robot.</p></li>
<li><p><strong>Trajectories:</strong> The possibility to create a 3D trajectory in the 
space seems to be not a standard package of ROS. I found <strong>Octomap</strong> 
which allows the creation of 3D maps from sensor datas. Very interesting
and for sure very useful. But...I don't think it could be useful for 
creating 3D trajectories.  Should I in that case create an extra package
to compute 3D trajectories to be feed into the quadrotor? Or there 
already something like that?</p></li>
</ol>

<p>There is already an existing project <em>hector_quadrotor</em> which seems to 
acclaim a good success ans it is very considered in the field. Most 
people refer to that project when speaking or answering question 
regarding quadrotors in ROS. I saw many times that project...since 
weeks. And due to the total lack of documentation I didn't try anymore 
to understand how it works. Really too difficult. 
Another interesting project, ArDrone, has comments in the source 
code...in Russian!!! @_@</p>

<p>Could you me give any good suggestions? Or point me in the right direction 
please?
It would help me to understand how to focus my searches and which 
package I can/cannot use.</p>

<p><strong>UPDATE:</strong> my goal is to let the quadrotor flying and using gmapping to localize itself. I've heard and read al lot of stuff about that but I found all this tutorials very hard to understand. I cannot get a <em>global</em> vision of the software and sometime I run in problems like: ""is there a package for this task, or should I invent it from scratch?""</p>

<p>Thanks!</p>
","ros quadcopter"
"4888","How to make a robot following a virtual eight figure pattern without using microcontrollers?","<p>The robot should go around 2 boxes and stop at the starting point after tracing a 8 figure pattern. With micro-controllers, i guess it can be easily done using sensors or navigation algorithms. Please suggest how can be one made without them.</p>
","mobile-robot"
"4890","Will AIs ever be as advanced as the human brain?","<p>I'm reading a book about a hypothetical economy in which robots work for us because they eventually became able to do everything we do (""Our work here is done, visions of a robot economy"" by Nesta).</p>

<p>I was wondering though: is it theoretically possible for a human brain (an extremely complex  artificial intelligence the way I and many others see it) to comprehend itself in its deepest details and produce an artifical intelligence which is exactly identical? It sounds unlikely. If not then, how close can we get?</p>

<p>This is not a philosophic question, the ideal answer would be a rigorous demonstration based on metrics simplifying the problem to be able to answer it, however objective answers with valid arguments are always interesting, too.</p>
","artificial-intelligence"
"4891","Quadcopter Hovering Problem","<p>My quadcopter can lift off the ground, but it kinds of circles around. Here is my video
<a href=""https://www.youtube.com/watch?v=Rxjpwhbgiw0"" rel=""nofollow"">https://www.youtube.com/watch?v=Rxjpwhbgiw0</a></p>

<p>Anyone helps me?</p>
","quadcopter"
"4897","Are there surface texture sensors for integration in circuits?","<p>I have a project in mind for a robot which is able to recognize surfaces, and thought about including the following sensors:</p>

<ul>
<li>a temperature sensor</li>
<li>a colour sensor, or a complex of electronic components to determine a colour</li>
<li>and a texture sensor, or, as above, a complex of components to fulfill the purpose</li>
</ul>

<p>Now, I did some research on finding a(preferably small) texture sensor for soldering into an electronic circuit, similar to those little temperature sensors one can buy.
I already thought that ""small"" would probably turn out to be not small at all before I searched.
But my research has been fruitless.
Not just fruitless like in ""I can't find exactly what I want."" but fruitless like in ""I cannot find anything similar to what I want.""...</p>

<p>Most things that turned up either were scientific papers about whole devices, or whole devices for purchase.
Some company even choose ""Structure Sensor"" as the name of their iPad-compatible 3D scanner, which made the search utterly depressing as every second article I found is about buying some pre-built iPad device.</p>

<p>All I need is the electronic component, nothing else.
So hope that any of you people can spare me some research time and recommend me a company/site/whatever which sells such texture sensors.</p>

<p>(Btw., I do know that surface sensors are probably a bit way more complex than temperature sensors, and my hope for getting what I want is low, but just because I cannot find something, ot does not mean that it doesn't exist.)</p>
","sensors"
"4900","Inverse Kinematics Constant End Effector Angle","<p>I have a simple RRR manipulator where one motor controls the base rotation, and the other two allow movement in a plane extending forward from the base and upwards/downwards. Are there any standard ways to ensure the angle of the end effector remains constant?</p>

<p>My current solution uses explicit trigonometric expressions based on distance between joints, but if there is a better way to solve it to include restraints I'd be open to suggestions.</p>

<h3>Edit</h3>

<p>The manipulator is essentially like the image below, but with an additional base rotation. This allowed for the inverse kinematics to be simplified. As a reference here is the site <a href=""http://www.hessmer.org/uploads/RobotArm/Inverse%2520Kinematics%2520for%2520Robot%2520Arm.pdf"" rel=""nofollow"">http://www.hessmer.org/uploads/RobotArm/Inverse%2520Kinematics%2520for%2520Robot%2520Arm.pdf</a></p>

<p><img src=""http://i.stack.imgur.com/K8kGd.png"" alt=""enter image description here""></p>
","inverse-kinematics"
"4901","How to tune the PID parameters using Fuzzy Logic?","<p>I previously used the Ziegler method to tune the parameters of my PID controller to control my robot's position. I then implemented fuzzy logic for self-tuning the parameters. 
I have two inputs to the fuzzy logic controller; one is the position error and the error rate.</p>

<p>I know that my problem might be due to not understanding the effect of each parameter very well.</p>

<p>The problem is that I am confused in setting up the fuzzy rules. When do I need to use high and low values for Kp, Kd and Ki to achieve the best tuning? Is it that Kp must be very low when the error is almost zero (hence, the robot is at the desired position)? The same question applies for all of the three parameters.</p>
","control pid tuning"
"4902","LM2576 and LM2596 regulator make microcontroller hang","<p>I am making a self balancing scooter which runs off 2 x 12V SLA batteries connected in series to make 24V. Everything works as expected except for the power supply which makes me pull most of the hair in my head for 2 weeks now. Hope someone could help.</p>

<p>The 2 24V motors run off the batteries directly. Now for my scooter, I need a +12V line for the half bridge drivers, and a +5V line for the signal part. For +12V I am using a LM2576-12 hooked to the batteries (+24) and for the +5V signal I am using a LM2596 ADJ, also hooked directly to the batteries (or is it supposed to be hooked to the output of LM2576-12 for better performance??).</p>

<p>The problem is that, when the motors are under load this power supply system makes the microcontroller hang (or reset I am not too sure, since everytime I have to try to turn off the power switch immediately, because the motor keeps running with whatever value they are fed with right before this happens), usually within 1 minute of riding, which is very dangerous when someone is onboard.</p>

<p>I have read and re-read the datasheet of LM2576 and LM2596 many times, and have tried many settings, from recommended to different values of capacitor and inductor. For the diode, I am using the SS34.</p>

<p>I guess its not because of electromagnetic interference, since while I do have the PCB located near the motors, the PCB is actually put inside a homemade Faraday cage which is grounded (Battery -), and the motor cases are also grounded. Plus that the microcontroller only hangs when motors are under load (i.e. me on board), especially when I go from forward to backward.</p>

<p>The motor controller is self made, using 8 x AUIRF2804S MOSFETs. I also put 4 x 1000uF caps between the motors and the +24V.</p>

<p>Would anyone be so kind to throw some light. What would a power supply for this kind of application is supposed to be?</p>
","motor"
"4903","Self-locking actuator: Friction versus worm gear","<p>I am planning to control my bicycle derailleur with an electronic circuit, the advantages being multiple but not topic of this question.</p>

<p>The actuation would be performed by placing the assembly very close to the derailleur (but not on it) and by winding the usual steel cable around a spool placed on axis of a gear, and by using a motor to turn the gear. </p>

<p>This question concerns the alternatives for the spool self-locking mechanism (and eventually the kind of motor to use).</p>

<p>In the literature I found <a href=""http://nabilt.github.io/diy_eshift/"" rel=""nofollow"">http://nabilt.github.io/diy_eshift/</a> and other similar ones that directly modify the derailleur with a stepper motor, and then are forced to keep the stepper motor powered up all the time to keep some torque on the shaft. I consider this inefficient, therefore I require a self-locking system to be able to remove power.</p>

<p>I come up with two ideas:</p>

<ul>
<li>worm gear operated by a DC motor, where the steel cable is wound around the gear. This system is self-locking or almost self-locking, according to the gear ratio: the gear cannot (easily) drive the worm.</li>
<li>a motor driving normal gears with an appropriate reduction factor, but with an additional friction element, whose friction force is greater than the strength of the spring mounted on the derailleur (sorry if I mixed the technical terms). This is what normal bicycles already have: the friction along the cable and in the element placed n the handle is high and keeps the derailleur in place.</li>
</ul>

<p>Both system would be assisted by a position-sensitive element (a trimmer?) to detect the actual position of the gear and/or of the derailleur, all configured in a closed feedback loop.</p>

<p>I don't consider additional options for the gear such as this one: <a href=""http://www.agma.org/assets/uploads/publications/10FTM17_Kapelevich.pdf"" rel=""nofollow"">http://www.agma.org/assets/uploads/publications/10FTM17_Kapelevich.pdf</a> that consists of parallel-axis gears, whose teeth are however shaped in a manner to achieve self-locking without the need of a low-efficiency worm system.</p>

<p>From my point of view, I cannot see any clear advantage of worm gear vs friction except for:</p>

<ul>
<li>the worm gear may allow me to build a more compact assembly, thanks to the two axes being perpendicular</li>
<li>speed vs torque: worm gears reduce the torque requirements, but the motor has to spin a lot and I cannot wait 3 seconds for each gear change.</li>
</ul>

<p>Concerning the choice of the motor type (this is not the main question though), I think that:</p>

<ul>
<li>a worm gear allows me to easily use a DC motor, since torque requirements are low and I don't need to detect the position of the shaft. Moreover, DC motors increase torque with decreasing speed, while stepper motors have the maximum torque defined by the stepping frequency.</li>
<li>DC are more compact and cheaper, important if I decide to offer this assembly as a kit and not unique, personal prototype.</li>
<li>I am working with 5V supply and I fear most easy-to get DC motors (old printers, scrap electronics) work on 12V, with a significant reduction of the available torque when operated at 5V.</li>
</ul>

<p>I was looking for a ""mechanics"" section in Stack Exchange but I couldn't find it, so I opted for Robotics instead of Electronics.</p>
","brushless-motor stepper-motor"
"4904","Is there build your own garage robotic assembly lines out there?","<p>I am new to robotics, and would like to build a smaller robotic arm than in manufacturing facilities.
I want a small robotic material handlers that can pick up or handle small objects around 12""x12""x12"" objects. Essentially a small robotic assembly line in my garage.</p>

<p>Are there any kits I can purchase that deals with robotic assembly lines?
I was wondering has anyone dealt with this before any suggestions on this?</p>
","robotic-arm"
"4911","Robot to manipulate poultry","<p>I am a software engineer and also a poultry farmer.</p>

<p>I periodically have to manipulate my poultry in such a way as to grab their head and hold them for a brief period, approximately 10 - 30 seconds.  This is an extremely labor-intensive process and it occurred to me that I might be able to use robotics to do the same task.  I am a software engineer so I know very little about robotics and am hoping that someone can point me in the right direction.  Can someone please refer me to companies and/or robotic systems that might be able to help me with this task?</p>

<p>I currently load the poultry into cages specifically designed for this process.  I am thinking that these cages could still be used as they keep the birds from running and make it much easier to capture their heads.</p>

<p>I recently read about a Raspberry PI that had a port of Deep Belief image recognition SDK and thought this might be a promising start.</p>
","robotic-arm"
"4913","Gyro rate gets increase problem","<p>I am using PID controller to stabilize quadcopter. Its working well on the jig. But when I removed jig it went up and hit ceiling. I analyzed my data and from which I come to the conclusion that at static poisition my gyro sensor is outputing +-6deg/sec but when I start motors (without being control in) gyro rate jupms to +-30deg/sec. This increase in rate due to vibrational noise is causing quadcopter to liftup without beign my intension. Any suggestions to get rid from this vibrational noise? </p>
","sensors"
"4914","Robotics simulation from PNG map","<p>I am a complete beginner at this.</p>

<p>I have an arbitrary map as a PNG file (black and white, only). And I'm supposed to draw a robot at a position(x,y) and then I'm supposed to simulate the robot taking a laser-scan measurement for an opening angle of 130 degrees, separation of each scan-line is 2 degrees. So, obviously: each laser scanline is supposed to be obstructed by black pixels in the PNG map, while the other lines just keep on going for an n distance.</p>

<p>I've attempted drawing scanlines around the object, but I'm running into issues when it comes to getting each line to be obstructed by the black-lines. To my mind, it requires a completely different approach.</p>

<p>Any helpful advice on how to get started would be greatly appreciated.</p>
","python mapping simulation"
"4928","Simplest and cheapest way to create a spring back latch","<p>My and a friend are hacking together a Nespresso Coffee pod dispenser. We have our heart set on a particular design after thinking up countless ways of dispensing a single pod. </p>

<p>The design has a single flavour of pods in a vertical tube so they tend to fall down. One or more latches around the base of the tube stop any pods from falling out. Releasing the latch for 45ms will allow the pod through (10mm fall, well past the lip of the pod) while catching the next one.</p>

<p>The latch is the problem component. I haven't yet found a suitable product off the shelf. Ideally, the solution would be compact and cheap (&lt; $5). </p>

<p>Here are some of the latch ideas to date (most of which are considered linear in motion):</p>

<ol>
<li>Solenoid - Seems over-kill and tend to be > 5 dollars each</li>
<li>Ultrasonic Motor - Can't find any</li>
<li>Linear Actuator - Usually around 50 dollars and quite bulky</li>
<li>Piezoelectric Actuator - Mostly tuned for nM scale precision, and are hard to come by.</li>
<li>Rotating disk with release notch, driven by stepper motor - still > $5 and moderately bulky.</li>
<li>Rotating latch with string attached to rack and pinion powered by electric motor - Don't think it's a simple enough solution.</li>
<li>Rotating cam - how a gumball machine works (I suspect). (This was also suggested in an answer, but would involve both a mechanical and electronic motor component, not as simple as option [5])</li>
</ol>

<p>I have a 3D printer, so I am open to mechanical solutions - a custom latch with crude electromagnet for example.</p>

<p><img src=""http://i.stack.imgur.com/Jo6qG.png"" alt=""enter image description here""></p>

<p>Note the desired size of the latch (Yellow), holding pods (Orange) in a tube (Black). Yes, motors can work, but they would be quite bulky. I'm not after the obvious solution, but a clever one, or one which finds the suitable product.</p>

<p>(I understand that with only one latch on one side, the pods will not sit perfectly vertical, and the latch would need to be higher up.)</p>
","mechanism"
"4930","Chassis materials for Hobby-weight (5.44Kg) battle robot","<p>I have sorted out all the internals for my robot (drive systems and weaponry) and now I need to put it all together in a chassis which will be about 40 cm wide by 35 cm long by 7 cm high. I have examined different options, including Perspex, Acrylic and Polycarbonate as well as aluminum, in a number of thicknesses. However I have excluded Perspex and Acrylic because, unlike Polycarbonate, they tend to shatter if bent.</p>

<p>So now it is down to Polycarbonate and/or Aluminum.</p>

<p>So here is the problem, up for discussion or a solution.. but first I must point out (a) that the overall weight limitation in turn imposes chassis weight limitations (b) that this is my first ever robot wars entry, and (c) I am likely to be up against cutting and tearing devices. </p>

<p>I already have the weights of the different materials in hand, so all of the following options are possible in terms of weight.</p>

<p>Option 1: Do it all in 6 mm Polycarbonate.</p>

<p>Option 2: Combine a thin (2 mm aluminum) outer shell with an underlying 3 mm polycarbonate one to get a good mix of the properties of both (rigid and hard, thin and heavy + flexible and strong, thick and light. </p>

<p>Option 3: As Option 2 but the other way round - 3mm Polycarbonate on the outside and 2 mm Aluminum on the inside.</p>

<p>Should I go with Option 1, 2, 3 or something else altogether that maybe I am not seeing? (Note: having it all in 3 mm aluminum is not possible as it will be too heavy - I checked)</p>

<p>Should I have the aluminum on the outside as a heavy duty shell or on the inside as a ""last resort"" layer? (Note: In my mind these layers would be held together with nuts and bolts with washers to spread any impact loads; but even here should the nuts and bolts be tightened for rigidity or left slightly loose for impact absorption?)</p>

<p>Any advice, especially from people seasoned in the art of robot warfare please?</p>
","wheeled-robot chassis"
"4933","Power switch standards and suitability for purpose","<p>I scavenged a 4 terminal power switch (Legion EPS 21) from some electronic device (don't remember what it was) and it has the following markings on it:</p>

<p>Legion EPS21
10A 250VAC TV-5
8A/128A 250V &micro;T85 </p>

<p><img src=""http://i.imgur.com/TNYkzqQ.jpg"" alt=""Legion EPS21""></p>

<p>I would like to use this as the main power switch for my robot which will have nothing higher than 12 volts and a ""normal"" total Amperage (i.e. with all the motors in it running) of around 25 Amps, but of course if a motor stalls then the current will rise much higher. </p>

<p>First of all I cannot understand how the same switch can be rated for different amperages. I cannot find any datasheets for this switch that might help nor any reference to the standards (TV-5 and &micro;T85). So I would like to know if this can handle 128 Amps at 12 Volts. If it helps at all, the wires currently connected to the terminals are quite thick and have ""18 AWG 600 volts"" written on them.</p>

<p>Secondly I would like to ask whether I need to cater for normal running amperage levels or for stall current levels, which are obviously much higher. Although people talk about stall currents running over 100 Amps in some cases - which quite frankly cause me some concern - I cannot seem to be able to find any such switches on any robot component websites, so I am starting to think that it is the ""normal"" level of current I should plan for and not the ""stall"" one. Am I right in this?</p>
","current"
"4934","Perfect implementation of Asimov's 3 Laws","<p>After seeing the movie, <i>I, Robot</i>, I got this question.</p>

<p>If Asimov's 3 Laws (actually implementing law 1 automatically implements the other 2) are perfectly implemented on a quantum computer that controls an army of humanoid robots, and it  decides that taking complete control over the politics and economics via revolution is the best way to ensure human happiness, shouldn't it be allowed to proceed peacefully to ensure minimal loss of life? Isn't the hero's decision to destroy the computer fundamentally wrong? </p>
","theory"
"4937","Can I use QT to communicate with a Lego NXT robot?","<p>QT has native bluetooth support, but can it be used to communicate with the Lego NXT robot?</p>
","nxt robotc"
"4939","Why can't i use different ESCs together on a multirotor?","<p>I'm working on a diy quadcopter build from scratch and have bought a 4pack ESC from Castel Creations.While i currently have my quad up and running(sort of), from what i've read on the various sources and forums on the internet, i am not able to/ not recommended to use different ESCs together on the same quad.</p>

<p>As i bought my ESCs together as a 4 pack, and am not able to buy any replacements unless i were to switch out all 4 of them, this has me worried in the eventual case of a spoilt ESC in the future.</p>

<p>From what i can gleam from various posts on the internet, it seems to have something to do with the rate at which ESCs communicate with the flight controller.If so, can i not simply buy a esc programmer and program all of them to communicate at the same rate?</p>

<p>I've asked the dude at my local hobby shop, and he said that i cannot/should not be using different escs from different brands or even the same brand but different models( i.e 35v &amp; 20V ) ESCs together.</p>

<p>I would really appreciate it if someone were to clarify what exactly is the issue with using different ESCs together on the same quadcopter.</p>

<p>P.S If it helps, i'm currently using the APM 2.6 as my flight controller on a WFLY transmitter and a f450 frame.</p>
","quadcopter microcontroller electronics esc multi-rotor"
"4943","What servos does this robot use?","<p>Well, I wanted to use some very small servos for a project and the smallest I could find there these: 
<a href=""https://www.hobbyking.com/hobbyking/store/uh_viewItem.asp?idProduct=33401"" rel=""nofollow"">https://www.hobbyking.com/hobbyking/store/uh_viewItem.asp?idProduct=33401</a></p>

<p>But Danny Choo (a Japanese blogger) started a business with robotic dolls some time ago and I remember him mentioning somewhere on his site that he uses servos in his dolls.</p>

<p><img src=""https://farm4.staticflickr.com/3836/14271013147_41a0820560_o.jpg"" alt=""servo in doll"">
(also this pic, containing doll nudity: <a href=""https://farm4.staticflickr.com/3889/14502508165_fde682636b_o.jpg"" rel=""nofollow"">https://farm4.staticflickr.com/3889/14502508165_fde682636b_o.jpg</a>)</p>

<p>This is about 60cm in height and therefore the servo in my first link is obviously too big, to e.g. fit inside the arm.</p>

<p>I was wondering what kind of servo(or motor in general if it's not a servo in the end) he is using that is so tiny it can fit in there.</p>

<p>Does anyone here have any idea?</p>
","motor servomotor"
"4944","Does anyone have a working example of using Qt to communicate with NXT?","<p>I'm stumped. I've been looking through all of the Qt classes and I'm so completely and utterly lost. There are only three examples of bluetooth use in Qt, but none of them work for me. I just need a program that can talk with NXT and analyze an image from a webcam.</p>

<p>Has anybody gotten this to work before?</p>
","nxt troubleshooting"
"4946","Joint angle correction using LM","<p>I have a camera mounted on a rotational joint. I need to calibrate the extrinsics of this camera. I can fix the camera at an estimated angle (facing the ceiling). Then I want to get the real angle.</p>

<p>For that I track key-points in the ceiling while moving my robot forward. Supposing that odometry is perfect, I will see a difference between real key-points shift and estimated shift from the odometry.</p>

<p>I thought about using <a href=""https://en.wikipedia.org/wiki/Levenberg%E2%80%93Marquardt_algorithm"" rel=""nofollow"">Levenberg Marquardt</a> to find the optimal solution which is the angle and of my camera in the robot frame but what would my equation look like?</p>
","cameras odometry joint"
"4948","Submarine screw and the isolation from the water","<p>How does the submarine prevent water flow through the screw mechanism? The mechanism rotates the screw so there should be a little gap, how come the water doesn't come through this gap in to the submarine?</p>
","motor design"
"4949","Has hierarchical learning been embodied in a robot before?","<p>I've been reading about hierarchical reinforcement learning (HRL) and it's applications. A well-written literature review on the subject can be found <a href=""http://people.cs.umass.edu/~mahadeva/papers/hrl.pdf"">here</a>. However, I was wondering if research has ever been done where an HRL system has been implemented on an individual robot? <a href=""https://mindmodeling.org/cogsci2014/papers/221/paper221.pdf"">This paper</a> seems to imply that it has been, by saying that the delivery task that it models ""is commonly used in HRL, both in computational and experimental settings"". However, my Google Scholar searches haven't turned up any fruit as to what this real-world experimental setting might be. Help would be appreciated for finding either model-based or model-free implementation of hierarchical reinforcement learning in a robot.</p>
","machine-learning reinforcement-learning reference-request"
"4953","Measuring the performance and response rate of ESCs","<p>How would i go about measuring and quantifying the performance of an ESC? I am looking to measure the response rate(in Hz) of an ESC, as well as it's performance(i.e how fast it starts and stops, as well as how much it increases/decreases the motor's speed per tick).</p>

<p>I'm assuming that i can manually program an ESC and it's response rate with a programming card/module that is compatible with said ESC, but i would still not be able to measure the exact performance of the ESC.</p>

<p>I would appreciate any and all inputs and suggestions.</p>

<p>P.S This question is linked/asked in conjunction with a previous question of mine here on the Robotics StackExchange here   <a href=""http://robotics.stackexchange.com/questions/4939/why-cant-i-use-different-escs-together-on-a-multirotor"">Why can&#39;t i use different ESCs together on a multirotor?</a></p>
","sensors quadcopter electronics esc multi-rotor"
"4954","Raspberry Pi vs BeagleBone Black Rev C on vending machine","<p>I wish to start my vending business, but none of the existing vending machines fit my needs.
So, I need to choose the ""brains"" to my vending machine under current development.</p>

<p>The user experience of my vending machine will be:</p>

<blockquote>
  <p>User change their products on touchscreen display (firegox open rails application running in the ""brains""), insert moneys, after that products will be returned to the user and notification (json query) will be send to it saas.</p>
</blockquote>

<p>There are requirements:</p>

<ol>
<li>Popular (I want to use a widely used computer for better support)</li>
<li>Debian-like or CentOs like system (easy to develop rails apps on them)</li>
<li>Big count of GPIOs</li>
<li>Working with touch-screen and large display (at least 15"")</li>
<li>Working with mdb protocol (for currency detector needs)</li>
</ol>

<p>So, I need your hints. It seems that BeagleBone is more powerful then Raspberry Pi, but there is one problem: It doesn't support many of the video outputs. Is there any solution to make good video output on BeagleBone? Do other such computers exist?</p>
","raspberry-pi beagle-bone"
"4955","Comparison of the efficiency of DC motor current limiting / control methods?","<p>I am using the wheels and motors of an RC toy car as a simple robotics platform. The car has 2 motors, one drives the back wheels, the other steers the front wheels. The steering motor is stalled by design when steering, it is blocked at a fixed angle by the plastic chassis. It draws 0.85 A when stalled (i.e. anytime when steering).  </p>

<p>Due to this marvel of toy engineering I have to use an oversized motor driver IC (L293B – 1A continuous) and this motor draws about 3W of power (0.85A x 3.6V). I’m using this IC to control the other (“normally rotating”) motor as well, which appears to be the same type: 0.85A stall current, around 100mA no-load, and 250-400mA at normal loads. </p>

<p>Testing with various series resistors I have found out that 0.3A are sufficient to turn the steering wheels and keep them in position. Using a resistor might allow me to use a driver IC with a lower Amp rating (L293D – 0.6 A), however the same energy is still wasted, only as heat. While this is not a serious issue with this toy setup, I am planning to build bigger robots with significantly more power, so energy conservation and current control will be important in the long run, and motors may also stall accidentally.</p>

<p>Looking into DC motor current limiting, I’ve found the following approaches:</p>

<ol>
<li><p>Series resistor – simple, cheap, bidirectional, wastes energy, dissipates heat</p></li>
<li><p>Current source with 2-3 transistors and sensing resistor – relatively simple, however I’ve only found unidirectional circuits, which would get shorted when switching motor direction. Is there a way to use this method bidirectionally? (and/or with a 2-channel H-bridge IC?  - I cannot place it before the ICs common supply, because the 2 motors draw different currents). </p></li>
<li><p>Chopper circuits/PWM – Will this reliably protect the IC from overload? Is it energy-efficient? </p></li>
<li><p>Are there other other methods I am unaware of? Something on the principles of switching supplies?</p></li>
<li><p>Would it be simpler in my application to use 2 separate drivers/h-bridges and place a voltage divider between them, so that a lower voltage is provided for the inefficient stalling motor and more to the one that moves the robot?</p></li>
</ol>

<p>So how do the above  methods compare in terms of efficiency and simplicity of design? What is the preferred method in robotics/other DC motor applications?  Also, is it standard practice to limit DC motor current, or a motor is most efficient if allowed to draw as much current as it needs? Is it acceptable to use a DC motor that is mechanically stalled by design, or is this only used in cheap crap toy cars?</p>
","motor power driver current h-bridge"
"4956","What kind of lidar is necessary for SLAM?","<p>I've read about various robots using a 2D lidar system for SLAM ( such as at IGCV, <a href=""http://www.igvc.org/"" rel=""nofollow"">http://www.igvc.org/</a> ) but I'm wondering how good exactly does the sensor have to be? Specifically: </p>

<p>What accuracy is necessary? </p>

<p>What field of view is necessary? Is it enough just to have lidar scanning forward in a 90 degree sweep?</p>

<p>What angular resolution is needed?</p>

<p>I realize that probably with super clever software you could probably do SLAM with a couple ultrasonic sensors, but using standard packages for software navigation what's a reasonable minimum value for these parameters? (and any other important ones I've forgotten) </p>
","slam navigation lidar"
"4958","How to program an Inovatic USB interface?","<p>In my possession I have an <a href=""http://www.inovatic-ict.com/didactic-equipment/emoro-kits-overview/emoro-usb-classic.html"" rel=""nofollow"">Inovatic USB Interface</a>. (In Detail: UI-8x8 v1.1 2009) I would like to program it to do some simple stuff and things. I am familiar with C# Programming but from what i have heard its not possible to program this interface with C#.</p>

<p>What it looks like:</p>

<p><a href=""http://i.stack.imgur.com/ZsJAf.jpg"" rel=""nofollow""><img src=""http://i.stack.imgur.com/ZsJAfs.jpg"" alt=""Interface from the top""></a>
<a href=""http://i.stack.imgur.com/X1OZD.jpg"" rel=""nofollow""><img src=""http://i.stack.imgur.com/X1OZDs.jpg"" alt=""The USB port on the Interface""></a>
<a href=""http://i.stack.imgur.com/zbern.jpg"" rel=""nofollow""><img src=""http://i.stack.imgur.com/zberns.jpg"" alt=""The USB Cable used to connect the Interface to the computer""></a></p>

<ul>
<li>Where can I find the Drivers for this Interface?  I have checked the Inovatic website but they only have the v1.0 version of the drivers and I'm pretty sure that I need v1.1 !</li>
<li>How can i program it? What language do i use?</li>
</ul>
","microcontroller"
"4959","Connecting multiple servos to a robotic arm","<p>I am computer programmer and it's really been long since I have done electronics. I need help on connecting my servos to an Arduino to power my robotic arm. <a href=""http://jjshortcut.wordpress.com/2010/04/19/my-mini-servo-grippers-and-completed-robotic-arm/"" rel=""nofollow"">This</a> is the robotic arm that I am trying to build.</p>

<p>I have come up with the connections as shown in the below diagram with my basic knowledge and browsing the internet. I have omitted the signal wires for clarity.</p>

<p><img src=""http://i.stack.imgur.com/CEFam.png"" alt=""Robot Arm Power Connection""></p>

<p>What I would like to know is </p>

<ul>
<li>Will this work?</li>
<li><p>Is this a good/decent design? I think it isn't as I have 4 battery packs.</p>

<p>I would like to have a single power source that would save me the trouble of maintaining so many batteries. To do this I have thought of using a voltage regulator but I am concerned about how this would perform if one servo starts drawing too much load. It might suck up all most the power leaving very little for the other servos.</p></li>
</ul>

<p>Any suggestions would be greatly helpful.</p>
","arduino robotic-arm servos"
"4960","Stereo vision on a moving vehicle","<p>When I put stereo camera on a moving platform and sync the cameras, what kind of delay between the cameras is tolerable? This would probably depend on the motion, but are there some guidelines?</p>

<p>Are there any practitioner guides on how to build a moving stereo system?</p>
","stereo-vision"
"4965","How to use gear motor 9 with Arduino","<p>I am new to robotics. I want to make a robot using Arduino Uno R3. I need to use Gear Motor 9 for that <a href=""https://solarbotics.com/product/gm9/"" rel=""nofollow"">here is the link</a>. </p>

<p>The problem is that motor needs 50mA current. But arduino only outputs only 40mA current. </p>

<p>I want to supply the motors with another power source and use a switch to connect both the circuits. Can you please tell me what type of switch I can use.</p>

<p>Thanks in Advance. 
P.S. Sorry if I used any wrong technical terms</p>
","arduino motor"
"4966","Keeping two wheeled wall following robot straight","<p>I have a two-wheeled (two DC motors) robot that needs to follow the wall beside the robot.</p>

<p>The issue is that the DC motors spin at different rates (because they are not identical, of course), so the robot does not go straight when the same voltage is provided to each motor.</p>

<p>How can I use IR distance sensors (and op-amps) to keep the distance from the wall constant?</p>

<p>(the robot must travel parallel to the wall)</p>
","two-wheeled"
"4967","Keeping two wheeled wall following robot straight","<p>I have a two-wheeled (two DC motors) robot that needs to follow the wall beside the robot.</p>

<p>The issue is that the DC motors spin at different rates (because they are not identical, of course), so the robot does not go straight when the same voltage is provided to each motor.</p>

<p>How can I use IR distance sensors (and op-amps) to keep the distance from the wall constant?</p>

<p>(the robot must travel parallel to the wall)</p>
","motor"
"4972","Can triangulation by measuring angles to 3 beacons to find location work over a large outdoor area","<p>To determine an outdoor location I think that I need to measure the angles between at least 3 beacons and take into account the order I sweep the beacons.  Is this a workable solution to get positional accuracy of about 30cm in a house block sized area?</p>

<p>Rewrite of question, note no distance measurement is suggested only angle measurements.
I am proposing that it might be possible to have a minimum of 3 local rf beacons and a robot device that sweeps an antenna through a circle identifying the angles between these beacons and to use this information and the order that the beacons are seen to find an absolute location.  I tried to prove this geometrically and it seems that with the 3 beacons there is 2 unique solutions without knowing the order and 1 solution if the order is known.  There would (I believe) be no need to try to find the distance to the beacons.  My question is, could this be implemented for a reasonable cost with some nRF24L01 based transcievers and some sort of rotating antenna?</p>
","localization"
"4980","Could a robot be programmed to be human?","<p>This is all hypothetical. If it was possible, it would have been done by now.</p>

<p>I realise that this area has been touched upon in many sci-fi movies but I wondered that if it was even feasible, how could it be achieved?</p>

<p>I know that it would raise a lot of ethical questions, I don't doubt that but I'm interested in the science.</p>

<p>What would a robot's brain have to be like to function like a human brain? For example, for it to have emotion (e.g. love, empathy), learn new things and remember them, make all those connections that a human brain does?</p>

<p>Thanks to all who reply!</p>
","mobile-robot humanoid"
"4983","DualCopter Degree Of Freedom","<p>I am a newbie in this drone field. I am curious to know what type of rotation and translation a dualcopter can achieve ? By rotation and translation i mean can it be able to roll, pitch and yaw like quadcopters?
If not, in any copter what makes them to roll pitch and yaw? Furthermore are there any dualcopter design that have movable wings that will rotate the rotors itself or do up and down motion while flying?</p>
","quadcopter multi-rotor"
"4986","Backstepping Integrator: changing the virtual control","<p>given the following differential equation 2°ODE in the following form:</p>

<p>$\ddot{z}=-g + ( cos(\phi) cos(\theta))U_{1}/m $</p>

<p>found in many papers (<a href=""http://e-collection.library.ethz.ch/eserv/eth:7848/eth-7848-01.pdf"" rel=""nofollow"">example</a>) and describing the dynamic model of a quadrotor (in this case I'm interested as an example only for the vertical axis $Z$) , I get the movement about $Z$ after integrating the variable $\ddot{z}$ two times. As control input I can control $U_{1}$, which represents the sum of all forces of the rotors.</p>

<p>A Backstepping Integrator (as in many of papers already implemented) defines a tracking error for the <em>height</em> $e_{z} = z_{desired} - z_{real}$ and for the <em>velocity</em> $\dot{e}_{z} = \dot{z}_{desired} - \dot{z}_{real}$ to build virtual controls.</p>

<p>Through the virtual controls one can <em>find</em> the needed valueof $U_{1}$ to drive the quadrotor to the desired height (see the solution later on)</p>

<p>But wait...as said above I need to track both: <em>position error</em> and <em>velocity error</em>.</p>

<p>Now I asked myself, how can I transform such equation and the corresponding virtual controls to track only the velocity??</p>

<p>In my code I need to develop an interface to another package which accepts only velocity inputs and not position information.
I should be able to drive my quadrotor to the desired position using only velocity informations, tracking the error for the z displacement it not allowed.</p>

<p>The solution for the more general case looks like:</p>

<p>$U_{1}=(m/(cos(\phi)cos(\theta))*(e_{z} + \ddot{z}_{desired} + \alpha_{1}^{2}\dot{e}_{z} - \alpha_{1}^{2}e_{z} + g + \alpha_{2}\dot{e}_{z})$</p>

<p>for  $\alpha_{1}, \alpha_{2} &gt; 0$</p>

<p>I could simply put brutal the $\alpha_{1} = 0$ for not tracking the position on Z but I think that is not the correct way.</p>

<p>Maybe could you please point me in the right direction?</p>

<p>Regards</p>
","control quadcopter"
"4987","How do I determine the heading of a six wheeled robot?","<p>I have a robot simulation that simulates Mars Exploration Rover with six steerable wheels.</p>

<p>In case of the following steering configuration</p>

<pre><code>// --- //  front wheels 
//     //  middle wheels 
// --- //  rear wheels
</code></pre>

<p>I'd say the heading of the rover with respect of the rover's body is about 45 to the right.
<strong>My question is what is the right approach of calculating heading with respect of the rover body?</strong> </p>

<p>Do I simply sum the steering angles of steering actuators and divide it by the total number of the steering actuators? </p>

<p><strong>Additional Note:</strong>
Assuming no slippage on a perfectly flat plane.</p>
","wheeled-robot"
"4990","Open source implementations of EKF for 6D pose esimation","<p>I am looking for open source implementations of an EKF for 6D pose estimation (Inertial Navigation System) using at minimum an IMU (accelerometer, gyroscope) + absolute position (or pose) sensor.</p>

<p>This seems to be such a recurring and important problem in robotics that I am surprised I cannot find a few reference implementations. Does everyone just quickly hack together his own EKF and move on to more interesting things? Is that not rather error-prone?</p>

<p>I would ideally like a well-tested implementation that can serve as a reference for fair evaluation of possible improvements. </p>
","kalman-filter ekf pose"
"4991","Seamless motor movement","<p>With the lego NXT Mindstorm kit I would like to have a rotating carousel that has ""perfect"" movement. This carousel has baskets and therefore it has quite a bit of inertia. I would like to find a method to calculate the perfect time to slow it down--taking into account motor friction, and momentum etc.</p>

<p>Here is some data I've collected:</p>

<p><img src=""http://i.stack.imgur.com/3TvpX.jpg"" alt=""enter image description here""></p>

<p>The motor power is the power to the motor. The break time was the time it took to stop from the time that the motor power was set to 0. The over-turn dist was amount of rotation in degrees that the motor continued to rotate after the power was set to 0.</p>

<p>Is there a specific method or approach to optimize the motors movement so movement can be precisely rotated to <strong>X</strong> degrees?</p>
","motor motion motion-planning"
"4994","Best UGV platform?","<p>My lab is interested in a good all-terrain UGV that can also be used indoors. We are particularly interested in the Clearpath Husky, Clearpath Jackal, and the Robotnik Summit XL (or XL HL), though we would welcome any other suggestions. Does anyone happen to have experience with more than one of these, and can speak to their pros and cons?</p>
","ugv platform"
"4996","Nano Quadcopters Microcontroller and battery","<p>I am looking into building a nano quadcopter, But as i watch more resources and videos i get more confused, regarding some of the things that i hope would be answered here. I am in very basic level of expertise here, i haven't built any robots or quadcopters to be exact.</p>

<p>What i want to know is, when i program a quadcopter say using intel edison chip, how do i power the quadcpter? i could not find that small size battery to move the propellers and start the chip.</p>

<p>Further more what is the procedure i should follow while developing a nano or small quadcopter, i saw a link on instructable that uses python on raspberry pi and then that raspberry pi control the arduino to control the robot. Can it be done only by using raspberry pi itself? </p>

<p>I am getting confused and i would like to know if i have to make small or nano quadcopter what should i be doing to get started?</p>

<p>Most of the latest chip support linux and high level programming language like python, so i hope i can go about programming the entire quadcopter using python or similar high level language and i don't suppose i have to stick with c langauge now. If i am wrong please help me understand the matter, there is high chance that i could be wrong.</p>
","arduino quadcopter raspberry-pi python"
"4998","Good 3D simulator for outdoor autonomous navigation","<p>What's an appropriate tool for simulating a car driving in a simple closed-loop racetrack? I'm trying to implement the control logic for an autonomous vehicle, and I'd like to be able to first simulate the behaviour before testing on a physical platform. The target environment is mostly 2D, but there are some 3D obstacle like small ramps and arches, so I can't use a strictly 2D simulator.</p>

<p>I've looked at some robotics simulators, as listed <a href=""http://stackoverflow.com/questions/2533321/robotics-simulator"">here</a>, but they seem like overkill and none of them seemed designed to model outdoor environments. I've done a little work with Gazebo, and I can't find any guide of texturing the ground/sky/background.</p>

<p>All I really need is some way apply a texture map to the ground and sky, create a handful of obstacles, and then to calculate a camera feed as a simple two-wheeled chassis moves along a mostly 2D course. However, I need the video input to be as realistic as possible because I don't have access to the real world racetrack. I need to be able to test and train the control logic in the simulator, and then load that logic onto the real mobile platform and have it navigate the course.</p>
","navigation simulator cameras"
"5001","How does power get to the flywheel in a motorized gyroscope?","<p>When I look at my toy gyroscope (I have never seen the inside of a motorized gyroscope), the central flywheel is suspended within the various gymbals and needs a lot of freedom of movement.  It's hard to see how an electric motor in the flywheel hub could be supplied with power.</p>

<p>How do ""real"" gyroscopes maintain angular velocity in their flywheel?</p>
","gyroscope"
"5006","Turning position level FK to motion level FK","<p>I have a position level forward and inverse kinematics blocks that I built on simulink by using s-function. I need to obtain the motion level FK and IK as well. </p>

<p>FK input is two motor angles and output is planar x,y coordinates and IK is the other way around.</p>

<p>Now I wonder if I simply put a derivative block at the output of each block, would it work ? I tried and this and cascaded the blocks to see if the input overlaps with output but it didn't, so apparently my idea is wrong ?</p>

<p>Can someone explain why it is ?</p>
","inverse-kinematics forward-kinematics"
"5011","Linear State space model for mobile robot","<p>How can I write a linear state space model for a 4 wheel mobile robot with Ackerman steering in terms of error. I want the robot to follow a line. The robot is rear wheel drive </p>
","mobile-robot line-following"
"5014","control circuit of a humanoid robot (something like iCub or Asimo)","<p>My friend and I are building the upper body of a humanoid robot for our M.Sc thesis project. There are 24 DC motors in this robot. The thing I want to know is <strong>what is the best way to command these motors simultaneously?</strong></p>

<p>The design I had in mind is for each motor to have its own micro for position and velocity control and then one master micro to command and control the slave micros. <strong>If this is the best way to go how does the master micro command slave ones simultaneously?</strong></p>

<p>Another question I have is what is the best micro for the robot to go with between ARM and PIC? I want the master micro to receive its command from a PC. Any help would be appreciated.</p>
","control microcontroller communication"
"5016","remote control laser meter","<p>I am looking to buy a laser distance meter and to connect it to a motor and a 3g cellular to control both the motor and to mesure the distance. I will appriciate your advice on how to do so. thanks</p>
","laser"
"5020","I am new in robotics....I want to know about SLAM algorithm....How should I proceed?","<p>Please give me guidance how should I proceed to know about SLAM algorithm? I am following some youtube videos but those are not so much helpful for me..... </p>
","computer-vision"
"5023","Is the accuracy of estimated position in localization better than estimated position in SLAM?","<p>We estimate position of robot in localization and SLAM. My intuition says we get better position estimation in localisation than in SLAM because we have better sensor model likelihoods in localization because of given complete environment than in SLAM.</p>

<p>I would like to know the difference in accuracy of estimated position in localization and SLAM.</p>
","localization slam particle-filter"
"5028","Trying to design a mechanical system with vertical and horizontal movement","<p>I'm trying to devise a system to lift a 10kg weight a distance of 1.4m vertically, and allow it to move in/out a distance of 30cm. I'd like the motions to be able to occur simultaneously if possible. </p>

<p>I'm thinking for the vertical motion I can use a suspended climber system. However I am unsure as to how I devise a system for the horizontal motion (in the horizontal plane I need nothing to protrude - only when the device is told to extend, so a horizontal suspended climber system isn't a possible solution.</p>

<p>I'm thinking I will need to use 2 electric motors.</p>

<p>Also - I'd like to mount it to the side of a car - so lightweight and low power draw is a must.</p>

<p>Does anyone know if there is anything available that will do this? Or suggest how I could combine a couple of systems to make this work?</p>

<p>Any information is appreciated.</p>
","mechanism"
"5031","Connect to video stream with Java app instead of console and mplayer","<p>I'm building a quadcopter using Raspberry Pi. There is the Pi Camera connected to the Raspberry Pi which is streaming the captured video. I can connect to this stream via Wi-Fi on my notebook (Linux) by using the console command ""nc"" and then show it by ""mplayer"".</p>

<p>What I want to do though is avoid the console commands and connect to this stream directly through my Java application. The reason is I want to do some image processing operations with this video so I need to have it in my application.</p>

<p>Is there anyone able to help me?</p>
","quadcopter raspberry-pi cameras linux"
"5034","Control circuit of humanoid robot: is it worth to learn and use ROS?","<p>I am building the upper body of a humanoid robot for my M.Sc thesis project with 24 DC motors and multiple sensors (something like i-cub or Nao). I have basic knowledge of communication protocols and I have worked with micros before but I have no knowledge and experience on working with ROS. The question I have is whether or not it is worthy and practical for me to learn ROS and use this for my robot or should I stick with what I already know.</p>
","microcontroller ros humanoid"
"5036","Calculating the force of this system","<p>My native language is not English, so I don't know all the specific terms you may expect me to use. I apologize for that.</p>

<p><img src=""http://i.stack.imgur.com/XEKxY.jpg"" alt=""enter image description here""></p>

<p>Anyway, I have a motor and three connecting rods (in French, <em>bielles</em>). So point C will have a circular trajectory and A, thanks to the sliding pivot (<em>pivot glissant</em>, I really hope I am using the right translations), should have a perfectly vertical trajectory.</p>

<p>My question is, how could I calculate the force F? I need this to emboss a piece of paper.</p>

<p>Thanks a lot for your attention!</p>
","force"
"5037","Kalman filter model values or state space original value? Which values to use?","<p>I am using <a href=""http://www.pololu.com/file/0J563/L3GD20.pdf"" rel=""nofollow"">L3GD20</a> and I am trying to implement a kalman filter for it on the stm32f3 discovery board. I have though a few questions about that:</p>

<ol>
<li><p>After the filter gave me the values, do I have to make an average between them and those of the original model or should I use them as they are?</p></li>
<li><p>According to <a href=""http://academic.csuohio.edu/simond/courses/eec644/kalman.pdf"" rel=""nofollow"">this document</a>, we don't use the original state space vectors in the filter, so how could we have ""correct"" space state estimated values?</p></li>
</ol>
","kalman-filter gyroscope"
"5039","Best localization method?","<p>I am making a robot that is supposed to roam inside my house and pick up trash using openCV. I plan to send information from my arduino mega to my arduino nano connected to window pc using radio transceivers. I also plan to send video feed from raspberry pi camera over WiFi to the windows PC. The windows PC then uses openCV and processes other information from the sensors and sends command back to arduino mega.</p>

<p>I have right now:</p>

<ul>
<li>Arduino mega</li>
<li>raspberry pi + usb camera + wifi dongle</li>
<li>Xbox 360 kinect </li>
<li>wheel encoders</li>
<li>sonar distance sensor</li>
<li>arduino nano</li>
<li>windows PC</li>
</ul>

<p>I want to know how to keep track of the robot like the room it is. I think what I am trying to do is SLAM, but I just need to make the map once because the rooms don't change much. I am open to ideas.  Cost is a factor.</p>
","arduino mobile-robot localization"
"5041","Bluetooth integration with MSP430","<p>I am trying to integrate bluetooth in a project with MSP430 so to be able to communicate between it and my PC. Doing a search on eBay I found the following item:</p>

<p>HC-05 06 Transceiver Bluetooth Module Backboard Interface Base Board Serial</p>

<p><img src=""http://i.stack.imgur.com/vQfu3.png"" alt=""enter image description here""></p>

<p>There are also a lot of other bluetooth modules that appear to be a lot more expensive and their boards are populated with IC's that this one doesn't have. So I am wondering if this is what I need or it has another use.</p>
","arduino"
"5044","Wiring 5V sensors to Beaglebone Black","<p>Is there a ""cape"" to make wiring sensors into a Beaglebone Black easier?</p>

<p>Whenever I read some guide for wiring a sensor into the Beaglebone (like <a href=""http://beagleboard.org/support/BoneScript/Ultrasonic_Sensor/"" rel=""nofollow"">this one</a>) it always recommends attaching wires directly to GND, +V and signal pins, which is horribly messy and unmaintainable. Even for small projects, you end up having several wires connected to the same GND/5V+ pins, so if you need to replace or repair something, you end up disrupting the wiring for every other component in your project.</p>

<p>Most Arduino guides assume this bad practice too, but at least I've found various <a href=""http://sandboxelectronics.com/wp-content/uploads/2014/01/SLD-000010.main_-399x399.jpg"" rel=""nofollow"">""GVS"" shields</a> to help organize groups of GND/5V/Signal pins so I can attach individual sensor cables.</p>

<p>Is there anything similar for the Beaglebone? I can't find anything appropriate Googling ""breakout"" or ""IO"" cape. I could only find one <a href=""http://beagleboard.org/project/BBB-GVS/"" rel=""nofollow"">GVS cape</a>, but it's less than ideal, since it only exposes 5 5V GPIO pins, and everything else it exposes as 3.3V or 1.8V which are incompatible with most peripherals.</p>
","beagle-bone"
"5045","How to choose the state space model for 1 axis gyroscope to implemnt a good kalman filter","<p>I am using <a href=""http://www.pololu.com/file/0J563/L3GD20.pdf"" rel=""nofollow"">this gyroscope</a> in order to measure the rotation of my robot around the z axis.
I want to implement a kalman filter in order to improve the values.
What i came with since now is this space model:</p>

<p>$$
 θ(k+1)=θ(k)+dt*θ'(k)+w(k) 
$$
$$
 y(k)=θ(k)+z(k)
$$
where $θ$ is the angle, $θ'$ is the angular rate given by the gyro and $w$ is the noise. (I hold up my gyro and measured 50 values while it was steady and find out that the variance is equal to 0.0002).
What i want to ask:</p>

<ol>
<li>is what i did is correct?</li>
<li>How can i find out $z(k)$? .According to the data sheet noise density is equal to 0.03 dps/sqrt(hz),how can i use this information to find out $z(k)$ and correct $w(k)$ if it is wrong.</li>
</ol>
","kalman-filter gyroscope noise"
"5047","Find Centre Of Circle, when robot can ""see"" a partial arc","<p>I originally asked this <a href=""http://math.stackexchange.com/questions/891093/work-out-center-of-a-partial-circle"">here</a> but I am struggling to take the concepts, particularly ""Circumcenter"", discussed there and apply it in my context. So here it is in my context...</p>

<p>I have a small robot that can ""see"" (via a camera) a partial arc (from a birds-eye view)</p>

<p>Therefore I know the height and the width of the square and therefore the width and height of the arc, I am sure there must be a way of approximating the circles centre?</p>

<p>What would be the quickest (not necessarily the most accurate) way of helping my robot find the centre.</p>

<p>In my head these are the steps of the problem:</p>

<ol>
<li><strong>Break up the arc into evenly spaced vectors ?</strong></li>
<li>Work out the angle between the vectors</li>
<li><strong>Use this information to complete the circle ?</strong></li>
<li>Work out radius of circle </li>
<li><strong>Find the center of the circle</strong></li>
</ol>

<p>Basically I would love some input on this problem because I think i know</p>

<ol>
<li>?</li>
<li>Dot Product</li>
<li>?</li>
<li>Add up the lengths of all the vectors to get circumference and then divide by pi, then divide by 2 (or divide by Tau : ) ) </li>
<li>I <strong>think</strong> this is where circumcentre comes in</li>
</ol>

<p>Basically I feel I have some pieces of the puzzle but I am not sure how they fit together. </p>

<p>I am currently using python with OpenCV and you may have guessed, I am not great at understanding math unless its expressed in algebraic formula or code. </p>

<p>Here are some illustrive pictures to reference:
<img src=""http://i.stack.imgur.com/4Fmyj.png"" alt=""enter image description here""><img src=""http://i.stack.imgur.com/ts26N.png"" alt=""enter image description here""></p>
","computer-vision navigation"
"5053","Why does the Pixhawk have 2 IMUs","<p>I was looking at the Pixhawk specs and noticed that it has 2 different IMUs- Invensense and STM. Is it for redundancy or does it have any other higher purpose?</p>
","uav"
"5058","2D Robot Arm Inverse Kinematics with minimum joint loads","<p>Suppose I have a robot arm with $n$ linkages of fixed length and equal density whose motion constrained within a 2D plane.  I want the end effector to reach a particular pose $(x^*,y^*,\theta^*)$.  </p>

<p>I know that in general, there can be multiple solutions that can reach this pose.  For my particular application, I'm interested in a solution that minimizes the maximum torque exerted over any joint under the influence of the weights of all the linkages, combined.  </p>

<p>Is there a way I can reformulate the inverse kinematics problem as a minimization problem over the joint loads?  Can I formulate my objective function to be differentiable (i.e. so that I can use traditional optimization techniques)?  Would this yield an unique solution (in a least squares sense) for the 2D planar motion problem?</p>
","inverse-kinematics"
"5059","Implementing a torque-controlled method on a position-controlled robot","<p>I am working with a position-controlled manipulator. However, I want to implement a torque-controlled method on this robot. Is there any way to convert a torque command to a position command?</p>

<p>I try to find research papers on this but I have no idea where I should start or which keywords I should use in searching. Do you have any suggestion?</p>
","robotic-arm industrial-robot manipulator"
"5062","What motors should I use that do not require gearboxes? This is a car like robot","<p>I am new to robotics, and ime looking for a &lt;12v motor that can be used to power a car-like robot.I will have two of these, so I can turn on-spot. Furthermore I want them not to require a gearbox, so I can just attach them to the wheels. I don't really know where to start looking for one. </p>

<p>I have heard servos have built in gearboxes, but don't they only have 180 degree rotation?</p>

<p>So does any body know a motor like I described in paragraph 1, or at least point me in the right direction? </p>
","motor"
"5068","Simulator for an adaptive, under-actuated robotic gripper","<p>I am looking to build an adaptable robotic arm with under-actuated three (or four) fingered hands. Before I start shelling out money, I want to test my prototypes in a simulator which would ideally</p>

<ul>
<li>allow me to try out various actuators, and also possibly a tactile sensor (like a pressure sensitive resistor or a pressure sensitive conductive sheet). </li>
<li>simulate different environments and tasks like gripping various shapes, sizes, weights etc.</li>
<li>be able to talk to an external learning/inference programs for the adaptive part (which, I think, goes under the name 'Dexterous Manipulation Planning' tasks), with sensory feedback from tactile sensors within the simulator and also camera input from a separate module.</li>
</ul>

<blockquote>
  <p>What are my options for such a simulator, including those that only
  partially address my requirements above?</p>
</blockquote>

<p>A bit about my background: I dont have any recent experience in building electronics hardware projects, although I have experienced it as part of my labs during electronics engineering under-graduation, a field that I have left a long time back. I am just a wannabe hobbyist now.</p>
","robotic-arm motion-planning simulator planning"
"5071","Plastic that's transparent to IR range sensors","<p>Sharp IR range finders are pretty popular sensors, but I usually see them externally mounted and directly exposed to the environment which makes them prone to being damaged or getting crufty.</p>

<p>I have a few that I'd like to use on an outdoor rover, and I'd like to cover them with some sort of transparent case to protect them dirt and impacts in the environment. What type of plastics would be completely transparent to these sensors, and where would I buy simple sheets of it?</p>
","sensors rangefinder"
"5074","Ubuntu ARM lacking /sys/devices/cape-bone-iio","<p>I'm trying to pull analog input from a beaglebone black using <a href=""http://www.linux.com/learn/tutorials/787511-how-to-get-analog-input-on-the-beaglebone-black"" rel=""nofollow"">this tutorial</a>. However when I go to <code>/sys/devices</code> there is no <code>cape-bone-iio</code>. I have spoken with several other programmers and one of them suggested that the cape-bone does not work with the newer versions of Linux. However downgrading could have negative impact on the rest of the project. Is there any other solution?</p>
","beagle-bone linux"
"5079","Communicating with syringe pump using PySerial","<p>Let's first start of by explaining that I do not have a decent background in electronics and interfacing with them, so maybe this is a stupid question. I am currently trying to connect an old Harvard 33 syringe pump (<a href=""http://www.harvardapparatus.com/webapp/wcs/stores/servlet/product_11051_10001_44004_-1_HAI_ProductDetail___"" rel=""nofollow"">website</a>, <a href=""http://www.instechlabs.com/Support/manuals/HA33Manual.pdf"" rel=""nofollow"">PDF manual</a>) to a computer, with the goal of controlling things like pump rates and direction. For this purpose, I connected the instrument to my computer using a D-sub/USB conversion dongle. I then connected to the dongle with PySerial without issues. However, whenever I try to send commands or request the instrument's output, for example <code>write(""RUN\r"")</code>, the instrument does not do anything at all. Requesting data (<code>read(100)</code>) returns only a couple of <code>\x00</code>. I suspect I am communicating with the dongle itself rather than the pump. When the pump is turned off or unplugged, I get exactly the same results!</p>

<p>Could anyone explain to me why my method does not work?</p>

<p>My Python code for reference:</p>

<pre><code>import serial # PySerial module

# open the connection
ser = serial.Serial(""/dev/ttyUSB0"", baudrate=9600, bytesize=8, stopbits=2, timeout=1)
print ser # returns [Serial&lt;id=0x1cc7610, open=True&gt;(port='/dev/ttyUSB0', baudrate=9600, bytesize=8, parity='N', stopbits=2, timeout=1, xonxoff=False, rtscts=False, dsrdtr=False)]

# see if the connection is truly open
print ser.isOpen() # prints True

# run the pump motor
ser.write(""RUN\r"")
</code></pre>

<p><strong>Additional observations:</strong> when the instrument is plugged in but the above code is not running, the pump does all sorts of things at random (move one way, stop, move the other way, etc.). This behaviour is much less pronounce but still present when the code runs (and 'locks' the channel or something?). This seems to suggest that the reference voltages (logical high and low) are not properly set at 2-5V and 0-0.5V respectively</p>
","control usb rs232"
"5082","Unable to hover my quadcopter","<p>I'm currently flying a f450 Quadcopter using a APM 2.6 flight controller.While i am able to get the quad off the ground and relatively steady horizontally(via the use of trims).However, i am unable to get the quad to hover no matter what i do.I've tried using trims on throttle,but i am still unable to get it hover.On my transmitter (WFLY WFT06II), where the throttle has ""ticks"", i am currently stuck between too little lift and too much lift, where i push the throttle up by 1 ""tick"" and the quad goes from slowly decending to ascending, and vice versa.</p>

<p>Is there any way i can get my quad to hover ( with my hands off the throttle if possible), as currently, evern with me trying to fly it, i can never get it to hover vertically as it alternates between ascending and descending whenever i fiddle with the throttle.</p>
","quadcopter multi-rotor radio-control"
"5095","following a trajectory with LQR controller","<p>We want our wheeled robot to follow a (rather short) trajectory. We wrote an LQR controller, which works well in simulation. However, our robot offers two problems:</p>

<p>1.) The reported state information does not seem to be very accurate.</p>

<p>2.) Its motion seems to underly some random deviations. We did not succeed in establishing a good model to predict the robots motion with a given control input.</p>

<p>Is it possible to manage these problems with the LQR controller? If yes, how?</p>
","control wheeled-robot kinematics"
"5097","optimal number of robots for cooperative surveillance","<p>Suppose we need to detect the occurrence of a special event in a large area(e.g. a city). If we need that each point being visited every h hours, how could we find the optimal number of robots for cooperative surveillance? </p>

<p>Notice that there is no control center!</p>
","multi-agent swarm"
"5099","Robot safety standards for software","<p>I am looking for possible ISO standards for robot safety specifically for software.</p>

<p>I have come across <a href=""http://www.eu-robotics.net/cms/upload/euRobotics_Forum/ERF2014_presentations/day_2/Industrial_HRC_-_ERF2014.pdf"" rel=""nofollow"">this presentation</a> that mentions many ISO standards and it's not very clear which exactly applies to software. The most probable ones are:</p>

<ul>
<li>ISO 10218-1</li>
<li>ISO 13849-1 / IEC 62061</li>
<li>IEC 61508</li>
<li>ISO/TS 15066</li>
</ul>

<p>The safety related to software seems to be categorized as Level 4 and Level 6 in the presentation above.</p>

<p>I would appreciate if anyone with knowledge in this area could point me to the right standard. They are quite expensive so I could simply go through them all to see which one applies.</p>

<p>As a side note, some standards like C have their standard ""draft"" freely available. Could there be free copies of drafts for those standards too?</p>
","software"
"5103","arduino -lcd screen has weird noise with multiple pictures","<p>I'm connecting an arduino to a <a href=""http://www.dfrobot.com/wiki/index.php?title=SPI_LCD_Module_%28SKU:DFR0091%29"" rel=""nofollow"">lcd screen</a> using the following <a href=""http://www.dfrobot.com/image/data/DFR0091/LCD12864RSPI%20v1.0.zip"" rel=""nofollow"">library</a>. For the display code I have written a simple piece of code that should display 2 picture os eyes (1 angry one friendly) and switching at regular intervals, however the display keeps showing me weird pixels around the borders that shouldn't be there, by making it show the same eyes twice this can be fixed however as long as I have both eyes being used it runs into trouble. Here is my code: </p>

<pre><code>    void setup()
    {
      LCDA.initDriverPin(2,7,10); 
      LCDA.Initialise(); // INIT SCREEN  
      delay(100);
    }
    void loop(){
      LCDA.CLEAR(); //empty screen 
      delay(100);//wait for command to finish 

      LCDA.DrawFullScreen(eyes);
      delay(5000); //wait 5 seconds with eyes on the screen 

      LCDA.CLEAR();//clear screen
      delay(100);//wait for command to finish 
      LCDA.DrawFullScreen(angry); //show me the angry face 
      delay(5000);//wait 5 seconds with angry face on screen 
    }

/*------------------------------------------------------------------------------
; DFrobot bitmap - size 128x64 pixels, black/white image
------------------------------------------------------------------------------*/ 
unsigned char eyes[]={
0x00 ,0x00 ,0x00 ,0x00 ,0x00 ,0x00 ,0x00 ,0x00 ,0x00 ,0x00 ,0x00 ,0x00 ,0x00 ,0x00 ,0x00 ,0x00 
,0x00 ,0x00 ,0x00 ,0x00 ,0x00 ,0x00 ,0x00 ,0x00 ,0x00 ,0x00 ,0x00 ,0x00 ,0x00 ,0x00 ,0x00 ,0x00 
,0x00 ,0x00 ,0x00 ,0x00 ,0x00 ,0x00 ,0x00 ,0x00 ,0x00 ,0x00 ,0x00 ,0x00 ,0x00 ,0x00 ,0x00 ,0x00 
,0x00 ,0x00 ,0x00 ,0x00 ,0x00 ,0x00 ,0x00 ,0x00 ,0x00 ,0x00 ,0x00 ,0x00 ,0x00 ,0x00 ,0x00 ,0x00 
,0x00 ,0x00 ,0x00 ,0x00 ,0x00 ,0x02 ,0xE0 ,0x00 ,0x00 ,0x00 ,0xF8 ,0x00 ,0x00 ,0x00 ,0x00 ,0x00 
,0x00 ,0x00 ,0x00 ,0x00 ,0x00 ,0x1E ,0xF8 ,0x00 ,0x00 ,0x00 ,0xFE ,0x00 ,0x00 ,0x00 ,0x00 ,0x00 
,0x00 ,0x00 ,0x00 ,0x00 ,0x00 ,0x3E ,0xF8 ,0x00 ,0x00 ,0x02 ,0xFE ,0x80 ,0x00 ,0x00 ,0x00 ,0x00 
,0x00 ,0x00 ,0x00 ,0x00 ,0x00 ,0xFE ,0xFC ,0x00 ,0x00 ,0x06 ,0xFE ,0xC0 ,0x00 ,0x00 ,0x00 ,0x00 
,0x00 ,0x00 ,0x00 ,0x00 ,0x00 ,0xFE ,0xFE ,0x00 ,0x00 ,0x0E ,0xFE ,0xF0 ,0x00 ,0x00 ,0x00 ,0x00 
,0x00 ,0x00 ,0x00 ,0x00 ,0x02 ,0xFE ,0xFE ,0x00 ,0x00 ,0x0E ,0xFE ,0xF8 ,0x00 ,0x00 ,0x00 ,0x00 
,0x00 ,0x00 ,0x00 ,0x00 ,0x0E ,0xFE ,0x7E ,0x00 ,0x00 ,0x1E ,0xC6 ,0xFC ,0x00 ,0x00 ,0x00 ,0x00 
,0x00 ,0x00 ,0x00 ,0x00 ,0x1E ,0xF8 ,0x3E ,0x00 ,0x00 ,0x1E ,0x80 ,0xFE ,0x00 ,0x00 ,0x00 ,0x00 
,0x00 ,0x00 ,0x00 ,0x00 ,0x3E ,0xF0 ,0x1E ,0x80 ,0x00 ,0x3E ,0x80 ,0xFE ,0x00 ,0x00 ,0x00 ,0x00 
,0x00 ,0x00 ,0x00 ,0x00 ,0x3E ,0xC0 ,0x1E ,0x80 ,0x00 ,0x3E ,0x00 ,0x7E ,0xC0 ,0x00 ,0x00 ,0x00 
,0x00 ,0x00 ,0x00 ,0x00 ,0x7E ,0x80 ,0x1E ,0x80 ,0x00 ,0x3E ,0x00 ,0x3E ,0xC0 ,0x00 ,0x00 ,0x00 
,0x00 ,0x00 ,0x00 ,0x00 ,0xFE ,0x00 ,0x0E ,0xC0 ,0x00 ,0x3E ,0x00 ,0x0E ,0xE0 ,0x00 ,0x00 ,0x00 
,0x00 ,0x00 ,0x00 ,0x00 ,0xFE ,0x00 ,0x0E ,0xC0 ,0x00 ,0x7E ,0x00 ,0x0E ,0xE0 ,0x00 ,0x00 ,0x00 
,0x00 ,0x00 ,0x00 ,0x00 ,0xFC ,0x00 ,0x06 ,0xC0 ,0x00 ,0x7E ,0x00 ,0x06 ,0xF0 ,0x00 ,0x00 ,0x00 
,0x00 ,0x00 ,0x00 ,0x02 ,0xFC ,0x00 ,0x06 ,0xE0 ,0x00 ,0xFE ,0x00 ,0x02 ,0xF8 ,0x00 ,0x00 ,0x00 
,0x00 ,0x00 ,0x00 ,0x02 ,0xF8 ,0x00 ,0x06 ,0xE0 ,0x00 ,0xFE ,0x00 ,0x02 ,0xF8 ,0x00 ,0x00 ,0x00 
,0x00 ,0x00 ,0x00 ,0x06 ,0xF0 ,0x00 ,0x06 ,0xE0 ,0x00 ,0xFC ,0x00 ,0x00 ,0xFC ,0x00 ,0x00 ,0x00 
,0x00 ,0x00 ,0x00 ,0x0E ,0xE0 ,0x00 ,0x02 ,0xE0 ,0x00 ,0xFC ,0x00 ,0x00 ,0xFE ,0x00 ,0x00 ,0x00 
,0x00 ,0x00 ,0x00 ,0x0E ,0xE0 ,0x00 ,0x02 ,0xE0 ,0x00 ,0xFC ,0x00 ,0x00 ,0xFE ,0x00 ,0x00 ,0x00 
,0x00 ,0x00 ,0x00 ,0x1E ,0xC0 ,0x00 ,0x02 ,0xE0 ,0x00 ,0xF8 ,0x00 ,0x00 ,0x3E ,0x00 ,0x00 ,0x00 
,0x00 ,0x00 ,0x00 ,0x1E ,0xC0 ,0x00 ,0x02 ,0xE0 ,0x00 ,0xF8 ,0x00 ,0x00 ,0x3E ,0x80 ,0x00 ,0x00 
,0x00 ,0x00 ,0x00 ,0x1E ,0x80 ,0xFE ,0xC2 ,0xE0 ,0x00 ,0xF8 ,0x7E ,0xF0 ,0x1E ,0x80 ,0x00 ,0x00 
,0x00 ,0x00 ,0x00 ,0x3E ,0x00 ,0xFE ,0xF2 ,0xE0 ,0x00 ,0xF8 ,0xFE ,0xF8 ,0x1E ,0x80 ,0x00 ,0x00 
,0x00 ,0x00 ,0x00 ,0x3E ,0x02 ,0xFE ,0xFA ,0xE0 ,0x00 ,0xF8 ,0xFE ,0xF8 ,0x1E ,0xC0 ,0x00 ,0x00 
,0x00 ,0x00 ,0x00 ,0x3E ,0x06 ,0xF2 ,0xFE ,0xF0 ,0x00 ,0xFA ,0xFC ,0xFC ,0x1E ,0xC0 ,0x00 ,0x00 
,0x00 ,0x00 ,0x00 ,0x7E ,0x0E ,0xC0 ,0xFE ,0xF0 ,0x00 ,0xFE ,0xC0 ,0x7E ,0x0E ,0xC0 ,0x00 ,0x00 
,0x00 ,0x00 ,0x00 ,0x7E ,0x0E ,0x80 ,0x7E ,0xF0 ,0x00 ,0xFE ,0xC0 ,0x3E ,0x0E ,0xC0 ,0x00 ,0x00 
,0x00 ,0x00 ,0x00 ,0x7E ,0x1E ,0xFE ,0x3E ,0xF0 ,0x00 ,0xFE ,0x8E ,0xFE ,0x0E ,0xC0 ,0x00 ,0x00 
,0x00 ,0x00 ,0x00 ,0xFC ,0x1E ,0xEE ,0x9E ,0xF0 ,0x00 ,0xFE ,0xBE ,0xFE ,0x0E ,0xE0 ,0x00 ,0x00 
,0x00 ,0x00 ,0x00 ,0xFC ,0x1E ,0x82 ,0xDE ,0xF0 ,0x00 ,0xFE ,0xBC ,0x6E ,0x86 ,0xE0 ,0x00 ,0x00 
,0x00 ,0x00 ,0x00 ,0xFC ,0x3E ,0x82 ,0xDE ,0xF0 ,0x00 ,0xFE ,0xB8 ,0x2E ,0xC6 ,0xE0 ,0x00 ,0x00 
,0x00 ,0x00 ,0x00 ,0xFC ,0x3E ,0x80 ,0xDE ,0xF0 ,0x00 ,0xFE ,0xB0 ,0x2E ,0xC6 ,0xF0 ,0x00 ,0x00 
,0x00 ,0x00 ,0x00 ,0xF8 ,0x3C ,0x3E ,0x5E ,0xE0 ,0x00 ,0xFE ,0x9E ,0x8E ,0xC2 ,0xF0 ,0x00 ,0x00 
,0x00 ,0x00 ,0x00 ,0xF8 ,0x3C ,0x7E ,0x9E ,0xE0 ,0x00 ,0xFE ,0xBE ,0xD6 ,0xC2 ,0xF0 ,0x00 ,0x00 
,0x00 ,0x00 ,0x00 ,0xF0 ,0x7C ,0x7E ,0xDE ,0xE0 ,0x00 ,0xFE ,0xBE ,0xC6 ,0xE2 ,0xF8 ,0x00 ,0x00 
,0x00 ,0x00 ,0x00 ,0xF0 ,0x7C ,0xFE ,0xDE ,0xE0 ,0x00 ,0xFE ,0xFE ,0xF6 ,0xE2 ,0xFC ,0x00 ,0x00 
,0x00 ,0x00 ,0x00 ,0xF0 ,0x7A ,0xFE ,0xDE ,0xE0 ,0x00 ,0xFE ,0xFE ,0xF6 ,0xE2 ,0xFC ,0x00 ,0x00 
,0x00 ,0x00 ,0x00 ,0xF0 ,0x7A ,0xFE ,0xDE ,0xE0 ,0x00 ,0xFE ,0xFE ,0xF2 ,0xE0 ,0xFC ,0x00 ,0x00 
,0x00 ,0x00 ,0x00 ,0xF0 ,0x7A ,0xFE ,0xDE ,0xE0 ,0x00 ,0xFE ,0xFE ,0xF2 ,0xE0 ,0xFC ,0x00 ,0x00 
,0x00 ,0x00 ,0x00 ,0xF0 ,0x7A ,0xFE ,0xDE ,0xE0 ,0x00 ,0xFE ,0xFE ,0xF2 ,0xE0 ,0xFC ,0x00 ,0x00 
,0x00 ,0x00 ,0x00 ,0xF0 ,0x7A ,0xFE ,0xDE ,0xE0 ,0x00 ,0xFE ,0xFE ,0xF2 ,0xE0 ,0xFC ,0x00 ,0x00 
,0x00 ,0x00 ,0x00 ,0xF0 ,0x7A ,0xFE ,0xDE ,0xE0 ,0x00 ,0xFE ,0xFE ,0xF2 ,0xE0 ,0xFC ,0x00 ,0x00 
,0x00 ,0x00 ,0x02 ,0xF0 ,0x7A ,0xFE ,0xFE ,0xE0 ,0x00 ,0xFE ,0xFE ,0xFA ,0xE0 ,0xFC ,0x00 ,0x00 
,0x00 ,0x00 ,0x02 ,0xF0 ,0x7A ,0xFE ,0xFE ,0xE0 ,0x00 ,0xFE ,0xFE ,0xFA ,0xE0 ,0xFC ,0x00 ,0x00 
,0x00 ,0x00 ,0x02 ,0xF0 ,0x7A ,0xFE ,0xFE ,0xE0 ,0x00 ,0xFE ,0xFE ,0xFA ,0xE0 ,0xFC ,0x00 ,0x00 
,0x00 ,0x00 ,0x02 ,0xE0 ,0x7A ,0xFE ,0xFE ,0xE0 ,0x00 ,0xFE ,0xFE ,0xFA ,0xE0 ,0xFC ,0x00 ,0x00 
,0x00 ,0x00 ,0x02 ,0xE0 ,0x7E ,0xFE ,0xFE ,0xE0 ,0x00 ,0xFE ,0xFE ,0xFA ,0xE0 ,0xFC ,0x00 ,0x00 
,0x00 ,0x00 ,0x02 ,0xFC ,0x7E ,0xFE ,0xFE ,0xE0 ,0x00 ,0xFE ,0xFE ,0xFE ,0xC0 ,0xFC ,0x00 ,0x00 
,0x00 ,0x00 ,0x02 ,0xFE ,0x3E ,0xFE ,0xFE ,0xE0 ,0x00 ,0xFE ,0xFE ,0xFE ,0xCE ,0xFC ,0x00 ,0x00 
,0x00 ,0x00 ,0x02 ,0xFE ,0xFE ,0xFE ,0xFE ,0xC0 ,0x00 ,0x7E ,0xFE ,0xFE ,0xFE ,0xFC ,0x00 ,0x00 
,0x00 ,0x00 ,0x02 ,0xFE ,0xFE ,0xFE ,0xFE ,0xC0 ,0x00 ,0x7E ,0xFE ,0xFE ,0xFE ,0xFC ,0x00 ,0x00 
,0x00 ,0x00 ,0x00 ,0xFE ,0xFE ,0xFE ,0xFE ,0xC0 ,0x00 ,0x7E ,0xFE ,0xFE ,0xFE ,0xF8 ,0x00 ,0x00 
,0x00 ,0x00 ,0x00 ,0x3E ,0xFE ,0xFE ,0xFE ,0xC0 ,0x00 ,0x3E ,0xFE ,0xFE ,0xFE ,0x80 ,0x00 ,0x00 
,0x00 ,0x00 ,0x00 ,0x06 ,0xFE ,0xFE ,0xFE ,0x80 ,0x00 ,0x3E ,0xFE ,0xFE ,0xFC ,0x00 ,0x00 ,0x00 
,0x00 ,0x00 ,0x00 ,0x00 ,0x3E ,0xFE ,0xE0 ,0x00 ,0x00 ,0x00 ,0xFE ,0xFE ,0x80 ,0x00 ,0x00 ,0x00 
,0x00 ,0x00 ,0x00 ,0x00 ,0x00 ,0x00 ,0x00 ,0x00 ,0x00 ,0x00 ,0x00 ,0x00 ,0x00 ,0x00 ,0x00 ,0x00 
,0x00 ,0x00 ,0x00 ,0x00 ,0x00 ,0x00 ,0x00 ,0x00 ,0x00 ,0x00 ,0x00 ,0x00 ,0x00 ,0x00 ,0x00 ,0x00 
,0x00 ,0x00 ,0x00 ,0x00 ,0x00 ,0x00 ,0x00 ,0x00 ,0x00 ,0x00 ,0x00 ,0x00 ,0x00 ,0x00 ,0x00 ,0x00 
,0x00 ,0x00 ,0x00 ,0x00 ,0x00 ,0x00 ,0x00 ,0x00 ,0x00 ,0x00 ,0x00 ,0x00 ,0x00 ,0x00 ,0x00 ,0x00 
,0x00 ,0x00 ,0x00 ,0x00 ,0x00 ,0x00 ,0x00 ,0x00 ,0x00 ,0x00 ,0x00 ,0x00 ,0x00 ,0x00 ,0x00 ,0x00 
,0x00 ,0x00 ,0x00 ,0x00 ,0x00 ,0x00 ,0x00 ,0x00 ,0x00 ,0x00 ,0x00 ,0x00 ,0x00 ,0x00 ,0x00 ,0x00 
,0x00 ,0x00 ,0x00 ,0x00 ,0x00 ,0x00 ,0x00 ,0x00 ,0x00 ,0x00 ,0x00 ,0x00 ,0x00 ,0x00 ,0x00 ,0x00 
,0x00 ,0x00 ,0x00 ,0x00 ,0x00 ,0x00 ,0x00 ,0x00 ,0x00 ,0x00 ,0x00 ,0x00 ,0x00 ,0x00 ,0x00 ,0x00 
};
unsigned char angry[]={
0x00 ,0x00 ,0x00 ,0x00 ,0x00 ,0x00 ,0x00 ,0x00 ,0x00 ,0x00 ,0x00 ,0x00 ,0x00 ,0x00 ,0x00 ,0x00 
,0x00 ,0x00 ,0x00 ,0x00 ,0x00 ,0x00 ,0x00 ,0x00 ,0x00 ,0x00 ,0x00 ,0x00 ,0x00 ,0x00 ,0x00 ,0x00 
,0x00 ,0x00 ,0x00 ,0x00 ,0x00 ,0x00 ,0x00 ,0x00 ,0x00 ,0x00 ,0x00 ,0x00 ,0x00 ,0x00 ,0x00 ,0x00 
,0x00 ,0x00 ,0x00 ,0x00 ,0x00 ,0x00 ,0x00 ,0x00 ,0x00 ,0x00 ,0x00 ,0x00 ,0x00 ,0x00 ,0x00 ,0x00 
,0x00 ,0x00 ,0x00 ,0x00 ,0x00 ,0x00 ,0x00 ,0x00 ,0x00 ,0x00 ,0x00 ,0x00 ,0x00 ,0x00 ,0x00 ,0x00 
,0x00 ,0x30 ,0x00 ,0x00 ,0x00 ,0x00 ,0x00 ,0x00 ,0x00 ,0x00 ,0x00 ,0x00 ,0x00 ,0x00 ,0x00 ,0x00 
,0x00 ,0x7C ,0x00 ,0x00 ,0x00 ,0x00 ,0x00 ,0x00 ,0x00 ,0x00 ,0x00 ,0x00 ,0x00 ,0x02 ,0x00 ,0x00 
,0x00 ,0x7E ,0x00 ,0x00 ,0x00 ,0x00 ,0x00 ,0x00 ,0x00 ,0x00 ,0x00 ,0x00 ,0x00 ,0x0E ,0x80 ,0x00 
,0x00 ,0x7E ,0xC0 ,0x00 ,0x00 ,0x00 ,0x00 ,0x00 ,0x00 ,0x00 ,0x00 ,0x00 ,0x00 ,0x3E ,0x80 ,0x00 
,0x00 ,0x1E ,0xE0 ,0x00 ,0x00 ,0x00 ,0x00 ,0x00 ,0x00 ,0x00 ,0x00 ,0x00 ,0x00 ,0xFE ,0x80 ,0x00 
,0x00 ,0x06 ,0xF8 ,0x00 ,0x00 ,0x00 ,0x00 ,0x00 ,0x00 ,0x00 ,0x00 ,0x00 ,0x00 ,0xFE ,0x00 ,0x00 
,0x00 ,0x00 ,0xFE ,0x00 ,0x00 ,0x00 ,0x00 ,0x00 ,0x00 ,0x00 ,0x00 ,0x00 ,0x06 ,0xF8 ,0x00 ,0x00 
,0x00 ,0x00 ,0xFE ,0x80 ,0x00 ,0x00 ,0x00 ,0x00 ,0x00 ,0x00 ,0x00 ,0x00 ,0x1E ,0xE0 ,0x00 ,0x00 
,0x00 ,0x00 ,0x3E ,0xC0 ,0x00 ,0x00 ,0x00 ,0x00 ,0x00 ,0x00 ,0x00 ,0x00 ,0x7E ,0xE0 ,0x00 ,0x00 
,0x00 ,0x00 ,0xFE ,0xF0 ,0x00 ,0x00 ,0x00 ,0x00 ,0x00 ,0x00 ,0x00 ,0x00 ,0xFE ,0xF8 ,0x00 ,0x00 
,0x00 ,0x00 ,0xFE ,0xFC ,0x00 ,0x00 ,0x00 ,0x00 ,0x00 ,0x00 ,0x00 ,0x02 ,0xFE ,0xFC ,0x00 ,0x00 
,0x00 ,0x02 ,0xFC ,0xFE ,0x00 ,0x00 ,0x00 ,0x00 ,0x00 ,0x00 ,0x00 ,0x0E ,0xF6 ,0xFE ,0x00 ,0x00 
,0x00 ,0x06 ,0xF0 ,0x7E ,0x80 ,0x00 ,0x00 ,0x00 ,0x00 ,0x00 ,0x00 ,0x3E ,0xE0 ,0xFE ,0x00 ,0x00 
,0x00 ,0x0E ,0xE0 ,0x1E ,0xE0 ,0x00 ,0x00 ,0x00 ,0x00 ,0x00 ,0x00 ,0x7E ,0x80 ,0x7E ,0x80 ,0x00 
,0x00 ,0x1E ,0xC0 ,0x1E ,0xF8 ,0x00 ,0x00 ,0x00 ,0x00 ,0x00 ,0x00 ,0xFE ,0x00 ,0x3E ,0xC0 ,0x00 
,0x00 ,0x3E ,0xE0 ,0x1E ,0xFE ,0x00 ,0x00 ,0x00 ,0x00 ,0x00 ,0x06 ,0xFE ,0x00 ,0x7E ,0xE0 ,0x00 
,0x00 ,0x3E ,0xF0 ,0x0C ,0xFE ,0x00 ,0x00 ,0x00 ,0x00 ,0x00 ,0x1E ,0xF2 ,0x00 ,0xE6 ,0xE0 ,0x00 
,0x00 ,0x7E ,0x30 ,0x04 ,0x3E ,0xC0 ,0x00 ,0x00 ,0x00 ,0x00 ,0x3E ,0xC2 ,0x8E ,0xE2 ,0xF0 ,0x00 
,0x00 ,0x7C ,0x30 ,0x0C ,0x0E ,0xF0 ,0x00 ,0x00 ,0x00 ,0x00 ,0xFE ,0x00 ,0x9E ,0x82 ,0xF0 ,0x00 
,0x00 ,0xFC ,0x3E ,0x1C ,0x06 ,0xFC ,0x00 ,0x00 ,0x00 ,0x02 ,0xFE ,0x02 ,0x98 ,0x00 ,0xF8 ,0x00 
,0x00 ,0xF8 ,0x3E ,0x18 ,0x00 ,0xFE ,0x00 ,0x00 ,0x00 ,0x06 ,0xF8 ,0x02 ,0x18 ,0x00 ,0xF8 ,0x00 
,0x00 ,0xF8 ,0x06 ,0x18 ,0x02 ,0xFE ,0x80 ,0x00 ,0x00 ,0x1E ,0xE0 ,0x06 ,0x18 ,0x00 ,0xF8 ,0x00 
,0x00 ,0xF0 ,0x02 ,0x18 ,0xFE ,0x1E ,0xE0 ,0x00 ,0x00 ,0x7E ,0xC0 ,0x06 ,0x78 ,0x00 ,0xFC ,0x00 
,0x00 ,0xF0 ,0x02 ,0x18 ,0xFE ,0x0E ,0xF8 ,0x00 ,0x00 ,0xFE ,0x66 ,0x8C ,0xF0 ,0x00 ,0x7C ,0x00 
,0x00 ,0xF0 ,0x02 ,0xFE ,0x80 ,0x02 ,0xFC ,0x00 ,0x02 ,0xFC ,0x7E ,0xC6 ,0xC0 ,0x00 ,0x7C ,0x00 
,0x00 ,0xF0 ,0x00 ,0xFE ,0x00 ,0x00 ,0xFE ,0x00 ,0x0E ,0xFC ,0x38 ,0xE6 ,0xC0 ,0x00 ,0x7C ,0x00 
,0x00 ,0xF0 ,0xE0 ,0xFE ,0x80 ,0x00 ,0xFE ,0xC0 ,0x3E ,0xFC ,0x00 ,0x76 ,0xE0 ,0x00 ,0x7C ,0x00 
,0x00 ,0xF0 ,0xFA ,0xFE ,0x80 ,0x00 ,0xFE ,0xF0 ,0xFE ,0xFC ,0x00 ,0x3E ,0xF6 ,0x80 ,0x7C ,0x00 
,0x00 ,0xF0 ,0xBA ,0xFE ,0xF0 ,0x00 ,0xF6 ,0xF0 ,0xFE ,0x78 ,0x00 ,0x3E ,0xFE ,0xE0 ,0x3C ,0x00 
,0x00 ,0xF0 ,0x8E ,0xFE ,0xF8 ,0x00 ,0xF0 ,0xF0 ,0xF8 ,0x7C ,0x00 ,0x1E ,0xFC ,0xF8 ,0xFC ,0x00 
,0x00 ,0xFC ,0x86 ,0xFE ,0x98 ,0x00 ,0xF0 ,0x60 ,0x60 ,0x7C ,0x70 ,0x1E ,0xF8 ,0x3E ,0xFC ,0x00 
,0x00 ,0xFE ,0x82 ,0xFE ,0x9C ,0x00 ,0xF0 ,0x00 ,0x00 ,0x7C ,0x7C ,0xFE ,0xF8 ,0x06 ,0x7C ,0x00 
,0x00 ,0xFE ,0x80 ,0xFE ,0x8E ,0xE0 ,0xF0 ,0x00 ,0x00 ,0x7C ,0x6E ,0xFE ,0xF0 ,0x00 ,0x7C ,0x00 
,0x00 ,0xF0 ,0x00 ,0xFE ,0x06 ,0xF0 ,0xF0 ,0x00 ,0x00 ,0x7C ,0x76 ,0x9E ,0xF8 ,0x00 ,0x7C ,0x00 
,0x00 ,0xF8 ,0x00 ,0xFE ,0x80 ,0x3A ,0xF0 ,0x00 ,0x00 ,0x7E ,0x30 ,0x0E ,0xFC ,0x00 ,0xFC ,0x00 
,0x00 ,0xF8 ,0x02 ,0x82 ,0x00 ,0x1E ,0xE0 ,0x00 ,0x00 ,0x3E ,0xF0 ,0x06 ,0xEC ,0x00 ,0xF8 ,0x00 
,0x00 ,0xFC ,0x06 ,0x02 ,0x00 ,0x0E ,0xE0 ,0x00 ,0x00 ,0x3E ,0xE0 ,0x06 ,0x0C ,0x00 ,0xF8 ,0x00 
,0x00 ,0xFC ,0x06 ,0x02 ,0x80 ,0x06 ,0xE0 ,0x00 ,0x00 ,0x3E ,0xC0 ,0x02 ,0x0C ,0x00 ,0xF8 ,0x00 
,0x00 ,0x7E ,0x06 ,0x00 ,0x80 ,0x0E ,0xC0 ,0x00 ,0x00 ,0x1E ,0x80 ,0x06 ,0x0C ,0x02 ,0xF0 ,0x00 
,0x00 ,0x7E ,0x06 ,0x00 ,0x80 ,0x1E ,0xC0 ,0x00 ,0x00 ,0x1E ,0x80 ,0x0E ,0x0C ,0x02 ,0xF0 ,0x00 
,0x00 ,0x3E ,0x3E ,0x00 ,0xC0 ,0x1E ,0x80 ,0x00 ,0x00 ,0x0E ,0xC0 ,0x1C ,0x0C ,0x06 ,0xE0 ,0x00 
,0x00 ,0x3E ,0xBC ,0x00 ,0xC0 ,0x3E ,0x80 ,0x00 ,0x00 ,0x0E ,0xE0 ,0x38 ,0x0C ,0x0E ,0xE0 ,0x00 
,0x00 ,0x1E ,0xE0 ,0x00 ,0xC0 ,0xFE ,0x00 ,0x00 ,0x00 ,0x06 ,0xF0 ,0x70 ,0x0E ,0x1E ,0xC0 ,0x00 
,0x00 ,0x0E ,0xF0 ,0x00 ,0xC0 ,0xFE ,0x00 ,0x00 ,0x00 ,0x02 ,0xFC ,0x60 ,0x06 ,0xFE ,0x80 ,0x00 
,0x00 ,0x06 ,0xFC ,0x00 ,0xF6 ,0xFC ,0x00 ,0x00 ,0x00 ,0x00 ,0xFE ,0x60 ,0x00 ,0xFE ,0x00 ,0x00 
,0x00 ,0x02 ,0xFE ,0x00 ,0xFE ,0xF8 ,0x00 ,0x00 ,0x00 ,0x00 ,0xFE ,0xE0 ,0x06 ,0xFE ,0x00 ,0x00 
,0x00 ,0x00 ,0xFE ,0xFA ,0xFE ,0xF0 ,0x00 ,0x00 ,0x00 ,0x00 ,0x7E ,0xFE ,0xFE ,0xFC ,0x00 ,0x00 
,0x00 ,0x00 ,0xFE ,0xFE ,0xFE ,0xE0 ,0x00 ,0x00 ,0x00 ,0x00 ,0x3E ,0xFE ,0xFE ,0xF8 ,0x00 ,0x00 
,0x00 ,0x00 ,0x3E ,0xFE ,0xFE ,0x80 ,0x00 ,0x00 ,0x00 ,0x00 ,0x0E ,0xFE ,0xFE ,0xE0 ,0x00 ,0x00 
,0x00 ,0x00 ,0x0E ,0xFE ,0xFE ,0x00 ,0x00 ,0x00 ,0x00 ,0x00 ,0x02 ,0xFE ,0xFE ,0x80 ,0x00 ,0x00 
,0x00 ,0x00 ,0x00 ,0xFE ,0xF0 ,0x00 ,0x00 ,0x00 ,0x00 ,0x00 ,0x00 ,0x7E ,0xFC ,0x00 ,0x00 ,0x00 
,0x00 ,0x00 ,0x00 ,0x00 ,0x00 ,0x00 ,0x00 ,0x00 ,0x00 ,0x00 ,0x00 ,0x00 ,0x00 ,0x00 ,0x00 ,0x00 
,0x00 ,0x00 ,0x00 ,0x00 ,0x00 ,0x00 ,0x00 ,0x00 ,0x00 ,0x00 ,0x00 ,0x00 ,0x00 ,0x00 ,0x00 ,0x00 
,0x00 ,0x00 ,0x00 ,0x00 ,0x00 ,0x00 ,0x00 ,0x00 ,0x00 ,0x00 ,0x00 ,0x00 ,0x00 ,0x00 ,0x00 ,0x00 
,0x00 ,0x00 ,0x00 ,0x00 ,0x00 ,0x00 ,0x00 ,0x00 ,0x00 ,0x00 ,0x00 ,0x00 ,0x00 ,0x00 ,0x00 ,0x00 
,0x00 ,0x00 ,0x00 ,0x00 ,0x00 ,0x00 ,0x00 ,0x00 ,0x00 ,0x00 ,0x00 ,0x00 ,0x00 ,0x00 ,0x00 ,0x00 
,0x00 ,0x00 ,0x00 ,0x00 ,0x00 ,0x00 ,0x00 ,0x00 ,0x00 ,0x00 ,0x00 ,0x00 ,0x00 ,0x00 ,0x00 ,0x00 
,0x00 ,0x00 ,0x00 ,0x00 ,0x00 ,0x00 ,0x00 ,0x00 ,0x00 ,0x00 ,0x00 ,0x00 ,0x00 ,0x00 ,0x00 ,0x00 
,0x00 ,0x00 ,0x00 ,0x00 ,0x00 ,0x00 ,0x00 ,0x00 ,0x00 ,0x00 ,0x00 ,0x00 ,0x00 ,0x00 ,0x00 ,0x00 
};
</code></pre>

<p>each time I re upload the image it changes the way the noise patterns look suggesting this is some kind of overflow problem. However changing the last byte of the bitmaps creates lines at the bottom of the screen, right where the noise is on one of the images. Note that with different images this noise can very much ""cut"" into the images even creating not active pixels (0's) rather then just set ones. Suggesting that the images themselves to at least fit the display. </p>
","arduino"
"5104","robotic cell simulation software plc","<p>I need to simulate robotic cell where cartesian robot trims a PCB arriving on conveyor, picks it up with vacuum cup and and places in another device. After receiving signal from device the robot would pick it up and place on another belt. I want to make the cartesian robot myself using servomotors and control cell using a PLC. Would there be software that can simulate all this? I would also need to integrate sensors and possibly machine vision.</p>
","simulation"
"5106","How do I set up a rubber hand experiment with precise latency?","<p>The rubber hand illusion (<a href=""http://en.wikipedia.org/wiki/Multisensory_integration#Rubber_hand_illusion"">Wikipedia</a>) involves touching both a fake arm and a subject's real arm simultaneously. This causes the subject to feel that the fake arm belongs to him. Normally a human delivers both touches, so the timing is approximate. I want to vary the latency between the fake touch and real touch precisely (~5 ms at minimum) to probe how close they need to be to create the illusion. What can I use to touch a human and fake hand lightly at variable but precise times?</p>
","research"
"5108","Looking for a cheap(ish) micromouse that I can program with C/C++","<p>I'm looking to buy a micromouse (i.e. a small single-board unit with wheels and IR sensors that can move around freely). I've done a lot of searching but have only found resources relating to building one from components bought separately. However, I'm more of a programmer than an electrician so I fear I would struggle with this.</p>

<p>Anybody know where to buy one of these in the UK? (PICAXE does some suitable stuff but they're BASIC only unfortunately). My budget is about £60.</p>
","arduino mobile-robot wheeled-robot micromouse"
"5109","Regarding Long distance wireless communication","<p>I have a requirement to transmit some sensor data through wireless to a distance of 2 kilometers. I am a newbie to these technologies and concepts. Can anyone help me by providing some pointers to start with this. </p>
","wireless"
"5110","3D Magnetometer calibration algorithm","<p>I want to calibrate my compass, which is installed on a board which inherits a GPS module. Because the GPS antenna is up-side-down the compass is 180° inverted. The easiest way to correct the problem would be to invert the rotation matrix 180°. </p>

<p>However I got interested how a general approach to calibrate a compass would look like. I found some approaches like <a href=""http://www.varesano.net/blog/fabio/freeimu-magnetometer-and-accelerometer-calibration-gui-alpha-version-out"" rel=""nofollow"">this</a>. They seem to collect magnetometer readings an project them on a sphere. But what is actually the point in this? </p>

<p>Does someone know how a general calibration algorithm of a 3D magnetometer looks like?</p>
","calibration compass magnetometer"
"5111","redundant arm path planning and trajectory following","<p>I have a 7dof robotic arm and a set of end effector trajectories in cartesian space I need it to follow. </p>

<p>How do I deal with the redundancy in the arm when planning to follow these trajectories both with and without obstacle avoidance?</p>
","robotic-arm motion-planning c++ planning"
"5116","How to find a solution for quadcopter PID control","<p>I've built a quadcopter and a rig to safely test it on. I'm working on the PID for controlling the roll pitch and yaw. I understand how a PID works on a more simple plant like say a robot with wheels and I'm just really in the dark ( i believe ) with controlling and stabilizing a quad.</p>

<p>My question, how do I make these sensor readings effectively alter the motors' throttle? </p>

<p>Firstly, my approach is based on this model,  </p>

<pre><code>CW motors    A,C
CCW motors   D,B

           Front
           +1 pitch
          C   D
-1 roll    \-/     +1 roll   right
           /-\
          B   A
           -1 pitch
</code></pre>

<p>My IMU calculates the roll and pitch as a value between +-1.0 where being perfect balance will read as 0.0.  Now a degree of +-1.0 means approximately 90 degrees from the original axis.<br>
A normal input to the pitch to go forward would be something like 0.33, meaning tilt 30 degrees forward.</p>

<p>Now my motors take some value between 0 and 100. Originally I thought this would mean i would have to modify my motor values like so. </p>

<pre><code>c = throttle - roll + pitch + yaw
d = throttle + roll + pitch - yaw
b = throttle - roll - pitch - yaw
a = throttle + roll - pitch + yaw
</code></pre>

<p>Finally, I'm taking those floating point numbers, from the IMU and computing them like with this method, which appears to be the normal way as far as I've found.</p>

<pre><code>RollPId.Compute( steering.roll - gyro.roll );  
// pid_t is either #define pid_t float    or double, I know its a reserved type but, a pre-processor definition will change that before it would matter. 
pid_t Compute(pid_t input) {
        uint64_t now = millis();
        if( ( now - last_time ) &gt;= sample_time ) {
            pid_t error      = set_point - input;
            error_sum        += error;
            pid_t d_error    = error - error_last;

            *output = kp * error + ki * error_sum + kd * d_error;

            error_last = error;
            last_time  = now;
        }
    }
</code></pre>

<p>I don't know where to go from here? Also I have angular rate calculated from my IMU i just haven't encountered a solution that called for it. </p>

<p>EDIT. Below is a graph of roughly 300 readings (20ms apart) so roughly six seconds where i hold it in one hand and roll it roughly 45degrees right. with kp=1 ki=0 kd=0<br>
 <img src=""http://i.stack.imgur.com/8TKkr.png"" alt=""IMU and PID output""> </p>
","quadcopter pid design"
"5118","Finding the position of a servo","<p>I am building a collision avoidance system for my robot. As a part of this system I am using a pan and tilt kit </p>

<p>(<a href=""http://www.robotshop.com/en/lynxmotion-pan-and-tilt-kit-aluminium2.html"" rel=""nofollow"">http://www.robotshop.com/en/lynxmotion-pan-and-tilt-kit-aluminium2.html</a>)</p>

<p>My aim is to pan the sensor attached to this kit, and thus plan the route the robot should take.</p>

<p>In order to pan this sensor, I need to know the angle the kit has panned, and need to be able to call upon that angle at point in time. </p>

<p>Basically the sensor keeps panning, and at a point in time when certain conditions are met, it stops panning. When those conditions are met, I need to be able to extract the angle the sensor is panned at.</p>

<p>The servo being used on is: <a href=""http://www.robotshop.com/en/hitec-hs422-servo-motor.html"" rel=""nofollow"">http://www.robotshop.com/en/hitec-hs422-servo-motor.html</a></p>

<p>If someone could help me find a way to extract this information that would be helpful. I did read somewhere that the servo could be hacked and changed into a closed loop system where the effective angle would be shown, but that option is not viable.</p>

<p>Thanks</p>
","mobile-robot sensors servos"
"5126","APM Planner on Linux/Ubuntu ""open from file"" not working","<p>I can't open parameter list with APM Planner.</p>

<p>Moreover I can't find anyone with same problem.
I run it on Ubuntu/trusty 14.04</p>

<p>It does not see files with any extensions including <code>param</code> and <code>txt</code> downloaded from internet or created with my version of APM Planner.</p>

<p>Any ideas how can I fix it?</p>

<p><img src=""http://i.stack.imgur.com/nZXI0.png"" alt=""enter image description here""></p>

<p><strong>PS</strong> <code>ls -la</code> from terminal</p>

<pre><code>user@laptop:~/apmplanner2/parameters$ ls -la
total 16
drwxrwxr-x 2 user user 4096 Dec  9 12:57 .
drwxrwxr-x 7 user user 4096 Dec  9 21:05 ..
-rw-rw-r-- 1 user user 6881 Dec  9 12:57 paramter.param
</code></pre>

<p>I have <code>param</code>, <code>txt</code> files in Downloads folder also.</p>

<p><strong>PS</strong></p>

<p><img src=""http://i.stack.imgur.com/XRgAA.png"" alt=""enter image description here""></p>
","ardupilot"
"5130","PID for Quadcopters","<p>As already mentioned the PID output values that correspond to the error from the desirable error and current error has no units. Let's say we are using only the proportional part of the PID. Is it better to map the output of the PID values to the corresponding thrust values on each motor, or is it better to increase the Proportional coefficient <code>Kp</code> until the output values correspond to the proper thrust value to the motors? </p>

<p>For example if my desired angle is 0 and the angle that the sensor is reading is 40 degrees the difference is multiplied by <code>Kp</code> and the output is added or subtracted from the current thrust depending on the motor.</p>

<p>If I increase <code>Kp</code> too much, then the quadcopter is oscillating and not listening to the controller command that I am sending for the desired degrees from the joystick. If I map the values then it is listening to the joystick commands and not oscillating so much. Why is this happening? Isn't mapping the PID output values to bigger values the same as increasing <code>Kp</code>?</p>
","arduino pid"
"5132","What is the maximum payload weight for create 2/can I use old create accessories with the Create 2?","<p>I'm attempting to build a heavy platform on the Create 2 but am worried about weight on the platform.  What is the maximum weight for the platform and is there an optimum?</p>

<p>I have an old create and want to know if any of my existing cables and accessories can be used with the new Create 2? </p>
","mobile-robot irobot-create"
"5133","What is recommended prerequisite knowledge to get kid started with Create 2?","<p>What aged children is the Create 2 appropriate for?  What is prerequisite knowledge?  Is this an appropriate first robot kit for a child?</p>
","irobot-create"
"5138","Connecting an Arduino uno with a beaglebone black over USB","<p>I have an arduino uno and a BeagleBone black and would like them to talk with eachother. Both the beagleBone and the arduino have their own 5V power supplies so no need for power to be transferred over usb. The used communication line we would like to use is the direct serial line. Preferably in such a way that the arduino can just call Serial.read() in order to get the bytes it needs (there won't be many). How do I get this to work on the BeagleBoneBlack? I suspect we can call the serial.write somewhere inside the BBB(we mainly program in C++). But how do we achieve this? </p>
","arduino serial usb c++ beagle-bone"
"5140","Simple yet effective angular position sensor to be used in robotic hand","<p>What is the best yet simple to use angular position sensor? I am building a robotic hand and I want to implement this sensor at the joints of the fingers. I don't need a module, just an analogue output.
Thank you.</p>
","sensors hall-sensor"
"5148","Step Motor and Bevel gear calibration","<p>I am new to robotics. I want to understand how gears state is preserved as the gears turned to same positions repetitively. 
I have a bevel gear and a step motor connected to one gear. This gear will turn 45*n = degrees. That is to say there are 8 states gears will stay in. The problem here is there will be force on the gear, which is not connected to motor, in any direction. That force must not change worm position even in micrometers. I think there should be a locking mechanism. Is there any applications of that you can give example of?</p>
","stepper-motor stability"
"5149","Defining a trajectory for a quadrotor","<p>I' m looking for a trajectory generator (the algorithm doesn't matter, since I m going to write it using C++ ) that generates a trajectory (a parametric curve in space) defined point by point which is going lately to be feed into my quadrotor drivers.</p>

<p>I'm honest: I don't know where to start.</p>

<ol>
<li><strong>Reading</strong> the following interesting <a href=""http://robotics.stackexchange.com/a/2260/7097"">answer</a>. But the problem here is: the trajectory has a PD controller with it. My quadrotor should take just a parametric curve as input.</li>
<li><a href=""http://ompl.kavrakilab.org/OMPL_Primer.pdf"" rel=""nofollow""><strong>OMPL</strong></a>: this library seems very powerful and interesting. It let the user to define a planner which many different algorithms. The problem is that it is not well documented, good examples and explanations are missing and till now I could'nt find anything related to quadrotors, which does use that library. There is an example for a quadrotor, which doesn't find my expectations and I cannot figure out, how to implement it in my package. I don't want just copy and paste code that I don't understand.</li>
<li><strong>B-Spline and Bezier Curve</strong>...and the whole family of parametric curves. I found very interesting libraries in internet that implement those algorithm directly C++. The problem here is: I can define some points in space, generate a spline that contains them and interpolate points for the PID controller in the quadrotor. The basic idea is like a dog chasing a rabbit. A point is generated from the start point of the spline and regularly sent to the quadrotor. The latter flies behind the point, trying to reach it until a goal point has been reached. What is the problem here?!? In this case I can only generate a curve based on geometric properties and not considering the <strong>dynamic</strong> and the <strong>kinematic</strong> of the quadrotor (which I would consider for a future project). The rabbit runs and has a tighter curve radius than the dog, which could result in a strange behavior of the quadrotor.</li>
</ol>

<p>I'd like same good tips to point to the right direction.</p>

<p>Which kind of trajectory planner are usually developed fr such an application?</p>

<p>Thanks!
Regards</p>
","mobile-robot ros quadcopter movement"
"5150","what is the easiest method to plot a temperature in my pc?","<p>After a lot of learning, I'm launching a reballing business and I feel the need to have a realtime plot of the temperatures involved (ideally 3 or 4) and I have an arduino uno and a few K type thermocouples, I was researching the subject and saw a lot of different approachs, most of them use arduinos to send serial data to a pc port, then from there they process it with phyton, other guys matlab, some use ms excel plus a free add on in vb for apps. etcetera, and now after some reading I feel overwhelmed by all the different methods, so I wonder, perhaps I'm already losing perspective here? may be there is a simple method I can use and KISS way of get it done? thank you.</p>
","arduino sensors microcontroller electronics"
"5152","To control an omni wheel robot wirelessly using bluetooth and arduino","<p>I am trying to control an omni wheel robot which has 4 motors using 2 joysticks, plus there are some actuation switches which I want to control too. I am using arduino mega and a pair of bluetooth wireless module(HC-05).</p>

<p>This bluetooth modules works on serial communication. How should I program arduino to send both the analog values provided by the joystick and the input from the switch continuously?</p>
","arduino serial communication"
"5156","Sun tracking with +/- 0.1degree accuracy?","<p>For a school project I am looking to track the sun with +/- 0.1 degree accuracy for use with a parabolic dish reflector. Say we need a final output torque of about 20Nm, what kind of gearing/motors and feedback would you guys use to accomplish this? The sun position will be found with a solar positioning algorithm.</p>

<p>I am pretty new to this all but from my research stepper motors seem the easiest but brushless DC motors, from what I have read, can yield better results with smoother tracking. I am confused how you even use regular brushless dc motors to achieve precision positioning. I am aware of encoders but I don't really understand why the BLDC are preferred for this particular application, and how one would implement them.. Any ideas that can help kick start my researching?</p>
","motor brushless-motor stepper-motor servomotor"
"5164","How to send a new Mavlink message from Ardupilot?","<p>I'm trying to add a new message to the MAVLink interface. Following <a href=""http://dev.ardupilot.com/wiki/code-overview-adding-a-new-mavlink-message/"" rel=""nofollow"">this page</a>, there are the steps I took:</p>

<ol>
<li><p>Added the message to ardupilotmega.xml. Right at the end of the file:</p>

<pre><code>&lt;message name=""TESTING_TESTING_TESTING"" id=""182""&gt;
  &lt;description&gt;A testing message&lt;/description&gt;
  &lt;field type=""int16_t"" name=""placeholder""&gt;Does nothing. Simply a placeholder&lt;/field&gt;
&lt;/message&gt;
</code></pre></li>
<li><p>Regenerated the mavlink messages headers using <code>./libraries/GCS_MAVLink/generate.sh</code>. It worked okay and the new headers appeared.</p></li>
<li><p>Then I added a function to the GCS class to make sure I'm sending on the right channel:</p>

<pre><code>void GCS_MAVLINK::send_testing_testing_testing()
{
    mavlink_msg_testing_testing_testing_send(chan, 0);
}
</code></pre></li>
<li><p>Now it's time to send the message, I added my own function to the scheduler (on last priority). I made sure the function is called by sending text first and seeing it on the mission planner console. Here is the function I added:</p>

<pre><code>static void a_testing_loop(void)
{
    for (uint8_t i=0; i&lt;num_gcs; i++) 
    {
        if (gcs[i].initialised)
        {
            // gcs[i].send_text_P(SEVERITY_HIGH,PSTR(""Testing String""));
            gcs[i].send_testing_testing_testing();
        }
    }
}
</code></pre></li>
</ol>

<p>My message, however, isn't received on the mission planner end. It might have been received and ignored by the mission planner, but anyway it doesn't appear on the console window (even with 'Mavlink Message Debug' on)</p>

<p>Is there configuration to be made to the Mission Planner for it to receive new messages? Or am I sending the message wrong?</p>

<p>Also, is there a way to filter out messages from the console when using 'Mavlink debug mode'?</p>

<p>I'm using SITL for testing</p>

<p>(I don't have enough reputation - But this should be under the tag 'mavlink')</p>
","ardupilot mavlink"
"5169","Error as a State space","<p>I am reading the following research paper regarding Trajectory tracking of mobile robots.</p>

<p><a href=""http://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&amp;arnumber=5415188"" rel=""nofollow"">http://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&amp;arnumber=5415188</a></p>

<p>There are two things at the start of the paper that i do not understand.</p>

<p>1) The author derives equation(14) as the state space model of the system in which he considers the error as the state. Can anyone please elaborate on why he is using the error as the state space model of the system and not the Vx, Vy, and w(Omega, angular speed) of the robot.</p>

<p>2) Why does the author linearize the system around the reference trajectory?</p>
","mobile-robot wheeled-robot differential-drive"
"5171","How would i go about learning to code a flight controller?","<p>I'm interested in quadcopters/multi-rotors and want to eventually code my own flight controller ala an APM and/or Pixhawk. I've got a little experience in programming (i.e i know about if/else/else if conditionals), and have done a little programming with PHP, though it was procedural code.</p>

<p>I currently have a quadcopter that i built/assembled myself that is running on a f450 frame, using a APM 2.6 flight controller,so i have a reasonable grasp of how a quad works, and i would like to take it a step further and make my own adjustments to the code base, with the eventual aim of coding my own flight controller.</p>

<p>I've had a look at the code base, but am still unable to get a grasp of what the code is actually doing....yet. How would i go about learning how to code a flight controller?</p>

<p>I'm thinking that i would have to learn C++ &amp; OOP first, but how familiar/proficient would i have to be with C++ before i can reasonably attempt to edit the code base?Also, what else would i need to learn apart from C++ &amp; OOP?I am looking at setting a 6 month timeframe/deadline for me to do this, would it be possible?</p>
","arduino quadcopter microcontroller multi-rotor"
"5178","I want to control a sewing machine motor; need help with choices","<p>I've got an industrial sewing machine (think ""can sew with thread that looks like string, and has no trouble pounding through 20 layers of Sunbrella fabric""); it's got a 1 HP motor to power it. (I've got a smaller machine as well, w/ a 1/2 or 3/4 HP motor, which I might work on first.) The motor is a ""clutch motor"" which is always on, and a foot-pedal engages a clutch, so unless you ""slip the clutch"", you're basically always sewing fast or stopped. I'd like better control. In particular, I'd like to </p>

<ol>
<li>Be able to stop with the needle ""up""</li>
<li>Be able to stop with the needle ""buried"" (i.e., most of the way down)</li>
<li>Be able to press a button to move forward exactly one stitch</li>
<li>Be able to adjust -- probably with a knob -- the top speed of the motor</li>
<li>Have the motor off when I'm not actually sewing</li>
</ol>

<p>The 1 HP motor is probably overkill for what I'm doing. I don't suppose I've ever used more than about 1/4 HP even on the toughest jobs. </p>

<p>I'd appreciate any comments on my thinking so far: </p>

<p>From what I've read, it looks as if a DC motor is the way to go (max torque at zero speed, which is nice for that initial ""punch through the material"" thing, and the ability to ""brake"" by shorting the + and - terminals). Brushless would be nice...but expensive. And I have a nice DC treadmill motor, and if I drive it at about 12-24V, it'll give me more or less the right speed; adjusting pulleys will do the rest. Such DC motors are powered (in electric lawnmowers, for instance) by running AC through a diode bridge rectifier to produce pulsating DC, and I've got such a bridge rectifier handy. I also have an old autotransformer that I can use to get 24VAC pretty easily. Thus I can get 24V pulsating DC to drive the thing ... but that may or may not be a good idea. </p>

<p>I've also got an Arduino and the skills to program it, and several years of electronics tinkering, and some RC experience...but no experience handling larger DC motors like this one. I've been told the magic words ""H-bridge"", and found this <a href=""http://www.ebay.com/itm/SX8847-240W-H-bridge-Motor-Driver-Module-SPI-Smart-Car-4WD-Arduino-USA-/111337627980?pt=LH_DefaultDomain_0&amp;hash=item19ec3d614c"">motor driver</a> which certainly seems as if it'll allow me to turn on/off the motor, and regulate the voltage going to the motor. I don't know whether, when presented with pulsating DC at the input, it'll still work. Any thoughts on this? </p>

<p>I also can't tell -- there doesn't seem to be a handy datasheet or instruction page -- whether this thing can do braking. </p>

<p>For position sensing, there are lots of places I can get information -- either from the needle baror the handwheel of the sewing machine, so I'm not too concerned about that part. To give a sense of the problem, a typical stitching speed is something like 1000 stiches per minute, so if I'm just sensing whether the needle is in the top 1/4 of its travels or the bottom quarter, we're talking about something on the order of 10-50Hz, which doesn't <em>sound</em> like a challenging control scenario. </p>

<p>I guess my questions are these:</p>

<ol>
<li>Will pulsating DC work with a controller like the one I've cited? </li>
<li>Would I be better off with an RC motor-controller? I can't seem to find one designed for the 24V range that can handle 50 amps, unless it's for a brushless motor, which I don't have. And I <em>think</em> that I want one that has braking ability as well. And I worry that with an RC controller, the software in the controller may prevent me from making the motor stop at a particular position. </li>
</ol>

<p>Any comments/suggestions appreciated. (BTW, I'm happy with mathematics and with reading data sheets, and I've read (a few years ago) about half of ""The Art of Electronics"", just so you have an idea of the level of answer that's appropriate.) </p>

<p>To answer @jwpat's questions:</p>

<ol>
<li><p>I got my voltage value by saying that the motor is rated for (I think) 130V, and is 2.5HP (yikes), but turns at something like 6700 RPM. (<a href=""https://www.youtube.com/watch?v=olxeXGonOIk"">Here's</a> one that looks just like mine).  Dividing by 5 or 6, I got ""about 24 V"" to give me about 1400 RPM. (I'm at the office; the motor's at home, so I can't tell you the exact part number, alas.) I honestly don't think that the no-load vs load condition is a big deal, because I can wangle a factor of 2 with pulleys. </p></li>
<li><p>The sewing machine is a <a href=""https://www.youtube.com/watch?v=exn_9WF6iqo"">Juki 562</a> </p></li>
<li><p>Current motor/clutch are similar to <a href=""http://rads.stackoverflow.com/amzn/click/B002L6KA92"">this</a></p></li>
</ol>

<p>Sorry for the lack of detail here, </p>
","motor control power"
"5180","making a robot that knows your location","<p>I've just made a radio frequency remote control using PIC microcontroller and I want to do something useful with it. I am thinking of a robot that gets things for you while you are at bed but here comes the question: How am I going to have the PIC determine the location of the remote control calling for it? It can't really be done using a GPS module because it will all be in the house.</p>

<p>What options do I have?</p>
","microcontroller localization"
"5182","interfacing arduino uno with 9 dof razor imu","<p>I have followed the tutorial for razor IMU and it worked perfectly when the IMU is directly connected to the PC. Currently, i am trying to interface the 9 dof razor imu with the arduino uno by simply connect the rx to tx and tx to rx. Sadly, it doesn't work! So, i am just wondering, has anyone done this before? or can anybody give me some hints? Much appreciated! </p>
","arduino imu"
"5188","Compound vision system or Megapixel camera reduction","<p>Are any commercially available compound vision sensors available?
Not a simple 8 sensor system using photo-diodes but a genuine sensor that can provide a >32x32 compound matrix. Would some form of reduction in the granularity of a megapixel camera be a better option? The real purpose is to reduce processing time to a minimum, while extracting the maximum basic information. </p>
","computer-vision"
"5189","Arduino depth sensor","<p>I'm looking for an arduino compatible depth sensor NOT for water. What I need is a sensor very similar to an Xbox kinect (but much smaller) that will tell me what is in front of the sensor and also the shape of the object. For example, if I place a cylindrical water bottle in front of the sensor I would like to be able to figure out how far away the bottle is and also the shape of the object (in 2d, I don't need to know whether it is actually a cylinder only the general shape). The sensor only needs to be accurate at most 1 meter away. Does this exist and if so where can I purchase one. If it does not exist wholly what pieces do I need to buy to put it together? Thanks.</p>
","arduino sensors kinect"
"5192","Alternatives to cameras for a dirt sensor for an autonomous vacuum robot?","<p>When I use a standard manual vacuum, I often notice that I have to pass over a spot several times because a single pass does not necessarily catch all the dirt.  My eyes/brain can easily perceive this information visually, but I don't know how an autonomous robot vacuum can detect whether a pass over a patch of dirt was successful or not.  What kind of sensor/action can I use to determine if the robot vacuum successfully picked up the dirt from a particular patch?  </p>

<p>I would prefer to avoid a visual camera if at all possible because it would necessarily have to be mounted above the robot and thereby limit the range of reachable locations.  Is there some other low-cost sensor that can accomplish the same task that can be placed low to the ground?</p>
","sensors"
"5193","Can mapping be done in real life applications without also solving the localization problem at the same time (i.e. SLAM)?","<p>I know that Occupancy Grid Mapping requires the assumption that the robots' pose is always known when generating a map.  However, I also know that in reality, position and orientation usually treated as uncertain in practical applications.  Assuming that my target mapping environment is inside a home, is there a way I can overcome inherent robot pose uncertainty in a real world application of Occupancy Grid Mapping without resorting to implementing a SLAM?  That is, what is a low-cost way to increase the certainty about my pose?  </p>

<p>Or is Occupancy Grid Mapping only useful in theory and not in practice?  </p>

<p><strong><em>Update</em></strong>:</p>

<p>It is clear to me, from the responses given, that occupancy grid mapping is just one possible way to represent a map, not a method in and of itself.  The heart of what I really want to know is:  <strong><em>Can mapping be done without also solving the localization problem at the same time (i.e. SLAM) in real life applications?</em></strong>   </p>
","slam mapping occupancygrid"
"5196","Is there a brushless motor controller accepting over 500 updates/s?","<p>I want to use brushless for my line follower.<br>
The problem is most ESCs don't accept more than 400-500 updates/s due to the characteristic of steering signal.<br>
Is there a way to overcome this with a custom flash or am I out of luck?</p>
","brushless-motor esc line-following"
"5199","SLAM Goal Babbling","<p>I am struggling to find good links to the use of <a href=""http://matthias-rolf.blogspot.com/p/goal-babbling.html?m=1"" rel=""nofollow"">goal babbling</a> in SLAM applications. Has this technique been used as a method for optimizing movement in a SLAM environment?</p>
","localization slam motion-planning mapping"
"5201","How can I power a wheel but let it spin freely when not under power?","<p>How can I power a wheel but let it spin freely when not under power?</p>

<p>I saw the question <a href=""http://robotics.stackexchange.com/questions/2/how-can-i-modify-a-low-cost-hobby-servo-to-run-freely"">How can I modify a low cost hobby servo to run &#39;freely&#39;?</a> but I'm more interested in knowing if there is some sort of gearbox that disengages ( moves to 'neutral' ) when no torque is being applied to it.</p>

<p>Two ideas that come to mind are:</p>

<ol>
<li><p>A drive gear on a spring loaded arm with a nominal amount of resistance before engaging. Perhaps when under power it would first use power to move in one direction, then it would engage with another gear, but without power the spring would return it to a non-engaged position</p></li>
<li><p>A centrifugal clutch - although I'd like something that works at low RPMs as well</p></li>
</ol>

<p>The idea is to create a small bot that can move up and down a track, but if someone interacts with it when not under power it will just roll and won't damage the gearbox.</p>
","motor power rcservo radio-control"
"5203","Simple vector problem, Weight vector components & sine and cosine of rotation?","<p>I have the quadcopter in the photo below. It has rotate <code>theta</code> degrees about the <code>-y</code> axis. I want to get the <code>x</code> and <code>z</code> components in the local frame for the weight <code>W</code> which always points along the vertical downward.</p>

<p><img src=""http://i.stack.imgur.com/1TuVk.png"" alt=""enter image description here""></p>

<p>We simply have:</p>

<pre><code>Wx = W sin(theta); Wz = W cos(theta);
</code></pre>

<p>Suppose that <code>W = 4N</code> and <code>theta = 30 deg</code>, then:</p>

<pre><code>Wx = -4 * sin(-30) = 2N;   Wz = -4 * cos(-30) = -3.464N
</code></pre>

<p>The negative sign in the angle was put because the rotation is about the <code>-y</code> axis (counterclockwise).</p>

<p><code>Wz</code> seems correct as it is pointing towards the negative local <code>z</code> axis but <code>Wx</code> is 2 which seems wrong because according to the diagram it is supposed to be -2 indicating that it point towards the negative local <code>x</code> axis.</p>

<p>What's wrong with my simple calculation?</p>

<pre><code>EDIT:
</code></pre>

<p>Using rotation matrices, we have the following rotation matrix when pitching (rotating about <code>y axis</code>):</p>

<p><img src=""http://i.stack.imgur.com/ki1Ck.png"" alt=""Rotation Matrix""></p>

<p>This matrix is used to transform vectors from inertial frame <code>Xn,Yn,Zn</code> to local frame <code>Xb,Yb,Zb</code>. To find the components of the weight <code>W</code>, we can multiply this matrix by <code>W</code>. Doing so, we get the same result:</p>

<pre><code>Wx = W sin(theta); Wz = W cos(theta);
</code></pre>
","quadcopter frame"
"5208","The algorithm for the following analog controller => digital controller?","<p>I have found a continous control in the following form:</p>

<p>$$
u(s) = \left( K_{p} + \frac{K_{i}}{s} + K_{d} \frac{N}{1 + \frac{N}{s}} \right)e(s)
$$</p>

<p>but since I need it to ""convert"" in a digital control I need something like:</p>

<p>$$
y_{k} = y_{k-1} + q_{0}e_{k-1} + q_{2}e_{k-2}
$$</p>

<p>or everything that I can use in a digital way.
Is there any algorithm to achieve such transformation?
Actually the problem is the term $N$ in the equation. At first I thought that it was a simply PID controller but the N term is far from my understanding</p>

<p>Thank you very much and happy Christmas!!</p>
","control pid"
"5209","How high of a gear ratio can a motor have?","<p>I want to make a robot arm where a joint is powerful enough to lift 8 kg up at a distance of 1 meter. </p>

<p>This requires torque $tau = r*F = r*m*g$ = about 80 nm.</p>

<p>So now I am trying to find the requisite parts to put this together, i.e. motor + gears and then controller. </p>

<p>It seems that I will require a very high gear ratio to make this happen. For example this motor: <a href=""http://www.robotshop.com/ca/en/banebots-rs-550-motor-12v-19300rpm.html"" rel=""nofollow"">http://www.robotshop.com/ca/en/banebots-rs-550-motor-12v-19300rpm.html</a> has stats:</p>

<p>stall torque = 70.55 oz-in = 0.498 nm
no load roation speed = 19300 rpm</p>

<p>To get my required torque, I need a gear ratio of 80/0.498 = about 161:1 (and the max speed drops to about 120 rpm).</p>

<p>My questions:</p>

<p>1) Do my calculations so far seem correct? It seems a bit suspect to me that an $8 motor with some gears can cause a 17.5lbs dumbbell to rotate about a circle of radius 1m twice a second (I'm barely that strong). This type of torque would be awesome for my application, but perhaps I'm missing something in the calculations and being too optimistic (e.g. efficiency).</p>

<p>2) Is it safe to operate a motor at such a high gear ratio? Gears are small, and I'm worried they'll easily crack/break/wear down quickly over time. Does anyone know of example gears that I should consider for this?</p>

<p>Thank you very much for any help, cheers.</p>
","motor robotic-arm gearing"
"5214","How does Fliike Smiirl counter mechanism work","<p>First of all please see this video : <a href=""http://www.youtube.com/watch?v=n0dkn4ZIQVg"" rel=""nofollow"">http://www.youtube.com/watch?v=n0dkn4ZIQVg</a></p>

<p>I think there is ony one stepper motor -or servo- working in the mechanisim. But as you can see each flip counter works alone and separately.</p>

<p>It is not like classical counter mechanism like this : <a href=""http://www.youtube.com/watch?v=rjWfIiaOFR4"" rel=""nofollow"">http://www.youtube.com/watch?v=rjWfIiaOFR4</a></p>

<p>How does it works?</p>
","mechanism stepper-motor"
"5218","I want to be a guy of robotics","<p>I will be beginner need a help-I want to gain knowledge about robotics so it need a basic theoretical knowledge what is the best way to start?</p>
","beginner"
"5220","Clicking NXT Brick","<p>Help! I have recently installed leJOS NXJ on to my NXT brick, and soon after my batteries died. I inserted new ones, and now I cant start my brick up. When I press the startup button(orange) it makes a clicking sound and when I let go it stops. I have tried reflashing the brick with both leJOS NXJ and the NXT software and both programs say something along the lines of ""unable to locate brick."" Any suggetions?</p>
","mindstorms"
"5223","Non-markovian problems/approaches in robotics","<p>As far as i can tell, the markov assumption is quite ubiquitous in probabilistic methods for robotics and i can see why.  The notion that you can summarize all of your robot's previous poses with its current pose makes many methods computationally tractable.</p>

<p>I'm just wondering if there are any classic examples of problems in robotics where the markov assumption cannot be used at all.  Under what circumstances is the future state of the robot necessarily dependent on the current and at least some past states?   In such non-markovian cases, what can be done to alleviate the computational expense?  Is there a way to minimize the dependence on previous states to the previous $k$ states, where $k$ can be chosen as small as desired?</p>
","probability"
"5224","Nano sized electric motors","<p>What is a good website for buying 1.5V continuous motors? I'm looking to build a clockwork robot but I cannot find a motor small enough to fit inside my power box and I don't want to mount it outside of the box. I have a 1"" by .75"" space that the motor needs to fit into. I have found a few websites but they look sketchy and none of them have good reviews.</p>
","motor brushless-motor motion"
"5226","I need more robots","<p>I know that this isn't a programming question, but it is robotics so I thought you could all be flexible since it's my first question?</p>

<p>Anyway. I love making robots using robot kits that come with instructions. It's always fun to use afterwards because of the controllers I build with it.</p>

<p>The problem is that I can't find anymore robots. They are all either too expensive, not what I'm looking for, or both.</p>

<p>Can anybody give me links to some good robot kits?</p>

<p>My price limit is £30 - £40.</p>

<p>Here are the three robot kits I have built. I need kits that are like these:</p>

<p>Robot Arm: <a href=""http://www.amazon.co.uk/gp/aw/d/B002HXTONC/ref=mp_s_a_1_9?qid=1419721030&amp;sr=8-9&amp;pi=AC_SX110_SY165"" rel=""nofollow"">http://www.amazon.co.uk/gp/aw/d/B002HXTONC/ref=mp_s_a_1_9?qid=1419721030&amp;sr=8-9&amp;pi=AC_SX110_SY165</a></p>

<p>Remote Control Robot Beetle: I can't post more than two links. Go to maplins and type in the name of the robot and you'll find it. It's a rover version of the robot arm.</p>

<p>3-in-1 All Terrain Robot Kit: <a href=""http://www.maplin.co.uk/p/3-in-1-atr-all-terrain-robot-n12dp"" rel=""nofollow"">http://www.maplin.co.uk/p/3-in-1-atr-all-terrain-robot-n12dp</a></p>

<p>I don't want to program this robot. I want it to be like it is in the examples above. Buy the kit, read the instructions, then build it. </p>

<p>Thank you all in advance!</p>

<p>PS. Any further information will be given if asked for.</p>
","control"
"5227","theory on rigid body motion in robotics book","<p>I am reading some theories related to rigid body motion from the book ""A Mathematical Introduction to Robotic Manipulation"" by Prof. Richard Murray. </p>

<p>I am focusing on chapter 2, Sec 4 to derive some formulation. According to his introduction of chapter ""we present a modern approach treatment of the theory of screws based on linear algebra and matrix groups"". I myself feel rather understandable and comprehensive explanation from this approach. </p>

<p>However, his scope in this chapter is limited in inertia coordinate frame where he refers to as spatial frame and moving frame as body frame. Is there any other references that treat the topic in the reversed order? spatial as moving/non-inertia frame and the other one is inertia frame?</p>

<p>Thank you!</p>
","control"
"5229","How can I replace acceleration in the dynamic model of the robot?","<p>In the dynamic model of the robot, it is obvious that we found the torques as functions of the angular acceleration of the joint as well as the linear acceleration of the link center of mass along the three axies.</p>

<p>My question is regarding the values of these accelerations. In general, step motors specifications do not give the acceleration.</p>

<p>Thank you</p>
","torque"
"5238","joint compatibility branch and bound (JCBB) data association implementation","<p>I would like to implement the joint compatibility branch and bound technique <a href=""http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=976019&amp;tag=1"" rel=""nofollow"">in this link</a> as a method to carry out data association. I've read the paper but still confused about this function $f_{H_{i}} (x,y)$. I don't know exactly what they are trying to do. They compared their approach with The individual compatibility nearest neighbor (ICNN). In the aforementioned method we have this function $f_{ij_{i}} (x,y)$. This function simply the inverse measurement function or what they call it in their paper the implicit measurement function. In Laser sensor, given the observations in the polar coordinates, we seek via the inverse measurement function to acquire their Cartesian coordinates. In ICNN, every thing is clear because we have  this function $f_{ij_{i}} (x,y)$, so it is easily to acquire the Jacobian $H_{ij_{i}}$ which is </p>

<p>$$
H_{ij_{i}} = \frac{\partial f_{ij_{i}}}{\partial \textbf{x}}
$$ </p>

<p>For example in 2D case and 2D laser sensor,  $\textbf{x} = [x \ y \ \theta]$ and the inverse measurement function is 
$$
m_{x} = x + rcos(\phi + \theta) \\
m_{y} = y + rsin( \phi + \theta )
$$</p>

<p>where $m_{x}$ and $m_{y}$ are the location of a landmark and 
$$
r = \sqrt{ (m_{x}-x)^{2} + (m_{y}-y)^{2}} \\
\phi = atan2\left( \frac{(m_{y}-y)}{(m_{x}-x)} \right) - \theta
$$</p>

<p>Using <code>jacobian()</code> in Matlab, we can get $H_{ij_{i}}$. Any suggetions?</p>
","slam ekf mapping data-association"
"5240","iRobot Create 2 discrepancy betweenOpen Interface Specifications and Create 2 Serial to 3.3V Logic","<p>I am cautiously moving forward with my new iRobot Create 2, planning on using a Raspberry Pi with ROSberry installed to control the Create 2. Discovered a problem with the pin out specs between the iRobot Roomba Open Interface (OI) Specification and the Create 2 Serial to 3.3V Logic document. Here is the discrepancy (marked by DISCREPANCY):</p>

<p>PIN ((OI)) Serial to 3.3V</p>

<ol>
<li>Vpwr Roomba battery voltage</li>
<li>Vpwr Roomba battery voltage</li>
<li>RXD  Roomba TX DISCREPANCY</li>
<li>TXD  Roomba RX DISCREPANCY</li>
<li>BRC  Ground DISCREPANCY</li>
<li>GND  Ground</li>
<li>GND  Roomba BRC DISCREPANCY</li>
</ol>

<p>The discrepancy is with pins 3,4,5 &amp; 7.</p>

<p>Don't want to fry my Raspberry Pi, any clarification and/or help appreciated.</p>
","raspberry-pi irobot-create"
"5248","How to attach a motor to a blade?","<p>The hole is a slightly too big for the motor's shaft. I thought about hot gluing them together.</p>

<p><a href=""https://www.dropbox.com/s/d9hfdbmi342n12t/2014-12-30%2013.38.01.jpg?dl=0"" rel=""nofollow"">Link to picture here</a></p>
","motor quadcopter multi-rotor"
"5250","Need a Relay Alternative","<p>I have an application that needs a XBee and another module to be turned on and off digitally via a microcontroller.</p>

<p>The Setup is 2 XBee's and an application board is connected to the Microcontroller. On PowerON I need 1 Xbee and the Microcontroller to come on and do its routines. After the uC gets the signal from the Xbee (wirelessly from a baseStation) the board has to turn on the other XBee and application board. And when the operation is over, the XBee and Board is to be powered back down. I dont want to put them in sleep or low power state, just power both those devices off. </p>

<p>I was thinking of using a relay. But i cannot find a 3.3v 1A SMD Equivalent system. I am looking for a SMD type of footprint to go on a very compact board. </p>

<p>What options do i have?</p>

<p>The XBee needs around 1A Power and the Application board 500mA.</p>
","power"
"5251","Feedback controller: Is there any influence between outer and inner loop when running at different frequencies?","<p>I've develop a quadrotor (only simulation on my PC using ROS) and the feedback controller resumes more or less the following structure:</p>

<p><img src=""http://i.stack.imgur.com/9FyeR.png"" alt=""Picture 1""></p>

<p>where you can think that process is the dynamic movement of the quadrotor (motion equations), the inner loop is the <em>attitude controller</em> (it just sets the orientation about all 3 axis) and the outer loop is the <em>position controller</em> that takes care where the quadrotor actually is. Why are they separated? Because in many papers I found out that the attitude controller (pitch, roll, yaw) need to run at higher frequency then any other controller in the system. The <em>position controller</em> instead needs to run at lower frequency.</p>

<p>The following picture is a better explanation of my description. Don't be scared...it is more simpler than one could think:</p>

<p><img src=""http://i.stack.imgur.com/Qf7Pd.png"" alt=""Picture 2""></p>

<p>Now I did it as in the paper. <strong>BUT</strong> I discovered that my quadrotor was really unstable and I spent days and days trying to correct the gains of the controller without getting a stable system. My intuition said to me that maybe they are running at wrong frequency, so I put a different frequency values for the position controller being sure it is not a multiply of the main frequency (something like 1000Hz and 355 Hz for example.)</p>

<p>Lately I removed the timer in my program (C++) and let the <em>position controller</em> run at the same frequency as the <em>attitude controller</em> just because I run out of ideas and suddenly worked everything nice.</p>

<p>So here is my question. What should I consider when my system has an outer/inner controllers? How to be aware of that?</p>

<p>Regards and happy new year!!!</p>
","control quadcopter"
"5255","Where is the function _MAV_RETURN_ defined?","<p>I'm trying to understand the source code of ArduPlane. The MAVLink message is decoded using a set of <code>_MAV_RETURN_????</code> functions, e.g. <code>_MAV_RETURN_float</code></p>

<p>When I grep recursively for <code>_MAV_RETURN_float</code>, I could not find where it is defined. I wonder if I'm missing anything.  </p>

<hr>

<p><strong>UPDATE</strong><br>
Here is the source code of Ardupilot, including ArduPlane.<br>
<a href=""https://github.com/diydrones/ardupilot"" rel=""nofollow"">https://github.com/diydrones/ardupilot</a></p>
","ardupilot"
"5259","Is my quadcopter loop fast enough?","<p>I've been working on a quad copter for awhile now, recently I've finished the interface for PID tuning and its leading me to question several design decisions. </p>

<p>The quad uses a RaspberryPi as its pilot, the entire loop takes less than 20ms. IMU data is gathered, the throttle speeds are calculated, and then finally sent to an Arduino(micro) over an SPI interface. Where they are <code>analogWrite(...)</code>, to each ESC.</p>

<p>Can a quadcopter fly with a loop that slow? 20ms = 50Hz? </p>
","arduino quadcopter raspberry-pi"
"5260","The aerial refueling problem: sketch of a feedback controller","<p>At first happy new 2015!!!</p>

<p>I'm looking for my next simulator development: a Tanker is flying at constant speed (350 Knots) (no acceleration, no change of altitude or direction). The Tanker is approached from behind by a UAV which needs to refuel or transfer data through a wire. The UAV knows the direction, the speed and <em>relative position</em> from the tanker in order to approach it smoothly. It knows that at about 5 m from the tanker is the contact successful.
Here a picture I found on internet but it is clear more than thousand words:</p>

<p><img src=""http://i.stack.imgur.com/XR2Rv.jpg"" alt=""Bild 1""></p>

<p>To achieve the task I thought to implement a ""simple"" PID which controls the position and the velocity, but for this I have in my mind two different designs approaches:</p>

<ol>
<li><strong>Solution one</strong>: the motion equation of my system provide the position $x,y,z$ and velocity $Vx, Vy, Vz$ of the UAV (to simplify things I will consider just $x$ but of course $y,z$ must be eventually considered too). Those are feedback with the desired position (5m) and velocity (350 Knots) of the tanker. The feedback line is separated for each state and PIDs are working quite indipendently as in the following picture:</li>
</ol>

<p><img src=""http://i.stack.imgur.com/q6VbZ.png"" alt=""PID 1""></p>

<p>please note that to simplify things I never considered the acceleration.</p>

<ol start=""2"">
<li><strong>Solution two</strong>: this is the most tricky one and I was yesterday thinking about it all the time. In this case <strong>only one state</strong> vector is going to be feedback to the desired setpoints. In this case I would feedback only the velocity then integrate it and feed the result into the second PID. Maybe the following picture is clearer:</li>
</ol>

<p><img src=""http://i.stack.imgur.com/OEcFF.png"" alt=""PID 2""></p>

<p>But here I'm not really sure if the second idea is conceptually wrong or could be affordable. I'm pretty sure that the first one is working and leads to good result, but I was wondering if the second one is affordable or is not recommended for a control design.</p>

<p>Regards</p>
","control pid uav"
"5261","How to program parallel PID control loops? So I can give my robot multiple set points to follow","<p>So I'm in the process of building my robot and it has encoders on every wheel measuring speed and position and a compass sensor measuring heading.</p>

<p>I have 3 seperate PID loops at the moment, I can either control the robots speed or I can control the robots position or I can make it follow a heading using a line following type algorithm.</p>

<p>Now I am completely stuck on how to merge these control loops, how do I control the robots heading AND its speed. So I can say to it, ""go at 20 degrees at 3m/s"" or ""go at 45degrees for 5 metres then stop""</p>

<p>Really I would like to be able to control all 3, heading speed and position so I could say ""go at 20 degrees for 10 metres at a speed of 5m/s"" however I have no idea how to merge the loops.</p>

<p>At the end of the day there are 3 inputs (heading, speed and position) and 1 output (PWM to motors) so do I need to use some kind of MISO control scheme, if so which one? </p>

<p>I've looked at cascaded control but that only accepts 1 set point where I want to set 3 different set points. Maybe some kind of state machine? I'm so stuck!!</p>
","motor sensors control pid"
"5267","Questions on gears for a robot actuator","<p>I have some questions regarding building a gearbox for a motor to create a robot actuator. Given my desired motor and loads, I've determined that a gear ratio in the 400-700 range is adequate for my application. </p>

<p>Here is my motor choice for reference: <a href=""http://www.robotshop.com/ca/en/banebots-rs-550-motor-12v-19300rpm.html"" rel=""nofollow"">http://www.robotshop.com/ca/en/banebots-rs-550-motor-12v-19300rpm.html</a></p>

<p>Here are my questions:</p>

<p>1) Mounting gears to a motor: if I have a motor with a shaft diameter of 0.12in (3.2mm), what size gear bore should I use, and how do I attach a gear to the shaft in practice? I'm not that mechanically inclined (yet). </p>

<p>2) Say I build a gearbox of ratio 625:1, as such: <a href=""https://www.youtube.com/watch?v=lF-4qVBWy88"" rel=""nofollow"">https://www.youtube.com/watch?v=lF-4qVBWy88</a> I have no idea of how ""durable"" such a set up would be. For my application, I am looking at moving an 8kg mass from 0.6 meters away, coming out to a total torque of 47 newton meters. How can I tell if the gears will break or not? </p>

<p>For reference, these are the gears I'm looking at (and I'm pretty sure they're the same ones in the video): <a href=""http://www.vexrobotics.com/276-2169.html"" rel=""nofollow"">http://www.vexrobotics.com/276-2169.html</a> </p>

<p>3) Assuming those gears above were to fail, what type of gear material would be recommended for my load application of max 47nm? </p>

<p>4) What efficiencies can one expect from gears of different types? I've been assuming 50% conservatively as another answer mentioned. </p>

<p>Thank you for any help, and please let me know if anything was unclear.</p>
","motor robotic-arm torque gearing"
"5276","Why Make Bipedal Robots?","<p>A friend and colleague of mine who studies robotics says that bipedal robots present much greater challenges for stability and locomotion than those with more legs.</p>

<p>So why is there so much effort to develop bipedal robots?  Are there any practical advantages?</p>

<p>Of course, I see the advantage of having arm-like free appendages, but it seems like 4 legs and 2 arms would generally be a better design.</p>
","mobile-robot design legged"
"5279","Which encoder should I use with a 24V Dc motor and 6mm shaft?","<p>I would like to control the position and velocity of a DC motor like this <a href=""http://www.ebay.co.uk/itm/ZY-MY-1016-Electric-Motor-24v-200w-Brushed-E-Bike-Scooter-24-Volt-200-Watt-Chain-/171573828523?pt=LH_DefaultDomain_0&amp;hash=item27f298dbab"" rel=""nofollow"">one</a> (ZY MY 1016 - 24V - 10A - 200W). The motor comes with a 13 TOOTH SPROCKET and the diameter of the shaft is 6mm. It's not possible to apply an encoder to the back of the motor (see <a href=""http://i.ebayimg.com/00/s/NDIzWDU1MA==/z/0NYAAOxyDo1TjvN6/$_12.JPG?set_id=880000500F"" rel=""nofollow"">picture</a>)</p>

<p>The angular velocity in the description is 2750 RPM, which encoder do you recommend?  </p>
","motor quadrature-encoder"
"5282","Uploading edited code for Arducopter","<p>I'm attempting to customise some code for my DIY pentacopter frame.</p>

<p>To that end, i've modified the some existing code, and saved it under AP_MotorPenta.cpp and AP_MotorsPenta.h . I'm currently trying to upload the code onto my flight controller, but am currently unable to do so due to the following problems.</p>

<p>Problems</p>

<p>Unable to upload to my APM 2.6    ( #1)
Unable to select my pentacopter frame.    (#2)</p>

<p>Problem (#1)</p>

<p>I've saved my customised files in the AP_Motors library, and have compiled the Arducopter 3.2 code in ArduPilot-Arduino-1.0.3-gcc-4.8.2-windows , after which i upload it using mission planner. However, when i am uploading the hex file, i get the following error </p>

<blockquote>
  <p>""Uploaded Succeeded, but verify failed : exp E2 got 60 at 245760""</p>
</blockquote>

<p>However, when i try uploading it directly from the modified Arduino IDE, i get a series of warnings , followed by the messages</p>

<blockquote>
  <p>avrdude:verification error, first mismatch at byte 0x3c000 0x60 !=
  0xe2 avrdude: verification error; content mismatch</p>
</blockquote>

<p>followed by the message </p>

<blockquote>
  <p>"" avrdude done.Thank you. ""</p>
</blockquote>

<p>Does this mean that the uploading of the firmware to my flight controller is successfull? Also, is there any difference between uploading via mission planner and the modified Arduino IDE?</p>

<p>Problem #2</p>

<p>In the mission planner, originally there is the option to choose one of several frames, (i.e Quad/HexaOcto) etc. After uploading my firmware, how would i go about selecting my penta frame for use?Also is there any further thing that i would have to do?</p>

<p>Apologies in advance if the questions are rather inane, as i have little programming experience to speak of.</p>

<p>I would really appreciate any help i can get.</p>

<p>Thanks in advance !</p>
","arduino quadcopter microcontroller ardupilot"
"5283","Public dataset for monocular visual odometry","<p>I am planning to develop a monocular visual odometry system. Is there any indoor dataset available to the public, along with the ground truth, on which I can test my approach?</p>

<p>Note: I am already aware of the KITTI Vision Benchmark Suite which provides stereo data for outdoor vehicles.</p>

<p>If anyone has access to the datasets used in the following paper [SVO14], it would be even more great: <a href=""http://rpg.ifi.uzh.ch/docs/ICRA14_Forster.pdf"" rel=""nofollow"">http://rpg.ifi.uzh.ch/docs/ICRA14_Forster.pdf</a></p>
","computer-vision odometry"
"5292","Lifting Robot To Lift Small Crates","<p>I am trying to design a robot to lift tote-crates and transport them around in a localized area. I want to be able to carry 3 tote-creates at a time.  This robot needs to be able to pickup the creates. I only want the robot to carry three at a time so keep is small and mobile. I was thinking of a design with a central lift that could carry the crates. What would you suggest as a simple ingenious way to create this robot? </p>
","design wheeled-robot mechanism"
"5293","How do self driving cars really work?","<p>I'm absolutely fascinated by the notion of a driverless car.  I know there is a lot involved with it and there are many different approaches to the problem.</p>

<p>To narrow the scope of this question to something reasonable for the SE network, i'm curious to know if there is a common sequence of subproblems that every driverless car needs to solve at each timestep to make an autonomous car possible for real life, point to point transportation possible.  I imagine that once the starting point and target destination on a given map are set, a self driving follows an algorithm that loops through certain operations to solve certain problems along the way.  I'm more interested in knowing what those problems are specifically at a high level, rather than detailed algorithms to solve them.  Do all self driving cars solve the same subproblems along the way?</p>
","automatic"
"5300","Linear or Switching Power Supply for a Embedded Project","<p>Power Block Designing noob here.</p>

<p>I have a beaglebone 2x XbeePro(s) and another 500mAh device connected to the board i am building a PCB Around.</p>

<p>I need some advice on weather to use Linear Voltage Regulation vs Switching mode regulation. </p>

<p>Secondly if i am using linear voltage regulation setup do I need multiple regulators for the different devices? </p>

<p>My plan is to use A 2S 1000mAh Battery -> Fuse -> 2x 1.5A LM1084's in parallel output feed to the beaglebone and a LM3940 for both the XBees. Or its better to have each XBee on its on LM3940 drawing power from a seperate LM1084?</p>

<p>Linear Regulators tend to get hot on full load, hows the performance of Switching mode regulators ?</p>
","power battery"
"5302","Code control your arduino with keyboard","<p>I've seen lots of examples on how to communicate from Arduino to the computer, but the few that talked about computer to Arduino communications were very hard to understand.</p>

<p>My question is: <em>What code can I use to control my arduino Uno with my keyboard</em></p>

<p>if it helps, I'm trying to set up a WASD steering behavior.</p>
","arduino programming-languages"
"5307","3D Camera for beginner","<p>I want to buy a 3d camera with depth sensor. Can anyone can give me advice on which one will be the best?</p>

<p>I have a experience with kinect, but configuring kinect for Linux is painful and also kinect generate sometimes a big latency.</p>

<p>I am looking for low latency, a good depth sensor and a good api for Linux. I am thinking about a Currera R.</p>

<p><strong>Update:</strong></p>

<p>I found a quite cheap and good camera from XIMEA.</p>

<p>Thay have a very nice support for libraries like opencv/matlab etc, so for newbie like it is perfect.</p>
","sensors beginner"
"5310","Interfacing high-resolution image sensors with ARM Board","<p>I'm working on a project requiring HD (Stereo) Video Processing. Most of High Resolution (5MP+) Sensors use MIPI-CSI interface. </p>

<p>I managed to get a board with an Exynos5 SoC. The SoC Itself has 2 MIPI-CSI2 interfaces, the problem is that the pins to those interfaces are not exposed and It's (almost) impossible to reach them. So I decided to use the USB3.0 Buses.</p>

<p>The problem is when I get to Significant bandwidth (~5.36 Gibibits/s per sensor), I don't think USB3.0 will work out. <code>Bandwidth = Colordepth*ColorChannels*PixelCount*FPS</code> but this could be solved with a Compressed stream (via a coprocessor)</p>

<p>I was thinking that Cypress' CYUSB306X chip was a good candidate for the job, but one of the problems is that I can't do BGA Soldering by hand nor have been able to find a BGA Soldering Service in Switzerland.</p>

<p>Any Ideas on other interfaces I could implement or other coprocessors with MIPI-CSI2 Interface?</p>

<p>Just a final remark, space and weight are important as this is supposed to be mounted on a drone.</p>
","cameras usb stereo-vision"
"6310","What type of Antennas to use for XBeePRo 2.4Ghz","<p>I am planning to use 2.4Ghz XBeePro 63mW Devices for a project that requires a coverage area of around 1.5-2km. </p>

<p>When i go to select an antenna there are various options like Circular,Virtical, Horizontal polarized etc.</p>

<p>Which antenna would give a coverage for a field? I cant have it directional (one point to another point). By devices will be moving around on a field. </p>

<p>What type of polarization is recommended for this kind of a setup? My Base XBee will be on a elevation of around 40m from the ground so i have a clear line of sight for all the moving modules. </p>

<p>There are going to be around 20-30moving modules streaming data at around 2-5readings per second. </p>

<p>A +12dBi Antenna should suffice the application? And what about polarization? </p>
","battery wireless"
"6313","How do you calculate the moment of inertia of a Quadcopter?","<p>I'm building a quadcopter for my final year project. I have a set of equations describing attitude and altitude but they involve $I_{xx}$, $I_{yy}$ and $I_{zz}$. None of the papers I have read describe how these are calculated. they simply choose it before their simulation. Can anyone help?</p>
","quadcopter kinematics"
"6314","Communicating between a beaglebone black and an arduino using ttyO4","<p>I'm trying to get an arduino to talk with a beaglebone black. 
I have followed <a href=""http://hipstercircuits.com/enable-serialuarttty-on-beaglebone-black/"" rel=""nofollow"">this</a> tutorial for getting ttyO4 open on the BBB and used the following command on to set the serial line correctly: </p>

<p>wiring is set up according to <a href=""http://www.instructables.com/id/How-to-make-a-BeagleBone-and-an-Arduino-communicat/"" rel=""nofollow"">this</a> tutorial. 
    stty -F /dev/ttyO4 cs8 9600 ignbrk -brkint -imaxbel -opost -onlcr -isig -icanon -iexten -echo -echoe -echok -echoctl -echoke noflsh -ixon -crtscts</p>

<p>next data is sent using the following method: </p>

<pre><code>echo s123 &gt; /dev/ttyO4
</code></pre>

<p>the arduino uses the followingvoid loop(){
 code to check for serial communication:</p>

<pre><code>#include &lt;SPI.h&gt;

void setup(){ //////////////SETUP///////////////////////
  Serial.begin(9600);
}
void loop(){
      if(Serial.available()&gt;=4){
         digitalWrite(12, HIGH);
         delay(1000);               // wait for a second
         digitalWrite(12, LOW);   // turn the LED on (HIGH is the voltage level)
         delay(1000);               // wait for a second
         digitalWrite(12, HIGH);
         byte b1,b2,b3,b4; 
         b1=Serial.read();
         }
    }
}
</code></pre>

<p>However it seems no message is received. It does not give any error either. </p>

<p>As an alternative I have also tried a variant of the code suggested in the wiring tutorial resulting in the following code:</p>

<pre><code>import sys
from bbio import *


Serial2.begin(9600)
for arg in sys.argv:
    print arg
    Serial2.write(arg)
    delay(5)
</code></pre>

<p>called with <code>pyton test s123</code> this printed s123 but the arduino remained silent. 
Edit I have now also tried to exactly follow the wiring tutorial so that gave me the following sketch: </p>

<pre><code>char inData[20]; // Allocate some space for the string
char inChar=-1; // Where to store the character read
byte index = 0; // Index into array; where to store the character

void setup() {
  Serial.begin(9600);
  pinMode(13, OUTPUT);   // digital sensor is on digital pin 2
  digitalWrite(13, HIGH);
  delay(2000);
  digitalWrite(13, LOW);
  delay(500);
}


void loop()
{
  Serial.write(""A"");
  digitalWrite(13, HIGH);
  delay(100);
  digitalWrite(13, LOW);
  delay(100);
  if (Comp(""A"")==0) {
      digitalWrite(13, HIGH);
      delay(1000);
      digitalWrite(13, LOW);
      delay(500);
  }
}

char Comp(char* This) {
    while (Serial.available() &gt; 0) // Don't read unless
                                   // there you know there is data
    {
        if(index &lt; 19) // One less than the size of the array
        {
            inChar = Serial.read(); // Read a character
            inData[index] = inChar; // Store it
            index++; // Increment where to write next
            inData[index] = '\0'; // Null terminate the string
        }
    }

    if (strcmp(inData,This)  == 0) {
        for (int i=0;i&lt;19;i++) {
            inData[i]=0;
        }
        index=0;
        return(0);
    }
    else {
        return(1);
    }
}
</code></pre>

<p>and on the BBB we turn on the echo script with</p>

<pre><code>/PyBBIO/examples$ sudo python serial_echo.py
</code></pre>

<p>The effect remains that there is no error but also no data delivery. </p>
","arduino control serial communication beagle-bone"
"6315","Pose estimation, how to populate set of known edges and points?","<p>I am building an estimator that solves for the camera pose relative to a reference frame which contains a known set of features and edges. Currently, the system works with an unscented kalman filter with four known points (red leds) in the reference frame. I am now hoping to improve robustness by adding edges to the model as well as robust features. I would like to add additional points that are uncovered by some opencv feature finding function (fast,cornerHarris,...).</p>

<p>So far I found the paper ""Fusing Points and Lines for High Performance Tracking"" and ""Robust Extended Kalman Filtering For Camera Pose Tracking Using 2D to 3D Lines Correspondences"" which seem to detail how to fuse edge and feature matching for pose estimation.</p>

<p>Is there a strategy to populate the known set of edges and features when it is impractical to measure them with a ruler/tape measure? My first thought is to start with a small known set of features, my red leds, then run some slam algorithm and keep all features/edges that have some minimum certainty.</p>

<p>Thanks a bunch!</p>

<p>I have misunderstood the RANSAC algorithm. This is not appropriate for my application. </p>

<p>For those interested, I am hoping to use a similar approach to the one presented in the following paper.</p>

<p>Youngrock Yoon, Akio Kosaka, Jae Byung Park and Avinash C. Kak. ""A New Approach to the Use of Edge Extremities for Model-based Object Tracking."" International Conference on Robotics and Automation, 2005.</p>
","kalman-filter computer-vision pose"
"6322","Getting started with Jetson Tegra K1","<p>After working for a long time on my Arduino Due, I needed a better and more powerful prototyping platform for my future projects. For which, I have placed an order for NVIDIA Jetson Tegra K1 board which runs on linux and supports CUDA based development. Being a newbie to Linux, I have no idea where to start from and what to do for getting started with code execution on the Jetson board. Please suggest the initial steps required and from where can I get familiar to Linux environment...</p>

<p>Thank you</p>
","linux"
"6323","Powering a multirotor with dedicated batteries for each motor","<p>I'm currently thinking of extending the battery life of my quad by powering each motor and ESC individually. I  will be using 1 dedicated battery for each motor, and 1 dedicated battery for the flight controller itself, bringing the total to 5 batteries for the entire quad.</p>

<p>My thinking is that by powering each motor with a dedicated battery, given a power draw/consumption, the flight-time of my quad will be increased by 4x as each motor will have 4x the capacity to draw from. Putting the problem of weight aside, would this be a feasible idea?</p>

<p>Also, i am currently using just 1 battery to power all motors, and as such, i only have to plug in the single battery and i can calibrate my ESCs. How would i calibrate my ESCs if i am using dedicated batteries for my APM 2.6 and each motors?Would i be able to get away with powering my APM using the BEC on my ESCs?</p>
","quadcopter power battery"
"6325","Ambiguous definition of Error-State (Indirect) Kalman Filter","<p>I am confused by what precisely the term ""Indirect Kalman Filter"" or ""Error-State Kalman Filter"" means.</p>

<p>The most plausible definition I found is in Maybeck's book [1]:</p>

<blockquote>
  <p>As the name indicates, in the total state space (direct) formulation, total states
  such as vehicle position and velocity are among the state variables in the filter,
  and the measurements are INS accelerometer outputs and external source
  signals. In the error state space (indirect) formulation, the errors in the INS-
  indicated position and velocity are among the estimated variables, and each
  measurement presented to the filter is the difference between INS and external
  source data.</p>
</blockquote>

<p>20 years later Roumeliotis et al. in [2] write:</p>

<blockquote>
  <p>The cumbersome modeling of the specific vehicle and its interaction with a dynamic environment is avoided by selecting gyro modeling instead. The gyro signal appears in the system (instead of the measurement) equations and <strong>thus the formulation of the problem requires an Indirect</strong> (error-state) Kalman filter approach.</p>
</blockquote>

<p>I cannot understand the bold part, since Lefferts et al. in [3] write much earlier:</p>

<blockquote>
  <p>For autonomous spacecraft the use of inertial reference units as a model
  replacement permits the circumvention of these problems.</p>
</blockquote>

<p>And then proceed to show different variants of EKFs using gyro modeling that are clearly direct Kalman Filters according to Maybeck's definition: The state only consists of the attitude quaternion and gyro bias, not error states. In fact, there is no seperate INS whose error to estimate with an error-state Kalman filter. </p>

<p>So my questions are:</p>

<ul>
<li><p>Is there a different, maybe newer definition of indirect (error-state) Kalman Filters I am not aware of?</p></li>
<li><p>How are gyro modeling as opposed to using a proper dynamic model on the one hand and the decision whether to use a direct or indirect Kalman filter on the other hand related? I was under the impression that both are independent decisions.</p></li>
</ul>

<p>[1] Maybeck, Peter S. Stochastic models, estimation, and control. Vol. 1. Academic press, 1979.</p>

<p>[2] Roumeliotis, Stergios I., Gaurav S. Sukhatme, and George A. Bekey. ""Circumventing dynamic modeling: Evaluation of the error-state kalman filter applied to mobile robot localization."" Robotics and Automation, 1999. Proceedings. 1999 IEEE International Conference on. Vol. 2. IEEE, 1999.</p>

<p>[3] Lefferts, Ern J., F. Landis Markley, and Malcolm D. Shuster. ""Kalman filtering for spacecraft attitude estimation."" Journal of Guidance, Control, and Dynamics 5.5 (1982): 417-429.</p>
","localization kalman-filter navigation errors"
"6327","beaglebone black power supply for hexapod","<p>I am trying to build a <em>hexapod</em> with camera interfacing using a <strong>beaglebone black</strong> for college project. I'm not sure what <strong>power supply</strong> to give so it can power up to <em>bot,</em> having in mind that it should be <strong>portable (mobile)</strong> and it should power about <em>18 servo motors</em> along with the <em>camera</em>, <em>wifi</em> and the <em>processor</em>. Your help is needed very badly as i'm nearing the <em>deadline</em> for the project.</p>
","power servos beagle-bone hexapod"
"6328","language to code Beaglebone","<p>I'm trying to build a hexapod with beaglebone in the linux environment (Im thinking of using Ubuntu). What is the <strong>best language</strong> to use for <strong>coding purpose</strong> to make <em>robot controls, camera and wifi integration</em> etc.</p>
","control programming-languages beagle-bone linux"
"6331","Passing power through a motor","<p>How would one go about passing power through a motor?</p>

<p>Let's say we have some basic robot which has a motor that slowly spins a limb, on each end of that limb, there is a motor which again spins a limb. Because the first motor is always going to be spinning, any wires would twist and eventually break, so a wired approach wouldn't work. The same goes for the subsequent motors.</p>

<p>I know that dc motors use brushes to get past this, but how is this generally solved in engineering/robotics? This must be a problem that has come up before, and there must be a solution to it.</p>

<p>Any ideas? :)</p>
","motor stepper-motor power"
"6334","What is inverse depth (in odometry) and why would I use it?","<p>Reading some papers about visual odometry, many use inverse depth. Is it only the mathematical inverse of the depth (meaning 1/d) or does it represent something else. And what are the advantages of using it?</p>
","slam computer-vision odometry"
"6339","Understanding the Bode Plot","<p>I'm not sure if this is the correct forum for this question about Automatic Control, but I'll try to post it anyway.</p>

<p>So, I've just started to learn about Control Systems and I have some troubles understanding Bode plots. My textbook is really unstructured and the information I find on the Internet always seem a little bit too advanced for a beginner like me to grasp, so I hope you can help me to understand this. </p>

<p>I would like to know what the information we find in the Bode plot can tell us about how the corresponding step responses behave. I know that a low phase margin will give an oscillatory step response and that the crossover frequency decide the rise time. But how can we see if the step response has a statical error in the Bode plot and what does the phase graph of the Bode plot actually mean?</p>
","control automatic"
"6341","LT1157 Logic Level Question","<p>I plan to use the <a href=""http://cds.linear.com/docs/en/datasheet/lt1157.pdf"" rel=""nofollow"">LT1157</a> in my application PCB to act as a switch control from a micro controller side to control the On/Off state of 2 module boards which will be connected in the PCB. </p>

<ul>
<li>1st Load is 5V 1A.</li>
<li>2nd Load is 3.3V 500mA.</li>
</ul>

<p>The LT1157 will get a 5V input at the V<sub>s</sub> terminal. </p>

<p>Does anyone know how much voltage is required to be used at the IN1 and IN2 pins? The datasheet doesn't say how much voltage can be used here. I am guessing it will be 5V, but can it do logic level with 3.3V? My microcontroller board gives an output of 3.3V and not 5V so I'll have to make a logic Level converter before feeding the pins IN1 and IN2 if it's not 3.3V tolerant. </p>

<p>Please confirm, if anyone has used this IC before. </p>
","circuit"
"6345","Battery system for a robot with a RaspberryPi or microcontroller","<p>I'm building a robot which is actually a rotating ball. All my circuitry will be inside this ball. I'm using a Raspberry Pi as the brains. Apart from Raspberry Pi, I've an H-bridge IC (L298N), a 6-axis Accelerometer + Gyroscope (MPU6050), and probably some more additional digital components. These will work with a 5V or 3.3V supply. Another set of components are electromechanical devices like a 9kg torque servo and 2 1000RPM DC motors.</p>

<p><strong>Here are my questions:</strong></p>

<ol>
<li>Everything will work on battery. I can get a 3.3V and 5V supply from a 9V battery using L1117-3.3V and 7805 regulators respectively. I know that it's not at all reliable to share the power source of the control circuitry with high load devices like motors and servos. Should I have a dedicated separate supplies for electromechanical components and the control circuitry?</li>
<li>Servo will run on 6V supply and motor will run on a 12V supply. How should I go about this one? Again, separate batteries for servo and motors?</li>
<li>Can of this work on a single high capacity battery, somewhat like 10000mAh?</li>
</ol>

<p><strong>Here are some of my calculations:</strong></p>

<p><strong><em>Servo current (6V):</em></strong> <strong>at no load:</strong> ~450mA, <strong>at around 6kg load:</strong> ~800mA</p>

<p><strong><em>Motor current (12V):</em></strong> <strong>at no load:</strong> ~500mA, <strong>at around 6kg load:</strong> ~950mA</p>

<p><strong><em>RaspberryPi and other digital circuitry (5V + 3.3V):</em></strong> ~600mA (that includes an Xbee)</p>

<p>Thus, the overall current at a 6kg load (with two motors) comes around ~3.3A</p>

<p>It would be really awesome if this thing gets done with a maximum of 2 batteries. Else, it may get messy while placing the batteries inside the ball. Space is limited!</p>
","motor power servos battery"
"6348","What Does the Sensitivity Function Mean?","<p>I'm studying for a test in Automatic Control and I have some troubles understanding sensitivity functions and complementary sensitivity functions. </p>

<p>There's one assignment from an old exam that sais
""Someone suggests that you should reduce perturbations and measurement noise simultaneously. Explain why this is not possible.""</p>

<p>The correct answer sais:
""Since the sensitivity and complementary sensitivity transfer functions add up to 1, i.e. $S+T=1$, one cannot improve both the output disturbance and measurement error suppression at the same time.""</p>

<p>I don't really understand this answer and my textbook is not to much help either, so I would appreciate alot if someone could explain how they got to this answer? Also, is the sensitivity function always representing the perturbations in the system and the complementary sensitivity function the measurement noise? My textbook seem to imply this, but I'm really not sure if this is always true.</p>
","control noise automatic"
"6351","Accounting for error in multiple electric motors","<p>Our goal is to drive an autonomous robot with a differential locomotion system using two identical electric motors and an Arduino Uno (1 for each wheel). From our understanding, over time the motors can lose accuracy and may result in one motor rotating more than the other.
Is there a way to account for possible error in the speeds of the motors so that the robot can end up in a very precise location?</p>

<p>Our thoughts were to have an indicator which would allow us to count the number of rotations of each motor and compensate if a noticeable difference began to appear.</p>
","arduino mobile-robot two-wheeled"
"6355","Dealing with position inaccuracy and latency in PID Loop","<p>Background:
I am new to PID, for my first PID project I am using a simple P-Loop and 300 degree linear potentiometers for position feedback. I am using the roboclaw 2x60A motor controller.  The motor controller has 64 speeds between.  Sometimes the potentiometers can vary as much as +-4 degrees when not in motion.  I am using an Arduino mega with a 10bit ADC to control the motors.  </p>

<p>My Question:
How can I filter or reduce the variance in the potentiometers?  In addition, it takes a certain amount of time for the motors to react to the command, and it seems to throw off the P loop.  How do I account for the latency, in my program?</p>

<p>Example:
For this example the P loop was run every 33-36 milliseconds.
I will tell the motor to go to 250 deg/sec, and it will go to 275 deg/sec, the P loop then reacts by lowering the value sent to the motor however the speed then increase to 400 deg/sec and then the P loop lowers the value again, then the speed will drop to 34 deg/sec.</p>

<p>Thanks so much for any help,
Joel</p>
","arduino motor pid software avr"
"6356","How do I achieve this? Grid of dowels powered by piston-like movement","<p>I am completely new to this site and robotics, but I have experience in programming, and programming microcontrollers.  </p>

<p>I would like to create a grid of ""pixels"", where each ""pixel"" is a metal or wooden dowel that is programmed to push in and out, like a piston.  </p>

<p>I'm imagining a lot of pixels, maybe 40x40, where each could be quite small in diameter (1/4"").  The Arduino would have control over the linear movement - up and down - of each pixel.</p>

<p>Could anyone point me in the right direction for accomplishing this?</p>
","arduino mechanism servos servomotor"
"6366","How can I improve the range of an XBee S6B?","<p>Has anyone used the XBee WiFI modules? Done a range check on them?</p>

<p>With my laptop i get a range of around 400m on industrial level Accesspoints on a football field, well how good are these devices ? If i get a SMA Connector version and use a higher gain antenna am I looking at ranges from 250-500m ? (Talking 18-22dBi gains here).</p>
","wifi"
"6367","How to combine an accelerometer and a gyroscope to find robot location and orientation in 2D/3D space","<p>I have data from an accelerometer that measures X,Y,Z acceleration and data from a gyroscope that measure pitch, roll and yaw. How would I combine this data to find robot location and orientation in 2D or 3D space? </p>
","localization accelerometer algorithm gyroscope"
"6371","Basic programming in arducopter","<p>I am starting with a project using Arducopter. I am a person familiar with arduino, but seeing the arducopter for the first time. Commands codes and everything is completly different compared to normal Arduino programming. I am not getting any help or commandlist for specific purposes in arducopter. Any body can help me in leading to any links which can help me out..</p>
","arduino ardupilot"
"6374","ros send message on startup doesn't seem to work","<p>I have the following code: </p>

<pre><code>void NewCore::spin(){
    ros::Rate rate(10);

    int first =1;
    while(ros::ok()){

        if (first){         
            askMath();first=0;}
        ros::spinOnce();
        rate.sleep();
    }
}

int main(int argc, char **argv){

    ros::init(argc, argv, ""newCore"");
    NewCore nc;
    nc.init();

    nc.spin();
}

void NewCore::init(){

    mngrSub = handle.subscribe&lt;std_msgs::String&gt;(""/tawi/core/launch"", 10, &amp;NewCore::launchCallback, this);
    mngrPub = handle.advertise&lt;std_msgs::String&gt;(""/tawi/core/launch"", 100);

    mathSub = handle.subscribe&lt;std_msgs::String&gt;(""/display"", 10, &amp;NewCore::launchCallback, this);
    serSub = handle.subscribe&lt;std_msgs::String&gt;(""/tawi/arduino/serial"", 100,&amp;NewCore::serialCallback,this);
    mathPub = handle.advertise&lt;std_msgs::String&gt;(""/questions"", 100);

    ballPub = handle.advertise&lt;std_msgs::Int16&gt;(""/tawi/core/ballcount"", 100);
    nmbrPub = handle.advertise&lt;std_msgs::Int16&gt;(""/tawi/core/number"", 100);
}

void NewCore::askMath(){
    ROS_INFO(""addition1d&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;"");
    std_msgs::String question;
    question.data = ""1digitAddition"";
    mathPub.publish(question);
}
</code></pre>

<p>(code that isn't interesting has been removed)</p>

<p>Running this causes the following output to appear: </p>

<pre><code>$ rosrun glados newCore 
[ INFO] [1421236273.617723131]: addition1d&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;
</code></pre>

<p>However if I have the following code on during launch:</p>

<pre><code>$ rostopic echo /questions 
</code></pre>

<p>then it does not show me an initial message being sent.</p>

<p>Changing</p>

<pre><code>        if (first){         
            askMath();first=0;}
</code></pre>

<p>into </p>

<pre><code>            askMath();first=0;
</code></pre>

<p>does appear to work but it then sends a message every cycle rather then just the one at the start. </p>

<p>Does anybody know what is wrong here? </p>
","ros c++"
"6382","Can a Jacobian be used to determine required joint angles for end effector velocity/position?","<p>I'm in the early stages of working with a simple robot arm, and learning about the Jacobian and inverse kinematics.</p>

<p>From my understanding, the Jacobian can be used to determine the linear and angular velocity of the end effector, given the angular velocity of all joints in the arm. Can it also be used to determine the Cartesian position of the end effector, given the angles and/or positions of the joints?</p>

<p>Furthermore, suppose that I want to determine the required angular velocities of the joints, in order to bring about a desired linear velocity of the end effector. Can this be done by simply inverting the Jacobian and plugging in the desired parameters?</p>
","kinematics inverse-kinematics forward-kinematics manipulator jacobian"
"6383","PCB making at home","<p>I've made many PCBs at home but still there are some mistakes. I tried ironing, drawing methods but it doesn't work very well. I use eagle CAD for design PCBs. Please help me. </p>
","design"
"6386","Robotics advice needed","<p>For a high school project I will be building a robot that will draw an image on a whiteboard for you based on what instructions you give. To accomplish this a motor will move the pen on each axis similar to how a 3rd printer moves but without the Z axis. As far as code goes I'm fine but I was wondering if anyone could give me an insight on how to go about building the robot (i.e. what motors, best system for moving on axises etc) All help is appreciated thanks</p>
","motor mechanism"
"6395","Mobile Camera Calibration and rectification frame rate","<p>I've been searching the internet for an answer to this question, but I haven't come across anything that will help me. Basically, we have a Meka Humanoid Robot in our lab, which has a shell head in which a PointGrey USB 3.0 camera is embedded. I use the <a href=""http://wiki.ros.org/pointgrey_camera_driver"" rel=""nofollow"">pointgrey_camera_driver</a>  for obtaining the images from the camera. The head has 2 degrees of freedom (up/down, left/right). I am intending to use this camera with the ar_pose package to detect and track AR tags on objects. I understand that camera's must be calibrated for effective use (forgive me, I don't know much about camera) which I do with the camera_calibration package.</p>

<p>My question is: Since this camera is ""mobile"" meaning since the head can move so does the camera, how would I go about calibrating it? Currently, I have the head fixed at a position and I've calibrated the camera in that position and got the parameters in the yaml file which I can load for rectification. In particular, if the head moves does the calibration file that I obtained in the previous position become invalid? If so, as asked before, how would I calibrate this camera for all of its possible field's of view (which can be obtained by moving)?</p>

<p>This camera has different video modes and in the mode I'm using I can get a frame rate of 21Hz (i.e. after driver is launched I get 21Hz for rostopic hz /camera/image_raw). However, when I rectify the image using image_proc, I get a frame rate of only about 3Hz on rostopic hz /camera/image_rect_color. Is this normal? Is there a way for me to increase this frame rate?</p>

<p>Please let me know if any more information is required.
Thanks for your help!</p>
","ros cameras calibration"
"6398","Eliminating Electrical Noise from my motor driver","<p>Background:</p>

<p>I am using an Arduino Mega connected to a RoboClaw 2x60A motor driver.  I asked <a href=""http://robotics.stackexchange.com/questions/6355/dealing-with-position-inaccuracy-and-latency-in-pid-loop"">this question</a> about the system, but I have since narrowed the scope of the problem.  I tried adding a bunch of different size capacitor between the 5v and gnd, when the RoboClaw is switched off then a 470 micro farad capacitor seems to eliminate all noise but when I turn on the RoboClaw no capacitance valued I tried, (4.7,10,100,220,320,470,540,690,1000,1100)microfarads seems to eliminate any noise.  I even tried hooking up a 12v battery with a 5v regulator to the logic battery on the RoboClaw and connecting it to the ground on the Arduino.  Then I tried using a separate battery for the pots and connecting the AREF to the +5v on the battery.<br>
No matter what I try when the roboclaw is on the potentiometer value will vary as much as +-6 degrees.  I found the degrees using:</p>

<p>map(analogRead(A0),0,1023,0,300) </p>

<p>In addition I took a bunch of data and graphed it and found that if I took 25 instantaneous data points and averaged them together it would help significantly reduce the variance.  I chose 25 because it take 2.9 ms, 100 worked really well but it took 11 ms.  To help explain the averageing of analog read, here is my code:</p>

<p>unsigned int num = 0;</p>

<p>for (int i = 0; i&lt;25; i++){</p>

<pre><code>num+=analogRead(A0);
</code></pre>

<p>}</p>

<p>potReading = num/25;</p>

<p>My Question:</p>

<p>What is my next step in eliminating this noise?  Is there a formula I can use to find a better capacitance value? Should I try putting capacitors on each potentiometer between 5V and gnd?  Any other IC I should try to help with this?  On my previous question someone mentioned optocouplers, what size would work best and where in the circuit do they go?  Is there code I can write to help eliminate the size of the variance beyond what I have written?</p>

<p>Thanks so much for any help,
Joel</p>
","arduino motor electronics noise driver"
"6405","Difference between kinematic, dynamic and differential constraints","<p>I would like to know the simple difference between kinematic, dynamic and differential constraints in robotic motion planning.</p>
","motion-planning"
"6406","State space and control space","<p>I would like to know the difference between state space and control space in relation to motion planning. I would like a simpler explanation.</p>
","motion-planning"
"6408","Can ReplicatorG or MatterControl drive a RepRap RAMBo motherboard?","<p>I'm fairly new to 3D printing. I'm considering motherboards I might use to control my printer. I'm hoping to find a board that I can easily control using either:</p>

<ul>
<li>ReplicatorG</li>
<li>MatterControl</li>
</ul>

<p>I like these programs because they seem reasonably current, mature and straight-forward for beginners.</p>

<p>My question is <strong>can I control a Rambo V1.2 board from either of these programs?</strong> These programs don't include explicit support for the RAMBo as far as I can see, but maybe I'm missing how printing software works at this point?</p>

<hr>

<h1>What is a RAMBo?</h1>

<p>The RAMBo V1.2 board is a creative-commons/open source design. It integrates an Arduino, Stepper-Motor drivers, Heater &amp; Fan controls, Power Management and more.</p>

<p>An example implementation looks like this:
<img src=""http://i.stack.imgur.com/bJ6KR.jpg"" alt=""RepRap Rambo V1.2G""></p>

<p>For more background info on what a RAMBo board is, you may read about it on the <a href=""http://reprap.org/wiki/Rambo"" rel=""nofollow"">RepRap community wiki</a>.</p>
","3d-printing reprap"
"6409","The IDE using for programming the Atlas robots","<p><a href=""https://www.youtube.com/watch?v=27HkxMo6qK0"" rel=""nofollow"">ATLAS Gets an Upgrade</a> - the new video of the Atlas robot is out so I'm curious about the IDE with which they are coding this thing.</p>
","dynamics"
"6412","How do I control a servo using a beaglebone black running ubuntu","<p>I  have a BeagleBoneBlack and would like to use it to control a servo for my robot. I'm mostly programming in ros and as such am looking preferably for a c++ solution. Is there an easy way of controlling a servo on a BBB running ubuntu 14.04 on the kernal 3.8? Most tutorials I have tried referred to files I did not have so I'm unsure.</p>
","ros servos servomotor beagle-bone c++"
"6417","Comparing LQR and PID controllers for inverted pendulum problem","<p>As far as i can tell, both LQR and PID controllers can both be applied to the cart-pole (inverted pendulum) problem.  What are the pros/cons to using one controller over the other for this particular problem?  Are there any reasons/situations where I should prefer one over the other for this problem?</p>
","control pid"
"6418","Tracking Landspeed Underwater","<p>I am hoping someone might be able to nudge me in the right direction (apologies for the long post but wanted to get all the information I have gained so far down.</p>

<p>Basically I am after a solution to record the path my vessel took under water for later analysis…like a bread crumb trail.</p>

<p><strong>Requirements:</strong></p>

<p>Ideally have a range of at least 30meters however if there were no other options I would accept down to 10meters.</p>

<p>Working fresh and salt water.</p>

<p>The vessel is (25cm x 8cm) so size and power consumption are a factors.</p>

<p>It would be traveling roughly parallel to the sea bed at variable distances from the sea bed (range of 0-30 meters)</p>

<p>Does not need to be super accurate, anything less than 5 meters would be fine.</p>

<p>Measurement speed range of 0 – 4 mph.</p>

<p>Measure direction the object was moving in (i.e. forwards, sideways, backwards)…I am planning to use a compass to ascertain N, S, E, W heading.</p>

<p><strong>Options I have discounted:</strong></p>

<ul>
<li>Accelerometers:</li>
</ul>

<p>This was my initial thinking but in doing some reading it seems they are not suited to my needs (unless you spend loads of money, and then the solution would end up being too heavy anyway).</p>

<ul>
<li>Optical Flow:</li>
</ul>

<p>Looks too new (from a consumer perspective) / complicated. I don’t know what its range would be like. Also requires additional sonar sensor.</p>

<p><strong>Current favorites:</strong></p>

<ul>
<li>Sonar:
<a href=""http://www.alibaba.com/product-detail/1mhz-waterproof-transducer-underwater-ultrasonic-sensor_1911722468.html"" rel=""nofollow"">http://www.alibaba.com/product-detail/1mhz-waterproof-transducer-underwater-ultrasonic-sensor_1911722468.html</a></li>
</ul>

<p>Simplest use is distance from object, however can use doppler effect to analyse speed of a moving object.</p>

<p>40m range, nice!</p>

<p>Presumptions:</p>

<p>If I fired this at an angle to the seabed I could deduce the speed the floor was ‘moving’ below which would give me the speed of my vessel?</p>

<p>I am also presuming that I could interpret direction of movement from the data?</p>

<p>I presume that the sensor would need to be aimed at an angle of around 45 degrees down to the seabed?</p>

<ul>
<li>Laser Rangefinder:</li>
</ul>

<p>Although it works differently to the Sonar the premise of use looks the same, and thus I have the same queries with this as I do with the Sonar above.</p>

<p>Presume if I mounted the sensor behind high quality glass (to waterproof it) then performance would not be impacted that much.</p>

<p>This is a lot more costly so if it does not give me any advantage over sonar I guess there is no point.</p>

<ul>
<li>Water Flow Meter:
<a href=""http://www.robotshop.com/en/adafruit-water-flow-sensor.html"" rel=""nofollow"">http://www.robotshop.com/en/adafruit-water-flow-sensor.html</a></li>
</ul>

<p>Super low cost and simple compared with the other options, I would potentially use a funnel to increase water pressure if it needs more sensitivity at low speed.</p>

<p>Would then just need to calibrate the pulses from the sensor to a speed reading.</p>

<p>Significant limitations of this is it would be registering a speed of 0 if the vessel was simply drifting with the current….its speed over the seabed that I am interested in.</p>

<p>Current favorite option is sonar (with the option of using water flow meter as second data source)…however are my sonar presumptions correct, have I missed anything?</p>

<p>Any better ideas?</p>
","sonar laser underwater rangefinder acoustic-rangefinder"
"6423","Particle filter weight function","<p>I am trying to implement a particle filter in MATLAB to filter a robot's movement in 2D but I'm stuck at the weight function. My robot is detected by a camera via two points, so a single measure is a quadruple (<code>xp1</code>, <code>yp1</code>, <code>xp2</code>, <code>yp2</code>) and states are the usual (<code>x</code>,<code>y</code>,<code>alpha</code>) to detect its pose in 2D. As far as my understanding goes I should assign a weight to each particle based on its likelihood to be in that particle position with regards to the current measurement.</p>

<p>I also have the measure function to calculate an expected measurement for a particle, so basically I have, for each instant, the actual measurement and the measurement that a single particle would have generated if it were at the actual state. </p>

<p>Assuming all noises are Gaussian, how can I implement the weight function? I kind of noticed the <code>mvnpdf</code> function in MATLAB, but I can't actually find a way to apply it to my problem.</p>
","mobile-robot wheeled-robot particle-filter"
"6428","Sensor that will produce a sinusoid phase locked to a high RPM Shaft","<p>Is there a sensor that will produce a sinusoidal signal phase locked to a high RPM (7000 RPM) shaft? I am attempting to build a coaxial helicopter based on the architecture described in <a href=""http://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&amp;arnumber=6696528"" rel=""nofollow"">this paper</a> which requires increasing and decreasing drive torque once per revolution, and I would like to do this modulation in hardware.</p>
","sensors"
"6429","Grid mapping probability calculation algorithmic complexity","<p>I have stumbled upon an equation (<a href=""http://i.stack.imgur.com/hv64E.png"" rel=""nofollow"">http://i.stack.imgur.com/hv64E.png</a>), where the probability of an occupancy grid map cell is calculated. My teacher insists that it's possible to approximate the algorithmic complexity of this long equation, but I'm not so sure.</p>

<p>The description of the factors used in this equation are described <a href=""http://roboticsclub.org/redmine/projects/colonyscout/repository/revisions/master/raw/docs/Research/nps424A.pdf"" rel=""nofollow"">here</a> on page 11 (item 26). With keeping in mind that this calculates occupancy probabilities of a 2 dimensional array from sensor measurements, is it really possible to approximate the algorithmic complexity of actually calculating occupancy with this equation in BigO by just taking a look at it and not delving much deeper into the details? </p>
","algorithm probability occupancygrid"
"6430","How does a two-gear pull-back car toy work?","<p>This is not a robotics question, but this Stack Exchange is the closest I could find to mechanical engineering. Please refer me to a better place to ask this, if one exists. Hopefully someone might just know this.</p>

<hr>

<p>I got a pull-back car for my boy at McDonalds, and it has two gears. It starts slow, then speeds up after about two seconds. It's impressive to me, especially given the inherent cheapness of toys sold by McDonalds. It feels solidly built as well.</p>

<p>I couldn't find anything related to this concept. The <a href=""http://en.wikipedia.org/wiki/Pullback_motor"" rel=""nofollow"">wiki on pullback motors</a> does not include any information on multiple gears.</p>

<p>Any ideas on how this works? </p>
","mechanism"
"6434","differential robot yaw estimation using kalman filter","<p>Hello i am building a differential drive robot which is equipped with quadrature encoders on both of the motors. My aim is to be able to predict the heading (yaw angle) of the robot using a kalman filter. I am using an MPU 9150 imu. As of now m just interested in yaw angle and not the roll/pitch. As i understand, i will be needing the z-axis of gyro to be fused with the magnetometer data in order to properly estimate the heading angle. My problem is how do i implement the bias and covariance required for the kalman filter to work. Gyroscope would be my process and magnetometer data would be my update step yeah?. From the datasheet i have found the Angular random walk of my gyroscope to be 0.3 degrees/second for 10 Hz motion bandwidth and a constant bias of 20 degrees/second at room temperature. If i am not mistaken i should include the bias in my state prediction equation?. Also how do i get the covariance matrix Q. </p>
","kalman-filter imu"
"6436","Choosing a WiFi antenna for outdoor robot","<p>A time has come for my robot to get some more permanent computer than my laptop balanced on top of it.
I have selected a mini itx board that can be powered directly from battery and some components that go with it including a wifi card and now I'm thinking about the antenna I will need.</p>

<p>Constraints I have identified so far:</p>

<ul>
<li>The robot's body is a closed aluminium box, so I think this rules out keeping the antenna inside.</li>
<li>The robot is intended to work outdoors, so it needs to be waterproof.</li>
<li>Vibrations might be an issue.</li>
</ul>

<p>And the questions:</p>

<ul>
<li>What parameters should I watch when selecting an antenna?</li>
<li>Is it ok to use indoor stick antenna and seal the mounting point with hot glue?</li>
<li>Does it change anything if the antenna will be sticking out of largish sheet of alluminium?</li>
<li>The robot will also have GPS, is it possible that the two will interact badly under some circumstances?</li>
</ul>
","mobile-robot wifi"
"6437","How to provide power to a robot/raspberry pi?","<p>I'm building a robot and powering it with a Raspberry Pi. I am looking at <a href=""http://www.adafruit.com/products/1566"" rel=""nofollow"">this battery pack</a>, but I am flexible with which one to use.</p>

<p>My problem is that I need to be able to charge the robot while it is still on, and apparently that is not good for a single battery pack to be charging while being used (they seemed to say so in the video). Am I wrong? Otherwise, how could I go about charging the robot while keeping the Raspberry Pi running?</p>

<p><strong>EDIT</strong>: This is my first robot (other than my lego NXT kit), so I don't have any prior experience with robot batteries.</p>
","mobile-robot battery"
"6442","Quadcopter multiple ESC angles glitch","<p>I'm developing my fligth controller board on Tiva Launchpad for quadcoper and while calibrating PID I discovered an unexpected behaviour: sometimes quadcopter seems to experience random angle errors. While trying to investigate it, I've figured out that my code if fairly trying to compensate tham, as soon as they appear - but do not cause them. Even more - i've discovered that such behaviour appears only when two (or more) motors are adjusted, while one motor system shows pretty good stabilisation.</p>

<p>Here is code for PMW output for different motors:</p>

<pre><code>torque[0] = (int16_t)(+ angles_correction.pitch - angles_correction.roll) + torque_set;
torque[1] = (int16_t)(+ angles_correction.pitch + angles_correction.roll) + torque_set;
torque[2] = (int16_t)(- angles_correction.pitch + angles_correction.roll) + torque_set;
torque[3] = (int16_t)(- angles_correction.pitch - angles_correction.roll) + torque_set;
</code></pre>

<p>and here is recorded angles for system with one motor and two motors:
<img src=""http://i.stack.imgur.com/7CJIU.png"" alt=""enter image description here""></p>

<p>To be sure that it's not the algorithm problem, while recording this angles only Integral part of PID was non-zero, so angles were not even stabilised.</p>

<p>My question is - could esc noise each other (in my quad they are quite close to each other - just few sentimeters away) to cause such behaviour?
Thanks a lot!</p>
","quadcopter pid esc"
"6445","Ways of reversing motor direction easily","<p>My set up consists of a brushed motor (ex cordless drill type) connected to a motor controller which is in turn connected to a LIPO battery and an r/c receiver. All my cables are fitted with XT 60 connectors except for the cable that goes to the receiver which is a 3 wire pin (usual white, red and black).</p>

<p>The above set up is one of a pair which I am using in my battle robot. The motors are connected to drive wheels, left and right respectively. the problem is the motors are turning in opposing directions.</p>

<p>For some reason I neglected to switch the polarity of the wires of one  motor at the time I attached the XT 60 connectors and I really am not looking forward to re-soldering.  </p>

<p>So my question is whether there is any fast way of reversing the direction of rotation without soldering?  For instance can the R/C transmitter (a turnigy 9x without any modding)be programmed to switch up for down (hence forward for reverse)? </p>

<p>Or can I maybe switch the pin connector going into the receiver (I don't think so because the ground is probably common, but worth asking just in case I guess). </p>

<p>Any ideas or should I just get soldering?</p>
","motor"
"6446","How do you design Quadcopter PID algorithm?","<p>Just to give a bit a context, here are the equations I'm using for the Angular accelerations.</p>

<p>φ** =(1/Jx)τφ</p>

<p>and</p>

<p>θ** =(1/Jy)τθ</p>

<p>So my plant gains would be </p>

<p>φ**/τφ =(1/Jx) along x axis</p>

<p>and</p>

<p>θ**/τθ =(1/Jy) along y axis</p>

<p>The basic PID structure is </p>

<p>Gain=Kp(Desired-measured)+Ki(integral(Desired-measured))+Kd(Differential(Desired-measured)</p>

<p>Lets just say my plant gain for angular accl around x axis is φg and my PID gain is Pg. To obtain a controller, do I do</p>

<p>(φg)(Pg)=open loop gain=L</p>

<p>and for closed loop L/(1+L). </p>

<p>My question is, am I right about what I'm doing and do I upload the algorithm in time domain form or frequency domain form (Silly question as frequency domain is for analysis but my only control experience is purely theory and entirely focused on analysis using root locus and nyquist)</p>
","arduino control pid"
"6447","role of chi2 in SLAM back-end optimization","<p>All-most all SLAM back-end implementation compute chi2 estimates. Chi2 computation is usually used to compute the best-fitness score of model to the data.
How it is related to optimization framework for SLAM?</p>

<p>Rgds
Nitin</p>
","slam theory"
"6452","Fuses for mechanical systems","<p>** If there's a better stack exchange site for this, let me know--the mechanical engineering one is closed so I'm not sure where else is appropriate **</p>

<p>Do mechanical systems have fuses? </p>

<p>In an electrical system, like a charging circuit for example, we prevent large loads from damaging the system with fuses and circuit breakers. These will disconnect the inputs from the outputs if the load get too high. </p>

<p>Do mechanical systems have a similar mechanism? Something that would prevent gears or linkages from breaking when the load gets too high by disconnecting the input from the output?</p>
","mechanism"
"6459","Relative frame calculation","<p>what's the quickest way to calculate a relative coordinate of a frame, as shown in the code below. The Kuka robot language this "":"" is referred to as the geometric operator. 
I would like to perform this calculation in matlab, scilab, smathstudio or java, could you please advise on which library to use and/or how to proceed?</p>

<pre><code>Frame TableTop=[x1 y1 z1 a1 b1 c1]
Frame RelCoord=[x2 y2 z2 a2 b2 c2]

Frame AbsCoord= TableTop:RelCoord
</code></pre>
","programming-languages"
"6462","beaglebone black doesn't autoboot when powered over jack or vdd pins","<p>I have a BBB and it works quite well however when I power it over the Barrel connector or over the vdd pins rather then use the usb connection it doesn't automatically boot. When I put the barrel connector and usb in at the same time and then remove the usb it continues running. This is running on ubuntu arm. I have tested that the power supply is between 4.95 and 5.04 V and is capable of sustaining this to just over 1 Ampere</p>

<p>Edit it appear the BBB does boot when supplied with 4.7 V lab power supply at ~0.4A power consumption. So that suggest something is wrong with the power supply. But how do I test it seeing I was able to verify that it can supply 1A at 5V.</p>

<p>This power supply works by feeding a 12V battery into a step down to convert it down to 5V if that matters.
power-supply power linux beaglebone-black beagleboard</p>
","beagle-bone linux"
"6463","Single Touch Based Sensor and Odometry SLAM in Noisy Rectilinear Environment","<p>A Co-worker said he remembered a 2011(ish) ICRA (ish)  paper which used just a touch/bump sensor and odometry to do SLAM. I can't find it - I've paged thru the ICRA and IROS agendas and paper abstracts for 2010-2012, no joy. I found a couple of  interesting papers but my coworker says these aren't the ones he remembered.</p>

<p>Foxe 2012 <a href=""http://staffwww.dcs.shef.ac.uk/people/C.Fox/fox_icra12.pdf"" rel=""nofollow"">http://staffwww.dcs.shef.ac.uk/people/C.Fox/fox_icra12.pdf</a>
Tribelhorn a &amp; Dodds 2007:  <a href=""http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.507.5398&amp;rep=rep1&amp;type=pdf"" rel=""nofollow"">http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.507.5398&amp;rep=rep1&amp;type=pdf</a></p>

<p>Background - I'm trying to make a Lego Mindstorms bot navigate and map our house. I also have a IR sensor, but my experience with these is that they are really noisy, and at best can be considered an extended touch sensor.</p>

<p>Regards, 
 Winton</p>
","slam"
"6467","Help for Cracking Robotics Interview","<p>I am studying electronics engineering. I am fond of robots since my childhood. I have small doubt i.e. if I want to get placed in
robotics and automation based company ,what should I must study(reference books/softwares etc) perticularly for cracking an interview ? In simple words,as Electronics Engineer what other specific skills (like embedded C programming etc) should I go through?</p>
","control microcontroller electronics"
"6469","Is there a name for the steering style/wheel actuation used on Curiosity?","<p>I read up on the wheels of Curiosity, and also about the suspension. But is there a name for the steering? It looks similar in nature to the front landing gear on an airplane, but searching those terms didn't turn up and answer. I've attached a picture with the area of interest highlighted. </p>

<p>(Image: Gene Blevins/Reuters)</p>

<p><img src=""http://i.stack.imgur.com/hD8ca.jpg"" alt=""enter image description here""></p>
","wheeled-robot"
"6471","How to connect HC-sr04 ultrasonic sensor to APM 2.6?","<p>I am working on my final project by autonomous Quadcopter. my tasks are to make a quadcopter  which should do object avoidance and it should auto land using ultrasonic sensors.
any possible ans to it, how should i connect HC-sr04 ultrasonic sensor to my APM 2.6 board?
Even APm 2.6 has a port I2C.</p>
","quadcopter"
"6475","what is the price of Pioneer P3DX","<p>I wanna know how much does <a href=""http://mobilerobots.com/ResearchRobots/PioneerP3DX.aspx"" rel=""nofollow""> Pioneer P3DX </a>cost? nothing mentioned in the website and I don't want to fill out the form regarding this matter. </p>
","mobile-robot"
"6476","How to run custom hardware from a laptop","<p>I am looking to build some custom hardware (nothing too fancy, just some motors, cameras and the like), and I need it to be controlled by my laptop (its going to have to do a non-trivial amount of image processing).</p>

<p>Is there a way to attach $n$ motors to a laptop where $n&lt;10$ via USB/e-SATA? It seems like something that should be very easy to solve, but I can't seem to find it anywhere.</p>

<p>I am not looking to get an Arduino/Raspberry Pi, really just connect the motors, and be able to control them individually. I am comfortable adding more power from a second source to supplement the USB power.</p>

<p>Ideas?</p>
","motor usb"
"6478","Rocker bogie suspension system - pitch angle","<p>What does this sentence mean :</p>

<p>""The chassis maintains the average pitch angle of both rockers.""</p>

<p>Put in other words, "" the pitching angle of the chassis is the average of the pitch angles of the two rocker arms"" </p>

<p>What is a pitching angle in this context? 
Please explain both pitching angles.</p>
","mobile-robot wheeled-robot"
"6488","Creating an xbox remote that replaces spectrum dx3c or dx3e without wifi","<p>Hello I'm a new rc enthusiast, </p>

<p>Is anyone interested in rc's controlled through xbox remotes? The project is to use an xbox one or xbox 360 remote to either hijack a dx3e or dx3c remote or create a transmitter compatible with the spectrum receiver out of the xbox remotes. I've seen applications that use wifi but I'm not sure thats the route I'm looking for. From what I've read there is limited range and signal loss through the wifi network plus It may create a lag larger than what would be desirable in racing. The rc is a losi scte short course race truck. I'm not to savvy with electronic jargon but will study and learn what I can. Thanks for your thoughts.  </p>
","brushless-motor"
"6493","2 wheeled, 2 motor robot control","<p>I decided to work on a 2 wheeled robot position mapping problem. For that I have 2 DC motors with encoders from pololu.
I have the following two questions:</p>

<ol>
<li><p>Do I need to know the model of each motor in order to tune a controller?</p></li>
<li><p>What are the steps/approach used to get a good mapping (let's say 1 mm maximum error)?</p></li>
</ol>
","pid wheeled-robot odometry"
"6495","How can I send a jpeg image to a microcontroller via USART?","<p>How can I send a jpeg image to a microcontroller via USART?</p>
","electronics embedded-systems"
"6500","DH Matrix to homogeneous transformation matrix for each joint","<p>Given a DH matrix for a set of joints, how would you convert the data into homogeneous transformation matrices for each joint? I've looked online, but can't find a good tutorial.</p>
","kinematics joint dh-parameters"
"6508","Low power computer for stereo vision","<p>I would like to build a motorized robot with stereo vision that sends visual input to a PC over wifi for processing. A Raspberry Pi would have been perfect for the task if it would be possible to connect 2 cameras. The Pi 2 would be powerful enough for two USB webcams (with minimum 8fps) I guess, but the shared USB seems to be a bottleneck (2 cameras + wifi dongle).</p>

<p>What other options are there to send input from 2 cameras and control a motor (or an arduino)?</p>
","computer-vision cameras"
"6510","Multiple PIDs in quadcopter","<p>I am wondering what the use is of two PID loops to control a quadcopter. One PID for stability control and another PID for rate control.</p>

<p>Why can't you just use one PID per axis to control a quadcopter where you take the current and desired angle as input and motor power as the output?</p>
","quadcopter pid"
"6511","What type of servos are used in industrial robot arms like Universal Robot UR5?","<p>I've noticed that the industrial robot arms have very smooth, fast, and strong movement.  Does anyone know what type of servos they use.  I'm trying to build my own and don't want to have the jerky movement that is seen in a lot of DIY robot arms?  Thanks. </p>
","robotic-arm servos"
"6515","Computer vision with single camera vs. distance sensors for obstacle detection","<p>I am going to start a new project consisting in implementing an autonomous driving RC car.
The car as it is now, has a camera installed on each side of the car, i.e. 4 cameras in total. They are connected to a board which is able to read and process the video input.</p>

<p>I have been researching about obstacle detection using a single camera (without stereo cameras, e.g. <a href=""http://robotics.stackexchange.com/questions/1348/single-camera-vision-and-mapping-system"">Single camera vision and mapping system</a> ) and although it seems possible it also seems quite complex. Modifying the cameras set-up is not an option. I already have some video processing algorithms, like dense optical flow, which might help me, but I am not sure whether I might me able to implement the system in the time I have (4 months). I also don't know how reliable would be the final solution.</p>

<p>If the first approach is not feasible, as an alternative option I also could install distance sensors in the car to detect obstacles. It seems that usually the most preferred choice is to use ultrasonic sensors. I would need to install them and I would not take advantage of the cameras, but it seems that the final complexity would be lower.</p>

<p>Is the first approach feasible? What are the pros and cons of each approach? If I implemented the second option, how many sensor would I need?</p>
","sensors computer-vision ultrasonic-sensors"
"6516","Stallable motor - 100% duty cycle higher torque motor that can be stalled without burning up","<p>I have a project where I need a motor that can turn some number of rotations which will spool up a cable attached to a spring-closed device to open it up. When power is disconnected, the spring closure will cause the spool to unwind and the device to close.</p>

<p>In the closed position, no power is available. (i.e. The closure mechanism needs to be 100% passive.)</p>

<p>In order to keep this open for some time, I will need a motor that is capable of being stalled for long periods without having the windings burn up. I know some motors can do this, such as the motors they use on spring closed HVAC dampers, but I don't know how to find them or if there's a particular name I should be using to find them. I know I could probably do this with a stepper motor, but that seems overkill for the application.</p>

<p>The only requirements are higher torque to open this mechanism, no gearing that prevents the motor from spinning when power is disconnected, and the ability to be stalled.</p>
","motor"
"6522","PID controller for trajectory with mutliple setpoints","<p>We implemented a PID controller for our quadcopter which allows us to fly from point A to B. The precise position of the quadcopter is measured using an external tracking system.</p>

<p>Now that we can fly from A to B, we would like to implement a controller to fly more complex trajectories with multiple set points. E.g. from A to B to C or flying in a circle using sample points.</p>

<p>We tried to use our regular PID controller for this but this of course doesn't work well since the PID controller forces the quadcopter to stabalize at any set point. We would like to have a controller that allows the quadcopter to fly a trajectory fairly smoothly. I think this has to be a controller that takes into account multiple set points in the trajectory at the same time so that it can already slow down/speed up based on the trajectory that is ahead.</p>

<p>Can someone point me to some controllers / algorithms that I can look at to realize this? Do I need a completely different controller to do this or will it be an adapted version of the PID controller that I have now?</p>
","quadcopter pid"
"6523","Building an open loop controller for a simple DC motor position problem","<p>In order to identify the dynamics of my DC motor, I am trying to command it with Xcos using the Arduino tool box. The problem that I am facing is how to give the motor an input command such that I get some given angle position as output. I can only control the input voltage to the motor via PWM.</p>

<p>I have been thinking about converting the angle to voltage but I can't figure it out.
Can somebody help me?</p>
","arduino motor control"
"6526","3-axis Magnetometer Question","<p>Apologies if this is a stupid question, but if I have a 3-axis magnetometer, and I calculate the vector magnitude as</p>

<pre><code>sqrt(magX * magX + magY * magY + magZ * magZ)
</code></pre>

<p>...then should I not always get the same value, regardless of the sensor's orientation? Mine is all over the place, and I feel as though I'm missing something obvious.</p>
","imu magnetometer"
"6531","Using hobby servo as axle","<p>I am designing a pan-tilt camera mount using standard hobby servos.    Many existing designs use the servo shaft as a revolute joint, as opposed to simply a torque producing element.  As a revolute joint the servo mechanism is subject to different torques and forces.  Is using a servo shaft as a revolute joint recommended practice or should a bearing be used?</p>
","rcservo"
"6532","What is the scope of Electronics Engineering in Robotics/Automation?","<p>I am beginner in Robotics .I have taken admission for Electronics engineering one year back as we don't have specifically Robotics Engineering Branch in my Country.Now I am suffering from questions like what is the scope of Electronics( <strong>not Electrical</strong>) Engineering in Robotics/ Automation?I am <strong>unable to distinguish between the role of Electronics engineer and Computer Engineer</strong> in Robotics <strong>as in both cases programming is required</strong></p>

<p><strong>Also,if I don't like to do programming(coding),are there any other options to stick to Robotics / Automation field as per my branch(Electronics Engineering ) is concerned?</strong>. </p>
","control electronics embedded-systems"
"6536","Motor for a hydraulic pump in a hydraulic system","<p>I'm not entirely sure if this is the right area to post this question, but looking at the other subjects on StackExchange, this seems to be the best fit.</p>

<p>I am a complete beginner to hydraulic systems, and I've wanted to learn more about this area. I'm designing a hydraulic system that involves using hydraulics to push/pull objects using pistons. I have looked at what the basic requirements are for a hydraulic system, but there is one thing that escapes me.</p>

<p>I come from an electronic background, and I noticed that the hydraulic pumps (for example, this one) seem to lack a motor to drive the fluid. Am I wrong? If not, I've been looking everywhere for a motor that can/should be attached to said pump, but I cannot seem to find anywhere that sells them. Is it just a simple DC motor (with correct specs), or should there be a specific motor designed for hydraulic pumps?</p>

<p>Looking around, I came across this, but looking through the specs, I don't see a power requirement, and being used to seeing power consumption in datasheets, I'm not even sure it is a motor!</p>
","motor"
"6541","BeagleBone - PRU questions","<p>I will be using at least one programmable real-time unit (PRU) to send pulses to a stepper motor driver but before I begin, I am trying to lay out the structure of my programs.</p>

<p>I am using this library <a href=""http://processors.wiki.ti.com/index.php/PRU_Linux_Application_Loader_API_Guide#prussdrv_map_prumem"" rel=""nofollow"">PRU Linux API</a> 
for loading assembly code into the PRU instruction memory but there doesn't seem to be much documentation other then whats at that wiki and the source:
<a href=""https://github.com/beagleboard/am335x_pru_package/tree/master/pru_sw"" rel=""nofollow"">github-pru-package</a>h</p>

<p>My c program will be calculating the position of the sun using an algorithm and executing the assembly/writing a pulse count to the PRU(s) data memory so they can just switch on/off a gpio at my desired frequency and for the number of pulses required to turn a stepper the appropriate number of steps. I am not even sure if this is an acceptable method but I am pretty new at this and it seems like a simple way to accomplish my task</p>

<p>My Questions regarding the library functions are:</p>

<ol>
<li>Is there a significant performance difference between using <code>prussdrv_map_prumem</code> or <code>prussdrv_pru_write_memory</code> to give the PRU(s) access to the pulse count?</li>
<li>Would it be better halt the PRU assembly program after has completed the tasks for each pulse count then re-execute it with new values, or keep the PRU program running and poll for a new pulse count to be written in?</li>
</ol>

<p>I plan to send a pulse count every 10 seconds or so.</p>

<p>Any suggestions on revisiting the whole structure and logic are welcome as well.</p>
","c beagle-bone"
"6544","What's the smallest rotation a brush-less motor can perform?","<p>For example, I have a brush-less outrunner with 14 poles and 12 stator windings. It has three connectors. Can this motor be controlled in a way that it performs a single step of 30 degrees (360/12)?</p>
","motor brushless-motor"
"6553","Create Artificial Integelent Robot Ability to communicate with Computer : Which language i should use","<p>I know some languages like PHP, C/C++ and Java but I'm not expert in these languages. I want to create an Artificial Intelligent Robot that can do these Task;</p>

<ol>
<li>able to communicate with Computer (USB, Bluetooth or other)</li>
<li>able to perform some specific task</li>
<li>present a  Visual interface (finding path, speed and others)</li>
<li>Access its Micro Controller device and attached devices</li>
<li>and so on..  (editor note: solve world hunger?) </li>
</ol>

<p>Can any one please suggest which programming language will be good for programing this type of robot. I have heard about C/C++ and Assembler and ROBOTC and LABVIEW but I am unable to decided which language to use for my project.</p>

<p>Sorry for my bad English!</p>
","microcontroller artificial-intelligence robotc"
"6560","Don't understand how sensor works, metal wired directly to I/O pin of arduino","<p>I'm trying to understand how an electronic musical instrument (called an e-chanter) works (imagine a recorder or other wind instrument, but with the holes replaced with metal contacts, and the sound played electronically, so no air is needed).</p>

<p>Basically, there are several metal contacts, as shown in this link: <a href=""http://www.echanter.com/home/howto-build#TOC-WIRES-SCREW-SENSORS"" rel=""nofollow"">http://www.echanter.com/home/howto-build#TOC-WIRES-SCREW-SENSORS</a></p>

<p>They each appear to be wired only directly to one pin of the arduino:</p>

<p><a href=""http://www.echanter.com/_/rsrc/1329242710187/home/howto-build/circuitdiagram.png"" rel=""nofollow""><img src=""http://www.echanter.com/_/rsrc/1329242710187/home/howto-build/circuitdiagram.png"" alt=""E-chanter circuit diagram""></a></p>

<p>I can't figure out for the life of me how this works.  Can anyone explain it, are the fingers being used as some kind of ground or what on earth is going on.  I have a physics background so can understand some technical info, but just can't fathom how on earth this magic works.</p>

<p>Thank you very much</p>
","arduino sensors"
"6564","Jacobian Matrix of 6DOF Body (with IMU)","<p>I am trying to derive the analytical Jacobian for a system that is essentially the equations of motion of a body (6 degrees of freedom) with gyro and accelerometer measurements.
This is part of an Extended Kalman Filter.</p>

<p>The system state is given by:
$
\mathbf{x} = \left(
\begin{array}{c}
\mathbf{q}\\
\mathbf{b_\omega}\\
\mathbf{v}\\
\mathbf{b_a}\\
\mathbf{p}\\
\end{array}
\right)
$</p>

<p>where $q$ is the quaternion orientation of the body expressed in the global frame, $b_\omega$ and $b_a$ are the biases in the gyro and accelerometer respectively (expressed in the body frame) and $v$ and $p$ are the velocity and position of the body expressed in the global frame. All vectors are [3x1] except $q$ which is [4x1] in $[w,x,y,z]^\top$ format, and $R$ (below) which is [3x3].</p>

<p>The equations of motion $\frac{dx}{dt}=\dot{x}$ (t is time) are:
$$
\dot{\mathbf{q}} = \frac{1}{2}\mathbf{q} \otimes 
\left(
\begin{array}{c}
0\\
\hat{\omega}\\
\end{array}
\right) \\
\dot{\mathbf{b_\omega}} = 0 \\
\dot{\mathbf{v}} = R^\top (\hat{\mathbf{a}} + [\hat{\mathbf{\omega}}\times]R \mathbf{v})+ g \\
\dot{\mathbf{b_a}} = 0 \\
\dot{\mathbf{p}} = \mathbf{v}
$$
Second-order terms are ignored. $\hat{a} = a - b_a$ and $\hat{\omega} = \omega - b_\omega$ are the corrected accelerometer and gyro biases ($a$ and $\omega$ are known) and are expressed in the body frame. $R$ is the rotation matrix (DCM) formed from $q$ and $g$ is the gravity vector $[0,0,9.81]^\top$. These equations have been validated against an aerospace engineering software library.</p>

<p>I need the jacobian $F = \frac{d\dot{x}}{dx}$ but I cannot find this jacobian in any texts (I do find the error-state jacobian eg <a href=""http://www-users.cs.umn.edu/~stergios/papers/IROS07-IMUCam.pdf"" rel=""nofollow"">this paper</a>).
I am struggling with doing this myself because I don't know how to handle the quaternion norm constraints. I also am concerned about the validity of a solution given through numerical differentiation.</p>

<p>Any help or explanation would be greatly appreciated. This is going towards an open-source robot localisation project.</p>
","control kalman-filter jacobian"
"6565","Communicating between a BeagleBone Black and a servo controller","<p>I am a complete newbie and recently joined a robot team at my school in order to gain some experience. I have been assigned a task of driving a servo using a Pololu Mini Maestro USB Servo Controller. I am using the BeagleBone Black (BBB) with the Python adafruit library. How do I make the BBB communicate with the Servo Controller? If you guys could point me in the right direction, I'd really appreciate that. Right now, I don't even know where to start. Not sure if it matters, but this is the servo I am using: <a href=""https://www.pololu.com/product/1053"" rel=""nofollow"">https://www.pololu.com/product/1053</a></p>
","motor microcontroller"
"6566","Robot Navigation Feedback using Image Processing","<p>In my project, I've successfully analyzed the arena and have detected the obstacles using an overhead webcam. I also have computed the shortest path. The path data is transmitted to the robot via Zigbee, based on which it moves to its destination.</p>

<p>The problem is: My robot is not taking accurate turns which would cause error in the rest of path it follows.</p>

<p>Could anyone please suggest any methods/techniques for feedback from the robot so as the path is corrected and the robot follows the original path computed without any deviation? (Basically a tracking mechanism to avoid deviation from the original computed path)</p>
","computer-vision navigation motion-planning"
"6570","Fusion of GNSS position data and prefused 9-dof AHRS data","<p>Bosch, FreeScale, InvenSense, ST and maybe others are releasing 9-dof AHRS platforms containing their own fusion software and outputting <strong>filtered/sane/fused</strong> data (attitude as quaternion and linear acceleration).</p>

<p>I would like to use these for the quality of their respective company fusion algorithm. And would like to merge GNSS position and velocity data to it.</p>

<p>I have found multiple examples of heavy (> 20) states Kalman filters merging <strong>raw</strong> 9-dof IMU data and GNSS position/velocity.</p>

<p>But I have a hard time finding a <strong>computationally lighter</strong> version of GPS+AHRS fusion as these new 9-dof AHRS already fuse IMU raw data themselves and this process should'nt be done twice.</p>

<p>Would you maybe have pointers on the algorithm(s) or type of filter to use ? Thank you.</p>
","quadcopter kalman-filter imu gps sensor-fusion"
"6571","Kalman Filter with incremental encoder + optical mice","<p>Currently I am building a robots with 2 incremental encoders with a optical mice sensor. The reason to install a optical mice sensor is to provide better feedback when slippage happen on the encoders.</p>

<p>I wonder if I could apply a kalman filter to get a better distance feedback with these 2 kinds of sensors? Especially when the control input is unknown?(For example I push the car with my hand, but not applying a voltage to the motors)</p>

<p>I have read some examples to use kalman filter (gyro+accel / encoder+gps), either one of the variable used is in absolute measurement, while in my case, two feedbacks are dead-reckoning. </p>

<p>Any help is appreciated =] !!!!!</p>
","kalman-filter quadrature-encoder"
"6572","What kind of linear actuator can be used for very small forces over relatively long distances","<p>What kind of linear actuator can be used for very small forces (pushing ~0,1 kg over a flat, smooth surface) in steps of ~50mm over a total length of about 250mm? I only seem to find actuators that are very large and are overpowered.</p>

<p>extra details:
speed requirements: >5mm/s
duty cycle: 12 times in half an hour, once per day
It must be compact and not much longer than 350mm when concealed
price must be under $100</p>
","actuator"
"6575","How to distribute tension load on a three footed claw?","<p>Good day!  </p>

<p>I am helping my little ones 6 and 7 to develop a robot that can pick up and stack cubes three high as well as gather blocks.  They came up with the design that enables them to pick up three cubes at a time when lined up and then pull up the claw, turn, drive to another cube, place the cubes on the stand still cube and release.<br>
   Well they got the claw made with two rods connected to gears to motor and the rods reinforced, then they made insect like legs 3 - pairs of two on the rods, with gripper feet pads on the ends of the legs. All of this works as it opens and closes! The problem is that when they try to close the claw on the cubes and pick up all three cubes, the first - closest to motor feet have a nice tight grip, the second - middle feet - have a lighter grip and just barely can lift the block, and third - farthest from motor doesn't even have a grip on the blocks.<br>
   I think it's because the second and third set of feet are farther from the motor. But how can they evenly disperse the tension load so the claw can pick up all three blocks?
I tried putting elastics on the feet for better grip and unless we put ten on each foot for the third set and maybe five on the second set it wont work.  Even though it's a quick fix I would like to help them figure it out the proper way of spreading the load so to speak.  We also tried putting a small band on the third set of legs.  The robot could still open and close and that worked for the third set but not the second.  We tried putting a band on the second and third but the legs wouldn't open anymore.  I could use a lighter band but is there another way? We only have one little motor to run it so we can't give all the leg sets it's own motor and even if we did there would be weight issues.<br>
  THank you in advance!</p>
","design mechanism vex"
"6576","Robotics jargon question: How to conjugate 'teach'?","<p>As a non-native speaker I have a (maybe trivial, but to me not clear) question concerning the verb 'to teach'. Of course, from school (and online dictionaries) I know the past tense of 'teach' is 'taught' not 'teached'. But in robotics 'to teach' has a special meaning (like: 'to make special ponts/orientations known to the (arm-)robot', e.g. by guiding the robot to those points/orientations.) </p>

<p>Does it make sense to have a different past tense for 'teach' (i.e. 'teached') in this case ? Maybe a reference were it is used/explained ?</p>

<p>(I would say 'No. The past of teach is taught, and that's it.', but some of my colleagues - also not native speakers - have a different opinion.)</p>
","reference-request"
"6581","Stereo camera Vs Kinect","<p>Any one can advice me on the ideal perception sensors for pick and place application using a robotic manipulator with ROS support. I have generally looked at things like Kinect and stereo cameras (Bumblebee2) which provide depth that can be used with PCL for object recognition and gripper positioning. Are there any other sensors would be preferred for such application and if not what are the drawbacks of stereo cameras in comparison to Kinect or other sensing capability. </p>

<p>Thanks,</p>

<p>Alan</p>
","sensors robotic-arm kinect stereo-vision"
"6583","Is our laptop wifi single channel or multiple channel? This is for controlling a bot","<p>We are building a hobby drone(Quad-coptor) with a camera for footage. So to control the quad, i have been suggested(<a href=""http://www.rchelicopterfun.com/helicopter-radios.html"" rel=""nofollow"">Web</a>,<a href=""http://blog.oscarliang.net/choose-rc-transmitter-quadcopter/"" rel=""nofollow"">Here</a>) to use minimum of four channels.For power and for turning etc., So which means i need one channel for every separate task to be done. For eg., if i want to rotate the camera, then i suppose i need the 5th channel and so on..</p>

<p>Now my question would be - i have seen a lot of drones(ardrone, walkera) which are controlled by an Android or an iPhone app. So the wifi used to connect the drones is, single channel or multi-channel? If single channel then how can i control different tasks like to control turning of the quad or camera in different axis?</p>

<p>Also if i want the GPS location from the quad, do i require to have another transmitter?</p>

<p>I am using(Planned to) a Raspberry Pi 2, OpenPilot-CC3D for flight control.</p>

<p>P.S this is my first drone, Kindly show some mercy if i ask/don't understand your comments.</p>
","quadcopter navigation radio-control wifi"
"6586","Denavit-Hartenberg convention or the product of exponentials formulation, when dealing with the manipulator kinematics?","<p>Though, Denavit-Hartenberg notation is commonly used to describe the kinematics of robot manipulator, some researcher prefer the product of exponential instead; and even the claim that it's better.</p>

<p>Which one should I use, and which one is generally better; is final solution same for both kinematics and dynamics?</p>

<p>Any suggestions?
<a href=""https://www.researchgate.net/go.Deref.html?url=http%3A%2F%2Fwww.cds.caltech.edu%2F%7Emurray%2Fbooks%2FMLS%2Fpdf%2Fmls94-complete.pdf"" rel=""nofollow"">A mathematical introduction to robotic manipulation</a></p>
","robotic-arm industrial-robot dh-parameters product-of-exponentials"
"6587","FRC RoboRio Eclipse can't find edu.wpi.first.wpilibj","<p>The first time after importing a project into the Eclipse workspace we find that eclipse cannot find the WPILIBj.</p>

<p>On any import line:<code>import edu.wpi.first.wpilibj.*</code> Eclipse says ""unresolved import edu.</p>
","microcontroller wheeled-robot"
"6590","How do quadcopters turn left and right?","<p>In this picture, a sketch of a quadcopter is displayed with rotor's direction of motion.  The magnitude of the rotational velocity is depicted by the thickness of the lines (thicker lines are higher velocity, thinner lines are lower velocity).</p>

<p><img src=""http://i.stack.imgur.com/WomRo.png"" alt=""enter image description here""></p>

<p>I'm told this is the correct way to produce turning motion, but my intuition (which is usually wrong) tells me that the two pictures should be reversed.  My argument is  as follows:  For the picture on the left, the two rotors of higher velocity are spinning clockwise.  If the motion of the rotors of greater velocity are clockwise, shouldn't the quadcopter also rotate clockwise?  What am I missing here?</p>
","quadcopter dynamics"
"6592","Can I use a PWM HHO Controller to control a brushless DC motor?","<p>I'm an electronics newbie.</p>

<p>The part I'm talking about is <a href=""http://www.ebay.in/itm/221636836855?ssPageName=STRK:MEWNX:IT&amp;_trksid=p3984.m1439.l2649"" rel=""nofollow"">this</a>:</p>

<pre><code>DC 10-50v 12V 24V 48V 3000W 60A amps DC Motor Speed Control PWM HHO Controller
</code></pre>

<p>My question is, can this type of part be used to control brushless DC motors?</p>
","arduino brushless-motor"
"6593","Guidance & Info about Quadcopter Project","<p>I'm new in forum and since I made some research the past days I'd like to get some guidance about constructing &amp; programming a Quadcopter from scratch since I'm completely new on a project like that.</p>

<p>Quadcopter Frame: Thinking about to construct an aluminum 70cm diameter frame which will weight around 500g. What kind of motors should I get in order the frame with the board,motors etc. will be able to lift?</p>

<p>Board: I'm thinking to use Arduino Uno or Raspberry Pi 2.0 ( With a little bit of research I made I conclude that Raspberry could make my life a little bit easier since you can add wifi on it. The quadcopter will be controlled via a pc/laptop through wifi). What can you suggest and why?</p>

<p>ESC: As far as I've seen in most of similar projects people using ESCs in order to control the motors throttle. Can you avoid that, with programming PIDs that make the same job in order not to use more hardware?</p>

<p>About PIDs and Code in General: Thinking about to simulate the whole project in Simulik, Matlab and somehow (if it's possible) to convert the Matlab Code into C++ and download it on the chip. What do you think about that?</p>

<p>About the whole project: I'm trying to minmize the hardware as much as it's possible (use only 4x motors, the board with the chip on it, cables and probably some sensors) in order to minimize the total weight of construction and ofc the price.</p>

<p>That's all for a start. I'm gladly waiting for your answers and ideas.</p>
","quadcopter"
"6596","Suggestions for beginer in Robotics","<p>I am beginner in Robotics.I want to make serious start from scratch with interest but confused from where to start.So can anyone give some suggestions for 1.As a beginner in robotics ,<strong>are there some simple and basic robots or circuit
designs which I can make by myself in the home</strong>(so that I can gain practical knowledge of robots)? 2.or should I first read books (can anyone suggest some good reference  <strong>book names,articles ,links,free on-line  video lecture series</strong>)?</p>
","beginner"
"6597","Need of Kalman filters in unimodal measurement model","<p>I have recently been studying Kalman filters. I was wondering that if sensor model of a robot gives a <strong>unimodal Gaussian</strong> ( as is assumed for LKF) and the environment is pre-mapped, then the sensor reading can be completely trusted( ie. max value of Kalman gain), removing the need for odometry for localization or target tracking purposes and hence the need for the Kalman filter. Please Clarify.</p>
","kalman-filter"
"6599","How to get the 4*4 matrix from the twist using product of exponentials, in robot kinematics?","<p>In robot kinematics, we have $e^{(\theta*twist)}$, where $twist$ is 6*1 vector.  How do I get the 4 by 4 transformation matrix by using product of exponentials?</p>
","robotic-arm kinematics forward-kinematics product-of-exponentials"
"6601","Help with code that is supposed to drive a servo controller","<p>I posted a similar question before as I was just getting started with the project but I wasn't specific enough so got a weak response from the SE community. But now I am at a point where I have Python code which is supposed to rotate a servo through Pololu's Maestro Serial servo controller (<a href=""https://www.pololu.com/product/207"" rel=""nofollow"">https://www.pololu.com/product/207</a>). Based on the ""Serial Interface"" section of the user's guide (<a href=""https://www.pololu.com/product/207/resources"" rel=""nofollow"">https://www.pololu.com/product/207/resources</a>), I sent a sequence of numbers starting with decimal 170 and 12, which are the ""first command byte"" and the ""device number data byte"", respectively. The user guide says that 12 is the default device number, so I tried changing it to 18 because that's how many servos my servo controller can drive. But that doesn't make much difference because the servo doesn't rotate at all. The numbers after that are the same as the example from the user's guide. I am not sure what the 4, 112 and 46 are doing, but the 0 targets the servo port ""0"" on the servo controller (the port to which my servo is connected). The servo doesn't move, regardless of what sequence of numbers I put in. I have very little experience, so if you guys could point me in the right direction or at least point to some useful resources on the web, I'd be very grateful.</p>

<pre><code>import serial
import struct
import time
import Adafruit_BBIO.UART as UART

drive_Motor_Port = serial.Serial(port = ""/dev/ttyO1"", baudrate=9600)
drive_Motor_Port.close()
drive_Motor_Port.open()

drive_Motor_Port.write(chr(170)); # User guide says we must start with OxAA = 170
drive_Motor_Port.write(chr(18)); # Device number
drive_Motor_Port.write(chr(04));
drive_Motor_Port.write(chr(00));
drive_Motor_Port.write(chr(112));
drive_Motor_Port.write(chr(46));

time.sleep(5);
</code></pre>
","python"
"6604","choose ZigBee modules for full wireless mesh","<p>What is the most common <strong>ZigBee IP Modules</strong> to create <strong>full wireless mesh</strong> mode? 
I know that it should be <em>1) coordinator and 2) router</em> to <strong>create full-mesh</strong></p>

<p>But I am interesting about what kind of modules it would be better to buy by <strong>comparing price, quality and tutorial material.</strong> It would be graet if you know some ZigBee modules based on ARM(Cortex) or Atmel MCUs and if you have some additional <strong>tutorial materials to control those modules</strong> and  understand it.</p>

<p>I am looking for ZigBee modules only, but <em>NOT XBee</em>!!!
because they have difference in organization of network. 
Ex.: 
<strong><em>First of all,</em></strong> ZigBee Mesh can create own zones to control devices. 
but in <em>Xbee Mesh</em> which from Digi Comany can create only full mesh and only one big or huge zone to control devices. </p>

<p><strong>Secondly,</strong> <em>ZigBee</em> modules - AES encryption. Can lock down network and prevent other nodes from joining.<br>
<em>Xbee</em> - AES encryption.** and it is coming soon...</p>
","mobile-robot microcontroller radio-control communication wireless"
"6606","Cheap wheeled robot without tether (does not need to be programable)","<p>I am looking for <strong>cheap</strong> wheeled robot that can be controlled remotely. I do not really care about how (RF, Bluetooth, WiFi, IR, other?), as long as I can control around 10 robots without interference in a small arena (they are always in the line of sight).</p>

<p>I would like to emphasize that I do not need them to be programmable and it is important that they are cheap.</p>
","control wheeled-robot"
"6607","Best practice to write a ROS service for a serial-communication class with many options","<p>I have been asked to write code to implement serial communications with a camera in order to control its pedestal (movable base) as well as set a few dozen other camera options. The catch is that I have to make it usable by ROS. </p>

<p>What would be the best practice to implement this functionality in ROS? I understand  the concept of services, but I think that there should be a better way than creating a different service/file for each option.</p>

<p>Thanks,
Daniel.</p>
","ros cameras serial"
"6613","Multiple limbs on small robots","<p>Note: I'm a firmware developer experienced with sensors and networks, but not much with motors.</p>

<p>I am trying to build a small hobby robot, like a cat-sized spider. I am thinking of using servo motors with position control, so I don't have to use encoders to know where the motor is. Assuming six legs (I know, spiders have eight), with each leg being able to move up-down and left-right, that already translates to 12 motors. If you want to bend a knee, that gets the number to 18.</p>

<p>18 motors on such a small robot is overkill, isn't it?</p>

<p>I have thought of a couple of ideas, but not having a strong mechanical background, I cannot tell whether they are doable/sane.</p>

<p>One of my ideas is to use a magnet on the end of the limb (the end inside the chassis) and a small permanent magnet above it. The magnets attract each other and this keeps the limb firm under the weight of the robot. A stronger controllable magnet (a coil) would attract the limb even more and let it lift in the air. The following drawing may help:</p>

<p><img src=""http://i.stack.imgur.com/XFdlT.png"" alt=""magnet to control limb motion""></p>

<p>This would allow the up-down movement of the leg, and a servo would control its left-right movement. However, I fear that such a system would not be strong enough to hold under the weight of the robot, or whether a reasonable coil would be compact enough.</p>

<p>In short, my question is, how can I control six legs each with two or three degrees of freedom with a reasonable number of motors? Is having one motor per degree of freedom the only possibility?</p>
","mobile-robot servomotor"
"6617","Jacobian-based trajectory following","<p>I would like to control my 7 DOF robot arm to move along a Cartesian trajectory in the world frame.  I can do this just fine for translation, but I am struggling on how to implement something similar for rotation.  So far, all my attempts seem to go unstable.  </p>

<p>The trajectory is described as a translational and rotational velocity, plus a distance and/or timeout stopping criteria.  Basically, I want the end-effector to move a short distance relative to its current location.  Because of numerical errors, controller errors, compliance, etc, the arm won't be exactly where you wanted it from the previous iteration.  So I don't simply do $J^{-1}v_e$.  Instead, I store the pose of the end-effector at the start, then at every iteration I compute where the end-effector <em>should</em> be at the current time, take the difference between that and the current location, then feed that into the Jacobian.</p>

<p>I'll first describe my translation implementation.  Here is some pseudo OpenRave Python:</p>

<pre><code># velocity_transform specified in m/s as relative motion
def move(velocity_transform):
  t_start = time.time()
  pose_start = effector.GetTransform()
  while True:
    t_now = time.time()
    t_elapsed = t_now - t_start
    pose_current = effector.GetTransform()
    translation_target = pose_start[:3,3] + velocity_transform[:3,3] * t_elapsed
    v_trans = translation_target - pose_current[:3,3]
    vels = J_plus.dot(v_trans) # some math simplified here
</code></pre>

<p>The rotation is a little trickier.  To determine the desired rotation at the current time, i use Spherical Linear Interpolation (SLERP).  OpenRave provides a quatSlerp() function which I use.  (It requires conversion into quaternions, but it seems to work).  Then I calculate the relative rotation between the current pose and the target rotation.  Finally, I convert to Euler angles which is what I must pass into my AngularVelocityJacobian.  Here is the pseudo code for it.  These lines are inside the while loop:</p>

<pre><code>rot_t1 = np.dot(pose_start[:3,:3], velocity_transform[:3,:3]) # desired rotation of end-effector 1 second from start
quat_start = quatFromRotationMatrix(pose_start) # start pose as quaternion
quat_t1 = quatFromRotationMatrix(rot_t1) # rot_t1 as quaternion

# use SLERP to compute proper rotation at this time
quat_target = quatSlerp(quat_start, quat_t1, t_elapsed) # world_to_target
rot_target = rotationMatrixFromQuat(quat_target) # world_to_target
v_rot = np.dot(np.linalg.inv(pose_current[:3,:3]), rot_target) # current_to_target
v_euler = eulerFromRotationMatrix(v_rot) # get rotation about world axes
</code></pre>

<p>Then v_euler is fed into the Jacobian along with v_trans.  I am pretty sure my Jacobian code is fine.  Because i have given it (constant) rotational velocities ok.  </p>

<p>Note, I am not asking you to debug my code.  I only posted code because I figured it would be more clear than converting this all to math.  I am more interested in why this might go unstable.  Specifically, is the math wrong?  And if this is completely off base, please let me know.  I'm sure people must go about this somehow. </p>

<p>So far, I have been giving it a slow linear velocity (0.01 m/s), and zero target rotational velocity.  The arm is in a good spot in the workspace and can easily achieve the desired motion.  The code runs at 200Hz, which should be sufficiently fast enough.  </p>

<p>I can hard-code the angular velocity fed into the Jacobian instead of using the computed <code>v_euler</code> and there is no instability.  So there is something wrong in my math.  This works for both zero and non-zero target angular velocities.  Interestingly, when i feed it an angular velocity of 0.01 rad/sec, the end-effector rotates at a rate of 90 deg/sec.</p>

<p><strong>Update:</strong> If I put the end-effector at a different place in the workspace so that its axes are aligned with the world axes, then everything seems works fine.  If the end-effector is 45 degrees off from the world axes, then some motions seem to work, while others don't move exactly as they should, although i don't think i've seen it go unstable.  At 90 degrees or more off from world, then it goes unstable. </p>
","kinematics jacobian"
"6620","What is the best software/package to draw the robot manipulator and indicate DH parameters and different axes?","<p>I'm wondering about good software/package to draw the robot manipulator and indicate DH parameters and different axes?</p>

<p>Any suggestions!</p>
","robotic-arm industrial-robot robotc simulation"
"6622","Is Khepera II still adequate for learning","<p>A professor in my university is asking me to study robotics with him. By robotics I understand programming a robot to move around, avoid obstacles, figure out a maze, etc.. He sent me some manuals for Khepera II. </p>

<p>When I first read the specs, I was surprised by the low specs:</p>

<ul>
<li>Motorola 68331 CPU @ 25 MHz</li>
<li>512 KB RAM</li>
<li>512 KB Flash</li>
</ul>

<p>But then I looked at some of the new Arduino boards and they had similar specs.
So maybe that's OK, I guess the CPU speed and RAM aren't that important if I'm going to control the robot from a normal computer that can handle real time computation.</p>

<p>What about the software? I glanced at the manuals and saw only C and assembly code.</p>

<p>Khepera I is from 1995 and Khepera II is from 2001. I think robots have advanced much since 2001.</p>

<p><strong>Is using Khepera II adequate for university level learning, considering I can probably give 200-300$ for a newer one?</strong></p>

<p>I ask in terms of hardware of the board, motors and sensors, as well as in the programmability. This question might seem vague. I'm ready to improve it by giving more detail upon request.</p>
","mobile-robot"
"6624","Ball and plate possible sensors use","<p>I am looking  for sensors to give me the position of a ball  on a plate in order to make a ball and plate problem  .</p>

<p>What came to my mind is to use image processing but since i never did some serious image processing i don't know if it is a good idea.</p>

<p>Eventually can you please help me to find some 'cheap' sensors in order to get the position of the ball on the plate.</p>

<p>Thank you for your attention.</p>
","control sensors"
"6632","Ros, iai_kinect2 issues","<p>here is what I did on <strong>Ubuntu 14.04 LTS</strong> running on a Toshiba Satellite, <strong>intel</strong> i7, <strong>nvidia</strong> with <strong>usb 3.0 and 2.0 ports</strong>.</p>

<p>1, 2, 3 refer to scripts found <a href=""https://github.com/Parth-Mehrotra/Tools/tree/master/InstallScripts"" rel=""nofollow"">here</a></p>

<ol>
<li><p>Setup ros by running this install-ros.sh script</p></li>
<li><p>Setup opencv by running this install-opencv.sh script</p></li>
<li><p>Setup PCL by running this install-pcl.sh script</p></li>
<li><p>Installed  libfreenect2 via the instructions at the master branch</p></li>
<li><p>Made the changes and installed</p></li>
<li><p>Cloned the repository into an empty catkin workspace</p></li>
<li><p>Sourced the respective setup.zsh files from my <strong>/opt/ros/...</strong> and from the <strong>devel/</strong> folders.</p></li>
</ol>

<p>At this point, I have encountered no issues</p>

<p>I tried running: </p>

<pre><code>rosrun kinect2_bridge kinect2_bridge
</code></pre>

<p>and I get the following message:</p>

<pre><code>[ERROR] [1424391698.413758209]: [registerPublisher] Failed to contact master at [localhost:11311].  Retrying..
</code></pre>

<p>So I assume I need to run roscore or something like that. So if I run: </p>

<pre><code>roscore
</code></pre>

<p>in one terminal and:</p>

<pre><code>rosrun kinect2_bridge kinect2_bridge
</code></pre>

<p>in another terminal, I get the following segmentation fault:</p>

<pre><code>[ERROR] [1424393345.496836066]: [registerPublisher] Failed to contact master at [localhost:11311].  Retrying...
[ INFO] [1424393446.243884489]: Connected to master at [localhost:11311]
parameter:
        base_name: kinect2
           sensor: 
        fps_limit: -1
       calib_path: /home/parthmehrotra/catkin_ws/src/iai_kinect2/kinect2_bridge/data/
          use_png: false
     jpeg_quality: 90
        png_level: 1
     depth_method: opengl
     depth_device: -1
       reg_method: default
       reg_devive: -1
        max_depth: 12
        min_depth: 0.1
       queue_size: 2
 bilateral_filter: true
edge_aware_filter: true
       publish_tf: false
     base_name_tf: kinect2
   worker_threads: 4

[1]    2679 segmentation fault (core dumped)  rosrun kinect2_bridge kinect2_bridge
</code></pre>

<p>I must be overlooking something really trivial. Thanks for taking the time to help me. </p>
","ros kinect"
"6639","how to plot $\pm 3 \sigma$ of a landmark in EKF-SLAM","<p>I have implemented 2D-SLAM using EKF. The map is based-feature in which there is only one landmark for the sake of simplicity. I've read some papers regarding this matter. They plot the $\pm3\sigma$ plus the error. I would like to make sure that I'm doing the right thing. In my project, I have the estimate of the landmark's position and its true values. The true values here are the ones that the sensor measure not the ideal case. For example, the ideal case of the landmark position  is (30,60) but this value is not accessible by any means, therefore I will consider the true values the ones that are coming from the sensor. </p>

<p>Now the error in the landmark's position in x-axis is formulated as follows </p>

<p>$$
\text{error}_{x} = \hat{x} - x
$$</p>

<p>The below picture shows the error in blue color. The red color represents the error bounds which is $\pm 3 \sigma_{x}$</p>

<p>My question is now is this the way people plot the errors in the academics papers because I've seen some papers the bounds doesn't not look like mine. Even though mine decreases monotonically however in some papers it is more curved and it seems more reasonable to me. Any suggestions?</p>

<p><img src=""http://i.stack.imgur.com/heEm9.png"" alt=""enter image description here""></p>
","slam ekf simulation mapping"
"6642","""Time-varying"" and ""nonautonomous"" dynamical systems and their Lyapunov analysis","<p>It is possible to distinguish the properties ""time-varying"" and ""nonautonomous"" in dynamical systems regarding Lyapunov stability analysis?</p>

<p>Does it make a difference if the system depends explicitly on $t$ or indirectly on $t$ due to a time-varying parameter?</p>

<p>I want to explain the problem in detail:</p>

<p>Let a dynamical system denoted by $\dot x = f$, with state $x$.
We say that a dynamical system is nonautonomous if the dynamics $f$ depend on time $t$, i.e. $$\dot x = f(t,x).$$</p>

<p>For instance the systems $$ \dot x = - t x^2 $$ and $$ \dot x = -a(t)x,$$ are nonautonomous. Let $a(t)$ be a bounded time-varying parameter, i.e. $||a(t)||&lt;a^+$ and strictly positive, i.e. $a(t) &gt; 0$. </p>

<p>Particularly, the second example is more likely denoted as a time-varying linear system, but of course it is nonautonomous. </p>

<p>In Lyapunov stability analysis autonomous and nonautonomous systems must be strongly distinguished to make assertions about stability of the system, and the Lyapunov analysis for nonautonomuos systems is much more difficult.</p>

<p>And here for me some questions arise. When i want to analize stability of the second example must i really use the Lyapunov theory for nonautonomous systems? </p>

<p>It follows for the candidate $V = 1/2 x^2$</p>

<p>$$\dot V = -a(t)x^2,$$</p>

<p>which is negative definite. Is the origin really asymptotically stable, as i suppose, or must i take the nonautomous characteristic into account in this case?</p>

<p>I would suppose it makes a difference if a system depends explicitly on $t$ as in the first example or just indirect due to a time-varying parameter, since $t$ approaches infinity, but a parameter does not.</p>
","control dynamics"
"6643","Can a quadcopter hover while tilted?","<p>Is there a way to make a quadcopter maintain steady hovering (no lateral movement, constant altitude) while tilted left or right?  If so, how can i accomplish this? </p>
","quadcopter"
"6648","Automatic agricultural robot using 8051","<p>I want to build a automatic agricultural robot for my final year diploma project. The basic idea is to program 8051 to drive the robot in a fixed path in farm for ploughing the farm which i am planning to do by setting a particular distance till which it will go straight and then take a U turn and plough in next lane. Width of the farm will also be set so when it completes full farm it'll stop and go back to starting point. the only catch is to reprogram it as per size of the farm of the person who uses it. So i want to add a number pad with which he can set the length and width of the farm as well as width of each lane as per his needs without professional help. Can this be done using 8051 or should i go for AVR or PIC microcontrollers. I have just started studying programming and interfacing of 8051 so I am not that good in programming. If its possible how do i do it. can someone please help me with circuit diagram for this project. After everything i said i need in my project if i still get an empty port in microcontroller I would love to add a fertilizer sprayer or water irrigation system and a GSM module so that a farmer can simply ask the robot to start working using his mobile phone. As I am making just a prototype i want it to be as small as possible. Suggestions are welcomed.</p>
","microcontroller wheeled-robot electronics artificial-intelligence"
"6649","PID control of tank-like robot and IMU","<p>Consider a tank like robot with a motor driver channel for each side of the robot (two motors on the left and two motors on the right) and an IMU.
I'm interested in driving the robot in a straight line using the yaw data from the gyro and magnetometer of the IMU, removing the noise caused by slightly different behaving motors, and the possibility to change the desired direction angle. For example some event comes and I want the car to switch the desired direction to +120 degrees and turn while driving.</p>

<p>I'm using Arduino Uno, MinIMU-9 v3 and two DRV8838 Single Brushed DC Motor Driver from Pololu.</p>

<p>Can you please give me some hints and a short pseudo-code example?
Thanks!</p>
","arduino pid imu"
"6650","Microcontroller for running Linux RTAI","<p>I'm a beginner in Robotics. </p>

<p>I'd like to ask what are the minimum/recommended specs for a microcontroller to run a real-time system such as Linux RTAI?</p>

<p>What is the popular microcontroller for running Linux RTAI? </p>

<p>Thank you.</p>
","microcontroller embedded-systems real-time"
"6651","Are robotic pollenators being designed?","<p>With the bee hive collapses, growers are desperate for pollenation options. Is anyone working on swarms of tiny flying robots to augment the bees? They could look for a certain color, poke around inside the flower for a moment, and move on to the next. When they need recharging, they fly back to their hive (<em>the same reason bees fly back</em>). </p>

<p>Of course, replacing germinators that run the seeds through their digestive systems would be a different problem.</p>
","mobile-robot"
"6652","How to get a python node in ROS subscribe to multiple topics?","<p>How a ROS node written in Python could subscribe to multiple topics and publish to multiple topics?</p>

<p>All examples I found were for a single topic. Is this an event-driven model so subscription to multiple ""events"" is allowed or it is more like a loop, so it can listen only to one ""source"" at a time?</p>
","ros"
"6657","What dynamic system could these equations represent?","<p>I have equations of a dynamic system. I need to figure out what this physical system is. </p>

<p>The equations are:</p>

<p>\begin{align}
\dot{x}_1&amp;=bx_1+kx_2+x_3\\
\dot{x}_2&amp;=x_1\\
\dot{x}_3&amp;=\alpha (u-x_2)-\beta x_3
\end{align}</p>

<p>All I can figure out is that it is maybe a mass-spring-damper system, plus a feedback control, but I am not quite sure about the terms $x_3$ and $\dot{x}_3$. What do these two terms mean?</p>
","control dynamics"
"6659","GraphSLAM: why are constraints imposed twice in the information matrix?","<p>I was watching Sebastian Thrun's video course on AI for robotics (freely available on udacity.com).  In his final chapter on <a href=""http://youtu.be/nLEbJZFm5-E"" rel=""nofollow"">GraphSLAM</a>, he illustrates how to setup the system of equations for the mean path locations $x_i$ and landmark locations $L_j$.</p>

<p><a href=""http://youtu.be/V41eTlGU0gw"" rel=""nofollow"">To setup the matrix system</a>, he imposes each robot motion and landmark measurement constraint twice.  For example, if a robot motion command is to move from x1 by 5 units to the right (reaching x2), i understand this constraint as</p>

<p>$$-x_2+x_1= -5$$</p>

<p>However, he also imposes the negative of this equation 
$$x_2-x_1=5$$
as a constraint and superimposes it onto a different equation and i'm not sure why.  In his video course, he briefly mentions that the matrix we're assembling is known as the <a href=""http://youtu.be/U8iFMhtJyek"" rel=""nofollow"">""information matrix""</a>, but i have no why the information matrix is assembled in this specific way.</p>

<p>So, I tried to read his book Probabilistic Robotics, and all i can gather is that these equations come from obtaining the minimizer of the negative log posterior probability incorporating the motion commands, measurements, and map correspondences, which results in a quadratic function of the unknown variables $L_j$ and $x_i$.  Since it is quadratic (and the motion / measurement models are also linear), the minimum is obviously obtained by solving a linear system of equations.</p>

<p>But why are each of the constraints imposed twice, with once as a positive quantity and again as the negative of the same equation?  Its not immediately obvious to me from the form of the negative log posterior probability (i.e. the quadratic function) that the constraints must be imposed twice.  Why is the ""information matrix assembled this way?  Does it also hold true when the motion and measurement models are nonlinear?</p>

<p>Any help would be greatly appreciated.</p>
","slam"
"6661","FreeIMU External Magnetometer","<p>I have been using the FreeIMU library successfully but now I want to add an external magnetometer that I can mount away from my motors.</p>

<p>I've figured out how to modify the FreeIMU library to use an external magnetometer and I am getting data.</p>

<p>What I can't figure out is what I need to change now that my magnetometer orientation has changed.</p>

<p>On the free IMU it is mounted like this 
<img src=""http://i.stack.imgur.com/iuU4m.jpg"" alt=""enter image description here""></p>

<p>The external compass is mounted like this - upside down - rotated 180° around x
<img src=""http://i.stack.imgur.com/k2UNg.jpg"" alt=""enter image description here""></p>

<p>I am changing the value inside the </p>

<p><code>void HMC58X3::getRaw(int16_t *x,int16_t *y,int16_t *z)
{
    *x = cache_x;
    *y = cache_y;
    *z = cache_z;
}
</code>
function as all the other code calls this to get the magnetometer data.</p>

<p>So far I have tried:</p>

<ol>
<li><p>Changing the sign for the y and z values after I have got them from the magnetometer.</p></li>
<li><p>Changing the sign for the y value only</p></li>
<li><p>Changing the sign for the z value only</p></li>
<li><p>Added 180 to both the y and z values</p></li>
<li><p>Subtracting 180 from both the y and z values</p></li>
<li><p>Subtracting 180 from y and adding 180 to Z</p></li>
<li><p>Adding 180 to y and subtracting 180 from z</p></li>
<li><p>Changing nothing</p></li>
</ol>

<p>The calibration GUI gives always gives me strange results, and doing the changes above just rotated/mirrored the magnetometer red and green graphs. I am unable to get rid of the key hole shape.
<img src=""http://i.stack.imgur.com/VbFrZ.jpg"" alt=""enter image description here""></p>

<p>The red is XY. The green YZ. The blue is ZX. Does the fact that ZX works mean that my issue is with the Y value?</p>

<p>This is how it looks using the on board magnetometer.
<img src=""http://i.stack.imgur.com/IOYoQ.jpg"" alt=""enter image description here""></p>

<p>What should I try next?</p>

<p>Thanks
Joe</p>

<p>EDIT</p>

<p>I tried rotating the external magnetometer so it is in the same orientation as the FreeIMU magnetometer and I still get the same result so I don't think its the difference in orientation that is causing the problem.</p>

<p>So then I thought maybe its because the FreeIMU is mounted central to the rotation axis and the external magnetometer is mounted about 20cm above. I tested this by rotating only the external magnetometer around itself and I still got the same result. </p>

<p>This is all seems strange, do you think its possible that the external magnetometer I have brought is faulty? Any way to confirm it is working properly on its own?</p>

<p>Thanks</p>

<p>EDIT</p>

<p>Managed to get circles plotting by changing the gain from 0 to 1. It seem my new magnetometer was being saturated.</p>

<p>Now I just need to work out how to change my values around so the orientation is correct.</p>
","quadcopter imu calibration magnetometer"
"6662","How to track robot position","<p>I'm a software researcher, who in my spare time mentors a robotics team, helping on the software side of things.  For years, I keep coming back to the same question.  How to determine the robots position, and heading during our competitions.  We have tried a number of things with varying degrees of success/failure. Encoders on the drive wheels, accelerometers, gyroscopes, etc.  I recently bought an IMU with a 3 axis accelerometer, 3 axis gyro, and 3 axis magnetometer, all preprocessed by an Arduino, and outputting the value to a serial port.  I thought surely there must be a way to take all these measurements, and get a composite view of position and heading.  We are using mechanum wheels on this particular robot, so wheel encoders are not particularly useful.  I've looked around and there's a lot of talk about orientation using quaternion with sensor fusion using similar boards, but it very unclear to me how to take the quaternion and the estimation and come up with x,y distance from the starting position.  Now my time window for these measurements is small, ~15 seconds, but I need it to be pretty accurate within that window.  I'm about ready to abandon using the IMU, and try something else.  One idea is to use a usb ball mouse to try and track robot motion but I'm certain that the mouse is going to get banged around way too much leading to noise and invalid results.  As a side note: robot's about 2ft x 3ft base weighting in at 120 lbs.  Any thoughts or suggestions appreciated.</p>
","imu sensor-fusion"
"6667","Heavy omnidirectional platform suspension","<p>I'm planning to build an omnidirectional platform that will support about 180kg robotic arm. The platform will be equipped in meccanum wheels. I would like to have some kind of suspension to avoid wheels loosing contact with the floor on small bumps (let's say 2cm). </p>

<p>The first suspension type I thought about was rocker bogie type, but I'm afraid, that changes of arm center of mass during its movement will introduce too much stress on rocker bogie mechanism. </p>

<p>What other choices would you recommend? Or maybe rocker bogie will be fine after all?</p>
","mobile-robot wheeled-robot design mechanism"
"6672","The uncertainty is big while the sensor is rather accurate at measuring a landmark in EKF-SLAM","<p>I've a 2D sensor which provides a range $r$ and a bearing $\phi$ to a landmark. In my 2D EKF-SLAM simulation, the sensor has the following specifications 
$$
\sigma_{r} = 0.01 \text{m} \ \ ,\sigma_{\phi} = 0.5 \ \text{deg}
$$ </p>

<p>The location of the landmark in x-axis is 30. EKF imposes the Gaussian noise, therefore the location of the landmark is represented via two quantities namely the mean $\mu_{x}$ and the variance $\sigma_{x}$. In the following graph </p>

<p><img src=""http://i.stack.imgur.com/3ChZN.png"" alt=""Graph""></p>

<p>The green is the mean $\mu_{x}$ which is very close to the true location (i.e. 30). The black is the measurements and red is $\mu_{x} \pm 3 \sigma_{x}$. I don't understand why the uncertainty is big while I'm using rather accurate sensor. The process noise for the robot's pose is $\sigma_{v} = 0.001$ which is small noise. I'm using C++. </p>

<hr>

<p>Edit: for people who ask about the measurements, this is my code </p>

<p>$$
r = \sqrt{ (m_{j,y} - y)^{2} + (m_{j,x} - x)^{2}} + \mathcal{N}(0, \sigma_{r}^{2}) \\
\phi = \text{atan2} \left( \frac{m_{j,y} - y}{m_{j,x} - x} \right) + \mathcal{N}(0, \sigma_{\phi}^{2})
$$</p>

<pre><code>std::vector&lt;double&gt; Robot::observe( const std::vector&lt;Beacon&gt;&amp; map )
{
    std::vector&lt;double&gt; Zobs;

    for (unsigned int i(0); i &lt; map.size(); ++i)
    {
        double range, bearing;

        range = sqrt( pow(map[i].getX() - x,2) + 
                      pow(map[i].getY() - y,2)   );

        // add noise to range
        range += sigma_r*Normalized_Gaussain_Noise_Generator();

        bearing = atan2( map[i].getY() - y, map[i].getX() - x) - a;

        // add noise to bearing
        bearing += sigma_p*Normalized_Gaussain_Noise_Generator();

        bearing = this-&gt;wrapAngle(bearing);


        if ( range &lt; 1000 ){
          // store measurements (range, angle) for each landmark. 
          Zobs.push_back(range);
          Zobs.push_back(bearing);
          //std::cout &lt;&lt; range &lt;&lt; "" "" &lt;&lt; bearing &lt;&lt; std::endl;
        }
    }


    return Zobs;
}
</code></pre>

<p>where <code>Normalized_Gaussain_Noise_Generator()</code> is ( i.e. $\mathcal{N}(0, 1) )$</p>

<pre><code>double Robot::Normalized_Gaussain_Noise_Generator()
{
    double noise;
    std::normal_distribution&lt;double&gt; distribution;
    noise = distribution(generator);

    return noise;
}
</code></pre>

<p>For the measurements (i.e. the black color), I'm using the inverse measurement function  given the estimate of the robot's pose and the true measurement in polar coordinates to get the location of a landmark. The actual approach is as follows </p>

<p>$$
\bar{\mu}_{j,x} = \bar{\mu}_{x} + r \cos(\phi + \bar{\mu}_{\theta}) \\
\bar{\mu}_{j,y} = \bar{\mu}_{y} + r \sin(\phi + \bar{\mu}_{\theta})
$$</p>

<p>This is how it is stated in the Probabilistic Robotics book. This means that the measurements in the above graph are indeed the predicted measurements not the true ones. </p>

<p>Now under same conditions, the true measurements can be obtained as follows</p>

<p>$$
\text{m}_{j,x} = x + r \cos(\phi + \theta) \\
\text{m}_{j,y} = y + r \sin(\phi + \theta)
$$</p>

<p>The result is in the graph below, which means there is no correlations between the true measurements and the robot's estimate. This leads me to the same question - why the uncertainty behaves like that? </p>

<p><img src=""http://i.stack.imgur.com/tpPD3.png"" alt=""Graph""></p>
","mobile-robot slam ekf noise"
"6673","Do you have to have a rate controller for a Quadcopter?","<p>Most academic papers characterise the rate of rotation along the x axis as φ""=(1/Jx)τφ. As far as I cant tell, this characterises the rate and not the actual angle φ itself and yet the PID controllers academics use to control this takes φsetpoint-φmeasured as its error signal. Should the error signal not be φ""setpoint-φ""measured (using gyro values) instead. Why are they using the euler angle instead of its second derivative to control the rate?</p>

<p>Is it possible to stabilise a quadcopter using euler angles only?</p>
","control quadcopter pid"
"6674","Drive motor voltage / other specifications of Roomba 650","<p>I salvaged some parts off my dead Roomba 650, and I'm trying to use the drive motor assembly . I got the pinout of the connector but I don't know what voltage / PWM / other specifications are there for this motor. </p>

<p>I've attached the picture of the drive motor assembly.</p>

<p>Any help would be appreciated!</p>

<p>Thank you,
Pratik</p>

<p><img src=""https://plus.google.com/photos/+PratikPanchal2011/albums/6119164925515534577/6119164929351874418?authkey=COOW4Njkmr7xQg&amp;pid=6119164929351874418&amp;oid=106454827636977474727"" alt=""Drive motor unit""></p>

<p>Edit: The Image is here: <a href=""https://plus.google.com/photos/+PratikPanchal2011/albums/6119164925515534577/6119164929351874418?authkey=COOW4Njkmr7xQg&amp;pid=6119164929351874418&amp;oid=106454827636977474727"" rel=""nofollow"">Drive Motor Module</a></p>
","motor pwm irobot-create h-bridge roomba"
"6680","I am doing a project on robotic surgeries! Can anyone help me and give me some details related to this topic?","<p>Can anyone help me, because I am doing a project on robotical surgeries and I would like someone to help me and advise me. I wonder if anyone could give me some data on tests he or she has run in a surgical robot... Thank you for your attention!
Anything else will be much appreciated!</p>
","sensors robotic-arm automatic"
"6683","What is the difference between motion planning and trajectory generation?","<p>What are the major differences between motion planning and trajectory generation in robotics? Can the terms be used interchangeably?</p>
","motion-planning"
"6686","iRobot Create 2 and Open Interface 2 Spec not syncing up with incoming data","<p>I have the create 2 and have it hooked up to an arduino. Almost all the commands work fine except when retrieving sensor information. If i send a request for packet 18 I get back values that while consistent don't match up, unless I am missing something. So if I press the clean button I get 127 or 11111110 and if i then press spot I get something like 11111010. I might be messing up my endianness but regardless the data isnt formatted how I expected it to be according to the spec sheet. I have 3 create 2s and they all do the same thing. Any ideas? I am using a 2n7000 along with the tutorial from the site but i dont think that has anything to do with the formatting of the byte.</p>

<p>this is the library I am using: <a href=""https://github.com/DomAmato/Create2"" rel=""nofollow"">https://github.com/DomAmato/Create2</a></p>

<p>Sorry to take so long to get back on this, anyways the data we get is always formatted this way. It is not a baud rate issue since it understands the commands properly.</p>

<hr>

<pre><code>        day     hour    minute  schedule    clock   dock    spot    clean
day     3       x       x       x           x       x       x       x
hour    6       7       x       x           x       x       x       x
minute  13      14      15      x           x       x       x       x
schedule    x   x       x       x           x       x       x       x
clock   x       x       x       x           x       x       x       x
dock    27      29      30      x           x       31      x       x
spot    55      59      61      x           x       62      63      x
clean   111     119     123     x           x       125     126     127
</code></pre>

<p>Note that the schedule and clock buttons return nothing</p>
","arduino irobot-create"
"6694","How to find the body jacobain, for each link in a robot manipulator?","<p>The links twist could be obtained, and thus The spatial manipulator Jacobian could be done, but when it comes to the body Jacobian, it is becomes difficult. Moreover, the adjoint transformation relates both Jacobain, but however that is 4*4 while the Jacobian is 6*n; how does it works? as in the picture, he is getting a body jacobian for each link, not one jacobian matrix for the whole robot, I don't know.
Any help is highly regarded.
<a href=""http://i.stack.imgur.com/yURZJ.png"" rel=""nofollow"">Like this example</a> or here for
<a href=""http://www.cds.caltech.edu/~murray/books/MLS/pdf/mls94-complete.pdf"" rel=""nofollow"">full details</a></p>
","robotic-arm kinematics inverse-kinematics manipulator jacobian"
"6696","DC Motor PID control with unstable velocity feedback","<p>Currently I am building a omnidirectional robot with 4 DC Motors with embedded incremental encoder.</p>

<p>However, with a constant pwm input, i am not able to control the motor to rotate in a ""relatively stable"" state, refer to the figure, it can be observed that the linear speed of the motors can varied in 10cm/s range. I believe one possible reason is the PWM signal generated from my Arduino Mega Controller is not good enough.</p>

<p>And my problem is how can I implement a stable PID controller in this case? As the speed of the motor varies even with the same input, I believe extra work like adding a filter is needed?</p>

<p>Any advice is appreciated >.&lt; Thank you</p>

<p><img src=""http://i.stack.imgur.com/9FGAU.png"" alt=""Velocity feedback (cm/s) against time""></p>
","motor pid"
"6700","help me in quadcopter controlling","<hr>

<pre><code>&gt; hi my name is zahed kamangar i am from iran and i created the
&gt; quadcopter and i must controll it with my computer can you help me to
&gt; work in the true direction because i should controll my quad with EEG
&gt; signal and, i just controll it with computer instead radio controller
&gt; do you have any way sir it is my email **zahed_kaman@yahoo.com**
&gt; thanks a lot
</code></pre>

<hr>
","pid"
"6701","Drones and camera streaming","<p>How do you stream video feed from a camera on a drone? I would think that at high altitudes Wi-Fi won't work. </p>

<p>So what would you usually do, and how?</p>
","quadcopter cameras"
"6703","Angles in a Rocker bogie system","<p>How do you select the following two angles in the design of a Rocker bogie system:</p>

<ol>
<li>Angle between two arms of the main rocker, and;</li>
<li>Angle between two arms in the bogie.</li>
</ol>
","mobile-robot wheeled-robot"
"6705","Detect human in proximity?","<p>I'm looking for ways to detect human presense behind walls in close proximity (around 10 feet) in whatever way possible! Problem is I can't code! (I hope it's ok I'm posting here.) 
I know there are different sensors but they all seem to be for detecting by motion of target humans. 
How do you detect still persons?
Is there a sound amplification device that magnifies human breathing x 20?
Or detect body heat?
Or pick up radiation waves or something off humans?</p>
","sensors sensor-fusion ultrasonic-sensors"
"6706","Converting a linear acceleration command into a DC motor command?","<p>I'm constructing a 2 wheels balancing robot which uses a PID controller.  I've tuned my parameters on numerical simulations based on a continuous inverted pendulum system so that the simulated inverted pendulum balances by controlling the horizontal (linear) cart acceleration $\ddot{x}$.</p>

<p>Now that I've done this, I want to take the next step and turn my PID control commands into electrical commands onto a DC motor to give the desired linear acceleration $\ddot{x}$. However I'm not sure how exactly to do this for my specific robot's motors.  Are there experimental tests should I run to determine how to convert PID commands into DC motor acceleration commands? Or is there a formula to do this based on the motor's specifications?</p>

<p><strong><em>Update</em></strong></p>

<p>The non-linear dynamic equation I'm using is</p>

<p>$$L\ddot{\theta}=gsin(\theta)+\ddot{x}(t)cos(\theta)+Ld(t)$$</p>

<p>where $\ddot{x}(t)$ is the linear acceleration, $g$ is the acceleration due to gravity, and $\ddot{\theta}$ is the angular acceleration, and $d(t)$ is an external disturbance to the system. To simplify things, I've linearized the equations around $\theta\approx0$, yielding</p>

<p>$$L\ddot{\theta}=g\theta+\ddot{x}(t)+Ld(t)$$</p>

<p>I've assumed that the only control input is the cart's linear acceleration $\ddot{x}(t)$, and chose this control command as $\ddot{x}(t)=K_1\theta(t) + K_2\int_0^t\theta(t) dt + K_3\dot{\theta}$, where $K_i$ are the PID gains.</p>
","motor control pid actuator dynamics"
"6708","I need help with choosing a computer on the board","<p>I need a computer on the board like raspberry pi for vending machine (I want to replace the original controller). This is list of some requirements:</p>

<p>1) It should have pins to connect to the <a href=""http://en.wikipedia.org/wiki/Multidrop_bus"" rel=""nofollow"">mdb protocol</a> &amp; other stuff through gpio.</p>

<p>2) Good performance. There will be a display with browser showing rails application running. I've tried a raspberry pi B+, but it's too slow (it can't even run a browser with speed like a laptop pc). So, I want to choose a more powerful system like odroid, wandboard etc.</p>

<p>3) Custom video output. Sometimes I need to display FullHD(1920x1080), sometimes I need to show at 768x1024 (yes, the computer should simply rotate video output)</p>

<p>4) I don't want to connect microcomputer to display directly, not through HDMI, DVI or something like this. (This is not required, but very desirable).</p>

<p>Please help me choose. Nowadays I try to choose from odroid, wandboard or pandaboard. Are there any other computers? What version of the computer is advisable?</p>
","arduino raspberry-pi"
"6714","What is the cheapest way to make led sensitive to sound around?","<p>There is always a way to do this using arduino, rasberry pi etc.  However, in many cases in discussion in forums i've come across things where whole 'logic' can be uploaded to $0.50 chip. Instead of $50 dollar part. DRASTIC change. This defines a line between a one time thing that you made as a hobby, and something you can sell around. </p>

<p>So basically if i want led to get brightest at loud sound and almost off on silence.
Or with button that switch to 100% on all the time. </p>
","design circuit"
"6715","Is it possible for a robot to navigate through predefined coordinates?","<p>I am a total newbie in robotics so please bare with me. </p>

<p>I have a school project where my team has to design a robot that is capable of picking up 3 golf balls in different sizes at predefined locations. Then it has to drop these balls into their respective holes. </p>

<p>We are using an arduino chip in our robot. </p>

<p>I thought I could perhaps define a path for the robot, an invisible virtual path you may call. So imagining the platform as Cartesian plane, can I tell the robot go to where I want it to go? For example, go to (5,12)</p>

<p>Or do I need some sort of sensors so the robot figures it out by itself. Thanks for your time!  </p>
","arduino sensors navigation"
"6720","Raspberry Pi quadcopter thrashes at high speeds","<p>I am attempting to build a Raspberry Pi based quadcopter. So far I have succeeded in interfacing with all the hardware, and I have written a PID controller that is fairly stable at low throttle. The problem is that at higher throttle the quadcopter starts thrashing and jerking. I have not even been able to get it off the ground yet, all my testing has been done on a test bench. I have ruled out bad sensors by testing each sensor individually, and they seem to be giving correct values. Is this a problem with my code, or perhaps a bad motor? Any suggestions are greatly appreciated.</p>

<p>Here is my code so far:</p>

<p>QuadServer.java:</p>

<pre><code>package com.zachary.quadserver;

import java.net.*;
import java.io.*;
import java.util.*;

import se.hirt.pi.adafruit.pwm.PWMDevice;
import se.hirt.pi.adafruit.pwm.PWMDevice.PWMChannel;

public class QuadServer {
    private static Sensor sensor = new Sensor();

    private final static int FREQUENCY = 490;

    private static double PX = 0;
    private static double PY = 0;

    private static double IX = 0;
    private static double IY = 0;

    private static double DX = 0;
    private static double DY = 0;

    private static double kP = 1.3;
    private static double kI = 2;
    private static double kD = 0;

    private static long time = System.currentTimeMillis();

    private static double last_errorX = 0;
    private static double last_errorY = 0;

    private static double outputX;
    private static double outputY;

    private static int val[] = new int[4];

    private static int throttle;

    static double setpointX = 0;
    static double setpointY = 0;

    static long receivedTime = System.currentTimeMillis();

    public static void main(String[] args) throws IOException, NullPointerException {

        PWMDevice device = new PWMDevice();
        device.setPWMFreqency(FREQUENCY);

        PWMChannel BR = device.getChannel(12);
        PWMChannel TR = device.getChannel(13);
        PWMChannel TL = device.getChannel(14);
        PWMChannel BL = device.getChannel(15);

        DatagramSocket serverSocket = new DatagramSocket(8080);


        Thread read = new Thread(){
                public void run(){
                    while(true) {
                    try {
                            byte receiveData[] = new byte[1024];
                            DatagramPacket receivePacket = new DatagramPacket(receiveData, receiveData.length);
                            serverSocket.receive(receivePacket);
                            String message = new String(receivePacket.getData());
                            throttle = (int)(Integer.parseInt((message.split(""\\s+"")[4]))*12.96)+733;
                            setpointX = Integer.parseInt((message.split(""\\s+"")[3]))-50;
                            setpointY = Integer.parseInt((message.split(""\\s+"")[3]))-50;

                        receivedTime = System.currentTimeMillis();

                        } catch (IOException e) {
                            e.printStackTrace();
                        }
                    }
                }
        };
        read.start();

        while(true)
        {
            Arrays.fill(val, calculatePulseWidth((double)throttle/1000, FREQUENCY));

            double errorX = -sensor.readGyro(0)-setpointX;
            double errorY = sensor.readGyro(1)-setpointY;

            double dt = (double)(System.currentTimeMillis()-time)/1000;

            double accelX = sensor.readAccel(0);
            double accelY = sensor.readAccel(1);
            double accelZ = sensor.readAccel(2);

            double hypotX = Math.sqrt(Math.pow(accelX, 2)+Math.pow(accelZ, 2));
            double hypotY = Math.sqrt(Math.pow(accelY, 2)+Math.pow(accelZ, 2));


            double accelAngleX = Math.toDegrees(Math.asin(accelY/hypotY));
            double accelAngleY = Math.toDegrees(Math.asin(accelX/hypotX));

            if(dt &gt; 0.01)
            {

                PX = errorX;
                PY = errorY;

                IX += errorX*dt;
                IY += errorY*dt;

                IX = 0.95*IX+0.05*accelAngleX;
                IY = 0.95*IY+0.05*accelAngleY;

                DX = (errorX - last_errorX)/dt;
                DY = (errorY - last_errorY)/dt;

                outputX = kP*PX+kI*IX+kD*DX;
                outputY = kP*PY+kI*IY+kD*DY;
                time = System.currentTimeMillis();
            }

            System.out.println(setpointX);

            add(-outputX+outputY, 0);
            add(-outputX-outputY, 1);
            add(outputX-outputY, 2);
            add(outputX+outputY, 3);

            //System.out.println(val[0]+"", ""+val[1]+"", ""+val[2]+"", ""+val[3]);
            if(System.currentTimeMillis()-receivedTime &lt; 1000)
            {
                BR.setPWM(0, val[0]);
                TR.setPWM(0, val[1]);
                TL.setPWM(0, val[2]);
                BL.setPWM(0, val[3]);
            } else 
            {
                BR.setPWM(0, 1471);
                TR.setPWM(0, 1471);
                TL.setPWM(0, 1471);
                BL.setPWM(0, 1471);
            }

        }
    }

    private static void add(double value, int i)
    {
        value = calculatePulseWidth(value/1000, FREQUENCY);
        if(val[i]+value &gt; 1471 &amp;&amp; val[i]+value &lt; 4071)
        {
            val[i] += value;
        }else if(val[i]+value &lt; 1471)
        {
            //System.out.println(""low"");
            val[i] = 1471;
        }else if(val[i]+value &gt; 4071)
        {
            //System.out.println(""low"");
            val[i] = 4071;
        }
    }

    private static int calculatePulseWidth(double millis, int frequency) {
        return (int) (Math.round(4096 * millis * frequency/1000));
    }
}
</code></pre>

<p>Sensor.java:</p>

<pre><code>package com.zachary.quadserver;

import com.pi4j.io.gpio.GpioController;
import com.pi4j.io.gpio.GpioFactory;
import com.pi4j.io.gpio.GpioPinDigitalOutput;
import com.pi4j.io.gpio.PinState;
import com.pi4j.io.gpio.RaspiPin;
import com.pi4j.io.i2c.*;
import com.pi4j.io.gpio.GpioController;
import com.pi4j.io.gpio.GpioFactory;
import com.pi4j.io.gpio.GpioPinDigitalOutput;
import com.pi4j.io.gpio.PinState;
import com.pi4j.io.gpio.RaspiPin;
import com.pi4j.io.i2c.*;

import java.net.*;
import java.io.*;

public class Sensor {
    static I2CDevice sensor;
    static I2CBus bus;
    static byte[] accelData, gyroData;
    static long accelCalib[] = new long[3];
    static long gyroCalib[] = new long[3];

    static double gyroX = 0;
    static double gyroY = 0;
    static double gyroZ = 0;

    static double accelX;
    static double accelY;
    static double accelZ;

    static double angleX;
    static double angleY;
    static double angleZ;

    public Sensor() {
        //System.out.println(""Hello, Raspberry Pi!"");
        try {
            bus = I2CFactory.getInstance(I2CBus.BUS_1);

            sensor = bus.getDevice(0x68);

            sensor.write(0x6B, (byte) 0x0);
            sensor.write(0x6C, (byte) 0x0);
            System.out.println(""Calibrating..."");

            calibrate();

            Thread sensors = new Thread(){
                    public void run(){
                        try {
                            readSensors();
                        } catch (IOException e) {
                        System.out.println(e.getMessage());
                    }
                    }
            };
            sensors.start();
        } catch (IOException e) {
            System.out.println(e.getMessage());
        }
    }

    private static void readSensors() throws IOException {
        long time = System.currentTimeMillis();
        long sendTime = System.currentTimeMillis();

        while (true) {
            accelData = new byte[6];
            gyroData = new byte[6];
            int r = sensor.read(0x3B, accelData, 0, 6);
            accelX = (((accelData[0] &lt;&lt; 8)+accelData[1]-accelCalib[0])/16384.0)*9.8;
            accelY = (((accelData[2] &lt;&lt; 8)+accelData[3]-accelCalib[1])/16384.0)*9.8;
            accelZ = ((((accelData[4] &lt;&lt; 8)+accelData[5]-accelCalib[2])/16384.0)*9.8)+9.8;
            accelZ = 9.8-Math.abs(accelZ-9.8);

            double hypotX = Math.sqrt(Math.pow(accelX, 2)+Math.pow(accelZ, 2));
            double hypotY = Math.sqrt(Math.pow(accelY, 2)+Math.pow(accelZ, 2));


            double accelAngleX = Math.toDegrees(Math.asin(accelY/hypotY));
            double accelAngleY = Math.toDegrees(Math.asin(accelX/hypotX));

            //System.out.println((int)gyroX+"", ""+(int)gyroY);

            //System.out.println(""accelX: "" + accelX+"" accelY: "" + accelY+"" accelZ: "" + accelZ);

            r = sensor.read(0x43, gyroData, 0, 6);
            if(System.currentTimeMillis()-time &gt;= 5)
            {
                gyroX = (((gyroData[0] &lt;&lt; 8)+gyroData[1]-gyroCalib[0])/131.0);
                gyroY = (((gyroData[2] &lt;&lt; 8)+gyroData[3]-gyroCalib[1])/131.0);
                gyroZ = (((gyroData[4] &lt;&lt; 8)+gyroData[5]-gyroCalib[2])/131.0);

                angleX += gyroX*(System.currentTimeMillis()-time)/1000;
                angleY += gyroY*(System.currentTimeMillis()-time)/1000;
                angleZ += gyroZ;

                angleX = 0.95*angleX + 0.05*accelAngleX;
                angleY = 0.95*angleY + 0.05*accelAngleY;

                time = System.currentTimeMillis();
            }
            //System.out.println((int)angleX+"", ""+(int)angleY);
            //System.out.println((int)accelAngleX+"", ""+(int)accelAngleY);
        }
    }

    public static void calibrate() throws IOException {
        int i;
        for(i = 0; i &lt; 3000; i++)
        {
            accelData = new byte[6];
            gyroData = new byte[6];
            int r = sensor.read(0x3B, accelData, 0, 6);
            accelCalib[0] += (accelData[0] &lt;&lt; 8)+accelData[1];
            accelCalib[1] += (accelData[2] &lt;&lt; 8)+accelData[3];
            accelCalib[2] += (accelData[4] &lt;&lt; 8)+accelData[5];

            r = sensor.read(0x43, gyroData, 0, 6);
            gyroCalib[0] += (gyroData[0] &lt;&lt; 8)+gyroData[1];
            gyroCalib[1] += (gyroData[2] &lt;&lt; 8)+gyroData[3];
            gyroCalib[2] += (gyroData[4] &lt;&lt; 8)+gyroData[5];
            try {
                Thread.sleep(1);
            } catch (Exception e){
                e.printStackTrace();
            }
        }
        gyroCalib[0] /= i;
        gyroCalib[1] /= i;
        gyroCalib[2] /= i;

        accelCalib[0] /= i;
        accelCalib[1] /= i;
        accelCalib[2] /= i;
        System.out.println(gyroCalib[0]+"", ""+gyroCalib[1]+"", ""+gyroCalib[2]);
    }

    public double readAngle(int axis)
    {
        switch (axis)
        {
            case 0:
                return angleX;
            case 1:
                return angleY;
            case 2:
                return angleZ;
        }

        return 0;
    }

    public double readGyro(int axis)
    {
        switch (axis)
        {
            case 0:
                return gyroX;
            case 1:
                return gyroY;
            case 2:
                return gyroZ;
        }

        return 0;
    }

    public double readAccel(int axis)
    {
        switch (axis)
        {
            case 0:
                return accelX;
            case 1:
                return accelY;
            case 2:
                return accelZ;
        }

        return 0;
    }
}
</code></pre>

<p><strong>Edit:</strong></p>

<p>I have re-written my code in C++ to see if it will run faster but it's still running at about the same speed(about 15 ms per cycle or about 66 Hz).</p>

<p>This is my new code in C++:</p>

<pre><code>#include &lt;wiringPi.h&gt;
#include &lt;wiringPiI2C.h&gt;

#include &lt;sys/socket.h&gt;
#include &lt;netinet/in.h&gt;

#include &lt;string.h&gt;
#include &lt;string&gt;
#include &lt;iostream&gt;
#include &lt;unistd.h&gt;

#include &lt;boost/thread.hpp&gt;

#include &lt;time.h&gt;

#include &lt;cmath&gt;

#define axisX 0
#define axisY 1
#define axisZ 2

#define kP 20
#define kI 0
#define kD 0

#define FREQUENCY 490

#define MODE1 0x00
#define MODE2 0x01
#define SUBADR1 0x02
#define SUBADR2 0x03
#define SUBADR13 0x04
#define PRESCALE 0xFE
#define LED0_ON_L 0x06
#define LED0_ON_H 0x07
#define LED0_OFF_L 0x08
#define LED0_OFF_H 0x09
#define ALL_LED_ON_L 0xFA
#define ALL_LED_ON_H 0xFB
#define ALL_LED_OFF_L 0xFC
#define ALL_LED_OFF_H 0xFD

// Bits
#define RESTART 0x80
#define SLEEP 0x10
#define ALLCALL 0x01
#define INVRT 0x10
#define OUTDRV 0x04

#define BILLION 1000000000L

using namespace std;

double accelCalX = 0;
double accelCalY = 0;
double accelCalZ = 0;

double gyroCalX = 0;
double gyroCalY = 0;
double gyroCalZ = 0;

double PX;
double PY;

double IX = 0;
double IY = 0;

double DX;
double DY;

double lastErrorX;
double lastErrorY;

int throttle = 1471;

int sensor = wiringPiI2CSetup(0x68);
int pwm = wiringPiI2CSetup(0x40);

array&lt;int,4&gt; motorVal;

struct timespec now, then;

int toSigned(int unsignedVal)
{
    int signedVal = unsignedVal;
    if(unsignedVal &gt; 32768)
    {
        signedVal = -(32768-(unsignedVal-32768));
    }
    return signedVal;
}

double getAccel(int axis)
{
    double X = (toSigned((wiringPiI2CReadReg8(sensor, 0x3B) &lt;&lt; 8)+wiringPiI2CReadReg8(sensor, 0x3C)))/1671.8;
    double Y = (toSigned((wiringPiI2CReadReg8(sensor, 0x3D) &lt;&lt; 8)+wiringPiI2CReadReg8(sensor, 0x3E)))/1671.8;
    double Z = (toSigned((wiringPiI2CReadReg8(sensor, 0x3F) &lt;&lt; 8)+wiringPiI2CReadReg8(sensor, 0x40)))/1671.8;

    X -= accelCalX;
    Y -= accelCalY;
    Z -= accelCalZ;
    Z = 9.8-abs(Z-9.8);

    switch(axis)
    {
        case axisX:
            return X;
        case axisY:
            return Y;
        case axisZ:
            return Z;
    }
}

double getGyro(int axis)
{
    double X = (toSigned((wiringPiI2CReadReg8(sensor, 0x43) &lt;&lt; 8)+wiringPiI2CReadReg8(sensor, 0x44)))/1671.8;
    double Y = (toSigned((wiringPiI2CReadReg8(sensor, 0x45) &lt;&lt; 8)+wiringPiI2CReadReg8(sensor, 0x46)))/1671.8;
    double Z = (toSigned((wiringPiI2CReadReg8(sensor, 0x47) &lt;&lt; 8)+wiringPiI2CReadReg8(sensor, 0x48)))/1671.8;

    X -= gyroCalX;
    Y -= gyroCalY;
    Z -= gyroCalZ;


    switch(axis)
    {
        case axisX:
            return X;
        case axisY:
            return Y;
        case axisZ:
            return Z;
    }
}

void calibrate()
{
    int i;
    for(i = 0; i &lt; 1500; i++)
    {
        accelCalX += (toSigned((wiringPiI2CReadReg8(sensor, 0x3B) &lt;&lt; 8)+wiringPiI2CReadReg8(sensor, 0x3C)))/1671.8;
        accelCalY += (toSigned((wiringPiI2CReadReg8(sensor, 0x3D) &lt;&lt; 8)+wiringPiI2CReadReg8(sensor, 0x3E)))/1671.8;
        accelCalZ += (toSigned((wiringPiI2CReadReg8(sensor, 0x3F) &lt;&lt; 8)+wiringPiI2CReadReg8(sensor, 0x40)))/1671.8;

        gyroCalX += (toSigned((wiringPiI2CReadReg8(sensor, 0x43) &lt;&lt; 8)+wiringPiI2CReadReg8(sensor, 0x44)))/1671.8;
        gyroCalX += (toSigned((wiringPiI2CReadReg8(sensor, 0x45) &lt;&lt; 8)+wiringPiI2CReadReg8(sensor, 0x46)))/1671.8;
        gyroCalX += (toSigned((wiringPiI2CReadReg8(sensor, 0x45) &lt;&lt; 8)+wiringPiI2CReadReg8(sensor, 0x46)))/1671.8;
        usleep(1000);
    }

    accelCalX /= i;
    accelCalY /= i;
    accelCalZ /= i;
    accelCalZ -= 9.8;

    gyroCalX /= i;
    gyroCalY /= i;
    gyroCalZ /= i;

    cout &lt;&lt; accelCalX &lt;&lt; "" "" &lt;&lt; accelCalY &lt;&lt; "" "" &lt;&lt; accelCalZ &lt;&lt; ""\n"";
}

int calculatePulseWidth(double millis, int frequency) {
    return (int)(floor(4096 * millis * frequency/1000));
}

void add(double value, int i)
{
    value = calculatePulseWidth(value/1000, FREQUENCY);
    if(motorVal[i]+value &gt; 1471 &amp;&amp; motorVal[i]+value &lt; 4071)
    {
        motorVal[i] += value;
    }else if(motorVal[i]+value &lt; 1471)
    {
        //System.out.println(""low"");
        motorVal[i] = 1471;
    }else if(motorVal[i]+value &gt; 4071)
    {
        //System.out.println(""low"");
        motorVal[i] = 4071;
    }
}

void getThrottle()
{
    int sockfd,n;
    struct sockaddr_in servaddr,cliaddr;
    socklen_t len;
    char mesg[1000];

    sockfd=socket(AF_INET,SOCK_DGRAM,0);

    bzero(&amp;servaddr,sizeof(servaddr));
    servaddr.sin_family = AF_INET;
    servaddr.sin_addr.s_addr = htonl(INADDR_ANY);
    servaddr.sin_port = htons(8080);
    bind(sockfd,(struct sockaddr *)&amp;servaddr,sizeof(servaddr));

    while(true)
    {
        len = sizeof(cliaddr);
        n = recvfrom(sockfd,mesg,1000,0,(struct sockaddr *)&amp;cliaddr,&amp;len);
        mesg[n] = 0;
        string message(mesg);
        string values[5];

        int valIndex = 0;
        int lastIndex = 0;
        for(int i = 0; i &lt; message.length(); i++)
        {
            if(message[i] == ' ')
            {
                values[valIndex] = message.substr(lastIndex+1, i);
                lastIndex = i;
                valIndex++;
            }
        }
        values[valIndex] = message.substr(lastIndex+1, message.length());

        throttle = calculatePulseWidth(((stoi(values[4])*12.96)+733)/1000, FREQUENCY);
    }
}

void setAllPWM(int on, int off) {
    wiringPiI2CWriteReg8(pwm, ALL_LED_ON_L, (on &amp; 0xFF));
    wiringPiI2CWriteReg8(pwm, ALL_LED_ON_H, (on &gt;&gt; 8));
    wiringPiI2CWriteReg8(pwm, ALL_LED_OFF_L, (off &amp; 0xFF));
    wiringPiI2CWriteReg8(pwm, ALL_LED_OFF_H, (off &gt;&gt; 8));
}

void setPWM(int on, int off, int channel) {
    wiringPiI2CWriteReg8(pwm, LED0_ON_L + 4 * channel, (on &amp; 0xFF));
    wiringPiI2CWriteReg8(pwm, LED0_ON_H + 4 * channel, (on &gt;&gt; 8));
    wiringPiI2CWriteReg8(pwm, LED0_OFF_L + 4 * channel, (off &amp; 0xFF));
    wiringPiI2CWriteReg8(pwm, LED0_OFF_H + 4 * channel, (off &gt;&gt; 8));
}

void setPWMFrequency(double frequency) {
    double prescaleval = 25000000.0;
    prescaleval /= 4096.0;
    prescaleval /= frequency;
    prescaleval -= 1.0;
    double prescale = floor(prescaleval + 0.5);
    int oldmode = wiringPiI2CReadReg8(pwm, MODE1);
    int newmode = (oldmode &amp; 0x7F) | 0x10;
    wiringPiI2CWriteReg8(pwm, MODE1, newmode);
    wiringPiI2CWriteReg8(pwm, PRESCALE, (floor(prescale)));
    wiringPiI2CWriteReg8(pwm, MODE1, oldmode);
    usleep(50000);
    wiringPiI2CWriteReg8(pwm, MODE1, (oldmode | 0x80));
}

void initSensor()
{
    wiringPiI2CWriteReg8(sensor, 0x6B, 0x0);
    wiringPiI2CWriteReg8(sensor, 0x6C, 0x0);
}

void initPWM()
{
    setAllPWM(0, 0);
    wiringPiI2CWriteReg8(pwm, MODE2, OUTDRV);
    wiringPiI2CWriteReg8(pwm, MODE1, ALLCALL);
    usleep(50000);
    int mode1 = wiringPiI2CReadReg8(pwm, MODE1);
    mode1 = mode1 &amp; ~SLEEP;
    wiringPiI2CWriteReg8(pwm, MODE1, mode1);
    usleep(50000);
    setPWMFrequency(FREQUENCY);
}

double millis(timespec time)
{
    return (time.tv_sec*1000)+(time.tv_nsec/1.0e6);
}

double intpow( double base, int exponent )
{
    int i;
    double out = base;
    for( i=1 ; i &lt; exponent ; i++ )
    {
        out *= base;
    }
    return out;
}

int main (void)
{

    initSensor();
    initPWM();

    cout &lt;&lt; ""Calibrating..."" &lt;&lt; ""\n"";
    calibrate();

    boost::thread server(getThrottle);

    clock_gettime(CLOCK_MONOTONIC, &amp;then);

    while(true)
    {
        motorVal.fill(throttle);

        clock_gettime(CLOCK_MONOTONIC, &amp;now);

        double dt = (millis(now)-millis(then))/1000;

        then = now;

        double accelX = getAccel(0);
        double accelY = getAccel(1);
        double accelZ = getAccel(2);

        double hypotX = sqrt(intpow(accelX, 2)+intpow(accelZ, 2));
        double hypotY = sqrt(intpow(accelY, 2)+intpow(accelZ, 2));

        double accelAngleX = (180/3.14)*(asin(accelY/hypotY));
        double accelAngleY = (180/3.14)*(asin(accelX/hypotX));

        double errorX = -getGyro(0);
        double errorY = getGyro(1);

        PX = errorX;
        PY = errorY;

        IX += errorX*dt;
        IY += errorY*dt;

        IX = 0.95*IX+0.05*accelAngleX;
        IY = 0.95*IY+0.05*accelAngleY;

        DX = (errorX-lastErrorX)*dt;
        DY = (errorY-lastErrorY)*dt;

        lastErrorX = errorX;
        lastErrorY = errorY;

        double outputX = kP*PX+kI*IX+kD*DX;
        double outputY = kP*PY+kI*IY+kD*DY;

        add(outputY, 0);//-outputX+
        add(outputY, 1);//-outputX-
        add(outputY, 2);//outputX-
        add(outputY, 3);//outputX+

        setPWM(0, motorVal[0], 12);
        setPWM(0, motorVal[1], 13);
        setPWM(0, motorVal[2], 14);
        setPWM(0, motorVal[3], 15);
    }
}
</code></pre>

<p>In addition two of the motors seem like they are lagging when I turn the quadcopter fast in one direction. Also for some strange reason the quadcopter seems less responsive to P gain; I have it at 20 in the C++ version and it is working about the same as when I had it at 1.5 in the java version.</p>

<p><strong>Edit:</strong></p>

<p>After doing some more testing I have determined that reading from the MPU6050 and writing to the PCA9685 board that I am using to control the ESCs is the source of the delay. Does anybody know how to speed this up?</p>

<p><strong>Edit:</strong></p>

<p>I managed to speed up my code to about 200 Hz by changing the i2c baud rate, but the quadcopter is still thrashing. I have spent hours trying to tune the pid controller, but it doesn't seem to help at all.</p>
","quadcopter pid raspberry-pi"
"6721","How feasible is the idea of operating a robotic arm in a non-sophisticated way?","<p><img src=""http://i.stack.imgur.com/cpwJd.png"" alt=""enter image description here"">I and my team have to design a robot using an arduino chip. The objective of the robot is to grab golf balls at a set of golf pins at different heights and pre-defined locations. We couldn't figure out a possible mechanism that could collect the balls and drop them into the trailer except for a robot arm. However, we don't have experience and time in designing a sophisticated system for the arm like recognizing where the ball is and then grabbing it accordingly. What would a feasible option be compared to a non-sophisticated  robot arm?</p>

<p>Note:The robot must be autonomous.</p>
","arduino control robotic-arm"
"6733","Minimising lateral drift in a PID (Arduino) controlled quadcopter using a 6DOF IMU","<p>I'm developing a stabilisation system for an 'off-the-shelf' quadcopter by using an Arduino Mega and an IMU. The IMU is reading the angle of the quad, calculating motor commands by using a PID controller and applying them to the motors. It works well when constrained in a test bed, however in reality, although the quad is straight and level, it's drifting to one side because of its recent motor commands correcting the pitch/yaw. Is there any way I can (without using a vision system) keep the quad in one place without drifting?</p>

<p>I've looked into obtaining velocity by integrating the acceleration value, however it's extremely noisy and doesn't give a meaningful reading.</p>
","arduino quadcopter pid imu rcservo"
"6739","Turning an epilog laser into a 3d printer?","<p>We have an epilog laser cutter around here and I was wondering if it would possibly work as a base for a 3d printer? Here is a <a href=""https://www.dropbox.com/sh/ioeveoskrt5qc12/AAD4tfmqiSgii4uiq5jQl1BFa?dl=0"" rel=""nofollow"">Dropbox photo album</a> of the laser cutter. I am thinking I will have to get a new control system but I am unsure if I will be able to use the motor controllers or if they are embedded in the current control's board. I am also unsure if it has fine enough control on the z axis but if not that can be modified.</p>

<p>What would be a good head to look at?
Any other thoughts?</p>
","laser 3d-printing"
"6740","In the PID equation K[((s+a)^2)/s] what values correspond to the PID coefficients Kp, Ki, Kd?","<p>I'm trying to understand how to obtain the Kp, Ki, Kd values after finding a combination of K and a that works for me. Do I just expand the equation and take the coefficients? </p>
","control pid tuning"
"6741","How many methods can I use to acquire depth data?","<p>I am a newbie in Robotics. As far as I know, there're generally two ways to acquire depth information of a scene. One is the stereo vision method which applies two cameras. The other is the RGBD sensors such as Kinect and Primesense. It seems that both methods are in use currently. However, I do not know what are their advantages against each other.</p>

<p>I think the Kinect is a perfect solution over stereo vision ignoring its expense. So I have two questions here:</p>

<ol>
<li>Is there advantages of binocular methods over Kinect besides expense?</li>
<li>As I know, both the two methods are confined with a limited distance detection range. In real world applications, we sometimes also need depth data at distance. Is there a method that we can acquire or estimate data information at far distance( not considering  laser-based detectors ).</li>
</ol>

<p>Further more, my application may be a small flight vehicle. Which method and equipment should I choose? Will the traditional binocular camera be too slow for my application?</p>
","computer-vision stereo-vision"
"6750","Does an electronic compass work underwater","<p>I'm building a submersible ROV, so I need a way to navigate. So using a compass would help but this brings up the question, does an electronic compass work underwater?</p>

<p>My thoughts are the water might act as a faraday cage, and therefore interfere with the magnetic field. Therefore it might not even work. Maybe a gyroscope might be a better solution.</p>
","sensors compass"
"6754","Why I'm getting very long terms in the inertia matrix (or dynamics model) of the robot using matlab script?","<p>I'm working on the dynamics model of a RRRR articulated robot, I'm following Euler-Lagrange approach and developing my code in m-file in matlab; I'm looking for dynamic model of this form: $$
D(q) \ddot{q} + C(q,\dot{q})\dot{q}+ g(q) = \tau
$$
where $D$ and $C$ are $4 \times4$ matrices and $g$ and $\tau$ (torque) are $4\times1$ vectors; by formulating the kinetic and potential energies;;</p>

<p>The problem is that, I'm getting very long equations, and the term in $D$ matrix are very huge and nonlinear, involving sin and cos; I'm talking about a several pages per equation;
After I published the code - 7 pages - and the output I got around 45 pages in total;
I searched <a href=""http://compgroups.net/comp.soft-sys.matlab/very-long-equations-of-motion-simulink/860201"" rel=""nofollow"">around</a> there was some guy he faced the same problem before, but there was no helpful proposal.</p>

<p>Any Suggestions ?? </p>
","robotic-arm kinematics dynamics matlab"
"6764","How do I get MPU 6050 gyroscope data using ""MPU6050_6Axis_MotionApps20.h"" library","<p>I'm currently calibrating the MPU6050 chip using an arduino mega 2560. I am using the J Rowberg 12c dev libraries. I can get it to print raw accelerometer and gyroscpe values (very unstable, wildly changing values). In the digital motion processing chip library, I can get it to print euler angles, quaternions, real world acceleration and actual acceleration but there is no option to get gyroscope data. </p>

<p>can I use the DMP library to get gyro data or is it only possible to get raw unprocessed gyro values?</p>
","arduino imu accelerometer gyroscope"
"6765","Open Source software for Quadcopters","<p>my question is general so please bear with me. I'm now interested in buying a quadcopter and develop some functions that it does for example an android app to control it, or objects detection. 
So my question is what are the available quadcopters which has a software that allows me to do such things not just a flying toy?</p>

<p>P.S: I'm asked to buy a kit within 600$ and not build it by myself</p>
","quadcopter"
"6767","Magnetometer to measure high angular velocity in small object","<p>I have to measure the frequency of a little circle in rotation. You can image this circle flying in air, because this circle can't touch anything that is not in rotation. So I can't use some simple trick to count the number of complete rotation in an amount of time.</p>

<p>So I supposed my only chance was to use an accelerometer, a gyroscope, or a magnetometer. The accelerometer can detect the centripetal acceleration, and the gyroscope and the magnetometer with some calculus directly the frequency.</p>

<p>The problem is the high frequency of this circle (can reach up to 50 Hz). Doing some simple calculus we know we need a gyroscope that can measure big angular velocity: 50*360°/s = 18.000°/s. Also the accelerometer need big range of values (the radius of the circle is only 5 cm): w (angular velocity) = (2 * 3,14) / (1/50) = 314 rad/sec acc_centr = w^2 * R = 98596 * 0,05 ~ 5000m/s^2 ~ 500g</p>

<p>Now I have seen there are some accelerometer or gyroscope for industrial purposes with enough range, but my question is:</p>

<p>How I can understand if a magnetometer can used is this kind of application? Considering there is no magnetic field near the circle, can a magnetometer be used to measure quickly change in inclination? In the datasheet i can read how ofter the sensor can communicate with my arduino, but nothing about how quick the rotation can be.</p>

<p>Is the reason that a magnetometer don't have the limits of a gyroscope or an accelerometer?</p>
","electronics"
"6772","Is the crazyflie control board considered a microcontroller","<p>I am currently doing a project for school and we are told that we must use a micro controller that ends up controlling some external hardware, now i know the crazyflie is controlling the motors which counts as external hardware but is it a micro controller? My second question is i want to purchase the kit so i can assembly it myself however I saw that you can use an expansion board so you need not solder and also i plan on not buying a remote its possible to control the crazyflie via my iPhone correct? I would appreciate it if someone could answer my questions. Thank you in advance</p>
","quadcopter"
"6776","Iron man Jarvis like robot","<p>I'm very passionate about robots from my childhood.I'm a java developer.
I love sci-fi movies.I have a little bit knowledge in embedded systems and electronics.
My ambition is to build a robot like Jarvis (In Iron Man Movie).Which is a voice controlled robot.I would like to implement that in my house as a home automation system.It would take voice as input and take appropriate action.. Please help me to do this. 
Any kind of help is appreciated..</p>
","mobile-robot quadcopter microcontroller mechanism embedded-systems"
"6782","Sending commands to Roomba from PC","<p>I'm trying to send some commands to the Roomba. However is behaving strange.</p>

<p>This is the manual that I'm using.
<a href=""http://www.irobot.com/~/media/MainSite/PDFs/About/STEM/Create/create_2_Open_Interface_Spec.pdf"" rel=""nofollow"">http://www.irobot.com/~/media/MainSite/PDFs/About/STEM/Create/create_2_Open_Interface_Spec.pdf</a>  </p>

<p>First of all. I have consulted several manuals, some of them say that the default baudrate is 115200, however it works for me at 57200. </p>

<p>I'm trying to get a response from the Roomba sending the following comand </p>

<p>Examples:
• To turn on iRobot Create’s Play LED only:
128 132 139 2 0 0</p>

<p>However, the Roomba goes crazy and start going around. Any idea what's happening or what I'm not doing? Or what should I do first? </p>

<p>Thank you. </p>
","irobot-create roomba"
"6783","What equipment has been used to design this robot","<p>Look at this robot here <a href=""http://www.meccanotec.com/step781b.JPG"" rel=""nofollow"">http://www.meccanotec.com/step781b.JPG</a>
I can see rods which have a lot of holes and planes which also have holes. This seems to be a way to create flexibility in how the things are connected together to create the final robot. Is there a name for this type of equipment, metals with holes. Where can I get it? I am aware of people using lego blocks to create robots, but am not sure about what these metal rods and plates with holes are.</p>

<p>Is there a free application in which I can design a mechanical structure like the one in the image and add gears and then simulate it to see how it will rotate and bend should a real robot like that be created?</p>

<p>What would be the quickest way to create a robot like this?</p>

<p>Edit: Thankyou; Frank and lanyusea. If one wants to do a simulation of the mechanical model, in others words play with the robot on the computer before actually building it (with all those gears in action), which software is most suitable for that purpose?</p>
","robotic-arm"
"6784","How to know what type of stepper motors to use when designing a robot","<p>I am talking about robots like this one: <a href=""http://www.meccanotec.com/step781b.JPG"" rel=""nofollow"">http://www.meccanotec.com/step781b.JPG</a></p>

<p>How would a person know what type of motor to use in design of such a robot? What I want to understand is that stepper motors have different step sizes, different torques among other things. How do we determine what type of stepper motor is most suitable to be used in a given robot?</p>
","robotic-arm stepper-motor"
"6787","Is it tough to make a robotic workshop of your own","<p>I want to make a robotic workshop.. I recruited 10 members to work...  Please give some tips about robotic workshop</p>
","irobot-create"
"6791","Does anyone have any walking patterns for a Biped Scout? (LYNXmotion)","<p>I recently got a LYNX Biped Scout and found that it is really hard to actually come up with a working ""Gait"" or walking pattern. </p>

<p>Making a servo move is easy, that's not the problem, I previously built a robotic arm from scratch (I have pictures if anyone is interested) and that one can be controlled via Arduino and a few potentiometers as it only has 4 degrees of freedom so it's not too hard to keep track of the different limbs.</p>

<p>However the Scout is a different beast entirely. It's a purpose built kit with 12 servos and to control them I'm using the LYNX SSC-32 Sequencer which is distributed freely on their website. The only problem is that making them all move in sequence to produce a convincing walking motion is actually really hard.</p>

<p>Has anyone got any patterns for this robot they would be happy to share? </p>
","walk"
"6792","How can I know which system is easier to control using PID controller?","<p>I have a inverted inertia wheel pendulum.
I suppose that if I have a wheel with larger inertia at its top, the system would be more stable.</p>

<p>How can I prove or disprove my conjecture?</p>
","control pid stability"
"6801","Unilateral Torque Constraint on the foot-ground interface","<p>I was studying the basics of legged locomotion and came across the unilateral force and torque constraints at the foot-ground interface.</p>

<p>I understood the implication of the unilateral constraint on the force ( the ground can only push the foot but not pull it) but I am unable to understand what does the unilateral torque constraint translate into physically in this case. Can anyone clarify it?</p>
","mechanism motion force legged walk"
"6804","How can I filter Gyroscopic data?","<p>I am using an Arduino Mega with an MPU6050. I can get gyroscopic data as well as euler angles. The problem is my gyro data keeps going back and forth between 0 and -1 despite me not moving it at all (it stays on -1 the most). What can I do to filter what I assume is noise? I am going to use the gyro data for a quadcopter PID rate controller so I cant really have it telling me I am rotating at -1 deg/sec. That would be catastrophic for the quadcopter</p>
","arduino imu accelerometer gyroscope"
"6807","Driver Board to Control 16 Brushed DC Motors","<p>I am building a humanoid robot with DC motor actuated fingers. There are 16 brushed DC motors to be position controlled with help of hall effect sensors implanted at the joints of each fingers. I need a developed driver board to control these 16, 3 watt, 12 v,DC motors.
Also each motor is equipped with an incremental encoder for speed control. </p>

<p>thank you</p>
","motor servomotor driver"
"6808","Provides a 10 Degree-Of-Freedom IMU reduntant data?","<p>Basic question concerning sensor fusion:</p>

<p>A standard 10 DoF IMU, I mean this cheap things for the tinkerer at home, provides 10 values:
3 Accelerometers
3 Gyroscope
3 Magnetic Field Measurements
1 Pressure sensor (+ 1 Temperature) </p>

<p>I know that the accel-data provide long term stability, but are useless for short term and the gyroscope is more or less vice versa. </p>

<p>So there are tons of strategies to ""marry"" this values, but how does the magnetic field measurement fit into this framework?</p>

<p>Basically the magnetic field measurement should provide an attitude, too. Like the other two sensors combined. I guess this measurement alone is neither reliable.</p>

<p>So how do all these sensors fit together?</p>

<p>BR </p>
","imu sensor-fusion"
"6810","Particle Filter Sampling Step","<p>I emphasize that my question is about <strong><em>sampling</em></strong>, not <em>resampling</em>.  </p>

<p>I'm reading the Probabilistic Robotics book by Thrun et al, Chapter 4 on Non-Parametric Filters.  The section on Particle filters has an algorithm which states that for each particle index $m$ (see line 4):  </p>

<p>sample $x_t^{[m]} \sim p(x_t|u_t,x_{t-1}^{[m]})$</p>

<p>The text's explanation of this step is quoted as:</p>

<blockquote>
  <p>Line 4. generates a hypothetical state $x_t^{[m]}$ for time t based on
  the particle $x_{t-1}$ and the control $u_t$.  The resulting sample is
  index by $m$, indicating that it is generated from the $m$-th particle
  in $\chi_{t-1}$.  This step involves sampling from the state
  transition distribution $p(x_t|u_t,x_{t-1})$.  To implement this step,
  one needs to be able to sample from this distribution.  The set of
  particles obtained after $M$ iterations is the filter's representation
  of $\bar{bel}(x_t)$.</p>
</blockquote>

<p>If I understand correctly, this step says that the m-th <strong><em>sampled</em></strong> particle $x_t^{[m]}$ is obtained by advancing the previous m-th particle with control command $u_t$.  I assume that the motion is not deterministic, so the result of this motion is a conditional probability, based on the particle's previous position $u_t$.  </p>

<p>However, I'm confused over how exactly to construct this conditional probability $p(x_t|u_t,x_{t-1}^{[m]})$.  Is this information usually given?  Or is it constructed from the distribution of the other particles?  </p>
","particle-filter"
"6816","Determine current roomba state / operating mode","<p>Using the <a href=""http://www.robotappstore.com/files/KB/Roomba/Roomba_SCI_Spec_Manual.pdf"" rel=""nofollow"" title=""SCI messages"">SCI messages</a>, I would like to determine the current operating mode or state of a iRobot Roomba 780. Finally, I would like to detect and separate four states: </p>

<ul>
<li>Cleaning</li>
<li>In docking station</li>
<li>Returning to docking station</li>
<li>Error (e.g. trapped on obstacle)</li>
</ul>

<p>What is a fast and reliable way to detect those states using <a href=""http://www.robotappstore.com/files/KB/Roomba/Roomba_SCI_Spec_Manual.pdf"" rel=""nofollow"" title=""SCI messages"">SCI data</a>?</p>

<p>The Roomba SCI Sensor packets ""Remote Control Command"" and ""Buttons"" seem to return the currently called commands and not the currently executed ones.</p>
","roomba"
"6817","Basic general question about controllers","<p>Suppose I have a mechanical system which is free to move on a given rail [-5m, 5m] like a motorized cart. The whole system can be mathematically expressed through a <em>linear timeinvariant</em> system equations.</p>

<p>If I need to <em>control</em> <em>only</em> the position (for example saying the controller: ""move to +2.3"") I can simply design a PID controller that, given a set point moves the cart to that position.</p>

<p>Now I need much more and I want to <em>control</em> the <em>position</em> <strong>and</strong> the <em>velocity</em> of the cart. So I need for example to say: ""move to +2.3 with a specific velocity's profile"". Of course the <em>vel = 0</em> at the end position.</p>

<p>Question: how should I design such a controller? Do I need specifically a special type of controller? Or I have a huge choice?</p>

<p>Any help, graph, link and example is really appreciated.</p>

<p>Regards</p>
","control pid"
"6818","Remote Control Relative to Driver","<p>Is it possible to remote control a 'robot' relative to the driver with an angle sensor (or any other sensor)?  For example, if the robot starts in this position</p>

<pre><code>--------------
|    Front   |
|  --------  |
| |________| |     [robot]
|    Back    |
--------------
</code></pre>

<p>and the joystick is in this configuration</p>

<pre><code>--------------
|  Forwards  |
| [joystick] |
|  Backwards |
--------------
</code></pre>

<p>then if the robot turns around,</p>

<pre><code>--------------
|    Back    |
|  --------  |
| |________| |     [robot]
|    Front   |
--------------
</code></pre>

<p>pushing the controller forwards will still make the robot go forward</p>

<pre><code>--------------
|      ^     |
| [joystick] |
|  Backwards |
--------------

--------------
|      ^     |
|  --------  |
| |________| |     [robot]
|    Front   |
--------------
</code></pre>

<p><em>even though from the robot's POV, he's going backwards</em>.</p>

<p>Any ideas/solutions?</p>
","control sensors"
"6821","Is motor current proportional to thrust?","<p>So I was making sure my circuit for an airboat I was working on was safe. And checking a motor, it has 35 amps max current running 11.1v at 1000. (However my ESC has a 30 current, 40 burst amps). </p>

<p>The recommended tested prop for this is a 11x9 3-blade and runs the motor at 20 amps. Doing some quick calculations via on online calculator, it appears to give a value of 5.4lbs of force (way off to the 2.65 lbs measured, but regardless...). When I type in a prop I want (13x6) it gives a thrust value of 7.53 .</p>

<p>Now, if the motor current is proportional to the thrust, 5.4 lbs / 20 amps = 7.53 / running amps. And therefore the amount of amps the prop would be, in theory a little less than 30 if this is indeed the case. Thus safe for my application. </p>

<p>This would also make sense in physics terms as Power = Current*Voltage which is proportional to thrust, but just need to make sure.</p>

<p>So does this thought process work for choosing a prop? My device will  be doing very short runs (less than 25 seconds), so near-boundaries should be safe...</p>
","quadcopter brushless-motor"
"6822","What is the link between a quadcopter transmitter pulse and the roll/pitch/yaw angles?","<p>I want to design a data logger for my quadcopter using the Arduino Mega board. I want to record the roll, pitch and yaw angles each second or 5 seconds, so they can be viewed later after a flight has ended. There's just one thing I don't understand, and that's how to translate the pitch/roll/yaw angles into a pulse of a specific length that the flight controller receives.</p>

<p>For example, when I press the control for the pitch, the transmitter sends out a pulse to the receiver of the drone and the speed of the drones' motors change accordingly for it to pitch either forward or backward. I can tap into these commands between the flight controller and the transmitter, and be able to record the length of the pulse that was sent out. However, what is the link between the pitch angle and the size of the pulse? Basically, how can I convert the pulse that was recorded by the Arduino board and convert it into the pitch angle in degrees? </p>

<p>Generally, for the transmitter I use, a 1500us-pulse means zero pitch; from 1501-2000 means pitch forward, and from 1000-1499 means pitch backwards (of course, the actual values vary slightly, but this is just a general reference for this question). So for instance, if I sent a pulse of 1400us, how would that translate into an angle in degrees? What's the formula to convert it?</p>

<p>I hope I'm clear, and if this question sounds stupid, please excuse me, but I haven't been able to find good information on it!</p>

<p>Thanks!</p>
","arduino quadcopter"
"6824","How to tune a PIV controller?","<p><img src=""http://i.stack.imgur.com/9wT10.jpg"" alt=""PIV Controller""></p>

<p>How could I tune the above PIV controller? I am trying to get the system to have a settling time of &lt; 1 second, P.O &lt; 15% and zero steady state error.  </p>
","control"
"6826","Difference between planetary and precision gear motors","<p>i'm working on a building a rover and would like some advice on selecting motors. In particular, i want to understand the difference between precision and planetary gear motors.  My robot will way about 10-15lbs i think and would like it to be responsive and quick. I have two sabertooth 2x12 motor controllers (which can supply up to 12amps). I have been looking at these motors and i am not sure which is better choice for my application.  </p>

<p>These are the two sets of motors i am thinking about.
<a href=""https://www.servocity.com/html/precision_robotzone_gear_motor.html"" rel=""nofollow"">https://www.servocity.com/html/precision_robotzone_gear_motor.html</a>
<a href=""https://www.servocity.com/html/3-12v_precision_planetary_gear.html"" rel=""nofollow"">https://www.servocity.com/html/3-12v_precision_planetary_gear.html</a></p>

<p>googling does provide some info on planetary gears, but the application of these two is still is unclear to me.</p>

<p>Thanks</p>
","mobile-robot motor gearing"
"6829","Robotics SLAM datasets - scaling factor","<p>There are several robotics datasets for SLAM, like <a href=""https://vision.in.tum.de/data/datasets/rgbd-dataset/file_formats"" rel=""nofollow"">this one</a>.</p>

<p>In this webpage you can see that the depth image is scaled by a factor of 5000, so that float depth images can be stored in 16 bit png files:</p>

<pre><code>The depth images are scaled by a factor of 5000, i.e., a pixel value of 5000
in the depth image corresponds to a distance of 1 meter from the camera, 10000
to 2 meter distance, etc. A pixel value of 0 means missing value/no data.
</code></pre>

<p>I do not understand why this value is chosen. Why not simply 1000, so that there is a conversion of meters to millimeters? </p>
","slam"
"6831","where can I get openinterface.py?","<p>I am trying to program the create2 irobot using python. there is a script called openinterface.py. where can I download this script?</p>
","software irobot-create python"
"6833","Can't connect to BeagleBone Webserver?","<p>I'm following this <a href=""http://beagleboard.org/getting-started"" rel=""nofollow"">getting started</a> tutorial, connected the board to USB and it's detected as mass storage, got the driver installed (Win 64), and at the third step I wasn't able to connect to BeagleBone webserver at 192.168.7.2, anything I did wrong? Please help.</p>

<p>Here is some troubleshooting info from Getting Started page, I've followed them all. I'm using Chrome, tried the node-webkit based application, not in a Virtual Machine, not using SSH just trying to access the webserver.</p>

<blockquote>
  <p><strong>Troubleshooting</strong></p>
  
  <p>Do not use Internet Explorer.</p>
  
  <p>One option to browse your board is to use this node-webkit based
  application (currently limited to Windows machines):
  beaglebone-getting-started.zip.</p>
  
  <p>Virtual machines are not recommended when using the direct USB
  connection. It is recommended you use only network connections to your
  board if you are using a virtual machine.</p>
  
  <p>When using 'ssh' with the provided image, the username is 'root' and
  the password is blank.</p>
  
  <p>Visit beagleboard.org/support for additional debugging tips.</p>
</blockquote>

<p>UPDATE:
- I tried to install Ubuntu on my machine and connect the BeagleBone, it need not any driver and I can immediately access the webserver after ejecting the mass storage, enabling the 'USB-to-Ethernet Interface'. However in Windows ejecting the mass storage still does nothing. Still trying to make it connect in Windows.</p>
","beagle-bone"
"6835","Internal Pullup Sufficient for i2c in beaglebone Black?","<p>I plan to use P8.13 and P8.15 of the beaglebone in a i2c bitbang mode.
Do i need to use external pull up resistors in my circuit? or can i use the internal pull up which is available on the beaglebone black itself?</p>
","beagle-bone circuit"
"6837","Controlling an ESC for Brushless Motors with an RPi","<p>I'm looking to build a new (first) quadcopter without the conventional flight controller and radio, with an onboard RPi and applying some newfound knowledge on autonomous control to improve my coding skills.</p>

<p>Although, since I've never actually built a quadcopter, I don't actually have any experience in using brushless motors.</p>

<p>I'll be using a RPi B+, so controlling them over I2C was something I looked into. The B+ though only has two I2C interfaces. It also only has two hardware PWM pins and I'm unsure whether software PWM would be enough. I found the Afro SimonK-based ESCs from HobbyKing which have I2C (Intended for the MikroKopter).</p>

<p>I've looked around and people have used the Adafruit 16-channel PWM/Servo drivers to control them. Is this an option to look into? Or is there perhaps a better way?</p>

<p>Also, would be it particularly safe if the RPi is run off the ESC's BEC? It's confusing because, when the ESC is powered on, well, it'll be powered on before the RPi comes up. What do ESCs do when they have bad input?</p>
","quadcopter raspberry-pi brushless-motor esc multi-rotor"
"6838","Implementation of wall and obstacle avoidance","<p>I have a task of developing a simulation of an adaptive robot control system but I don't seem too have anyone to discuss my uncertainties with. I want to keep the simulation as simple as possible as I have a very tight deadline and it's only a one off project that most probably will never be used in my life again. </p>

<p>The minimal behaviour that the agent is supposed to exhibit is wall and obstacle avoidance. It can be extended to avoiding small objects and exploring large ones. </p>

<p>I've decided to go with a simple feedback control system. 
To begin with I'm struggling to decide how to represent the map of agent's environment. What I mean is, what if I want a wall to be from coordinate [0,0] to [0.5]. I could hard code it, e.g. have a matrix with coordinates of all obstacles but how small units do I make... I.e. what if I have two neighbouring coordinates [0,0.01] and [0,0.02] but the agents gets a 'clear to go' to coordinate [0,0.05]. In this case it doesn't know that it actually is about to walk into a wall. I've heard of something called occupancy grid map but I don't exactly get how it works and how to implement it. </p>

<p>Another thing that I am struggling with is how do I distinct between a wall and an obstacle? And then, how do I let the agent know how big that obstacle is so that it can either avoid it or explore it. </p>

<p>Eh, I'm really puzzled with this project.
I would really appreciate any thoughts or directions. Thank you. :-)</p>
","control localization simulation"
"6842","How to calculate the center of mass Jacobian matrix of a robot arm","<p>I have a 4-DOF robot arm system with 4 revolute joints arranged in an open-chain fashion like below:</p>

<p><img src=""http://i.stack.imgur.com/wU0VL.png"" alt=""4-DOF robotic arm""> </p>

<p>Assume that each link’s mass is a point mass located at p_i and each link’s center of mass is at p_i.</p>

<p>What I am trying to do is calculate the center of mass Jacobian matrix of the arm.
I found some related materials online <a href=""http://www.elysium-labs.com/robotics-corner/learn-robotics/biped-basics/com-jacobian/"" rel=""nofollow"">Center of Mass Jacobian</a>.But I am still not very sure about how to calculate it. Could anybody give me some hint? Thanks!</p>
","robotic-arm jacobian"
"6843","Sporadic sensing rates for hc-sr04 ultrasonic distance sensor","<p>Been working on a robot recently which uses ultrasonic sensors for an integral part of the navigation.
While testing the sensors I noticed a strange behaviour, the sensors seem to frequently stop functioning and bring the entire Arduino Mega I'm working with to a stop. The strange part is that these stops seem to be entirely random, on some occasions the sensor will read values consistently (at maybe 20 vals per second) for 10+ seconds, then all of a sudden the sensor will slow to reading only 2-3 values per second with stalls between.</p>

<p>I have tested several sensors and different codes for pinging distances yet the problem has persisted.</p>

<p>This leads me to believe the issue is with the arduino mega itself, but I am unsure how to verify this. Any advice?
Thanks in advance!</p>

<p>PS: other pins on the Mega seem to be working fine, i.e. analog pins for IR reflectance sensors and PWM pins for driving 2 DC motors.</p>
","arduino ultrasonic-sensors"
"6844","aerodynamics of quadcopter","<p>I want to build a quad copter. I want to know how do we calculate thrust or lift generated by using a motor, I am not aware about the capacities of motor. So can you explain how to calculate thrust or lift generated by assuming a motor. And what is the maximum payload that a quad copter can lift for a given thrust. </p>
","quadcopter"
"6845","Recommendation for good source of Robotic Components","<p>Im looking for a good source for robotic components like sheel/tracked robot chasis, motors, sensors, communication and mechanics. I thought about using raspberry and arduino as platforms for automation, is that an good idea? Im asking as i dont know yet much about the motors/drives uses for powering robots.</p>

<p>Thanks!</p>

<p>Uli</p>
","mobile-robot motor"
"6846","Datalogging from Arduino Mega to Dropbox","<p>I have an Arduino Mega board and the Adafruit Ultimate GPS Logger + GPS module shield. I have these two connected together using headers and have the entire thing mounted on my drone. Currently, I have a code that I found online and modified slightly to get GPS coordinates in NMEA format and parse them for the information I actually want. I can store these in an SD card.</p>

<p>The thing is, I want to use the Arduino GSM shield to somehow send this data, either from the SD card or directly, to a folder in Dropbox. I have no idea how to do that, if it's possible at all. I just started working with Arduino about a month ago, so I apologize if my question sounds particularly noob-ish.</p>

<p>Could anyone on this forum at least guide me on how to approach this problem? Thanks!</p>
","arduino gps"
"6849","Graph optimization with G2O","<p>I'm trying to do graph optimization with G2O, mainly in order to perform loop closure. However finding minimal working examples online is an issue (I've found <a href=""https://github.com/MiguelAlgaba/KinectSLAM6D/blob/9891798038fe0b693390513d36fb83346988bf91/kinect6DSLAM.cpp"" rel=""nofollow"">this</a> project, as well as <a href=""https://github.com/jstueckler/mrsmap/blob/2e1c378b3d22520dcc07f650b35b881fbd3e529a/src/slam/slam.cpp"" rel=""nofollow"">this one</a>. The second one though has the form of a library, so one cannot really see how the author uses things.)</p>

<p>In contrast to online loop closure, where people update and optimize a graph every time they detect a loop, I'm doing graph optimization only once, after pairwise incremental registration. So in my case, pairwise registration and global, graph-based optimization are two separate stages, where the result of the first is the input for the second.</p>

<p>I already have a working solution, but the way that works for me is quite different from the usual use of g2o:</p>

<ul>
<li>As nodes I have identity matrices (i.e. I consider that my pointclouds are already transformed with the poses of the pairwise reg. step) and </li>
<li>as edges, I use the relative transformation based on the keypoints of
the pointclouds (also the keypoints are transformed). So in this case
I penalize deviations of the relative pose from the identity matrix.</li>
<li>As Information matrix (inverse of covariance) I simply use a 6x6
identity matrix multiplied by the number of found correspondences
(like <a href=""https://github.com/MiguelAlgaba/KinectSLAM6D/blob/9891798038fe0b693390513d36fb83346988bf91/kinect6DSLAM.cpp"" rel=""nofollow"">this case</a>). </li>
<li>The result of the graph is an update matrix,
i.e. I have to multiply with this the camera poses. </li>
</ul>

<p>Although this works in many/most cases, it is a quite unusual approach, while one cannot draw the graph for debugging (all nodes are identities in the beginning, and the result after optimization is a 3d path), which means that if something goes wrong getting an intuition about this is not always easy.</p>

<p><img src=""http://i.stack.imgur.com/XPvL9.png"" alt=""enter image description here""> <img src=""http://i.stack.imgur.com/BGnxT.png"" alt=""enter image description here""> <img src=""http://i.stack.imgur.com/Kg30b.png"" alt=""enter image description here""></p>

<p>So I'm trying to follow the classic approach:</p>

<ul>
<li>The vertices/nodes are the poses of the pairwise registration</li>
<li>The edges are the relative transformations based on the keypoints/features of the raw pointclouds (i.e. in the camera frame, not transformed by the poses of the pairwise registration)</li>
<li>The output are the new poses, i.e. one simply replaces the old poses with the new ones</li>
<li>Drawing the graph in this case makes sense. For example in case of scanning an object with a turntable, the camera poses form a circle in 3d space.</li>
<li>I'm trying to form all the edges and then optimize only at one stage (this doesn't mean only 1 LM iteration though).</li>
</ul>

<p>However I cannot make things running nicely with the 2nd approach.
I've experimented a lot with the direction of the edges and the relative transformation that is used as measurement in the edges, everything looks as expected, but still no luck. For simplicity I still use the information matrix as mentioned above, it is a 6x6 identity matrix multiplied with the number of correspondences. In theory the information matrix is the inverse of covariance, but I don't really do this for simplicity (plus, following <a href=""https://github.com/jstueckler/mrsmap/blob/2e1c378b3d22520dcc07f650b35b881fbd3e529a/src/registration/multiresolution_surfel_registration.cpp"" rel=""nofollow"">this way</a> to compute the covariance is not very easy).</p>

<p><img src=""http://i.stack.imgur.com/Lvmdu.png"" alt=""enter image description here""> <img src=""http://i.stack.imgur.com/LfwqV.png"" alt=""enter image description here""> <img src=""http://i.stack.imgur.com/DYbgw.png"" alt=""enter image description here""></p>

<p>Are there any minimal working examples that I'm not aware of?
Is there something fundamentally wrong in what I describe above?
Are any rules of thumb (e.g. the first node in both approaches above is fixed) that I should follow and I might not be aware of them?</p>

<p><em>Update: More specific questions</em></p>

<ul>
<li>The nodes hold the poses of the robot/camera. It is unclear though at which reference frame they are defined. If it is the world coordinate frame, is it defined according to the camera or according to the object, i.e. first acquired pointcloud? This would affect the accumulation of the pose matrices during incremental registration (before the g2o stage - I try to form and optimize the graph only once at the end, for all the frames/pointclouds).</li>
<li>The edge (Src->Tgt) constraints hold the relative transformation from pointcloudSrc to pointcloudTgt. Is it just the transformation based on the features of the two in the local coordinate frame of pointcloudSrc? Is there and tricky point regarding the direction, or just consistency with the relative transformation is enough?</li>
<li>The first node is always fixed. Does the fixed node affect the direction of the edge that departs/ends_up from/at the fixed node?</li>
<li>Is there any other tricky point that could hinter implementation?</li>
<li>I'm working in millimeter instead of meter units, I'm not sure if this will affect the solvers of g2o in any way. (I wouldn't expect so, but a naive use of g2o that was giving some usable results was influenced)</li>
</ul>
","slam"
"6852","I need the specifications for iRobot Create 2","<p>I need the specifications for the Create 2. I need it for research purposes. So I think I'm going to need a high computational computer on board.</p>

<p>Please suggest some nice configuration. </p>
","irobot-create"
"6853","Dock command does not seem to work","<p>We bought a new Create2 robot and started using it. But when we issue the dock command the robot moves for a bit and does not go back to the base.</p>

<p>The base is not hidden or obstructed and the create2 is just a couple of feet away. we need help to figure out why it does not see the base.</p>

<p>Just to clarify that even using the DOCK button on the create2 does not make the create2 go back to the base</p>
","irobot-create"
"6854","the specifications graph showing the battery discharges in volt per time","<p>my name is dylan we are doing a project on irobot create and we would like to know the specifications graph showing the battery discharges in volt per time and my robot is the irobot ceate 1.</p>

<p>The battery is the roomba advanced power , it's a 14.4V Nickel metal hybride battery pack and she deliver 3000mah.</p>
","irobot-create"
"6859","how to implement tracking problem with PID controller","<p>I'm trying to implement the tracking problem for this <a href=""http://robotics.stackexchange.com/questions/4793/proportional-controller-error-doesnt-approach-zero"">example</a> using PID controller. The dynamic equation is </p>

<p>$$
I  \ddot{\theta} + d \dot{\theta} + mgL \sin(\theta) = u
$$</p>

<p>where </p>

<p>$\theta$ : joint variable. </p>

<p>$u$ : joint torque</p>

<p>$m$ : mass. </p>

<p>$L$ : distance between centre mass and joint. </p>

<p>$d$ : viscous friction coefficient</p>

<p>$I$ : inertia seen at the rotation axis.</p>

<p>$\textbf{Regulation Problem:}$</p>

<p>In this problem, the desired angle $\theta_{d}$ is constant and $\theta(t)$ $\rightarrow \theta_{d}$ and $\dot{\theta}(t)$ $\rightarrow 0$ as $t$ $\rightarrow \infty$. For PID controller, the input $u$ is determined as follows</p>

<p>$$
u = K_{p} (\theta_{d} - \theta(t)) + K_{d}( \underbrace{0}_{\dot{\theta}_{d}} -  \dot{\theta}(t) ) + \int^{t}_{0} (\theta_{d} - \theta(\tau)) d\tau
$$</p>

<p>The result is </p>

<p><img src=""http://i.stack.imgur.com/3EFpL.png"" alt=""enter image description here""></p>

<p>and this is my code  <code>main.m</code></p>

<pre><code>clear all
clc

global error;
error = 0;

t = 0:0.1:5;

x0 = [0; 0];

[t, x] = ode45('ODESolver', t, x0);

e = x(:,1) - (pi/2); % Error theta

plot(t, e, 'r', 'LineWidth', 2);
title('Regulation Problem','Interpreter','LaTex');
xlabel('time (sec)');
ylabel('$\theta_{d} - \theta(t)$', 'Interpreter','LaTex');
grid on
</code></pre>

<p>and <code>ODESolver.m</code> is </p>

<pre><code>function dx = ODESolver(t, x)

global error; % for PID controller

dx = zeros(2,1);

%Parameters:
m = 0.5;       % mass (Kg)
d = 0.0023e-6; % viscous friction coefficient
L = 1;         % arm length (m)
I = 1/3*m*L^2; % inertia seen at the rotation axis. (Kg.m^2)
g = 9.81;      % acceleration due to gravity m/s^2


% PID tuning
Kp = 5;
Kd = 1.9;
Ki = 0.02;

% u: joint torque
u = Kp*(pi/2 - x(1)) + Kd*(-x(2)) + Ki*error;
error = error + (pi/2 - x(1));

dx(1) = x(2);
dx(2) = 1/I*(u - d*x(2) - m*g*L*sin(x(1)));

end
</code></pre>

<p>$\textbf{Tracking Problem:}$</p>

<p>Now I would like to implement the tracking problem in which the desired angle $\theta_{d}$ is not constant (i.e. $\theta_{d}(t)$); therefore, $\theta(t)$ $\rightarrow \theta_{d}(t)$ and $\dot{\theta}(t)$ $\rightarrow \dot{\theta}_{d}(t)$ as $t$ $\rightarrow \infty$. The input is </p>

<p>$$
u = K_{p} (\theta_{d} - \theta(t)) + K_{d}( \dot{\theta}_{d}(t) -  \dot{\theta}(t) ) + \int^{t}_{0} (\theta_{d}(t) - \theta(\tau)) d\tau
$$</p>

<p>Now I have two problems namely to compute $\dot{\theta}_{d}(t)$ sufficiently and how to read from <code>txt</code> file since the step size of <code>ode45</code> is not fixed. For the first problem, if I use the naive approach which is </p>

<p>$$
\dot{f}(x) = \frac{f(x+h)-f(x)}{h}
$$</p>

<p>the error is getting bigger if the step size is not small enough. The second problem is that the desired trajectory is stored in <code>txt</code> file which means I have to read the data with fixed step size but I'v read about <code>ode45</code> which its step size is not fixed. Any suggestions!</p>

<hr>

<p>Edit:</p>

<p>For tracking problem, this is my code </p>

<p><code>main.m</code></p>

<pre><code>clear all
clc

global error theta_d dt;
error = 0;


theta_d = load('trajectory.txt');

i    = 1;
t(i) = 0;
dt   = 0.1;
numel(theta_d)
while ( i &lt; numel(theta_d) )
   i = i + 1;
   t(i) = t(i-1) + dt;
end

x0 = [0; 0];
options= odeset('Reltol',dt,'Stats','on');
[t, x] = ode45(@ODESolver, t, x0, options);

 e = x(:,1) - theta_d; % Error theta


plot(t, x(:,2), 'r', 'LineWidth', 2);
title('Tracking Problem','Interpreter','LaTex');
xlabel('time (sec)');
ylabel('$\dot{\theta}(t)$', 'Interpreter','LaTex');
grid on
</code></pre>

<p><code>ODESolver.m</code></p>

<pre><code>function dx = ODESolver(t, x)

persistent i theta_dPrev

if isempty(i)
    i = 1;
    theta_dPrev = 0;
end

global error theta_d dt ; 

dx = zeros(2,1);

%Parameters:
m = 0.5;       % mass (Kg)
d = 0.0023e-6; % viscous friction coefficient
L = 1;         % arm length (m)
I = 1/3*m*L^2; % inertia seen at the rotation axis. (Kg.m^2)
g = 9.81;      % acceleration due to gravity m/s^2


% PID tuning
Kp = 35.5;
Kd = 12.9;
Ki = 1.5;

if ( i == 49 )
    i = 48;
end
% theta_d first derivative
theta_dDot  = ( theta_d(i) - theta_dPrev ) / dt;
theta_dPrev = theta_d(i);


% u: joint torque
u = Kp*(theta_d(i) - x(1)) + Kd*( theta_dDot - x(2)) + Ki*error;
error = error + (theta_dDot - x(1));

dx(1) = x(2);
dx(2) = 1/I*(u - d*x(2) - m*g*L*sin(x(1)));

i = i + 1;
end
</code></pre>

<p>trajectory's code is </p>

<pre><code>clear all 
clc

a = 0:0.1:(3*pi)/2;

file = fopen('trajectory.txt','w');


for i = 1:length(a)
    fprintf(file,'%4f \n',a(i));
end

fclose(file);
</code></pre>

<p>The result of the velocity is </p>

<p><img src=""http://i.stack.imgur.com/Lr1EO.png"" alt=""enter image description here""></p>

<p>Is this correct approach to solve the tracking problem?</p>
","control pid dynamics"
"6860","DYNAMIXEL MX-106-R burnt","<p>I am using a MX 106-R dynamixel servo for a project that i am making. 
I am making a robotic arm controlled by this servo. 
I accidentally moved the horn of the servo while it was on and hence due to excess current input, it wont work anymore. 
I suspect that the H-bridge inside the motor got burnt. </p>

<p>Can somebody tell me what exactly went wrong ?
How can i test the motor ?
How can i repair it (if possible) or else where can i find a service centre (I live in India).</p>

<p>I am in deep trouble right now. Please Help !</p>
","robotic-arm servos h-bridge"
"6862","How to calculate the real time RPM of motor with rotary encoder?","<p>I want to measure the real time RPM of the wheels. I think incremental rotary encoder would be good. But i am confused on how to interface it with DC brushless geared motors. From the images i am not quite sure if only one rotary encoder would suffice or do i need any other sensor also with it?<br>
I am doing my project on arduino uno.  </p>
","motion forward-kinematics quadrature-encoder"
"6863","What wireless technology to use to control robots in classroom?","<p>I want to build a cheap robot programmable in Scratch graphical language, that could be employed during lessons in school. Scratch code is interpreted on a PC, so on the robot there should be only the code that receives specific commands (i.e. drive forward) and transmits sensors' measurements. </p>

<p>I'm looking for a wireless technology that will allow me to exchange information between robot and PC with at least 30Hz rate. It should also allow to work at least 16 robots simultaneously in the same room and have a range of at least 20m. </p>

<p>I did tests with BLuetooth, but sometimes there are connectivity issues, and pairing devices can be a hassle in a classroom. I have also tried WiFi modules, but pinging it showed average time of 19ms, but maximum of more than 500ms, so I'm afraid that it won't be able to control linefollower robot for example. </p>

<p>Can you point me to some other, preferably cheap (under 10$ per module) wireless technologies? Or maybe my worries about WiFi are exaggerated?</p>
","mobile-robot radio-control wireless wifi"
"6869","Roomba Create 2 problem reading distance traveled","<p>I am trying to work with the create2. In using the ""get distance traveled"" command (id 142) I am getting back incorrect data. My simple test case logic is</p>

<p>I am working with the Create2_TetheredDrive.py example
and adding this</p>

<pre><code>    elif k == 'PLUS' or k == 'MINUS': # Move 200mm forward or backward
        # reset distance measurement by sending request
        sendCommandASCII('142 19') 
        # ignore/discard the data returned
        recv_basic(connection)
        # set velocity mm/s
        v = 200;
        if k=='MINUS':
            v=-v
        # start moving
        cmd = struct.pack(""&gt;Bhh"", 145, v, v)
        sendCommandRaw(cmd)
        # pause 1 second
        time.sleep(1);
        # stop moving
        cmd = struct.pack(""&gt;Bhh"", 145, 0, 0)
        sendCommandRaw(cmd)
        # get distance traveled
        sendCommandASCII('142 19') 
        data = recv_basic(connection)
        dist = struct.unpack('&gt;h',data)
        print(dist)
</code></pre>

<p>I consistently numbers near -25 for moving forward, and +25 for moving backward.
If I wait for 2 seconds, I get -50 for moving forward, and +50 for moving backward. The documentation says it should return the distance traveled in mm, so these numbers seem to be off by a factor of -8.</p>

<p>Anyone have any suggestions? Thanks.</p>

<p>p.s. I had to add this function to the example as well</p>

<pre><code>def recv_basic(the_socket):
    the_socket.settimeout(0.1)
    total_data=[]
    while True:
        try:
            data = the_socket.recv(8192)
            total_data.append(data)
        except:
            break
    return ''.join(total_data)
</code></pre>
","irobot-create"
"6870","Controlling the iRobot Create 2 with MATLAB","<p>I teach a university sophomore level MATLAB programming class for engineers, and I am planning on using the create2 for their final project. There is a nice simulator and MATLAB toolbox for the Create, but the toolbox utilizes some of the commands that no longer exist on the Create 2, thus it doesn't work correctly. And of course is doesn't support any of the newer commands. In addition, I want to be able to ""cut the cord"" so I am using a Raspberry Pi on the Create to pipe data to the serial port, and TCPIP sockets to send the data from a remote computer running MATLAB to the Pi/Create. If anyone is working on a similar configuration, I'd love to trade notes and share the pain.</p>
","irobot-create"
"6880","Monocular vs. stereo computer vision robustness for object detection","<p>Are there some genaral rules for the robustness between monocular and stereo vision when considering object detection? I am especially interested in the automotive field - considering distance/obstacle/car detection (see video links below).</p>

<p>Someone told me monocular vision is more robust than stereo. I guess this may be true if the monocular algorithm is well written (and especially verified over lots of input data)... but once you input (image) data that has not been verified it may probably provide unexpected results, right? With stereo vision one does not really care about the contents of the image as long as texture/lighting conditions allow stereo matching and the object detection is then done within the point cloud.</p>

<p>I consider following usage:</p>

<ul>
<li><p><a href=""https://www.youtube.com/watch?v=LRnQUncmZEo"" rel=""nofollow"">Monocular</a></p></li>
<li><p><a href=""https://www.youtube.com/watch?v=UVdhilv2qmg"" rel=""nofollow"">Stereo</a></p></li>
</ul>

<p>The monocular sample video seems to have sometimes problems detecting the cars in front (the bounding boxes disappear once in a while). The stereo sample seems to be more robust - the car in front clearly is detected in all of sequnce image frames.</p>
","computer-vision stereo-vision"
"6882","Calculating acceleration and velocity","<p>I'm writing some Quad Copter software and beginning to implement an altitude hold mode. </p>

<p>To enable me to do this I need to get an accurate reading for vertical velocity. I plan to use a Kalman filter for this but first I need to ensure that I'm getting the correct velocity from each individual sensor.</p>

<p>I have done this but I'm not 100% sure its correct so I was hoping to get some confirmation on here.</p>

<p>My first sensor is a Lidar distance sensor, I calculated acceleration and velocity using the following code:</p>

<pre><code>float LidarLitePwm::getDisplacement()
{
    int currentAltitude = read();
    float displacement = currentAltitude - _oldAltitude;
    _oldAltitude = currentAltitude;

    return displacement; //cm
}

//Time since last update
float time = (1.0 / ((float)FLIGHT_CONTROLLER_FREQUENCY / 10.00)); // 50Hz, 0.02s

float lidarDisplacement = _lidar-&gt;getDisplacement();
_currentLidarVelocity = lidarDisplacement / time;
</code></pre>

<p>The second sensor is an accelerometer. I calculated acceleration and velocity using the following code:</p>

<pre><code>Imu::Acceleration Imu::getAcceleration()
{
    //Get quaternion
    float q[4];
    _freeImu.getQ(q);

    //Get raw data
    float values[9];
    _freeImu.getValues(values);

    //Extract accelerometer data
    float acc[3];
    acc[0]= values[0]; //x
    acc[1]= values[1]; //y
    acc[2]= values[2]; //z

    //Gravity compensate
    _freeImu.gravityCompensateAcc(acc, q);

    //Convert acceleration from G to cm/s/s
    _acceleration.x = acc[0] * 9.8 * 100;
    _acceleration.y = acc[1] * 9.8 * 100;
    _acceleration.z = acc[1] * 9.8 * 100;

    return _acceleration; //cm/s/s
}

//Time since last update
float time = (1.0 / ((float)FLIGHT_CONTROLLER_FREQUENCY / 10.00)); // 50Hz, 0.02s

//Get accel
Imu::Acceleration imuAcceleration = _imu-&gt;getAcceleration();

//Get velocity
currentZVelocity += imuAcceleration.z * time; //cm/s
</code></pre>

<p>It would be great if someone could confirm if this is correct (or not)</p>

<p>Thanks
Joe</p>
","sensors accelerometer lidar"
"6890","Verifying motor selection calculations","<p>I'm trying to select a brushed DC motor for a project. I tried following the advice on <a href=""http://www.scribd.com/doc/38698/Sizing-Electric-Motors-for-Mobile-Robotics"" rel=""nofollow"">sizing electric motors</a>, mentioned in <a href=""http://robotics.stackexchange.com/questions/913/are-power-and-torque-required-related-in-some-way"">this question</a>, but a few details were missing, and I'm unsure if I properly followed the procedure.</p>

<p>For my application, I need:</p>

<ul>
<li>Nm = number of motors = 2</li>
<li>Wd = wheel diameter = 12 cm</li>
<li>Wp = estimated weight of platform = 5 kg</li>
<li>Minc = maximum incline under load = 5 degrees</li>
<li>Vmax = maximum velocity under load = 5 km/hr</li>
<li>Fpush = maximum pushing force = 1.25 kg</li>
<li>Ur = coefficient of rolling friction = 0.015</li>
</ul>

<p>These are my calculations:</p>

<p>Step 1: Determine total applied force at worst case.</p>

<pre><code>Ftotal = Wp * (Ur*cos(Minc) + sin(Minc)) + Fpush = 1.7604933161 kilogram
</code></pre>

<p>Step 2: Calculate power requirement.</p>

<pre><code>Vradps = maximum velocity under load in radians/second = 23.1481481481 radian / second

Pmotor = required power per motor = (Ftotal * Vradps * Wd/2)/Nm = 1.22256480284 kilogram * meter * radian / second
</code></pre>

<p>Step 3: Calculate torque and speed requirement.</p>

<pre><code>Tmotor = required torque per motor = Pmotor/Vradps = 5281.47994829 centimeter * gram = 73.345953832 inch * ounce
RPMmin = required revolutions per minute per motor = Vradps / 0.104719755 = 221.048532325 rev / minute
</code></pre>

<p>Are my calculations correct? Intuitively, the final <code>Tmotor</code> and <code>RPMmin</code> values seem right, but my calculation for <code>Pmotor</code> doesn't exactly match the one used in the link, which doesn't explicitly do the conversion to radians / second and therefore doesn't result in the proper units.</p>

<p>Here's my Python script for reproducing the above calculations:</p>

<pre><code>from math import *
#http://pint.readthedocs.org/en/0.6/tutorial.html
from pint import UnitRegistry

ureg = UnitRegistry()

def velocity_to_rpm(v, r):
    kph = v.to(kilometer/hour)
    r = r.to(kilometer)
    d = r*2
    rpm = (kph / (2*pi*r)) * ((1*hour)/(60.*minute)) * rev
    return rpm

def velocity_to_radps(v, r):
    return velocity_to_rpm(v, r).to(radian/second)

# Units
km = kilometer = ureg.kilometer
meter = ureg.meter
newton = ureg.newton
cm = centimeter = ureg.centimeter
hr = hour = ureg.hour
mm = millimeter = ureg.millimeter
rev = revolution = ureg.revolution
minute = ureg.minute
sec = second = ureg.second
kg = kilogram = ureg.kilogram
gm = gram = ureg.gram
deg = degree = ureg.degree
rad = radian = ureg.radian
oz = ureg.oz
inch = ureg.inch

# Conversions.
km_per_mm = (1*km)/(1000000.*mm)
hour_per_minute = (1*hour)/(60.*minute)
minute_per_second = (1*minute)/(60*sec)
minute_per_hour = 1/hour_per_minute
gm_per_kg = (1000*gm)/(1*kg)
cm_per_km = (100000*cm)/(1*km)

# Constraints
target_km_per_hour = (5*km)/(1*hour) # average walking speed
estimated_platform_weight = 5*kg
maximum_incline_degrees = 5*deg
maximum_incline_radians = maximum_incline_degrees * ((pi*rad)/(180*deg))
maximum_pushing_force = estimated_platform_weight/4.
maximum_velocity_at_worst_case = (5*km)/(1*hour)
rolling_friction = 0.015 # rubber on pavement
velocity_under_max_load = target_km_per_hour
number_of_powered_motors = 2

# Variables
wheel_diameter_mm = 120*mm
wheel_radius_mm = wheel_diameter_mm/2
wheel_radius_km = wheel_radius_mm * km_per_mm
rev_per_minute_at_6v_unloaded = 33*rev/(1*minute)
rev_per_minute_at_6v_loaded = rev_per_minute_at_6v_unloaded/2.
mm_per_rev = (wheel_diameter_mm * pi)/(1*rev)
target_rpm = velocity_to_rpm(target_km_per_hour, wheel_radius_mm)
target_radps = velocity_to_radps(target_km_per_hour, wheel_radius_mm)

# Calculate total applied force at worst case.
total_applied_force_worst_case = estimated_platform_weight * (rolling_friction*cos(maximum_incline_radians) + sin(maximum_incline_radians)) + maximum_pushing_force
print 'Ftotal:',total_applied_force_worst_case

# Calculate power requirement.
vel_in_radps = velocity_to_radps(velocity_under_max_load, wheel_radius_mm)
print 'Vradps:',vel_in_radps
required_power = total_applied_force_worst_case * velocity_to_radps(velocity_under_max_load, wheel_radius_mm) * wheel_radius_mm.to(meter)
required_power_per_motor = required_power/number_of_powered_motors
print 'Pmotor:',required_power_per_motor

# Calculate torque and speed requirement.
required_angular_velocity = velocity_under_max_load/wheel_radius_km * hour_per_minute * minute_per_second * rad #rad/sec
required_rpm = required_angular_velocity / 0.104719755 * (rev/rad) * (sec/minute)
required_torque_per_motor = (required_power_per_motor/required_angular_velocity).to(gm*cm)
print 'Tmotor: %s, %s' % (required_torque_per_motor, required_torque_per_motor.to(oz*inch))
print 'PRMmin:',required_rpm
</code></pre>
","mobile-robot motor design torque force"
"6895","RGB-D SLAM - Compute Information Matrix","<p>currently im working on a RGB-D SLAM with a Kinect v1 Camera. In the front-end the SLAM estimates the pose with Ransac as an initial guess for the ICP. With the pose estimation i transform the pointcloud to a pointcloud-scene which represents my map.</p>

<p>To smooth the map im trying to implement a graph optimizing algorithm (g2o). 
Until now, there is no graph representation in my frontend, so i started to integrate that.</p>

<p>Im trying to build a .g2o file with the following fromat:</p>

<p><strong>VERTEX_SE3 i x y z qx qy qz qw</strong></p>

<p>where x, y, z is the translation and qx, qy, qz, qw ist the Rotation in respect to the initial coordinate system. And,</p>

<p><strong>EDGE_SE3 observed_vertex_id observing_vertex_id x y z qx, qy, qz, qw inf_11 inf_12 .. inf_16 inf_22 .. inf_66</strong></p>

<p>Translation and rotation for the edge is the pose estimate that i compute with Ransac and ICP (visual odometry). </p>

<p>Now im getting stuck with the information matrix.
I read the chapter 3.4 <em>THE INFORMATION FILTER</em> in Thrun's <em>Probabolistic Robotics</em> and several threads in this forum, such as:</p>

<p><a href=""http://robotics.stackexchange.com/questions/764/the-relationship-between-point-cloud-maps-and-graph-maps"">The relationship between point cloud maps and graph maps</a></p>

<p>and</p>

<p><a href=""http://robotics.stackexchange.com/questions/1180/information-filter-instead-of-kalman-filter-approach"">information filter instead of kalman filter approach</a></p>

<p>From the second link, i got this here. </p>

<blockquote>
  <p>The covariance update
  $$P_{+} = (I-KH)P$$
  can be expanded by the definition of K to be</p>
  
  <p>$$ P_{+} = P - KHP$$
  $$ P_{+} = P - PH^T (HPH^T+R)^{-1} HP$$</p>
  
  <p>Now apply the matrix inversion lemma, and we have:</p>
  
  <p>$$P_{+} = P - PH^T (HPH^T+R)^{-1} HP$$
  $$ P_{+} = (P^{-1} + H^TR^{-1}H)^{-1}$$</p>
  
  <p>Which implies:
  $$ P_{+}^{-1} = P^{-1} + H^TR^{-1}H$$</p>
  
  <p>The term $P^{-1}$ is called the prior information,$$H^TR^{-1}H$$ 
  is the sensor information (inverse of sensor variance), and this gives us  $P^{-1}_+$, which is the posterior information. </p>
</blockquote>

<p>Could you please point this out for me. 
What data do i need to compute the information matrix? </p>
","slam kinect matlab"
"6896","Jacobian transpose: how to calculate orientation error","<p>I'm confused about how to compute the error in orientation. All the documents I've read don't explain how to do it.</p>

<p>The error in position is simply the difference between the points. </p>

<p>Let's assume we have the orientation along the effector axis, and we represent the rotation with quaternions. I have two questions:</p>

<ul>
<li>Is describing the orientation with quaternions a good approach?</li>
<li>How can we compute the error in orientation with the quaternions to use this in jacobian transpose?</li>
</ul>
","inverse-kinematics jacobian"
"6900","Low power to motors -- motor power jumper issue","<p>I'm currently working on my first robotics project using the <a href=""http://4tronix.co.uk/store/index.php?rt=product/product&amp;product_id=171"" rel=""nofollow"">Initio kit from 4tronix</a> powered by Raspberry Pi. The setup was fairly simple, and I've been testing it out over the last couple of days. All of the sensors work as expected; however, my motor tests are failing. When I input commands to actually move the robot, I can hear the DC motors running but they're not getting enough power to do anything. In the instructions, it says if this issue is encountered, that the power selection jumper might not be set correctly and provides this diagram:</p>

<p><a href=""https://onedrive.live.com/redir?resid=3C398965F6E3D4A1!69271&amp;authkey=!AAPxSGXtZsVBWxY&amp;v=3&amp;ithint=photo%2cjpg"" rel=""nofollow""><img src=""https://xdteya-ch3301.files.1drv.com/y2mGTgMLv3fnWgVl5wjaInciwhc93ByI8lhDwjzUcpCIQkaDKpugWr-uJoKAalXvf06LRv2nNmqJ3tbryGkGJA5p4EvWG96dlBQ4H_C0Inw0FrOrewIvratJ6_zcX4-BIE-HknIwyKUGQ9bkqpPOSQlqA/pirocon.jpg?psid=1"" alt=""""></a></p>

<p>For comparison, here's how I have the wiring for the motors setup:</p>

<p><a href=""https://onedrive.live.com/redir?resid=3C398965F6E3D4A1!69260&amp;authkey=!ABFSrJ5e5ru42Dg&amp;v=3&amp;ithint=photo%2cjpg"" rel=""nofollow""><img src=""https://xttdya.dm2302.livefilestore.com/y2mqOa7YTsmXAtHAs-JEDc1px07PIEFjrLVM53pFYpDCuWrgDLZDzL7X7Rwd0nwhCGK4xChoR--yTBYSZnwbhb0RDnXPxRpKLC8Il2eHfHS1x1xCAetDTeE3cvStshsMWMc3VEMpTTnSo-N3cr6n7unXg/robo_pic_crop_one.jpg?psid=1"" alt=""""></a></p>

<p>I'm not entirely sure what it means to have the power selection jumper being set incorrectly and would greatly appreciate it if someone could explain this to me or point out if they see anything wrong with my setup.</p>
","mobile-robot motor raspberry-pi first-robotics"
"6905","SLAM noob here, a few questions regarding EKF-SLAM","<p>I've recently been learning about SLAM and have been attempting to implement EKF-SLAM in python. I've been using this <a href=""http://ocw.mit.edu/courses/aeronautics-and-astronautics/16-412j-cognitive-robotics-spring-2005/projects/1aslam_blas_repo.pdf"" rel=""nofollow"">great article</a> as a guide. Some progress has been made, but I'm still confused by certain stages.</p>

<p>Firstly, does the <em>inverse sensor model</em> have to compute range and bearing, as opposed to cartesian coordinates? Why is this approach used?</p>

<p>Secondly, what format should my robot provide its heading in? Currently I just use a running offset from the origin angle (0), without wrapping it between 0 and 360. Turning right yields positive degrees, and left negative. I ask this as I assume the sensor model expects a certain format.</p>

<p>Thirdly, when computing the jacobians for adding new landmarks, (page 35) is Jz simply the absolute rotation of the robot (-540 degrees for example) plus the bearing the landmark was detected at?</p>

<p>And finally, what's the best approach for managing the huge covariance matrix? I'm currently thinking of a good way to 'expand' P when adding new landmarks.</p>

<p>Here's my current implementation: <a href=""http://pastebin.com/r7wUMgY7"" rel=""nofollow"">http://pastebin.com/r7wUMgY7</a></p>

<p>Any help would be much appreciated! Thanks.</p>
","slam ekf python"
"6906","Quadcopter Propeller size + Motor","<p>i would like to build quad that uses bigger propellers like 15"" . My question is what kind of motor shall i use ? Low or high KV? Do all motors support this kind of propellers ? Will they burn because of it ?I found they say CW and CCW motors does that mean you can't set way they spin ? I'm totally new in this so thank you for answer .</p>

<p><a href=""http://www.ebay.com/itm/GARTT-ML3508-415KV-Brushless-Motor-For-Multirotor-Quadcopter-Hexa/261535221935?_trksid=p2047675.c100005.m1851&amp;_trkparms=aid%3D222007%26algo%3DSIC.MBE%26ao%3D1%26asc%3D29266%26meid%3Da4bdb746beac401b8a4ca982730d114e%26pid%3D100005%26rk%3D2%26rkt%3D6%26sd%3D261640837515&amp;rt=nc"" rel=""nofollow"">okey so given this one it should be able to hold 15"" prob since it's in description</a>
shall i get 12A ESC since on 15 size prob they used max 8.8 or shall i get 25A ESC cause max continous is 20 ?</p>
","quadcopter brushless-motor"
"6908","EKF-SLAM, how best to manage the 'P' covariance matrix, programatically","<p>I've recently been learning about SLAM and EKF-SLAM. </p>

<p>I've began my implementation in python, but have had trouble managing the updating of P, especially when it comes to adding new landmarks. Currently there is no 'P' but just a few separate matrices that I have to stitch together when needed.</p>

<p>My implementation can be seen here: <a href=""http://pastebin.com/r7wUMgY7"" rel=""nofollow"">http://pastebin.com/r7wUMgY7</a> </p>

<p>How best should I manage the large covariance matrix, should I be using one matrix, like the algorithm suggests? Thanks in advance.</p>
","slam ekf python"
"6909","EKF-SLAM Computing the jacobians for landmark updates","<p>I've been working through <a href=""http://ocw.mit.edu/courses/aeronautics-and-astronautics/16-412j-cognitive-robotics-spring-2005/projects/1aslam_blas_repo.pdf"" rel=""nofollow"">this informative guide</a> on EKF-SLAM but I'm having difficulty understanding the jacobians required for the 'landmark update', on page 35.</p>

<p>What exactly is Jxr and Jz taking as input? Is it taking the the current rotation of the robot, plus the addition of the odometry update? IE, the rotation that is now stored in the 'X' state vector. Or are they taking the angle from the Inverse Sensor Model, and if so, what's the 'delta' angle from?</p>

<p>Thanks.</p>
","slam ekf python"
"6910","How to find theta1 to theta5 after D-H parameter","<pre><code>Joint   θi    di    ai-1    αi-1
1     θ1-90  -d1     0      180
2      θ2     0      0      -90
3      θ3     0     a2       0
4     θ4-90   0     a3       0
5      θ5     0     0        90
</code></pre>

<p>I am confused about the right way to look for my theta1-theta5.
Probably from the offset limit of the angles or calculation from x0 to x5 angle rotation or from atan2(x,y).</p>
","robotic-arm dh-parameters"
"6913","What is the reduced form of this block diagram?","<p>What is the reduced form of this block diagram? I can't see any solution way :(</p>

<p><img src=""http://i.stack.imgur.com/2qRDH.png"" alt=""enter image description here""></p>
","control"
"6918","How do I work out the kinematic solution of a robot arm?","<p>I draw a robotic arm in Solidworks, but I'm not so sure about how to find out the <a href=""http://en.wikipedia.org/wiki/Degrees_of_freedom_%28mechanics%29"" rel=""nofollow"">DOF</a>, forward and backward kinematic.</p>

<p><a href=""https://grabcad.com/library/a-robotic-arm-for-maintaince-1"" rel=""nofollow""><img src=""http://i.stack.imgur.com/FOCb9.png"" alt=""A robotic arm for maintaince""></a></p>

<p>Could anyone help me understand how to work out the kinematic solution of this robot arm?</p>
","robotic-arm kinematics automatic matlab"
"6922","How is it possible to maintain the total thrust when controlling yaw of a quadcopter?","<p>I'm working on the control of a quadcopter and I'd like to understand how come controlling the yaw does not increase the overall thrust. My understanding is that the control is carried out through 2 PIDs per axis (roll, pitch and yaw). The output of the last PID is sent as a PWM signal to correct the rotor speeds of the propellers. The mixing looks something like that:</p>

<p>$T_{FrontLeft} = thrust + roll_{pid} + pitch_{pid} + yaw_{pid}$
$T_{FrontRight} = thrust - roll_{pid} + pitch_{pid} - yaw_{pid}$
$T_{RearLeft} = thrust + roll_{pid} - pitch_{pid} - yaw_{pid}$
$T_{RearRight} = thrust - roll_{pid} - pitch_{pid} + yaw_{pid}$</p>

<p>All the quadcopter controls seem to work that way from what I could gather. So the basic idea to control yaw is to add $yaw_{pid}$ to the clockwise motors and substract the same amount $yaw_{pid}$ to the counterclockwise motors to make the quadcopter turns clockwise. Which translates into a increase of speed of clockwise motors and a decrease of speed for counterclockwise motors from the same amount.</p>

<p>But we know that each motor produces thrust and torque according to those equations:</p>

<p>$T = C_T\rho n^2 D^4$</p>

<p>$Q = C_Q\rho n^2 D^5$</p>

<p>where $T$ is thrust, $Q$ is torque, $C_T$ and $C_Q$ are system dependent constants, $ρ$ is the air density, $n$ is rotor speed, and $D$ is rotor diameter. Which means that the thrust produced by each motor is proportional to the propeller speed squared. </p>

<p>So if $n$ is the speed of all propellers before correction, the thrust of the clockwise propellers after correction will be proportional to $(n+\Delta)^2$ and the thrust produced by the counterclockwise propellers to $(n-\Delta)^2$. The total thrust for these 2 propellers will be proportional to:</p>

<p>$(n+\Delta)^2 + (n-\Delta)^2 = 2n^2 + 2\Delta^2$</p>

<p>As you can see, there is an increase of $2\Delta^2$ in the overall thrust produced by those 2 propellers (and $4\Delta^2$ when we take the 4 propellers into account). Of course, in real life, when we control the yaw the quadcopter does not go up.</p>

<p>So what am I missing? </p>

<p>(the same stands for roll and pitch control but since the quadcopter turns around the roll or pitch axis, the total thrust is no longer entirely on the vertical axis and I could imagine that the projection on the vertical axis is not increasing, but that does not work with yaw)</p>
","control quadcopter torque"
"6927","How important are events like ""Robocup"" to the advancement of Robotics in general?","<p>Are events like Robocup advantageous to the development of robotic  advancement? <br>
Or is it merely entertainment which advances robotics by allowing entry level participation which helps maintain interest? <br>
Do the DARPA Grand's provide a better vehicle for advancement? (pun intended)   </p>
","design"
"6929","How to send commands to create 2 over Bluetooth","<p>I'm very new to create 2. I want to send commands using Bluetooth. I have already bought the bluetooth USB radio. What other devices do I need to get or how can I set up sending commands over bluetooth. Any help is appreciated. 
Thanks.</p>
","irobot-create"
"6931","Simulate IMU (2D gyro and accelerometer) data","<p>If I have a robot path in 2D space, </p>

<p>i.e. a vector of (x,y) locations, and I need to generate artificial IMU data (simulate them), how would I go about it? </p>

<p>How do I model equations to generate the values given a time frame and positions?</p>

<p>I've come across <em>imusim</em> I'd like to know how to model them and generate using Matlab or something similar.</p>
","imu accelerometer gyroscope simulation"
"6934","Glasses with eye sensors","<p>Someone could tell me if there are wearable devices such as glasses, with sensors that can detect eye movement?</p>

<p>In particular, I would need a device like google glass, having a sensor or a camera that is facing the eye, and it can capture the movement, possibly interfaced with a mobile device.</p>

<p>Alternatively, are there micro-cameras on the market, which can be connected via Bluetooth or USB to a mobile device?</p>
","sensors motion"
"6936","joint positions of a robot","<p>I would likte to find the joints positions using joint angles, link lengths etc.</p>

<p>How can I define the position of the each joint using DH parameters?</p>
","robotic-arm joint dh-parameters"
"6942","How to set up binocular cameras on a car?","<p>I am trying to set up my stereo vision system on a car. However, I meet several problems and do not know how to solve them.</p>

<ol>
<li><p>How to select the baseline? I want the distance measurement to be far at 30 or 50 meters and near at around 5-10m. Is it possible to choose a baseline that meets my requirement?</p></li>
<li><p>I have tried stereo calibration of two cameras and also learned how to compute depth value from disparity map. However I don't know how to compute depth value if the focal lengths of the two cameras are different. It seems all the theorems I can find on the Web only concern cameras of the same focal length. </p></li>
</ol>
","stereo-vision"
"6944","Dynamically detect changing obstacles","<p>So the idea is that there would be one robot acting as overwatch, which would detect all of the obstacles in an area (which are not necessarily static), and then send the data about the obstacles' positions to another robot that would navigate around the obstacles to a goal.</p>

<p>My initial thought was to have the overwatch robot be in an elevated position in the centre of the area, then sweep around using an ultrasonic sensor. This way, it could keep track of the obstacles in a set of polar coordinates (distance, angle). But then I realised that this method doesn't account for collinear obstacles.</p>

<p>So the question is, what is the best way to detect a bunch of non-static obstacles within an area?</p>

<p>As a side note, I have seen a system similar to this, where there was a robot detecting obstacles (in that case, a crowd of people) and another robot pathfinding around the obstacles (the people), but I'm unsure exactly how that system was detecting the obstacles.</p>
","sensors computer-vision sonar ultrasonic-sensors"
"6945","Help to dimension the right controller for the following Tranfer Function","<p>I have a generic problem to create a controller for the following system:
$$\ddot{x}(t) = a y(t)$$ 
where $a$ is a constant real value.
The system could be seen as an equivalent of a mass-spring-damper system, where damper and spring are removed. Also $x(t)$ is the $x$ dimension and $y$ is simply the force moving the mass. <strong>BUT</strong> in this case I need to drive the force using $x(t)$ and not the contrary.</p>

<p>Transforming according Laplace I get:
$$ y(t) = \frac{1}{a}\ddot{x}(t)$$
$$ Y(s) = \frac{1}{a}s^{2}X(s)$$
$$ G(s) = \frac{Y(s)}{X(s)} = \frac{s^{2}}{a}$$</p>

<p>Considering that $a = 1$ I implemented a possible example in Simulink. </p>

<p><img src=""http://i.stack.imgur.com/wuzjG.png"" alt=""enter image description here""></p>

<p><em>Please not that I put the output given by the scope for showing up the resulting answer of the system.</em></p>

<p>So I have 2 questions:</p>

<ol>
<li>Is it possible to develop such a system? As far as I know the degree of the numerator should be $=&lt;$ the degree of the denominator. So is the above system possible?</li>
<li>Is it possible to create a PID or PD controller to stabilize the output of the system?</li>
</ol>

<p>Regards</p>
","control pid matlab"
"6946","Square with hinge on all four sides","<p>I want to make a component that will be a square plate that will behave like it has a motorized hinge on all four sides. That is, it can ""open"" by pivoting around any one of its four sides. I want it to pivot by up to 45 degrees. </p>

<p>I thought about designing it so that 3 hinges could be detached while one pivots, but I wonder if there's a simpler way to do this. </p>
","design motion"
"6952","Using a Bitmap maze image to navigate the maze","<p>I'm working on an robot that would be able to navigate through a maze, avoid obstacles and  identify some of the objects in it. I have a monochromatic bitmap of the maze, that is supposed to be used in the robot navigation.</p>

<p>I am just a first year electrical engineering student, and so need help on how I can use the bmp image. I will be making my robot using the Arduino mega microcontroller.</p>

<p>So how should I get started on it.</p>

<p>If you need me to elaborate on anything kindly say so. </p>

<p>Link: <a href=""http://ceme.nust.edu.pk/nerc/files/theme_ind_2015.pdf"" rel=""nofollow"">http://ceme.nust.edu.pk/nerc/files/theme_ind_2015.pdf</a></p>
","arduino control localization"
"6953","How to calculate Euler Angles from gyroscope output?","<p>I am using a tri-axis accelerometer and tri-axis gyroscope to measure the linear acceleration of a body. I need to get the orientation of the body in euler form in order to rotate the accelerometer readings from the body frame into the earth frame. Please help I'm so stuck</p>
","accelerometer gyroscope frame"
"6955","how to make a robot move using arduino other than timing to predefined locations?","<p>how to make a robot move using arduino other than timing to predefined locations? and without the use of sensors?? I want to make my car move to different loactions on a board..want to know the possible options without using sensors and encoders??
 And how does cartesian robot work for predefined locations..does it require sensor too?</p>
","arduino navigation"
"6956","using range-only sensors for mapping in SLAM","<p>SLAM noob here but trying to implement an algorithm that fuses odometry data and mapping based on wifi signal strengths for a 2D robot.</p>

<p><strong>1)</strong>
After various readings of different resources,
I came across this - <a href=""http://www.qucosa.de/fileadmin/data/qucosa/documents/8644/Dissertation_Niko_Suenderhauf.pdf"" rel=""nofollow"">http://www.qucosa.de/fileadmin/data/qucosa/documents/8644/Dissertation_Niko_Suenderhauf.pdf</a>
that explained what sensors are used in mapping and how they are categorized.</p>

<p>There are range-bearing sensors (stereo cameras,RGB-d cameras) that provide both distance and angle (range and bearing), from which is easy to locate (x,y) coordinates of landmarks ---> I can develop a map.</p>

<p>But in case I'm using wifi signal strengths (Received signal strengths) etc, in which case it is range-only (meaning, I can only establish from a robot pose(x,y,theta) as to <strong>how far</strong> this signal is coming from), how am I developing a map at all?</p>

<p>My question is similar to this - <a href=""http://robotics.stackexchange.com/questions/53/what-algorithm-can-i-use-for-constructing-a-map-of-an-explored-area-using-a-numb"">What algorithm can I use for constructing a map of an explored area using a number of ultrasound sensors?</a> but not quite same.</p>

<p>Even if I were using IMU/GPS, how am I using GPS to develop a map? What is my state space there? If I am getting GPS signals / wifi signals/ radio signals, am I estimating the transmitter/AP's location as the map? or the walls of a room I'm navigating in, as a map?</p>

<p>A lot of SLAM literature talks about <strong>motion model</strong> and <strong>measurement model</strong>, the former gives me the pose of the robot quite easily because of the odometry and imu. </p>

<p>The latter though is more for development of a map. Am I right in understanding this? If yes, say 
a] I have walls in a room and I'm using Lidar scanner - 
this still gives me the location of the wall using the number of beams that give me bearing, and the average distance from all the beams.</p>

<p>b] Or if I have just a single laser scanner, I can still use a camera (distance) and the heading of the robot to calculate the location of wall (the map). <a href=""https://shaneormonde.wordpress.com/2014/01/25/webcam-laser-rangefinder/#more-403"" rel=""nofollow"">https://shaneormonde.wordpress.com/2014/01/25/webcam-laser-rangefinder/#more-403</a></p>

<p>But If I have wireless signal strengths, I have a distance (<strong>distance of the transmitter from which I'm getting the RSS</strong>, not the distance of the wall) as to where they are coming from. But how am I estimating the location of walls here?</p>

<p><strong>2)</strong> What does the term ""correspondences"" mean in SLAM literature? </p>
","localization slam artificial-intelligence mapping wireless"
"6957","Micro Quadcopter PID problem","<p>I designed a mini quadcopter which is about 4.5x4.5cm(Main Body). The main body is the PCB. </p>

<p>![enter image description here][1]</p>

<p>It weighs about ~20 grams with the battery. I'm using the MPU6050 with the DMP using the i2cDevLib. I am using raw radians for pitch, roll, and yaw these measures are read from the MPU6050's DMP. The motors are attached to the body using electrical tape(Black thing  around motors). The motors are 8.5mm in diameter and are driven by a n-channel mosfet. The mode of control right now is bluetooth(HC-05 module). The code being used is my own.
I have a control loop on all axes, the pitch and roll have the same values since the quadcopter is symmetrical. The problem I have is that PID tuning is next to impossible, the best I got was a ~2 second flight ([Video in slow-motion][2]).</p>

<p>At first I was using my own code for the control loop, but it wasn't as effective as the Arduino PID library. </p>

<p>The output of the PID loops are mapped to -90 to 90 on all axes. This can be seen in the code</p>

<pre><code>myPID.SetOutputLimits(-90, 90); //Y angle    
myPID1.SetOutputLimits(-90, 90); // X angle
myPID2.SetOutputLimits(-90, 90); // Yaw angle
myPID.SetMode(AUTOMATIC);
myPID1.SetMode(AUTOMATIC);
myPID2.SetMode(AUTOMATIC);
</code></pre>

<p>My full code is below, but what do you think the problem is?</p>

<h1>Code</h1>

<p><a href=""http://pastebin.com/cnG6VXr8"" rel=""nofollow"">http://pastebin.com/cnG6VXr8</a></p>
","arduino quadcopter pid"
"6969","Shallow underwater wireless sensor network","<p>I need to make shallow (max 2m) underwater wireless sensor network. Data payload is about 10kB/s. I know that VLF band (~3-30kHz)could be the best solutions for that, but cause of time-to-market I cannot make hardware and software from the ground.</p>

<p>Maybe someone could share own-self experience in this filed. <br>If band 100-900MHz could be enough to send 10kB/s from one device to another - from 2m underwater to over a dozen cm from water surface?<br> Maybe some IC for ultrasonic communication exist?<br> Another ideas?</p>
","sensors wireless communication"
"6979","Question about dynamic window approach?","<p>I have a my mobile robot and plan to use the dynamic window approach to collision avoidance. I have read <a href=""http://www4.cs.umanitoba.ca/~jacky/Teaching/Courses/74.795-LocalVision/ReadingList/fox97dynamic.pdf"" rel=""nofollow"">the paper</a> ,but have one inequality i can't derive it.
<img src=""http://i.stack.imgur.com/TbU3x.png"" alt=""enter image description here"">
could you tell me? thanks!</p>
","navigation motion-planning"
"6980","Beaglebone not accessible through LAN?","<p>Since the day I bought it I always use ethernet over USB connection, now I need to use RJ45 LAN cable to connect Beaglebone from my laptop, but my laptop can't even detect LAN connection from it, what could go wrong? Do I need straight or crossover cable? Do I need to configure something first on my BeagleBone?</p>

<blockquote>
  <p>UPDATE: Managed to connect it through Crossover cable and assign it IP
  address by running DHCP server on my laptop.</p>
</blockquote>

<p><img src=""http://i.stack.imgur.com/X3e16.png"" alt=""enter image description here""></p>

<blockquote>
  <p>As seen above my laptop assign IP 169.254.223.76, but when I tried to
  connect to that IP using puTTY it gives me connection refused.</p>
</blockquote>

<p><img src=""http://i.stack.imgur.com/LUNAe.png"" alt=""enter image description here"">
Please help.</p>
","beagle-bone"
"6982","EKF over-correcting?","<p>I've been implementing an extended kalman filter, following Thrun's Probabilistic Robotics implementation. I believe my correct step may be wrong, as the state appears to be corrected <em>far</em> too much.</p>

<p>Here's a screen capture showing the issue <a href=""https://youtu.be/gkSpFK27yvg"" rel=""nofollow"">https://youtu.be/gkSpFK27yvg</a> </p>

<p>Note, the bottom status reading is the 'corrected' pose coordinates.</p>

<p>This is my correct step:</p>

<pre><code>    def correct(self, reobservedLandmarks):
    for landmark in reobservedLandmarks:
        storedLandmark = self.getLandmark(landmark.id)

        z = Point(landmark.dist, math.radians(landmark.angle))
        h, q = self.sensorModel(storedLandmark)

        inv = np.array([[z.x-h.x], [wrap_radians(z.y-h.y)]])

        JH = np.zeros([2, 3 + (self.landmarkCount*2)])
        JH[1][2] = -1.0/q

        JH[0][0] = -((self.X[0] - storedLandmark.x) / math.sqrt(q))
        JH[0][1] = -((self.X[1] - storedLandmark.y) / math.sqrt(q))
        JH[1][0] = (storedLandmark.y - self.X[1]) / q
        JH[1][1] = -((storedLandmark.x - self.X[0]) / q)

        JH[0][3+(landmark.id*2)] = -JH[0][0]
        JH[0][4+(landmark.id*2)] = -JH[0][1]
        JH[1][3+(landmark.id*2)] = -JH[1][0]
        JH[1][4+(landmark.id*2)] = -JH[1][1]

        R = np.array([[landmark.dist*self.sensorDistError, 0],[0, self.sensorAngleError]])

        Z = matmult(JH, self.P, JH.T) + R
        K = matmult(self.P, JH.T, np.linalg.inv(Z))

        self.X = self.X + matmult(K, inv)
        self.P = matmult((np.identity(self.X.shape[0]) - matmult(K, JH)), self.P)
</code></pre>

<p>h = The range and bearing of state landmark.</p>

<p>q = (landmark.x - self.X[0])^2 + (landmark.y - self.X[1])^2</p>

<p>My sensor covariance errors are 1cm per meter, and pi/180 for the bearing. My assumption was that the correction should be relative to the size of the robot's pose error. Which is very small in this example, as it only moved forward less than 30cm.</p>

<p>Is the kalman gain applied correctly here, and if yes, what other factors would result in this 'over-correcting'?</p>

<p>Thanks.</p>
","slam ekf python"
"6984","Mapping algorithm without noise","<p>I have a simulated robot moving in a discretized 2D grid world that (for various simplification and time-restriction reasons) has no noise. The problem is how the robot creates its initial map of the world. Algorithms like SLAM and occupancy grid mapping are based on uncertainty, but in this case there is no uncertainty.</p>

<p>So I'm wondering if there is a relatively simple algorithm for mapping the environment with noiseless position.</p>
","mapping"
"6985","Enable Bluetooth Adapter for BeagleBone Black","<p>I recently bought a <a href=""http://plugable.com/products/usb-bt4le"" rel=""nofollow"">USB 2.0 Bluetooth Adapter</a>. It claims to have support from Linux kernels of versions 3.4 and higher. I have a <strong>BeagleBone Black</strong> with <strong>Debian GNU/Linux 7</strong> image and kernel 3.8. I am developing on BeagleBone Black by hosting it through USB with <code>ssh</code>.</p>

<p>I have tried both hot plugging and  plugging in before boot and failed. </p>

<p>Then, I tried <a href=""http://michaelhleonard.com/enable-bluetooth-on-beaglebone-black/"" rel=""nofollow"">this tutorial</a>. However, I cannot find the <code>connman</code> directory on my BeagleBone Black device. I looked up and assumed I needed to install the <code>connman</code> package, but my BeagleBone Black has no internet access.</p>

<p>I have also tried <code>lsusb -v</code>, as suggested by an answer of a <a href=""http://stackoverflow.com/questions/18754699/usb-bluetooth-dongle-isnt-recognized-by-beaglebone-black"">similar question</a> to this, with no luck. The weird thing is, while <code>lsusb</code> itself prints</p>

<blockquote>
  <p>Bus 001 Device 001: ID 1d6b:0002 Linux Foundation 2.0 root hub <br>
  Bus 002 Device 001: ID 1d6b:0002 Linux Foundation 2.0 root hub</p>
</blockquote>

<p><code>lsusb -v</code> only prints</p>

<blockquote>
  <p>Bus 001 Device 001: ID 1d6b:0002 Linux Foundation 2.0 root hub</p>
</blockquote>

<p>then hangs. Information regarding bus 002, which I believe the device is connected to, is not printed. I have to restart the <code>ssh</code> connection to get back to work. </p>

<p>How should I approach to get the dongle to work on my BeagleBone Black? If the <code>connman</code> package is sufficient, how do I install it on my BeagleBone Black without internet access. Why does <code>lsusb -v</code> hang?</p>

<p>Any help is appreciated!</p>
","beagle-bone"
"6987","Calculating thrust generated from electric engines","<p>I wanted to calculate the amount of thrust generated from the engines. I am using the blade 180 cfx model.</p>

<p><a href=""http://www.bladehelis.com/Products/Default.aspx?ProdID=BLH3450#quickSpecs"" rel=""nofollow"">http://www.bladehelis.com/Products/Default.aspx?ProdID=BLH3450#quickSpecs</a></p>

<p>After some research I have found a way to calculate the thrust using:</p>

<p>T = ( pi D^2 rho P^2)/2 where P is the Power multiplier and can be calculated using: P= prop constant * (rpm/100)^power factor</p>

<p>I am unable to find the values for the Prop constant and the power factor. Is there a way I can get this information? Or an alternative way to calculate the thrust generated?</p>
","power"
"6990","Detect polyethylene","<p>First of all, I am in high school[to tell you that I am a newbie and lack knowledge]</p>

<p>What I want to achieve for now is a thing that can differentiate between poly bags[polyethylene] and other stuffs. Or a thing that could detect polyethylene.</p>

<p>I have to built a robot and therefore we have only a few method accessible.</p>

<p>Anyway any knowledge or suggestion or external links provided by you, about this topic would be welcomed by me.</p>
","mobile-robot sensors"
"6992","My PID Controller in Java is not operating correctly","<p>I was looking for an implementation of a PID controller in Java and I found this one:</p>

<p><a href=""https://code.google.com/p/frcteam443/source/browse/trunk/2010_Post_Season/Geisebot/src/freelancelibj/PIDController.java?r=17"" rel=""nofollow"">https://code.google.com/p/frcteam443/source/browse/trunk/2010_Post_Season/Geisebot/src/freelancelibj/PIDController.java?r=17</a></p>

<p>So, for what I could understand about it I am using it this way:</p>

<pre><code>package lol.feedback;

public class dsfdsf {

    public static void main(String[] args) throws InterruptedException {
        final PIDController pidController = new PIDController(1, 1, 1);
        pidController.setInputRange(0, 200); // The input limits
        pidController.setOutputRange(50, 100); // The output limits
        pidController.setSetpoint(120); // My target value (PID should minimize the error between the input and this value)
        pidController.enable();
        double input = 0;
        double output = 0;
        while (true) {
            input = output + 30;
            pidController.getInput(input);
            output = pidController.performPID();
            System.out.println(""Input: "" + input + "" | Output: "" + output + "" | Error: "" + pidController.getError());

            Thread.sleep(1000);
        }
    }

}
</code></pre>

<p>But he never stabilizes. He doesn't behave like a PID at all... This is the output I get:</p>

<pre><code>Input: 30.0 | Output: 100.0 | Error: 90.0
Input: 130.0 | Output: 50.0 | Error: -10.0
Input: 80.0 | Output: 100.0 | Error: 40.0
Input: 130.0 | Output: 50.0 | Error: -10.0
Input: 80.0 | Output: 100.0 | Error: 40.0
Input: 130.0 | Output: 50.0 | Error: -10.0
Input: 80.0 | Output: 100.0 | Error: 40.0
Input: 130.0 | Output: 50.0 | Error: -10.0
Input: 80.0 | Output: 100.0 | Error: 40.0
Input: 130.0 | Output: 50.0 | Error: -10.0
Input: 80.0 | Output: 100.0 | Error: 40.0
Input: 130.0 | Output: 50.0 | Error: -10.0
Input: 80.0 | Output: 100.0 | Error: 40.0
Input: 130.0 | Output: 50.0 | Error: -10.0
Input: 80.0 | Output: 100.0 | Error: 40.0
Input: 130.0 | Output: 50.0 | Error: -10.0
Input: 80.0 | Output: 100.0 | Error: 40.0
Input: 130.0 | Output: 50.0 | Error: -10.0
Input: 80.0 | Output: 100.0 | Error: 40.0
Input: 130.0 | Output: 50.0 | Error: -10.0
Input: 80.0 | Output: 100.0 | Error: 40.0
Input: 130.0 | Output: 50.0 | Error: -10.0
Input: 80.0 | Output: 100.0 | Error: 40.0
Input: 130.0 | Output: 50.0 | Error: -10.0
Input: 80.0 | Output: 100.0 | Error: 40.0
Input: 130.0 | Output: 50.0 | Error: -10.0
Input: 80.0 | Output: 100.0 | Error: 40.0
Input: 130.0 | Output: 50.0 | Error: -10.0
Input: 80.0 | Output: 100.0 | Error: 40.0
Input: 130.0 | Output: 50.0 | Error: -10.0
Input: 80.0 | Output: 100.0 | Error: 40.0
Input: 130.0 | Output: 50.0 | Error: -10.0
Input: 80.0 | Output: 100.0 | Error: 40.0
Input: 130.0 | Output: 50.0 | Error: -10.0
Input: 80.0 | Output: 100.0 | Error: 40.0
Input: 130.0 | Output: 50.0 | Error: -10.0
Input: 80.0 | Output: 100.0 | Error: 40.0
Input: 130.0 | Output: 50.0 | Error: -10.0
Input: 80.0 | Output: 100.0 | Error: 40.0
Input: 130.0 | Output: 50.0 | Error: -10.0
Input: 80.0 | Output: 100.0 | Error: 40.0
Input: 130.0 | Output: 50.0 | Error: -10.0
Input: 80.0 | Output: 100.0 | Error: 40.0
Input: 130.0 | Output: 50.0 | Error: -10.0
Input: 80.0 | Output: 100.0 | Error: 40.0
Input: 130.0 | Output: 50.0 | Error: -10.0
Input: 80.0 | Output: 100.0 | Error: 40.0
Input: 130.0 | Output: 50.0 | Error: -10.0
Input: 80.0 | Output: 100.0 | Error: 40.0
Input: 130.0 | Output: 50.0 | Error: -10.0
Input: 80.0 | Output: 100.0 | Error: 40.0
Input: 130.0 | Output: 50.0 | Error: -10.0
Input: 80.0 | Output: 100.0 | Error: 40.0
Input: 130.0 | Output: 50.0 | Error: -10.0
Input: 80.0 | Output: 100.0 | Error: 40.0
Input: 130.0 | Output: 50.0 | Error: -10.0
Input: 80.0 | Output: 100.0 | Error: 40.0
Input: 130.0 | Output: 50.0 | Error: -10.0
Input: 80.0 | Output: 100.0 | Error: 40.0
Input: 130.0 | Output: 50.0 | Error: -10.0
Input: 80.0 | Output: 100.0 | Error: 40.0
Input: 130.0 | Output: 50.0 | Error: -10.0
Input: 80.0 | Output: 100.0 | Error: 40.0
Input: 130.0 | Output: 50.0 | Error: -10.0
Input: 80.0 | Output: 100.0 | Error: 40.0
Input: 130.0 | Output: 50.0 | Error: -10.0
Input: 80.0 | Output: 100.0 | Error: 40.0
Input: 130.0 | Output: 50.0 | Error: -10.0
Input: 80.0 | Output: 100.0 | Error: 40.0
Input: 130.0 | Output: 50.0 | Error: -10.0
Input: 80.0 | Output: 100.0 | Error: 40.0
Input: 130.0 | Output: 50.0 | Error: -10.0
Input: 80.0 | Output: 100.0 | Error: 40.0
Input: 130.0 | Output: 50.0 | Error: -10.0
Input: 80.0 | Output: 100.0 | Error: 40.0
Input: 130.0 | Output: 50.0 | Error: -10.0
Input: 80.0 | Output: 100.0 | Error: 40.0
Input: 130.0 | Output: 50.0 | Error: -10.0
Input: 80.0 | Output: 100.0 | Error: 40.0
Input: 130.0 | Output: 50.0 | Error: -10.0
Input: 80.0 | Output: 100.0 | Error: 40.0
Input: 130.0 | Output: 50.0 | Error: -10.0
Input: 80.0 | Output: 100.0 | Error: 40.0
Input: 130.0 | Output: 50.0 | Error: -10.0
Input: 80.0 | Output: 100.0 | Error: 40.0
Input: 130.0 | Output: 50.0 | Error: -10.0
Input: 80.0 | Output: 100.0 | Error: 40.0
Input: 130.0 | Output: 50.0 | Error: -10.0
Input: 80.0 | Output: 100.0 | Error: 40.0
Input: 130.0 | Output: 50.0 | Error: -10.0
Input: 80.0 | Output: 100.0 | Error: 40.0
Input: 130.0 | Output: 50.0 | Error: -10.0
</code></pre>

<p>Can someone help me tell me what I am missing?</p>

<p>Thank you!</p>
","control pid"
"6999","Quadcopter frame design","<p>Quadcopter frames seem to consistently follow the same X design. For example:</p>

<p><a href=""http://i.stack.imgur.com/IpaTl.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/IpaTlm.png"" alt=""Quadcopter X Frame""></a></p>

<p>I'm curious to know why that is. It certainly seems like the most efficient way to use space but is it the only frame design that would work for quadcopters?</p>

<p>For instance, would a design like this work?</p>

<p><a href=""http://i.stack.imgur.com/cMDxM.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/cMDxMm.png"" alt=""Quadcopter O Frame""></a></p>

<p>Why or why not?</p>
","quadcopter design frame"
"7002","Easiest way to submit a longer non standard character string via MAVLink","<p>I want to submit my gains for the PID regulator via MAVLink. 
Unfortunately, I am not very used to MAVLink and there are several functions which may be used for that purpose (I think). My string is currently JSON formatted and I was directly sending it to the serial port before. </p>

<p>Is there a straight forward way to submit the data like it is (see below) with MAVLink, or is it better not to transfer a JSON string with MAVLink and submit each single value? If yes, what is the function of choice. </p>

<p>So far I noticed that for most of the sensors, there are already MAVLink function defined. For the PID gains I found not so much.</p>

<pre><code>AP_HAL::UARTDriver *pOut = uartX == UART_C ? hal.uartC : hal.uartA;
pOut-&gt;printf( ""{\""t\"":\""pid_cnf\"",""
              ""\""p_rkp\"":%.2f,\""p_rki\"":%.2f,\""p_rkd\"":%.4f,\""p_rimax\"":%.2f,""
              ""\""r_rkp\"":%.2f,\""r_rki\"":%.2f,\""r_rkd\"":%.4f,\""r_rimax\"":%.2f,""
              ""\""y_rkp\"":%.2f,\""y_rki\"":%.2f,\""y_rkd\"":%.4f,\""y_rimax\"":%.2f,""
              ""\""p_skp\"":%.2f,\""r_skp\"":%.2f,\""y_skp\"":%.4f}\n"",
              static_cast&lt;double&gt;(pit_rkp), static_cast&lt;double&gt;(pit_rki), static_cast&lt;double&gt;(pit_rkd), static_cast&lt;double&gt;(pit_rimax),
              static_cast&lt;double&gt;(rol_rkp), static_cast&lt;double&gt;(rol_rki), static_cast&lt;double&gt;(rol_rkd), static_cast&lt;double&gt;(rol_rimax),
              static_cast&lt;double&gt;(yaw_rkp), static_cast&lt;double&gt;(yaw_rki), static_cast&lt;double&gt;(yaw_rkd), static_cast&lt;double&gt;(yaw_rimax),
              static_cast&lt;double&gt;(pit_skp), static_cast&lt;double&gt;(rol_skp), static_cast&lt;double&gt;(yaw_skp) );
</code></pre>
","c++ mavlink"
"7004","Euler’s Method Or ode45 for solving ODE for control systems","<p>The dominant approach for solving ODE in control systems books is <code>ode45</code> since the majority of these books use Matlab. I'm not acquainted with how the <code>ode45</code> works but lately I started reading about Euler's method in this book <a href=""http://rads.stackoverflow.com/amzn/click/0073401064"" rel=""nofollow"">Numerical Methods for Engineers</a>. If the step size is very small, then the results are satisfactory. For simulation, one can actually set the step size to be very small value. I've used <code>ode45</code> in <a href=""http://robotics.stackexchange.com/questions/6859/how-to-implement-tracking-problem-with-pid-controller/6867?noredirect=1#comment9843_6867"">here</a> for regulation and tracking problems. I faced some difficulties for using <code>ode45</code> for tracking problem since the step size is not fixed. Now for the same experiment, I've used the Euler's method with step size 0.001 sec. The results are amazing and so friendly in comparison with <code>ode45</code>. This is a snapshot from the result </p>

<p><img src=""http://i.stack.imgur.com/J5nLD.png"" alt=""enter image description here"">   </p>

<p>And this is the code </p>

<pre><code>clear all;
clc;

dt = 0.001;
 t = 0;

% initial values of the system
 a = 0; % angular displacement
da = 0; % angular velocity 

% PID tuning
Kp = 50;
Kd = 18.0;
Ki = 0.08;

error  = 0;

%System Parameters:
m = 0.5;       % mass (Kg)
d = 0.0023e-6; % viscous friction coefficient
L = 1;         % arm length (m)
I = 1/3*m*L^2; % inertia seen at the rotation axis. (Kg.m^2)
g = 9.81;      % acceleration due to gravity m/s^2

% Generate Desired Trajectory 
y = 0:dt:(3*pi)/2;
AngDes = y; % Ang: angle , Des: desired
AngDesPrev = 0;

for i = 1:numel(y)

   % get the first derviative of the desired angle using Euler method.
   dAngDes  =  ( AngDes(i) - AngDesPrev )/ dt;
   AngDesPrev = AngDes(i);

   % torque input
   u = Kp*( AngDes(i) - a ) + Kd*( dAngDes - da ) + Ki*error;
   % accumulated error
   error = error + ( AngDes(i) - a );

   %store the erro
   E(i) = ( a - AngDes(i) );
   T(i) = t;


   dda = 1/I*(u - d*da - m*g*L*sin(a));

   % get the function and its first dervative
   da = da + dda*dt;
    a = a  +  da*dt;

   %store data for furhter investigation 
   A(i) = a;
   dA(i) = da;

   t = t + dt; 

end

plot(T, AngDes, 'b', T, A, 'g', 'LineWidth', 1.0)
h = legend('$\theta_{d}(t)$', '$\theta(t)$');
set(h, 'Interpreter','LaTex')
</code></pre>

<p>My question is why <code>ode45</code> is preferred in many control books assuming the step size is very small. </p>
","control"
"7008","How to decide between LiPo or LiFePo for robot battery","<p>When choosing a battery for a robot, should you use a LiPo or LiFePo?</p>

<p>For LiFePo, the pros:</p>

<ul>
<li>can deliver higher sustained amps</li>
<li>many are built to be drop-in replacements for lead-acid batteries and can use the same charger</li>
</ul>

<p>The cons:</p>

<ul>
<li>enormously expensive (about $1/watt*hour/kg)</li>
<li>lower energy density than LiPo (around 110 watt*hour/kg)</li>
</ul>

<p>For LiPo batteries, the pros:</p>

<ul>
<li>cheaper (about $0.2/watt*hour/kg)</li>
<li>over twice the energy density than LiFePo (around 250 watt*hour/kg)</li>
</ul>

<p>The cons:</p>

<ul>
<li>more complicated and unsafe to charge (see videos of LiPos catching on fire)</li>
<li>most can't safely deliver high amps</li>
</ul>

<p>Is there anything I'm missing? I see LiFePo batteries used on a lot of larger platforms, probably due to the higher continuous amp rating. I see Ebay flooded with tons of cheap high-capacity Chinese LiPos, but almost none of them have documentation, which probably means they're junk.</p>

<p>When should I use LiFePo vs LiPo?</p>
","battery"
"7012","algorithm Simple Stereo Vision","<p>I'm working in a project implementing a vision system. I'm a student and this is the first time I'm doing something like this, it has been a challenge.</p>

<p>I'm using a controller (Netduino+2, .Net MicroFramework) and a camera (CmuCam5 - Pixy) and for now it's working well. I'm communicating with the robot(Fanuc M430iA) using Modbus, and aquiring the data from the camera using I2C. </p>

<p>But, the next challenge is using 2 cameras to implement stereo vision and I'm not shure how to achieve that. I'm reading a lot about that and I understand the process and generally how it works, but I think my case is very specific.</p>

<p>My cameras detect the center of an object and give me the coordinates, so, I have that, and that's good. </p>

<p>What do you think it's the more reasonable approach?</p>

<p>(sorry for my english, let me know if I'm not being explicit, I'll edit the question if I see there's not enough information)</p>
","microcontroller robotic-arm cameras stereo-vision"
"7014","What main factors/features explain the high price of most industrial computer vision hardware?","<p>I am a student who is currently working on a computer science project that will require soon computer vision and more specifically stereoscopy (for close depth detection). I am now looking for a great camera to do the job and I found several interesting options:</p>

<p>1- A custom built set of two cheap cameras (i.e. webcam);</p>

<p>2- The old, classic, economic and proven Kinect;</p>

<p>3- Specialized stereo sensors.</p>

<p>I found a couple months ago this sensor: <a href=""https://duo3d.com/product/duo-mini-lv1"" rel=""nofollow"">https://duo3d.com/product/duo-mini-lv1</a></p>

<p>I tought it was interesting because it is small, stereoscopic and brand new (encouraging a fresh USA company). However, if we take apart the additional APIs that come with it, I just don't understand why you would buy this when a Kinect (or ""cheap"" cameras) are at least 4-5 times less expensive and still have great if not better specifications.</p>

<p>Same applies for this one: <a href=""http://www.ptgrey.com/bumblebee2-firewire-stereo-vision-camera-systems"" rel=""nofollow"">http://www.ptgrey.com/bumblebee2-firewire-stereo-vision-camera-systems</a></p>

<p>Can someone please explain to me why I would need such a device and if not, why they are needed for some?</p>
","sensors computer-vision kinect"
"7016","Simple way of 3d perception","<p>I wonder is there any <strong>simple (can be computed in microcontroller level)</strong> option which is suitable for <strong>3d object perception</strong> (depth, position, pose or coordinate estimation) of flying robots except <em>LIDAR, stereovision, omnidirectional camera, laser scanner</em> or any other machine vision based techniques </p>
","mobile-robot sensors cameras stereo-vision lidar"
"7018","Irobot create 2 C# connection","<p>My department recently purchased Irobot create 2. We want to recreate the code from the Csharp create 2 driving Tether program to use as a base for our intro to computer science course. Currently the code we are using to talk to the Irobot is <a href=""http://www.robotappstore.com/Knowledge-Base/How-to-program-Roomba-for-NET-developers/23.html"" rel=""nofollow"">http://www.robotappstore.com/Knowledge-Base/How-to-program-Roomba-for-NET-developers/23.html</a>. Not sure if the irobot is getting the commands as well as if the serial port is making a connection. We are using Visual Studio 2012 as the programming environment. Any recommendation and or input would be appreciated.</p>

<p>Thank you </p>
","irobot-create roomba"
"7024","ROVER 5 with 2 encoders help","<p>am 2 weeks old to arduino projects..i had been using timing all this while to control my rover...now, i wanted to shift to using encoders..am facing quite some problems..am using arduino uno and a two amp motor shield..this is code i am trying to use..am using a 8V Li-po battery</p>

<p><a href=""http://www.myduino.com/index.php?route=product/product&amp;product_id=170&amp;search=rover+5"" rel=""nofollow"">http://www.myduino.com/index.php?route=product/product&amp;product_id=170&amp;search=rover+5</a> (link to rover)</p>

<p><a href=""http://www.myduino.com/index.php?route=product/product&amp;product_id=131&amp;search=motor+shield"" rel=""nofollow"">http://www.myduino.com/index.php?route=product/product&amp;product_id=131&amp;search=motor+shield</a> (link to motoshield)</p>

<p>my question is there are four pins coming out of encoders from each side...what i did was connected the red and black to 5V and GND respectively and the white and yellow of the first encoder to pin 2 and the white and yellow of second encoder to pin 3...is what am doing correct??</p>

<p>and sometimes when i use this code, in the motorshield both the green and red light starts thereby stalling the motor..why does that happen?</p>

<p>can anyone of you suggest a link to a simple encoder code to make the motors move forward in a straight using feedback..</p>

<p>thanks</p>

<p>// interupt 0 -> pin 2</p>

<p>// interupt 1 -> pin 3</p>

<p>volatile unsigned long positionL = 0;  //vehicle position count from left encoder</p>

<p>volatile unsigned long positionR = 0;  //vehicle position count from right encoder</p>

<p>int motorLa = 5;</p>

<p>int dirLa = 4;</p>

<p>int motorRa = 7;</p>

<p>int dirRa = 6;</p>

<p>void setup() </p>

<p>{</p>

<p>pinMode (motorLa, OUTPUT);</p>

<p>pinMode (dirLa, OUTPUT);</p>

<p>pinMode (motorRa, OUTPUT);</p>

<p>pinMode (dirRa, OUTPUT);</p>

<p>Serial.begin(9600);</p>

<p>}</p>

<p>void loop() </p>

<p>{</p>

<p>moveFWD(5300);</p>

<p>delay(2000);</p>

<p>moveREV(3000);</p>

<p>delay(2000);</p>

<p>while(1);</p>

<p>}</p>

<p>void encoder1()</p>

<p>{</p>

<p>positionR++;</p>

<p>}</p>

<p>void encoder2()</p>

<p>{</p>

<p>positionL++;</p>

<p>}</p>

<p>void moveFWD(unsigned int x)</p>

<p>{</p>

<p>positionL=0;</p>

<p>positionR=0;</p>

<p>attachInterrupt(0, encoder1, CHANGE);   </p>

<p>attachInterrupt(1, encoder2, CHANGE);</p>

<p>digitalWrite(dirLa, LOW); // Left a Forward </p>

<p>digitalWrite(dirRa, HIGH); //Right a Forward</p>

<p>while((positionL &lt;= x) || (positionR &lt;= x))</p>

<p>{</p>

<pre><code>if (positionL &gt; positionR) 

{

  analogWrite(motorLa, 220);

  analogWrite(motorRa, 255);



}  



else if (positionR &gt; positionL) 

{

  analogWrite(motorRa, 220);

  analogWrite(motorLa, 255); // Sets the motor speed at a value of 180

}



else 

{

  analogWrite(motorRa, 255);

  analogWrite(motorLa, 255); // Sets the motor speed at a value of 180



}











Serial.print(positionL); // This prints the current value of positionL in the serial monitor on the computer.

Serial.print(""\t""); // This creates a tab on the monitor 

Serial.print(positionR);

Serial.println(); // This creates a new line to print on  
</code></pre>

<p>}</p>

<p>// Stop all motors</p>

<p>analogWrite(motorLa, 0);</p>

<p>analogWrite(motorRa, 0);</p>

<p>// Disables the encoders interrupt</p>

<p>detachInterrupt(0);</p>

<p>detachInterrupt(1);</p>

<p>}</p>

<p>void moveREV(unsigned int x)</p>

<p>{</p>

<p>positionL=0;</p>

<p>positionR=0;</p>

<p>attachInterrupt(0, encoder1, CHANGE);   </p>

<p>attachInterrupt(1, encoder2, CHANGE);</p>

<p>digitalWrite(dirLa, HIGH); // Left a Forward </p>

<p>digitalWrite(dirRa, LOW); //Right a Forward</p>

<p>while((positionL &lt;= x) || (positionR &lt;= x))</p>

<p>{</p>

<pre><code>if (positionL &gt; positionR) 

{

  analogWrite(motorLa, 20);

  analogWrite(motorRa, 200);

}  



else if (positionR &gt; positionL) 

{

  analogWrite(motorRa, 20);

  analogWrite(motorLa, 200); // Sets the motor speed at a value of 180



}



else 

{

  analogWrite(motorLa, 200); // Sets the motor speed at a value of 180





  analogWrite(motorRa, 200);



}







Serial.print(positionL); // This prints the current value of positionL in the serial monitor on the computer.

Serial.print(""\t""); // This creates a tab on the monitor 

Serial.print(positionR);

Serial.println(); // This creates a new line to print on  
</code></pre>

<p>}</p>

<p>// Stop all motors</p>

<p>analogWrite(motorLa, 0);</p>

<p>analogWrite(motorRa, 0);</p>

<p>// Disables the encoders interrupt</p>

<p>detachInterrupt(0);</p>

<p>detachInterrupt(1);</p>

<p>}</p>
","arduino sensors"
"7040","How to get the projection matrix from odometry/tf data?","<p>I would like to compare my results of visual Odometry with the groundtruth provided by the KITTI dataset.
For each frame in the groundthruth, i have a projection matrix.
For example:</p>

<pre><code>1.000000e+00 9.043683e-12 2.326809e-11 1.110223e-16 9.043683e-12 1.000000e+00 2.392370e-10 2.220446e-16 2.326810e-11 2.392370e-10 9.999999e-01 -2.220446e-16
</code></pre>

<p>Here the instructions provided by the readme:</p>

<blockquote>
  <p>Row i represents the i'th pose of the
  left camera coordinate system (i.e., z
  pointing forwards) via a 3x4
  transformation matrix. The matrices
  are stored in row aligned order (the
  first entries correspond to the first
  row), and take a point in the i'th
  coordinate system and project it into
  the first (=0th) coordinate system.
  Hence, the translational part (3x1
  vector of column 4) corresponds to the
  pose of the left camera coordinate
  system in the i'th frame with respect
  to the first (=0th) frame</p>
</blockquote>

<p>But I don't know how to produce the same kind of data for me.
What I have for each frame in my case:</p>

<ul>
<li>The Tf transformation from the init_camera (the fix one from the (0,0,0)) to the left camera which is moving. So I have the translation vector and the quaternion rotation.</li>
<li>The odometry data: the pose and the twist</li>
<li>Camera calibration parameters</li>
</ul>

<p>With those data, How I compare with the groundtruth ? So I need to find the projection matrix from the data above but don't know how to do it.</p>

<p>Can someone help me ?</p>

<p>Thank</p>
","mobile-robot odometry stereo-vision"
"7043","How to tune the two PIDs for quadrotor","<p>I'm trying to implement two PIDs for stabilizing quadrotor for position tracking. The inputs are $x_{d}(t), y_{d}(t), z_{d}(t)$ and $\psi_{d}(t)$. For position tracking, usually the small angle assumption is assumed. This assumption allows for acquiring $\theta_{d}$ and $\phi_{d}$. These are the results </p>

<p><img src=""http://i.stack.imgur.com/TOsgv.png"" alt=""enter image description here""></p>

<p>The x-axis position is driving me crazy. After alot of attempts for tuning the PIDs, I felt something wrong is going on. Is this a normal behavior for PID controller? Also, what I've noticed is that once $\psi$ reaches to zero, the platform starts oscillating (after 1.5 second in the figure).   </p>

<p>For solving ODEs and computing the derivatives for the velocities, I use Euler methods. </p>

<hr>

<p>It is simulation in Matlab. </p>
","pid quadcopter"
"7048","Depth of view for a hypercatadioptric camera","<p>I am trying to determine the depth of view for a hypercatadioptric camera (camera lens system and a hyperbolic mirror) based on <a href=""http://i.stack.imgur.com/w7uTa.png"" rel=""nofollow"">1</a>. </p>

<p>The following illustration seems pretty clear. For an image point $p$, we are looking for a virtual point $p_u$, given the parameters of the optical system. </p>

<p><img src=""http://i.stack.imgur.com/w7uTa.png"" alt=""Image from [1]""></p>

<p>I have troubles finding the right equations in the paper, though. There is <code>the distribution of a virtual point</code> which seems to be connected to $p_u$, but is not defined anywhere.</p>

<p>My goal is to replicate the diagrams they have later in the paper, like e.g. this one:
<img src=""http://i.stack.imgur.com/fud50.png"" alt=""enter image description here""></p>

<p>Which for a mirror (blue) gives the virtual image points of the scene (red).
I would like to calculate the depth of view, so the area at which the image blur is below a threshold.</p>

<p><a href=""http://i.stack.imgur.com/w7uTa.png"" rel=""nofollow"">1</a> <a href=""http://oatao.univ-toulouse.fr/3672/1/Zhang_3672.pdf"" rel=""nofollow"">Zhang, S., Zenou, E.: Optical approach of a hypercatadioptric system depth of
field. In: 10th International Conference on Information Sciences, Signal Processing
and their Applications</a></p>
","computer-vision cameras"
"7050","Questions regarding 3D scanning and camera choice","<p>A few days ago, I just shared my concerns about the price of computer vision hardware on this same exact forum (see <a href=""http://robotics.stackexchange.com/questions/7014/what-main-factors-features-explain-the-high-price-of-most-industrial-computer-vi"">What main factors/features explain the high price of most industrial computer vision hardware?</a>) and I think a new but related post is needed. So here we go.</p>

<p>Here are some details to consider regarding the overall scanner I want to build:</p>

<ul>
<li><p>Restricted space: my overall scanner can't be larger than 3 feet cube.</p></li>
<li><p>Small objects: the objects I will be scanning shouldn't be larger than 1 foot cube.</p></li>
<li><p>Close range: the camera would be positioned approximately 1 foot from the object.</p></li>
<li><p>Indoor: I could have a dedicated light source attached to the camera (which might be fixed in a dark box)</p></li>
</ul>

<p>Here are the stereo cameras/sensors I was looking at (ordered by price):</p>

<ul>
<li><p>Two Logitech webcams (no model in particular)</p>

<ul>
<li><p>Cheap</p></li>
<li><p>Harder to setup and calibrate</p></li>
<li><p>Need to create your own API</p></li>
<li><p>Built for: what you want to achieve</p></li>
</ul></li>
<li><p>Intel RealSense: <a href=""http://click.intel.com/intel-realsense-developer-kit.html"">http://click.intel.com/intel-realsense-developer-kit.html</a></p>

<ul>
<li><p>$100</p></li>
<li><p>High resolution: 1080p (maybe not for depth sensing)</p></li>
<li><p>Workable minimum range: 0.2 m</p></li>
<li><p>Unspecified FOV</p></li>
<li><p>Built for: hands and fingers tracking</p></li>
</ul></li>
<li><p>Kinect 2.0: <a href=""https://www.microsoft.com/en-us/kinectforwindows/"">https://www.microsoft.com/en-us/kinectforwindows/</a></p>

<ul>
<li><p>$150</p></li>
<li><p>Low resolution (for depth sensing): 512 x 424</p></li>
<li><p>Unworkable minimum range: 0.5 m</p></li>
<li><p>Excellent FOV: 70° horizontal, 60° vertical</p></li>
<li><p>Built for: body tracking</p></li>
</ul></li>
<li><p>Structure Sensor <a href=""http://structure.io/developers"">http://structure.io/developers</a></p>

<ul>
<li><p>$380</p></li>
<li><p>Normal resolution with high FPS capability: 640 x 480 @ 60 FPS</p></li>
<li><p>Unspecified minimum range</p></li>
<li><p>Good FOV: 58° horizontal, 45° vertical</p></li>
<li><p>Built for: 3D scanning (tablets and mobile devices)</p></li>
</ul></li>
<li><p>ZED Camera: <a href=""https://www.stereolabs.com/zed/specs/"">https://www.stereolabs.com/zed/specs/</a></p>

<ul>
<li><p>$450</p></li>
<li><p>Extreme resolution with high FPS capability: 2.2K @ 15 FPS (even for depth sensing) and 720p @ 60 fps</p></li>
<li><p>Unviable minimum range: 1.5 m</p></li>
<li><p>Outstanding FOV: 110°</p></li>
<li><p>Built for: human vision simulation</p></li>
</ul></li>
<li><p>DUO Mini LX: <a href=""https://duo3d.com/product/duo-minilx-lv1"">https://duo3d.com/product/duo-minilx-lv1</a></p>

<ul>
<li><p>$595</p></li>
<li><p>Normal resolution with high FPS capability: 640 x 480 @ 60 FPS</p></li>
<li><p>Workable minimum range: 0.25 m (see <a href=""http://stackoverflow.com/questions/27581142/duo-3d-mini-sensor-by-code-laboratories"">http://stackoverflow.com/questions/27581142/duo-3d-mini-sensor-by-code-laboratories</a>)</p></li>
<li><p>Phenomenal FOV: 170° (with low distortion)</p></li>
<li><p>Built for: general engineering</p></li>
</ul></li>
<li><p>Bumblebee2: <a href=""http://www.ptgrey.com/bumblebee2-firewire-stereo-vision-camera-systems"">http://www.ptgrey.com/bumblebee2-firewire-stereo-vision-camera-systems</a></p>

<ul>
<li>Too much expensive (not even worth mentioning)</li>
</ul></li>
</ul>

<p><strong>Note: All prices are in date of April 18th 2015 and might change overtime.</strong></p>

<p>As you can see, some have really goods pros, but none seems to be perfect for the task. In fact, the ZED seems to have the best specifications overall, but lacks of minimum range (since it is a large baselined camera designed for long range applications). Also, the DUO Mini LX seems to be the best for my situation, but unlike the ZED which generates really accurate depth maps, this one seems to lack of precision (lower resolution). It might be good for proximity detection, but not for 3D scanning (in my opinion). I could also try to build my own experimental stereo camera with two simple webcams, but I don't know where to start and I don't think I will have enough time to deal with all the problems I would face doing so. I am now stuck in a great dilemma.</p>

<p>Here are my questions:</p>

<ol>
<li><p>What good resources on the internet give you a good introduction on 3D scanning concepts (theoretically and programmatically)? I will be using C++ and OpenCV (I already worked with both a lot) and/or the API provided with the chosen camera (if applies).</p></li>
<li><p>Should you have a static camera capturing a moving object or a moving camera capturing a static object?</p></li>
<li><p>Should I use something in conjunction with stereo camera (like lasers)?</p></li>
<li><p>Is it profitable to use more than two cameras/sensors?</p></li>
<li><p>Are resolution, FPS and global shuttering really important in 3D scanning?</p></li>
<li><p>What camera should I get (it can also be something I didn't mention, in the range of $500 maximum if possible)? My main criteria is a camera that would be able to generate an accurate depth map from close range points.</p></li>
</ol>

<p>Thanks for your help!</p>
","sensors computer-vision kinect"
"7052","How to select the parameters of the Sliding Mode Control of a Robotic Arm?","<p>I working on sliding mode control (SMC) of a 4 DoF manipulator, I don't know
how to select the discontinuity gain matrix, $ K$ , the surface constant (the diagonal gain matrix $\Lambda$  components).</p>
","control robotic-arm industrial-robot"
"7056","Locating Omni-Directional Robot","<p>I have an omni-directional robot, such as a X-Drive or mecanum Drive that I need to track the position of. I can put encoders on the wheels, but that is all I can do in terms of the sensors. I have no external beacons that I can link to. The issue is that I needed to keep track of X-Y position, including strafing, and my heading. Does anyone have any resources that could help me with this.</p>
","localization"
"7057","How to control Brushless Motor+ESC with BeagleBone?","<p>I can't really find <a href=""https://www.google.co.id/?gws_rd=cr,ssl&amp;ei=mIg0VZv5K4-HuASa4oCgBg#q=beaglebone%20esc%20brushless%20motor"" rel=""nofollow"">a real straightforward tutorial</a> for that. There are <a href=""https://www.google.co.id/?gws_rd=cr,ssl&amp;ei=mIg0VZv5K4-HuASa4oCgBg#q=arduino%20esc%20brushless%20motor"" rel=""nofollow"">a lot for Arduino</a> but I only have an original beaglebone, an ESC, and brushless motor with me. Please help.</p>
","brushless-motor beagle-bone"
"7059","How to approach an object","<p>My question is more on a basic/conceptual level.</p>

<p>I'm looking into a way to approach an object in map, that I have detected earlier. My robot is localized in a map using SLAM. And object position is 2D point that I recieve from my algorithm. (Object is actually a face picture on a wall). Is there a smart way to approach the point and ""look"" at it?</p>
","mobile-robot mapping"
"7062","create2 angle (packet ID 20)","<p>how to convert the value you get for the angle (packet ID 20) into degrees?
i am using the create2 robot and when I did not understand the data I am getting back. The documentation it says it's in degrees but what I get back is a huge number like 4864 when I turned the robot just 45 degrees.</p>
","irobot-create"
"7066","What is the technology used for no-resistance robot arms?","<p>I saw a high end robot arm once that I could move and bend it to any form I wanted with no resistance, as if the robot arm didn't have any weight. I'd like to know more about them. What is this class of robot arm design called? Where can I get some more information about its design and applications?</p>
","control robotic-arm joint"
"7067","How can I communicate wirelessly between a RasPi and servo?","<p>I want to use a Raspberry Pi to pan a camera mounted on a servo wirelessly from ~100 feet away. What are some good servos and transceivers for this?
To clarify, there should be no physical connection between the RasPi and servo.
Do I need an additional RasPi on the servo end?</p>
","motor raspberry-pi wireless"
"7068","How does the strategy changes in a RobCup soccer competition without connection to outside?","<p>How is a new team strategy during a robocup competition sent to each player of a robot team? Robots in the Standard Platform League (i.e. SPL), for example, are fully autonomous and there is no connection with non-team members (except pulling from the GameControl).</p>
","soccer"
"7069","Is GMIS really a breakthrough?","<p>The GMIS (General Machine Intelligence System) from a <a href=""http://www.codeproject.com/Articles/897132/GMIS-Turn-All-Your-Devices-into-Robots"" rel=""nofollow"">new article</a> posted at codeproject.com looks interesting.  Do you think that it could be a breakthrough in the field of robotics</p>
","mobile-robot robotic-arm"
"7070","Jacobian for point on robotic arm","<p>currently i am programming for a robotic simulation. I have a Endeffector which aproaches a target, on the way to the target is an Obstacle. Now i redirect my Endeffector, so that it does not hit the target.</p>

<p>When i want to do the same for the whole arm i want to push the arm away from the Obstacle as well. Now i have it working so far that i can redirect the arm. But my calculation for the Jacobian seems to be faulty.</p>

<p>For my setup, and what i need for that.</p>

<p>I have a robotic arm, 7DOF. Let $x_0$ be the closest point on the arm to the obstacle. And $J_0$ the corresponding Jacobian.
Also i have given the following term:
$\dot{x_0} = J_0 * \dot{\theta}$ </p>

<p>$\theta$ are my joint angles. I can calculate the Jacobian for the EndEffector, but do not know on how to calculate it for a point ob the arm.</p>

<p>Does anybody have an Idea on how to calculate the corresponding Jacobian.</p>

<p>Cheers</p>
","robotic-arm jacobian"
"7073","make hc-sr04 receive from another one","<p>I have 3 ultrasonic Sensor (HC-SR04), i want to use one of them as transmitter, and the other as receiver, i want to let the first one send ultra Sonic waves and the other receive these waves from the same transmitter.</p>

<p>how can i do that ? </p>

<p>i tried to send trigger for each ultrasonic and connect them on different pins on PIC, but its now work.</p>

<p>its something like this <a href=""https://www.youtube.com/watch?v=mWgInUW6A8w"" rel=""nofollow"">project</a> but using HC-SR04 </p>
","microcontroller ultrasonic-sensors"
"7080","building a quadcopter using STM32F3 Discovery board","<p>how do you calculate the PID values and stabilise the quadcopter using the on board sensors The gyro accelerometer and magnetometer</p>
","sensors"
"7089","Programable Drone with robotic arm and hand?","<p>I was just wondering if it was possible to buy or build a programmable drone with a robotic arm,hand, knife.</p>

<p>I want to program a drone to harvest crops.</p>

<p>-object recognition from live video stream to server</p>

<p>-identify and grab objects with arm, make cut if necessary</p>

<p>-transport produce to collection site</p>

<p>I know this would take much knowlege from many fields but do any you have any forsight into the limitations of doing this other than energy for power.</p>

<p>Estimates on cost of hardware?</p>
","robotic-arm design quadcopter dynamic-programming"
"7091","Design and construction of universal robotic arm (5kg, 1m)","<p>i am working on my master's thesis about design and construction of universal robotic arm. 
Goal of my work is to design 5 DOF robotic arm. Something like on the picture: <img src=""http://i.stack.imgur.com/RqzaB.jpg"" alt=""enter image description here""></p>

<p>I need it to be able to lift a weight about 5kg. It has to move in ""action radius"" 1m. Rotation speed should be about 1m/s. The conclusion of my work should be like: ""You can buy ABB robotic arm or you can but this..it can lift that much, can turn that speed and weighs that much"". Basic construction should be done too. Maybe with some simulation.</p>

<p>First of all - i picked really bad master's thesis for me, i know that know.
Second - i have like month to finish it.
I would like to ask someone how to proceed. 
I know that first step is to pick servos/actuators/gearboxes, but which one?
What is realistic weight of the whole arm which should lift another 5 kg of weight? How strong motors should i pick or with what gearboxes? </p>

<p>Is anyone able to help me via maybe emails? </p>
","robotic-arm servos actuator arm"
"7095","Depth image sensor for integration into robot","<p>I know there are lots of consumer depth image sensors: kinect, primesense, structure.io, leap motion, ... But I'm looking for something that is more suitable for integration into robot, something without case or with proper mount, compact and available for at least next five years if robot is going to production. Something similar to the sensors used in this drone <a href=""https://www.youtube.com/watch?v=Gj-5RNdUz3I"" rel=""nofollow"">https://www.youtube.com/watch?v=Gj-5RNdUz3I</a></p>
","sensors kinect"
"7097","Connecting USB Xbox Controller to National Instruments cRIO","<p>I have a FIRST Robotics spec National Instruments cRIO.  I would like to connect a USB wireless Xbox controller to it in order to control it from a distance with minimal extra hardware (which is why I am not using the more traditional WiFi radio method).  To this point I have been able to find either</p>

<p>A. A sidecar for the cRIO which allows it to act as a USB host or</p>

<p>B. A method that does not use NI specific hardware to connect the two together</p>

<p>If someone who is knowledgeable on the subjects of industrial system and robot control could provide some assistance that would be greatly appreciated, thanks!</p>
","control industrial-robot wireless usb first-robotics"
"7099","Got Difficulty in reading Specification?","<p>Guys I am making a home automation system very simple one with infrared remote ccontrol of tv remote Now the problem is I wanna buy a some relays to switch 230V AC using my arduino board but can't understand which one to buy I don't want to buy relay module but I wanna buy relay.</p>
","arduino"
"7105","Is it possible to use ROS on RPI with controller like MultiWii or similar?","<p>I have a quadcopter using a MultiWii(Arduino Mega) controller. I am curious if there is any way to connect it with ROS capable RPI. (That I could add to the quad itself).</p>
","arduino ros quadcopter"
"7109","Building industrial-like robotic arm","<p>I would like to build a 70cm articulated robotic arm (not a scara one) that can lift between 10kg and 15kg (10kg would be awesome already, this payload includes the weight of the arm+gripper) and moves at 1 m/s (dreaming again :)). The goal is to make it similar to an human arm since I want to control it ""remotely"", so the joint should not be able to rotate more than my arm ^^</p>

<p>So I know that I cannot uses servos available (like the overpriced dynamixel ones..)  with that payload. I also already excluded common linear actuators and less common ones like the pneumatic actuators (because of latency).</p>

<p>From what I read arms like the baxter ones uses elastic series actuators, so I guess that I should go that way, but there aren't a lot of details on how that works (getting a lot of 100x100 photos where you see nothing) and I have a lot of questions. 
The only thing I could understand is that it uses 2 motors and a spring.</p>

<p>Do they use brushed or brushless motors? DC motors or steppers ? I read that steppers aren't that good to handle collisions and have difficulties when used at their limits.</p>

<p>Also, how is the spring mounted on the motor ?</p>

<p>To sum up, I'm collecting any experience, diagrams, intel, or documents that you have on that topic :)</p>

<p>PS : my budget can't exceed +/- 1500$ for that arm.</p>
","motor robotic-arm stepper-motor servomotor actuator"
"7110","read from Ultrasonic HC-SR04","<p>i want to run two HC-SR04 on one PIC16F877A and send the value mesured by the two ultrasonic to serial port.</p>

<p>this is my code using PIC C Compiler :</p>

<pre><code>#use rs232(baud=9600,parity=N,xmit=PIN_C6,rcv=PIN_C7,bits=8) 
#define e1 PIN_B6
#define t1 pin_B7 
#define e2 pin_B4 
#define t2 pin_B5 
int a; 
int distanse(int,int); 

void main()
{

while(1){
int u1,u2;
u1=distanse(e1,t1);u2=distanse(e2,t2);
printf(""%3u"", u1);
printf(""%3u"", u2);
delay_ms(1000);

}
}

int distanse(int e,int t){
long long counter=0;
output_bit(t,1);delay_us(10);output_bit(t,0);
a=input(e);
while(a==0){a=input(e);}
while(a==1){counter=counter+1;a=input(e);}
return counter/3.333333;
}
</code></pre>

<p>but the computer received random values ! 
what is the problem ?</p>
","microcontroller programming-languages ultrasonic-sensors"
"7111","DH Forward Kinematics for a Cartesian Robot (CNC Mill)","<p>I am implementing a Denavit-Hartenberg forward transform for a 3-axes CNC mill. I know that the kinematic for such a machine is trivial and doesn't need DH, but I need to make appliable for other robots too. My implementation does the math correctly(I've verified that with another tool), but the transformation doesn't give me the results I would expect. </p>

<p>I assume that for 3-axes cartesian robot with orthogonal prismatic joints(=CNC mill) the resulting transformation matrix should give me the input parameters(d1-d3) back in its translation vector, but it somehow doesn't. Also, the resulting orientation matrix should have some ""nice"" values(90, 180, 270, etc.) and no odd ones(0.0528, 0.5987, etc,.). 
Is my assumption wrong?</p>
","forward-kinematics cnc dh-parameters"
"7112","What is the RobotC type for motors?","<p>I'm trying to create a function that allows me to more easily start a motor, but I'm running into a problem, I don't know the type to use for the <code>motorName</code> argument. I'm using a VEX 269 Motor. Here's the function.</p>



<pre><code>void runMotor(MotorTypeHere motorName, int speed, int time)
{
  startMotor(motorName, speed);
  wait(time);
}
</code></pre>

<p>I just don't know what type to put for the <code>motorName</code> argument. What type would it be?</p>
","robotc vex"
"7116","accelerate a stepper using arduino AccelStepper starting with a pre-defined speed","<p>I've read the <a href=""http://www.airspayce.com/mikem/arduino/AccelStepper/classAccelStepper.html"" rel=""nofollow"">AccelStepper documentation</a> on airspayce.com and it seems to be not possible to accelerate a stepper starting with a speed greater 0. Acceleration always starts from speed=0, I tried it with several variations of the code below...</p>

<pre><code>#include &lt;AccelStepper.h&gt;
int onOffPin = 9;

AccelStepper stepper(AccelStepper::DRIVER, 2, 10);

void setup()
{      
   stepper.setMaxSpeed(1000);
   stepper.setSpeed(200);
   stepper.setAcceleration(100);       
}

void loop()
{  
    //turn motor on
    digitalWrite( onOffPin, HIGH ); 

    // go forwards
    digitalWrite( onOffPin, HIGH );
    stepper.move(1300);
    stepper.runToPosition(); // stepper shall start from speed 200, but it starts from speed 0; 

    // Now go backwards
    stepper.move(-1300);
    stepper.runToPosition(); 

    //turn motor off
    digitalWrite( onOffPin, LOW ); 
    delay(2000);
}
</code></pre>

<p>I also tried to set the speed in the library's method <code>void AccelStepper::computeNewSpeed()</code> directly, but I'm not that good in c++ and don't get it to work.</p>

<p>Anybody any ideas?</p>

<hr>

<h2>UPDATE</h2>

<p>I tried to write some custom code in AccelStepper.cpp's method <code>void AccelStepper::computeNewSpeed()</code> 
My idea was to set the speed manually during acceleration/deceleration if the speed is below my intended value. At first I thought it couldn't be a big deal, but now I see that my cpp skills seems to be not good enough or I don't understand the library quite well. </p>

<p>I tried</p>

<pre><code>void AccelStepper::computeNewSpeed()
{
    long distanceTo = distanceToGo(); // +ve is clockwise from curent location
    long stepsToStop = (long)((_speed * _speed) / (2.0 * _acceleration)); // Equation 16 

    //now here goes my modification
    if (_speed &lt; 200.0 &amp;&amp; _speed &gt;= 0 ){
       setSpeed(200.0);
    }
    //I did no modification below this comment
</code></pre>

<p>This results in a very slow stepper movement..</p>
","arduino control stepper-motor c++"
"7120","Measuring angular displacement using the TI-SensorTag","<p>I've looked around but can't find the answer, to what I hope is a simple question. I'm working with a TI-SensorTag, and I want to be able measure the rotation around the unit's Z-axis. Basically I want to attach the tag to a clock pendulum, lie the clock on a table so the tag and clock face point up, and to measure the angular displacement of the pendulum as it swings back and forth. I'm hoping the mental image translated well! </p>

<p>My understanding is that I can solve for displacement by multiplying my gyroscope readings by my sampling period, but I'm not sure how to compensate for drift. So my questions are: is my approach sound, and is the answer to drift to use the changing x and y accelerations? Or would I need to somehow incorporate the magnetometer readings?</p>

<p>Thanks!</p>
","sensors kinematics gyroscope"
"7121","iRobot Create 2: Angle Measurement","<p>I have been working on trying to get the angle of the Create 2. I am trying to use this angle as a heading, which I will eventually use to control the robot. I will explain my procedure to highlight my problem.</p>

<p>I have the Create tethered to my computer. </p>

<p><ol>
<li>I reset the Create by sending Op code [7] using RealTerm.
The output is:</p>

<blockquote>
  <p>bl-start<br>
  STR730<br>
  bootloader id: #x47175347 4C636FFF<br>
  bootloader info rev: #xF000<br>
  bootloader rev: #x0001<br>
  2007-05-14-1715-L<br>
  Roomba by iRobot!<br>
  str730<br>
  2012-03-22-1549-L<br>
  battery-current-zero 252</li>
  </ol>
  (The firmware version is somewhere in here, but I have no clue what to look for--let me know if you see it!) </p>
</blockquote>

<ol start=""2"">
<li>I mark the robot so that I will know what the true angle change has been.</li>
<li>I then send the following codes [128 131 145 0x00 0x0B 0xFF 0xF5 142 6]. This code starts the robot spinning slowly in a circle and request the sensor data from the sensors in the group with Packet ID 2. The output from the Create seen in RealTerm is 0x000000000000, which makes sense.</li>
<li>I wait until the robot has rotated a known 360 degrees, then I send [142 2] to request the angle difference. The output is now 0x00000000005B.</li>
</ol>

<p>The OI specs say that the angle measurement is in degrees turned since the last time the angle was sent; converting 0x5B to decimal is 91, which is certainly not 360 as expected. </p>

<p><strong>What am I doing wrong here?</strong> Is the iRobot Create 2 angle measurement that atrocious, or is there some scaling factor that I am unaware of? are there any better ways to get an angle measurement?</p>
","irobot-create roomba"
"7124","Using accelorometer and gyroscope to get velocity, spin & flightpath of a ball in projectile motion","<p>I'm working on a project to make a SmartBall that can detect the velocity(km/h) , spin(degrees per second) and flightpath(trajectory) of the ball using Intel Edison with the 9DOF block (LSM9DS0 : 3-axis accelerometer, 3-axis gyroscope, and 3-axis magnetometer) &amp; the battery block, I'm reading values from the 9DOF block by RTIMULib(Library for IMU chips). I've been working on integrating the acceleration data from the accelorometer to get the velocity then get the position, I know that this method is not really accurate as the integration error cumulate very fast but I rely on that my calculations will be done in a very short time (about 3 seconds) then i re-calculate again from the beginning after every kick so that error doesn't cumulate hardly, Also i only need an acceptable accuracy not a very high one. I discovered then that i'm dealing with projectile motion(ball kicking), so after considering this &amp; searching in projectile motion equations i found that i must know the initial velocity and the angle of projection(theta) to be able to get my requirements. my problem that I don't know how to get any of these , I tried different approaches like getting the horizontal distance &amp; getting the height to get their resultant(using pythagoras) then get the angle(assuming it's a right angle) in a very small time at the beggining of the projection , but i still couldn't get the height. The gyroscope outputs roll, pitch &amp; yaw angles related to the sensor orientation but i'm still not using this as i'm assuming that the sensor will be fixed inside the ball so it's orientation will not be the same as the projection angle.Now What I really want is any approach/idea on how to get velocity &amp; flightPath of a projectile using accelorometer and gyroscope data. Hope I made it clear , Any help on how to get my requirements is really appreciated, Thanks so much.</p>
","gyroscope"
"7125","keeping 2 motors common for three circuits","<p>I am beginner in robotics and I want to create a line follower+ obstacle avoider+ remote controlled robot and I am using the drive differential algorithm but I want to keep 2 common motors for the three circuits, I have got the three circuits ready and I want it to switch among these 3 circuits wirelessly. Please tell me a solution.   </p>
","motor wireless line-following circuit"
"7128","Determining a robot's distance from a certain point when the robot's position is constantly changing","<p>I was wondering how I could determine a robot's distance from a fixed point when the robot itself is constantly changing positions. I can keep encoders on the wheels and can also get data from a gyroscope and an accelerometer.</p>
","localization"
"7129","(Dynamixel) Inverse rotation direction","<p>I'm wondering if there's a feature to ""flip"" the rotation direction with Dynamixel (I'm using MX-106). For example, if I give +1.57 to the motor, then it interprets it as <code>-1.57</code>. And the other way around.</p>

<p>I'm using <a href=""http://wiki.ros.org/dynamixel_motor"" rel=""nofollow"">its ROS driver</a> package that doesn't seem to explicitly claim that there's a feature to do this, although there is <a href=""http://answers.ros.org/question/59872/dynamixel-wheel_mode/?answer=117281#post-id-117281"" rel=""nofollow"">a question</a> where a user reported he was able to do this from source code. But I failed to replicate. So I first wanted to ask about the capability of the device itself since I don't know if the limitation comes from the Dynamixel device or from ROS driver.</p>

<p>Thank you.</p>

<hr>

<p>(UPDATE) Usecase of mine is that I have multiple robots where the direction of how Dynamixel is attached is different per robot, and ideally like to flip the motor's direction at driver's level so that I can keep using the same controller software.</p>
","servomotor dynamixel"
"7137","Understanding the various attitude estimation methods","<p>I am building a quadcopter using the Arduino Uno with a 6dof accelerometer and gyro. I will be adding a separate 3 axis magnetometer soon for heading. I have successfully implemented the code that reads the data coming in from these sensors and prints them out. </p>

<p>I have also checked for bias by averaging 100 measurements. My code calculates the pitch from the accel and pitch from the gyro respectively:</p>

<p>pitchAccel = atan2((accelResult[1] - biasAccelY) / 256, (accelResult[2] - biasAccelZ) / 256)*180.0 / (PI); </p>

<p>pitchGyro +=((gyroResult[0] - biasGyroX) / 14.375)*dt;</p>

<p>I am then using a complementary filter to fuse the two readings together like this:</p>

<p>pitchComp = (0.98*pitchGyro) + (pitchAccel*0.02);</p>

<p>I am stuck on how to proceed from here. I am using the same procedure for roll, so I now have readings for pitch and roll from their respective complementary filter outputs. </p>

<p>I have read a lot of articles on the DCM algorithm which relates the angles from the body reference frame to the earth reference frame. Should that be my next step here? Taking the pitch and roll readings in the body reference frame and transforming them to the earth reference frame? Repeat the entire procedure for yaw using the magnetometer? If yes, how should I go about doing the transformations? </p>

<p>I understand the math behind it, but I am having a hard time understanding the actual implementation of the DCM algorithm code-wise. </p>

<p>Any help is appreciated!</p>
","arduino quadcopter accelerometer gyroscope"
"7138","Pins in OpenROV that control the motors?","<p>I'm working on my own ROV project, but I find OpenROV have a <a href=""https://github.com/OpenROV/openrov-software/releases/tag/v2.5.0"" rel=""nofollow"">ready to use image</a> for my BB so want to use that instead of making my own program, and I already deployed the image, but I can't find which three pins find that send PWM signal for ESC's? Please help.</p>
","mobile-robot beagle-bone"
"7139","Determining transfer function of a PTU for visual tracking","<p>I have a PTU system whose transfer function I need to determine. The unit receives a velocity and position, and move towards that position with the given velocity. What kind of test would one perform for determining the transfer function...</p>

<p>I know Matlab provides a method. The problem, though, is that I am bit confused on what kind of test I should perform, and how I should use Matlab to determine the transfer function.</p>

<p>The unit which is being used is a Flir PTU D48E</p>

<p>---> More about the system </p>

<p>The input to the system is pixel displacement of an object to the center of the frame. The controller I am using now converts pixel distances to angular distances multiplied by a gain $K_p$. This works fine. However, I can't seem to prove why that it works so well, I mean, I know servo motors cannot be modeled like that.</p>

<p>The controller is fed with angular displacement and its position now => added together give me angular position I have to go to. 
The angular displacement is used as the speed it has to move with, since a huge displacement gives a huge velocity.</p>

<p>By updating both elements at different frequency I'm able to step down the velocity such that the overshoot gets minimized. </p>

<p>The problem here is: if I have to prove that the transfer function I found fits the system, I have to do tests somehow using the <code>ident</code> function in Matlab, and I'm quite unsure how to do that. I'm also a bit unsure whether the PTU already has a controller within it, since it moves so well, I mean, it's just simple math, so it makes no sense that I'll convert it like that.</p>
","control"
"7140","What is the difference between SAM and SLAM?","<p>What is the difference between <em>Smoothing and Mapping</em> (SAM) and <em>Simultaneous Localization and Mapping</em> (SLAM)? These general approaches seem closely related. Can someone describe the differences?</p>
","slam"
"7146","Estimating angular speed from position for control purpose","<p>I am new to robotics, however I am designing a PID to control the angular velocity (azimuth, elevation) of a couple of Faulhaber motors.
The input to the PID control is the actual angular velocity, which is not observed though, since it is derived from the position of each motor at time $t$.</p>

<p>The PID sample period is aprox. <code>30 ms</code>, whereas the input data rate from the joystick is aprox. <code>300 samples/s</code>, corresponding to a sample period of <code>3.33 ms</code>. The joystick input gets transformed into the desired angle speed, that the PID will control.</p>

<p>I was initially filtering position data using a normal 2D linear <strong>Kalman Filter</strong>, but the angular velocity is not linear (by formula), hence I switched to <strong>Extended Kalman</strong> filtering.</p>

<p>My questions are the following:</p>

<ol>
<li>Is this latter approach that makes use of EKF correct?</li>
<li>Which are the parameters that I have to check in order to properly set the update rate of the PID loop?</li>
</ol>

<p>Thx in advance!</p>
","pid ekf"
"7148","An easy way to exert desired load on a motor shaft?","<p>I am trying to exert a desired load of 0.07 N.m on a BLDC motor shaft whose length is 0.750in and diameter is 0.3125in (0.008m). I can go to a machine shop and get a small adjustable cylindrical coupling made for my shaft. But I need it to exert close to desired torque at a speed of 2100 rpm (220 rad/s). I tried doing some calculations, according to the formula </p>

<p>Torque = speed * mass * (radius)^2</p>

<p>If I solve this equation with T = 0.07 N.m, speed = 220 rad/sec, radius = 0.004 m, I get around 20 kg for mass, which is huge!!!. It is more than the mass of the motor. Can you please suggest a convenient way to load the motor. Thank you.</p>
","motor brushless-motor"
"7153","Is it possible to have A/V feed and serial communication on an RF transceiver at the same time on an Arduino?","<p>Is it possible to transmit live audio/video feed and at the same time, receive commands through UART using only 1 RF transceiver connected to the Arduino board?</p>

<p>I want to control the Arduino through serial communication (UART) which can be accomplished by using RF connection to control it from a remote. I also want to transmit live audio and video feed from the Arduino using the same RF transceiver. Is this possible?</p>

<p>I found AVCTP, but I'm not sure if it enables serial communication. Also, I don't like to use Bluetooth for some reasons.</p>

<p>Thanks in advance!</p>
","arduino microcontroller radio-control serial communication"
"7156","STM32F3 timers & computing","<p>I have an STM32F3 discovery board. I want to go to the next step and I want to try to use timers in a few configurations. </p>

<p>How can I calculate variables (such as prescaler, period)? I looked in all datasheets, manuals and didn't find anything that can describe these values as  - Input capture mode, OP, PWM, etc. </p>

<p>I think that prescaler is for downgrading a frequency from 1-65575. </p>

<p>So if I have f<sub>cpu</sub>=72MHz and want to generate a signal of frequency=40kHz, am I supposed to do: 72MHz/40kHz=1800? </p>

<p>Now should I subtract this prescaler with -1?</p>
","microcontroller"
"7158","C++ and Create 2","<p>I am trying to use C++ to talk to the Create 2 robot. Does anyone have basic code to write/read from the Create 2 using C++ or C?</p>

<p>I am having trouble with converting Create 2 commands (like <code>145</code>) into one <code>char</code>.</p>
","irobot-create c c++"
"7159","Do structured light camera sensors work outdoors?","<p>Do structured light camera sensors like the structure.io, Intel RealSense or Microsoft Kinect work outdoors?</p>

<p>I read these sensors wont work outdoors because of ambient IR light. Can someone provide this with proper references/tests? I mean what degree of IR illumination is needed for the sensor to stop working etc.</p>

<p>There are videos on YouTube that show Microsoft Kinect working outdoors:</p>

<ul>
<li><a href=""https://www.youtube.com/watch?v=9e4lYzTK6Mo"" rel=""nofollow"">Prairie Dog II: UGV Kinect Sensor Outside</a> - limited outdoors range</li>
<li><a href=""https://www.youtube.com/watch?v=rI6CU9aRDIo"" rel=""nofollow"">Outdoor Kinect Data Collection</a> - heavy interference with direct sunlight</li>
</ul>

<p>However, the (not yet released) new <a href=""https://software.intel.com/en-us/realsense/devkit"" rel=""nofollow"">Intel RealSence R200 specification</a> says ""range up to 3-4 meters indoors, longer range outdoors"" while the older F200 says ""0.2 meters - 1.2 meters, indoors only"". I am really interested in seeing if the R200 will really work outdoors.</p>
","kinect cameras"
"7161","ODroid XU3 speed issue with image processing","<p>Currently working on a UAV using an ODroid XU3 Lite as the center core running the out-of-box SD card image of Ubuntu.</p>

<p>This is (supposedly) an upgrade from the previous XU that we were running. We're using OpenCV functions HoughCircles, Lines, and RGB color detection, searching for shapes and color blobs of 3 different colors (white, Red, Green) for a competition. This is all running a lag time a little over 2 seconds on the OpenCV playback frame with an output every second or so on the terminal when that's off.</p>

<p>The XU3 is running even slower, though, despite being a better machine. What could I do to improve the speed or is the XU3 not fitting the power requirements? One thing I think is an issue could be that the stock Ubuntu version is bloated and I've been looking around for other images, and I'd like to avoid using Android. </p>
","computer-vision software uav c++ linux"
"7163","Hand Eye Calibration","<p>I'm trying to use a dual quaternion <a href=""https://github.com/hengli/camodocal/blob/master/include/camodocal/calib/HandEyeCalibration.h"" rel=""nofollow"">Hand Eye Calibration Algorithm Header</a> and <a href=""https://github.com/hengli/camodocal/blob/master/src/calib/HandEyeCalibration.cc"" rel=""nofollow"">Implementation</a>, and I'm getting values that are way off. I'm using a robot arm and an optical tracker, aka camera, plus a fiducial attached to the end effector. In my case the camera is not on the hand, but instead sitting off to the side looking at the arm.</p>

<p>The transforms I have are:</p>

<ul>
<li>Robot Base -> End Effector</li>
<li>Optical Tracker Base -> Fiducial</li>
</ul>

<p>The transform I need is:</p>

<ul>
<li>Fiducial -> End Effector</li>
</ul>

<p><img src=""http://i.imgur.com/EPfPTky.png"" alt=""HandEyeCalibrationQuestion""></p>

<p>I'm moving the arm to a series of 36 points on a path (blue line), and near each point I'm taking a position (xyz) and orientation (angle axis with theta magnitude) of Camera->Fiducial and Base->EndEffector, and putting them in the vectors required by the <a href=""https://github.com/hengli/camodocal/blob/master/include/camodocal/calib/HandEyeCalibration.h"" rel=""nofollow"">HandEyeCalibration Algorithm</a>. I also make sure to vary the orientation by about +-30 degrees or so in roll pitch yaw.</p>

<p>I then run estimateHandEyeScrew, and I get the following results, and as you can see the position is off by an order of magnitude. </p>

<p>[-0.0583, 0.0387, -0.0373] Real
[-0.185, -0.404, -0.59] Estimated with HandEyeCalib</p>

<p>Here is the full transforms and debug output:</p>

<pre><code># INFO: Before refinement: H_12 =
-0.443021 -0.223478  -0.86821  0.321341
 0.856051 -0.393099 -0.335633  0.470857
-0.266286 -0.891925   0.36546   2.07762
        0         0         0         1
Ceres Solver Report: Iterations: 140, Initial cost: 2.128370e+03, Final cost: 6.715033e+00, Termination: FUNCTION_TOLERANCE.
# INFO: After refinement: H_12 =
  0.896005   0.154992  -0.416117  -0.185496
 -0.436281    0.13281  -0.889955  -0.404254
-0.0826716   0.978948   0.186618  -0.590227
         0          0          0          1


expected RobotTipToFiducial (simulation only):   0.168   -0.861    0.481  -0.0583
expected RobotTipToFiducial (simulation only):   0.461   -0.362    -0.81   0.0387
expected RobotTipToFiducial (simulation only):   0.871    0.358    0.336  -0.0373
expected RobotTipToFiducial (simulation only):       0        0        0        1


estimated RobotTipToFiducial:   0.896    0.155   -0.416   -0.185
estimated RobotTipToFiducial:  -0.436    0.133    -0.89   -0.404
estimated RobotTipToFiducial: -0.0827    0.979    0.187    -0.59
estimated RobotTipToFiducial:       0        0        0        1
</code></pre>

<p>Am I perhaps using it in the wrong way? Is there any advice you can give?</p>
","robotic-arm stereo-vision calibration"
"7165","How to implement PID control for robotic arm?","<p>I'm wondering that, PID control is a linear control technique and the robot manipulator is a nonlinear system, so how it is possible to apply PID control, in this case. I found a <a href=""http://www.slideshare.net/popochis/pid-control-dynamics-of-a-robotic-arm-manipulator-with-two-degrees-of-freedom"" rel=""nofollow"">paper</a> named: PID control dynamics of a robotic arm manipulator with two
degrees of freedom. on slide share page, is this how we use PID control for robotic arm, is there any name for this approach? and how to remove the ambiguity that PID is linear control technique and the robot is nonlinear system. Any suggestions?</p>
","control pid robotic-arm industrial-robot dynamics"
"7167","Is this rubber/PVC coupling a good enough for small torque (0.1 N.m)","<p>I am working on a project that involves speed regulation of a BLDC motor under no-load and load conditions. I wish to use another machine operated as generator, acting as load on the motor, as shown in <a href=""https://www.youtube.com/watch?v=L8gvQIgOiGo"" rel=""nofollow"">this video</a>. </p>

<p>The coupling used in this motor/generator arrangement looks handmade out of a rubber tube or somethhing. I am considering using it as an alternative to a flexible coupling. Purchasing an actual flexible coupling is not an option for me. Moreover, I need the coupling on an urgent basis. </p>

<p>My question is, can this arrangement (or something similar) be used to couple a 15W motor to a similar rating machine, if the rated torque is not exceeding 0.1 N.m?</p>
","control brushless-motor"
"7175","Need suggestion about which microcontroller/processor and language to be used in my project","<p>I am very new to robotics. but I will be writing algorithm for my robot to move around and gather information from its surroundings and process it. It will also process audio-visual signals. but I am in confusion about which micro-controller to use so it would be performance efficient and consumes less power.</p>

<p>The controller should also be capable of communication with wireless network (internet through wi-fi) and should also support memory integration.</p>

<p>Also I know to program in Java and C. please suggest which would be the best language to use for programming.</p>

<p>Thanks.</p>

<p>P.S. I would really like to use a microprocessor as it is highly customizable. Please suggest the best to use</p>
","artificial-intelligence"
"7176","Using 20 Servos at once, with raspberry pi","<p>I have recently been asked to review a raspberry pi hat (from a programming view) that will allow PWM control of upto 16 servos, however I am hoping to use this time to work on a hexapod idea I have been thinking about for a while, which requires a minimum of 18 servo's, and preferably 20 (camera/sensor pan and tilt).</p>

<p>My question is:</p>

<p>What is a relatively cheap and uncomplicated way of extending my control over those extra 4 servo's?</p>

<p>It would appear most servo controller hat/shiels for arduino and raspi are upto 16 servos, and can be extended by buying another shield, are there any other options?</p>

<p>Any advice in this subject would be greatly appreciated, preferably dumbed down a bit, and I don't know a great deal about micro controller hardware (more of a software guy)</p>
","raspberry-pi servomotor rcservo"
"7177","How to test GAZEBO works properly. Save windows don't show any component","<p>How can I test whether my gazebo installation works properly or not? I'm trying to ""save myworld"" and ""save as"" options but no window is shown.</p>
","gazebo"
"7178","Is ROS (Robot Operating System) mandatory?","<p>Do we have to build ROS for robotic researchs/applications? What is the main advantage? When or in which situations ROS is mandatory?</p>
","ros"
"7181","Is the geometric inverse problem's solution ""continuous"" for a redundant robot?","<p>Let's say my redundant robot is at an operationnal position $x$. Is the set of all possible joint configuration ""continuous"", which would mean that it is possible to explore all the possible configurations without moving the end effector. Is there a way to show that it is true or false? I am using a Kuka LBR robot with 7 dof so maybe there is a specific answer for this one.</p>

<p>I have been searching it and did not find any result but I will gladly accept any link or answer that you may have.</p>
","control inverse-kinematics"
"7186","What would I need to control a DC servo using a 4-20ma linear analog signal?","<p>I'm looking to find a way to operate a small servo using a 4-20ma linear analog signal generated by a PLC in an industrial setting. </p>

<p>The purpose of this is to allow for automation for a task that currently is done by manually turning and adjusting a potentiometer with removable dial. Basically, I'm trying to ghetto together an oldschool Motor Operated Potentiometer (MOP) so it can be removed quickly and easily without affecting the operation of the original process.</p>

<p>I've spent hours looking for servo controllers/encoders that are capable of this, but I haven't been able to find any. Any way I could get pointed in the right direction would be fantastic. Surely such a thing must exist!
Thanks so much!</p>
","rcservo"
"7188","Can a Jacobian matrix be used to derive joint angles from end-effector linear and rotational velocity (without a filter)?","<p>I have a 2-link, 2 degree of freedom robotic arm, that only measures linear acceleration at each link(through an accelerometer), and rotational velocity on each joint (through a gyroscope).</p>

<p>I know that through using the Jacobian matrix, I can compute link velocity and acceleration from joint angles, and through the inverse of the matrix I can compute joint velocities from joint angles and link acceleration.</p>

<p>However, I am not sure if I can compute joint angles using only the link linear and rotational acceleration? I am aware that the joint angle could be estimated by integrating the joint velocities (and applying some sort of filter), but is there an algebraic way this can be computed? It doesn't seem likely to me. </p>
","robotic-arm accelerometer gyroscope jacobian"
"7189","create2 reading sensor date does not always work","<p>I noticed that the create2 does not always provide sensor data while it's moving. Am I supposed to stop the robot, request sensor data then start it again? or am I missing something? it seems to work most of the time but once in a while I get no data back. I am trying to make it move from one point to another by starting it and then reading distance to see how far it travels every .1 seconds but sometimes it I just keeping getting no data.</p>

<p>I noticed this using python and C code as well.</p>

<p>I am using the USB port with the bit rate they recommend (115200).</p>
","irobot-create"
"7190","Cheap and efficient 3D sensor?","<p>I'm searching for a <strong>cheap (under 100$) and efficient</strong> 3D sensor, which detects obstacles and moving objects, for robot applications like quadrotor navigation, swarm robotics, etc. Can you suggest a sensor that can be either a commercial product or a ""do it yourself"" project?</p>
","mobile-robot sensors swarm"
"7194","Beginner question about software for calculations","<p>I'm a complete beginner in robotics with background in programming...</p>

<p>I started thinking about a robot project yesterday and want to order some motors to test with. I saw the specs for the motors, torque etc, and I think I remember enough physics from high school to do simple calculations. For example, if the motor rotates an arm, then given the torque and the length of the arm, how much could it lift? Also, if it doesn't lift it straight up, but at an angle, I could with a bit of thinking tweak the calculations a bit... If there would be several joints attached to each other, the calculations would be more complex, but I could then probably create a program in node.js, for example, to be able to experiment with different values.</p>

<p>However, I would assume that these kinds of calculations would be very common when designing robots. So I would assume there are already programs for these types of calculations created. I don't know exactly what I mean with ""these types of calculations"", because I don't know yet what I don't know, so would like to ask which programs are there that you guys use for making calculations when you design your robots?</p>

<p>Preferable they should be open source...</p>
","robotic-arm mechanism software"
"7198","Dynamic model of a tank like robot","<p>I am planning a tank like robot for hobby purpose. I have control engineering background, however I never applied on robotics. 
I would like to test different control theory, namely MPC. I saw a lot of publications regarding the kinematics and inverse kinematics of such a robot, however I am wondering if somebody can point out regarding the dynamics modelling of such a system,taking into account the forces, mass etc?</p>
","wheeled-robot"
"7199","Pole placement gains tuning","<p>given my control system 
<img src=""http://i.stack.imgur.com/fXjuI.png"" alt=""enter image description here""></p>

<p>I have found the region of the complex space that satisfies my specifications, determining poles position in 0.5 +- 0.2i. 
<img src=""http://i.stack.imgur.com/McX5p.png"" alt=""enter image description here"">
Now I want to find the gains that fix the desider pole (with matlab), but I have not understand well how to do it: anyone can suggest me an example on how to do that, with or without matlab?
Thanks</p>

<p>Edit: in the first image the sum blocks are +-, not ++</p>
","control pid tuning matlab"
"7203","Education sources for robot building?","<p>I teach FTC robotics to high school students, and while I'm a proficient programmer and can teach them coding fairly well, my mechanical skills are a bit soft.  I'm looking for good sources for myself and the students to go through that gets a little more in depth than ""this is a gear, this is a chain, this is gear ratio, etc.,"" but maybe not quite the level of building professional / industrial robots.  </p>

<p>I've used the Vex Robotics Curriculum as a starting reference - <a href=""http://curriculum.vexrobotics.com/curriculum"">http://curriculum.vexrobotics.com/curriculum</a> - but it doesn't go through some more advanced topics (for example, how to drive a single gear / drive shaft with multiple motors to achieve more power without having to gear down and lose speed.)</p>

<p>Are there any good intermediate sources like this?  Do I need to just bit the bullet and get a college level mechanics text?</p>
","mechanism"
"7206","Building my first quadcopter","<p>I am trying to build a quadcopter from scratch. I have selected few parts but I have no idea whether the quadcopter will come together and fly.</p>

<p>I would appreciate your feedback on whether the parts I have selected are compatible (<a href=""http://www.hobbyking.com/hobbyking/store/__15205__Hobby_King_30A_ESC_3A_UBEC.html"" rel=""nofollow"">UBEC</a>, <a href=""http://www.hobbyking.com/hobbyking/store/__15193__HobbyKing_Donkey_ST3007_1100kv_Brushless_Motor.html"" rel=""nofollow"">Motor</a>). If not, I would appreciate suggestions.</p>

<p>The frame for my quadcopter is in the X configuration and I am making my own. I am expecting the average weight of the quad to be around 800g. I hope the motors and prop combination can hover it well.</p>
","multi-rotor"
"7210","recommendation of robot for special education","<p>I saw a video of a robot used in special education with children on the autism spectrum (<a href=""https://www.youtube.com/watch?v=FQcjfebQXgQ"" rel=""nofollow"">https://www.youtube.com/watch?v=FQcjfebQXgQ</a>).  My son isn't autistic, he has Tourette Syndrome, ADHD, executive function problems, and OCD.  A robot could be quite helpful for him.  Where can I buy one?  I don't need it to look like a human being.  It just needs to be interactive and reasonable cute.  As my son is getting ready for bed, he needs someone to talk him through his steps, give him positive feedback, and ask questions like ""Okay, you're in your pajamas.  Great!  What else do you need to do to get ready for bed?""  And the robot would have a mental list (preprogrammed) of everything that's needed (brush teeth, wash face, put on eczema ointment, put dirty clothes in hamper).  My son is 12 and would like to get ready by himself -- without Mama or Papa -- but he gets sidetracked when he's in his room on his own.  The robot doesn't need to be able to ""see"" him brushing his teeth.  He just needs to be able to hear my son saying, ""I brushed my teeth.""  Because when the two of them together decide he has made it through his routine, then they can call me in, and I'll check, and then we'll do our bedtime reading.</p>

<p>That's an example of what I have in mind.  There are other situations where I could imagine a robot being helpful for him.</p>
","artificial-intelligence"
"7212","estimate transfer function of stepper motor?","<p>I have a stepper motor which has an internal controller. 
I would like to determine them both, but don't know how i should approach the problem. 
the system receives a input velocity and position, and moves toward that position using that velocity. The input could also just be a velocity. </p>

<p>The plant is a Pan and tilt unit, which has 2 stepper motors. I Tried with ident but only got a fit of 5 %...  My input was a noisy signal, and output  was the position it writes out. </p>
","control"
"7215","Arduino-Create 2: Reading Sensor Values","<p>Over the past few weeks, I have been attempting to interface the iRobot Create 2 with an Arduino Uno. As of yet, I have been unable to read sensor values back to the Arduino. I will describe by hardware setup and my Arduino code, then ask several questions; hopefully, answers to these questions will be helpful for future work with the Create 2.</p>

<p><strong>Hardware:</strong>
The iRobot Create 2 is connected to the Arduino Uno according to <a href=""http://www.irobot.com/~/media/MainSite/PDFs/About/STEM/Create/Arduino_Tutorial.pdf?la=en"" rel=""nofollow"">the suggestions given by iRobot</a>. Instead of the diodes, a DC buck converter is used, and the transistor is not used because a software serial port is used instead of the UART port.</p>

<p><strong>Software:</strong>
The following is the code that I am implementing on the Arduino. The overall function is to stop spinning the robot once the angle of the robot exceeds some threshold. A software serial port is used, which runs at the default Create 2 Baud rate.</p>

<pre><code>#include &lt;SoftwareSerial.h&gt;
int rxPin=3;
int txPin=4;
int ddPin=5; //device detect
int sensorbytes[2]; //array to store encoder counts
int angle;
const float pi=3.1415926;
#define left_encoder (sensorbytes[0])
#define right_encoder (sensorbytes[1])
SoftwareSerial Roomba(rxPin,txPin);

void setup() {
  pinMode(3, INPUT);
  pinMode(4, OUTPUT);
  pinMode(5, OUTPUT);
  pinMode(ledPin, OUTPUT);
  Roomba.begin(19200);

  // wake up the robot
  digitalWrite(ddPin, HIGH);
  delay(100);
  digitalWrite(ddPin, LOW);
  delay(500);
  digitalWrite(ddPin, HIGH);
  delay(2000);

  Roomba.write(byte(128));  //Start
  Roomba.write(byte(131));  //Safe mode
  updateSensors();

  // Spin slowly
  Roomba.write(byte(145));
  Roomba.write(byte(0x00));
  Roomba.write(byte(0x0B));
  Roomba.write(byte(0xFF));
  Roomba.write(byte(0xF5));  
}

void loop() {
    updateSensors();
    // stop if angle is greater than 360 degrees
    if(abs(angle)&gt;2*pi){
      Roomba.write(173);
      delay(100);
    }
}

void updateSensors() {
  // call for the left and right encoder counts
  Roomba.write(byte(148));
  Roomba.write(byte(2));
  Roomba.write(byte(43));
  Roomba.write(byte(44));
  delay(100);

  // load encoder counts into an array
  int i = 0;
  while(Roomba.available()) {
    int c = Roomba.read();
    sensorbytes[i] = c;
    i++;
  }
  angle=((right_encoder*72*pi/508.8)-(left_encoder*72*pi/508.8))/235;
}
</code></pre>

<p><strong>Questions:</strong></p>

<ul>
<li><strong>Am I loading the sensor values into the array correctly?</strong> This same code works when a bump and run program is implemented, but that requires knowing only one bit rather than two bytes.</li>
<li><strong>How many bytes can be read over the serial connection at a time?</strong> A previous post (<a href=""http://robotics.stackexchange.com/questions/2530/help-sending-serial-command-to-roomba/2537#2537"">Help sending serial command to Roomba</a>) highlights that one byte can be sent at a time. Does this imply that the reverse is true? If so, would a solution be to use a char array to read the values instead and then to append two chars to form an signed int?</li>
<li><strong>Is serial communication synchronization a problem?</strong> I am assuming that synchronization is not a problem, but is it possible for the bytes to be split on the nibble boundaries? This would present a problem because there is not a nibble datatype. </li>
</ul>
","arduino irobot-create roomba"
"7216","What are the different ways to control distance to be covered by a robot?","<p>I want to move a robot to a certain distance say 1 meter.
What are the different ways that i can implement to do so?
For example I can measure the circumference of the wheel and assign time of rotation to move it. what are other techniques to achieve this?</p>
","wheeled-robot movement"
"7219","Issue with multiple bytes from Irobot Create 2","<p>I was having problems reading sensor information from my Irobot Create 2 and sent an email asking for help from the Irobot staff. They were super helpful and gave me an answer(the next day!!!) that helped push along my project. I was requesting data from the create2 to print to the screen so I could figure out how to write a code that would read the data. I started with this section of code that was not working for me (I trimmed some of the code off that controlled other functions):</p>

<pre><code>from Tkinter import *
from subprocess import call
import datetime
import serial
import ttk
import struct
import thread

port = '/dev/ttyUSB0' #sets the com port for Atlas
baud = 115200 #sets the baud rate
connection = serial.Serial(port, baud) #starts the serial communication

#program to read communication from create2
def program2(threadName):
    while True:
        x = connection.read()
        print x

#program to write to create2    
def program1(threadName):
    atlas = Tk() #starts a new GUI for atlas control
    atlas.geometry('1000x500') #sets the size of the control window
    atlas.title('Atlas Control Panal') #sets the name of the control window

    def sendCommandASCII(command):#used to send a command to the create2
    cmd = """"
    for v in command.split():
        cmd += chr(int(v))

    sendCommandRaw(cmd)

    def sendCommandRaw(command):#used to send a command to the create2
    global connection

    try:
        if connection is not None:
            connection.write(command)
        else:
            tkMessageBox.showerror('Not connected!', 'Not connected to a robot!')
            print ""Not connected.""
    except serial.SerialException:
        print ""Lost connection""
        tkMessageBox.showinfo('Uh-oh', ""Lost connection to the robot!"")
        connection = None

    def test():#sets a test command up to check connection
        global buttonpress
        buttonpress='test'
        sendCommandASCII('142 7')

    #makes a button on the GUI that starts the test command
    button1 = Button(atlas, text = 'test mode', command=test)
    button1.place(x=600, y=400)
    button1.config(width=10, height=5)

    atlas.mainloop() #runs the GUI as a loop to keep the window open.

#runs the read and the write program at the same time
try:
    thread.start_new_thread(program1, (""program1"",))
    thread.start_new_thread(program2, (""program2"",))
except Exception, e:
    print str(e)
</code></pre>

<p>They told me that the code was actually working fine but I was trying to print out the value of the sensor packet without parsing it in any way. They then recommended I change the code in program2 to this:</p>

<pre><code>while True:

    def toHexFromByte(val):
        return hex(ord(val))[2:}.rjust(2, '0').upper()

    x = connection.read()
    for b in x:
        print toHexFromByte(b)
</code></pre>

<p>this works beautifully and prints to the screen if the bumper is pressed or a wheel drops. My question is how to deal with responses that are longer than one byte (ie Packet ID: 22 for voltage)? </p>

<p>When I try Packet ID: 22 it prints to screen and it sends the high byte of 3F and a low byte of D7. I can manually combine them to get 3FD7 and convert it to a decimal of 16.343 Volts but I would like to figure a way to print to screen the the voltage by having my program do this for me. Many of the sensors send data in 2 bytes and I need a way to make sure that it is combined automatically. </p>

<p>Robb </p>
","irobot-create python roomba"
"7221","What is the difference between Positioning and Localization Systems","<p>I would like to know what are the differences between Positioning and Localization Systems. In most review papers they are used interchangeably. Are they the same?
 For example:
GPS(Global Positioning system): gives coordinates of receiver and
SLAM(Simultaneous localization and mapping): constructing or updating a map of an unknown environment</p>

<p>Is difference :</p>

<p><strong>Positioning:</strong> only gives information about receiver coordinates.No information about enviorement</p>

<p><strong>Localization:</strong> gives information about receiver coordinates and also enviorement. positioning is a subtopic of localization</p>
","localization slam gps"
"7225","Robotic winch force sensor","<p>I have a requirement for a motor that pulls a piece of rope until the rope is taught. However I'm at a loss as to how to achieve this, I'm sure it must've been done before but I'm not sure how to best describe this in a way that would get me more results. I wondered if there are any sensors or pre-established methods for sensing resistance to motion in electrical motors?</p>
","motor"
"7226","At what frequency should I input and read values?","<p>I am at the moment trying to identify a system using frequency sweep. I have been using Mathematica and have created a frequency sweep as such.</p>

<pre><code>g[t_] := 0.799760*Sin[2 Pi (3 t/333.3 + 1) t];
Plot[g[t], {t, 0, 1000000000000000000000000000000}]
g[#] &amp; /@ Range[0, 5, 0.001]
</code></pre>

<p>The max frequency is 10 Hz, I sample the data using 1000 Hz. But at what rate should I input it to the system, and what rate should I read from it?</p>
","control"
"7227","Building a micro cnc machine","<p>I am building a micro cnc machine, probably from 2 cd roms as the x and y axis and a floppy drive as the z, or 1 hard drive as the x, 1 cd drive as the y and 1 floppy as the z, but I am not sure how I wire this to work with EMC2, a linux cnc program that works with your parrallel port.
Do I connect my steppers directly to the control drivers(the board I buy off ebay right?) and then connect to my pc?Or do I need to interface the board instead then go to the driver and them my parralel port?
I also am wondering why people opt for x/y at the bottom and z at the top, why not put x/y/z stacked, so you could place it upside down for a top down view?(I would do this but it might be done for accuracy,3 layers of potential failure.</p>
","driver stepper-motor cnc"
"7229","iRobot Create 2: Encoder Counts","<p>This post is a follows from an earlier post (<a href=""http://robotics.stackexchange.com/questions/7121/irobot-create-2-angle-measurement"">iRobot Create 2: Angle Measurement</a>). I have been trying to use the wheel encoders to calculate the angle of the Create 2. I am using an Arduino Uno to interface with the robot.</p>

<p>I use the following code to obtain the encoder values. A serial monitor is used to view the encoder counts.</p>

<pre><code>void updateSensors() {
  Roomba.write(byte(149)); // request encoder counts
  Roomba.write(byte(2));
  Roomba.write(byte(43));
  Roomba.write(byte(44));
  delay(100); // wait for sensors 
  int i=0;
  while(Roomba.available()) {
    sensorbytes[i++] = Roomba.read();  //read values into signed char array
  }

  //merge upper and lower bytes
  right_encoder=(int)(sensorbytes[2] &lt;&lt; 8)|(int)(sensorbytes[3]&amp;0xFF);
  left_encoder=int((sensorbytes[0] &lt;&lt; 8))|(int(sensorbytes[1])&amp;0xFF);

  angle=((right_encoder*72*3.14/508.8)-(left_encoder*72*3.14/508.8))/235;
}
</code></pre>

<p>The code above prints out the encoder counts; however, when the wheels are spun backwards, the count increases and will never decrement. Tethered connection to the Create 2 using RealTerm exhibits the same behavior; <strong>this suggests that the encoders do not keep track of the direction of the spin.</strong> Is this true? </p>
","irobot-create roomba"
"7230","System identification on a physical system with constrains","<p>What kind of input output test can be peformed on at physical system which has constraints to identify the transfer function.  The system which is being discussed is a pan/tilt unit.. </p>

<p>The input it receives is either both a position and velocity or only a velocity.. </p>
","input"
"7234","Are LiPo really 100 times more energy dense than model rockets?","<p>Lately I've been interested in comparing the energy density of <a href=""http://www.estesrockets.com/rockets/engines/standard/1613-c6-3"" rel=""nofollow"">model rocket engines</a> to lithium polymer batteries (attached to motors and propellers) for propelling things upwards.</p>

<p>To get a feel for this, I decided to compare an Estes C6-5 motor to a 3DR Iris + quadcopter.</p>

<p>Estes C6-5 has initial mass of 25.8g, and produces 10 N s total impulse. So, the ""Impulse density"" is about 10 N s / 25.8g = 0.38 N s g^-1.</p>

<p>3DR Iris+ weighs 1282g without battery. 3.5ah battery weighs 250g and will power hover for about 20 minutes (so about 10.5a draw). Thrust produced to hover on Earth is 9.8N kg^-1 * 1.532 kg = 14.7N. ""Impulse density"" is 14.7N * 1200s / 250g = 70.6 N s g^-1 .</p>

<p>So, according to my math here, the LiPo is about 0.38/70.6 = 186 times more energy dense than the model rocket engine.</p>

<p>Of course, the model rocket engine will lose 12.48g of propellant by the end of the flight so it will be effectively a little lighter, but that's not going to affect things by a factor of 100.</p>

<p>Does this seem right to you? Am I missing anything?</p>
","quadcopter rocket lithium-polymer"
"7235","how to implement and code inner and outer PD controllers for quadrotor for position tracking","<p>The quadrotor system is  multi-ODEs equations. The linearized model is usually used especially for position tracking, therefore one can determine the desired x-y positions based on the roll and pitch angles. As a result,  one nested loop which has inner and outer controllers is needed for controlling the quadrotor. For implementation, do I have to put <code>while-loop</code> inside <code>ode45</code> for the inner attitude controller? I'm asking this because I've read in a paper that the inner attitude controller must run faster (i.e. 1kHz) than the position controller (i.e. 100-200 Hz). In my code, both loops run at 1kHz, therefore inside <code>ode45</code> there is no <code>while-loop</code>. Is this correct for position tracking? If not, do I have to insert <code>while-loop</code> inside <code>ode45</code> for running the inner loop? Could you please suggest me a pseudocode for position tracking?</p>

<p>To be more thorough, the dynamics equations of the nonlinear model of the quadrotor is provided  <a href=""http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.149.4367&amp;rep=rep1&amp;type=pdf"" rel=""nofollow"">here</a>, if we assume the small angles, the model is reduced to the following equations </p>

<p>$$
\begin{align}
\ddot{x}      &amp;= \frac{U_{1}}{m} ( \theta \cos\psi + \phi \sin\psi) \\
\ddot{y}      &amp;= \frac{U_{1}}{m} ( \theta \sin\psi - \phi \cos\psi) \\
\ddot{z}      &amp;= \frac{U_{1}}{m} - g   \\
\ddot{\phi}   &amp;= \frac{l}{I_{x}} U_{2} \\
\ddot{\theta} &amp;= \frac{l}{I_{y}} U_{3} \\
\ddot{\psi}   &amp;= \frac{1}{I_{z}} U_{4} \\
\end{align}
$$</p>

<p>The aforementioned equations are linear. For position tracking, we need to control $x,y,$ and $z$, therefore we choose the desired roll and pitch (i.e. $\phi^{d} \ \text{and} \ \theta^{d}$)</p>

<p>$$
\begin{align}
\ddot{x}^{d} &amp;= \frac{U_{1}}{m} ( \theta^{d} \cos\psi + \phi^{d} \sin\psi) \\
\ddot{y}^{d} &amp;= \frac{U_{1}}{m} ( \theta^{d} \sin\psi - \phi^{d} \cos\psi) \\
\end{align}
$$</p>

<p>Therefore, the closed form for the desired angles can be obtained as follows</p>

<p>$$
\begin{bmatrix}
\phi_{d} \\
\theta_{d} 
\end{bmatrix}
=
\begin{bmatrix}
\sin\psi &amp; \cos\psi \\
-\cos\psi &amp; \sin\psi 
\end{bmatrix}^{-1}
\left( \frac{m}{U_{1}}\right) 
\begin{bmatrix}
\ddot{x}^{d} \\
\ddot{y}^{d}
\end{bmatrix}
$$</p>

<p>My desired trajectory is shown below</p>

<p><img src=""http://i.stack.imgur.com/QDujZ.png"" alt=""enter image description here""></p>

<p>The results are </p>

<p><img src=""http://i.stack.imgur.com/UKJKc.png"" alt=""enter image description here""></p>

<p>And the actual trajectory vs the desired one is </p>

<p><img src=""http://i.stack.imgur.com/9hbl9.png"" alt=""enter image description here""></p>

<p>My code for this experiment is </p>

<pre><code>%%
%######################( Position Controller )%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

clear all;
clc;

dt = 0.001;
 t = 0;

% initial values of the system
 x = 0;
dx = 0;
 y = 0;
dy = 0;
 z = 0;
dz = 0;

   Phi = 0;
  dPhi = 0;
 Theta = 0;
dTheta = 0;
   Psi = pi/3;
  dPsi = 0;


%System Parameters:
m  = 0.75;      % mass (Kg)
L  = 0.25;      % arm length (m)
Jx = 0.019688; % inertia seen at the rotation axis. (Kg.m^2)
Jy = 0.019688; % inertia seen at the rotation axis. (Kg.m^2)
Jz = 0.039380; % inertia seen at the rotation axis. (Kg.m^2)
g  = 9.81;      % acceleration due to gravity m/s^2

errorSumX = 0;
errorSumY = 0;
errorSumZ = 0;

errorSumPhi   = 0;
errorSumTheta = 0;

pose = load('xyTrajectory.txt');

DesiredX = pose(:,1);
DesiredY = pose(:,2);
DesiredZ = pose(:,3);

dDesiredX = 0;
dDesiredY = 0;
dDesiredZ = 0;

DesiredXpre = 0;
DesiredYpre = 0;
DesiredZpre = 0;

dDesiredPhi = 0;
dDesiredTheta = 0;
DesiredPhipre = 0;
DesiredThetapre = 0;



for i = 1:6000

   % torque input
   %&amp;&amp;&amp;&amp;&amp;&amp;&amp;&amp;&amp;&amp;&amp;&amp;( Ux )&amp;&amp;&amp;&amp;&amp;&amp;&amp;&amp;&amp;&amp;&amp;&amp;&amp;&amp;&amp;&amp;&amp;&amp;
   Kpx = 50; Kdx = 8; Kix = 0; 
   Ux = Kpx*( DesiredX(i) - x  ) + Kdx*( dDesiredX - dx ) + Kix*errorSumX;

   errorSumX = errorSumX + ( DesiredX(i) - x );

     dDesiredX = ( DesiredX(i) - DesiredXpre ) / dt;
   DesiredXpre = DesiredX(i);

   %&amp;&amp;&amp;&amp;&amp;&amp;&amp;&amp;&amp;&amp;&amp;&amp;( Uy )&amp;&amp;&amp;&amp;&amp;&amp;&amp;&amp;&amp;&amp;&amp;&amp;&amp;&amp;&amp;&amp;&amp;&amp;
   Kpy = 100; Kdy = 10; Kiy = 0; 
   Uy = Kpy*( DesiredY(i) - y  ) + Kdy*( dDesiredY - dy ) + Kiy*errorSumY;


   errorSumY = errorSumY + ( DesiredY(i) - y );

     dDesiredY = ( DesiredY(i) - DesiredYpre ) / dt;
   DesiredYpre = DesiredY(i);



   %&amp;&amp;&amp;&amp;&amp;&amp;&amp;&amp;&amp;&amp;&amp;&amp;( U1 )&amp;&amp;&amp;&amp;&amp;&amp;&amp;&amp;&amp;&amp;&amp;&amp;&amp;&amp;&amp;&amp;&amp;&amp;
   Kpz = 100; Kdz = 20; Kiz = 0; 
   U1 = Kpz*( DesiredZ(i) - z ) + Kdz*( dDesiredZ - dz ) + Kiz*errorSumZ; 

   errorSumZ = errorSumZ + ( DesiredZ(i) - z );

      dDesiredZ = ( DesiredZ(i) - DesiredZpre ) / dt;
   DesiredZpre = DesiredZ(i);

   %#######################################################################
   %#######################################################################
   %#######################################################################
   % Desired Phi and Theta
   R = [  sin(Psi),cos(Psi); 
         -cos(Psi),sin(Psi)];


   DAngles = R\( (m/U1)*[Ux; Uy]);


   %Wrap angles
   DesiredPhi   = wrapToPi( DAngles(1) ) /2;
   DesiredTheta = wrapToPi( DAngles(2) );


   %&amp;&amp;&amp;&amp;&amp;&amp;&amp;&amp;&amp;&amp;&amp;&amp;( U2 )&amp;&amp;&amp;&amp;&amp;&amp;&amp;&amp;&amp;&amp;&amp;&amp;&amp;&amp;&amp;&amp;&amp;&amp;
   KpP = 100; KdP = 10; KiP = 0;
   U2 = KpP*( DesiredPhi - Phi ) + KdP*( dDesiredPhi - dPhi )  + KiP*errorSumPhi;

   errorSumPhi = errorSumPhi + ( DesiredPhi - Phi );

     dDesiredPhi = ( DesiredPhi - DesiredPhipre ) / dt;
   DesiredPhipre = DesiredPhi;


   %--------------------------------------
   %&amp;&amp;&amp;&amp;&amp;&amp;&amp;&amp;&amp;&amp;&amp;&amp;( U3 )&amp;&amp;&amp;&amp;&amp;&amp;&amp;&amp;&amp;&amp;&amp;&amp;&amp;&amp;&amp;&amp;&amp;&amp;

   KpT = 100; KdT = 10; KiT = 0;
   U3 = KpT*( DesiredTheta - Theta ) + KdP*( dDesiredTheta - dTheta ) + KiT*errorSumTheta;

    errorSumTheta = errorSumTheta + ( DesiredTheta - Theta );

     dDesiredTheta = ( DesiredTheta - DesiredThetapre ) / dt;
   DesiredThetapre = DesiredTheta;



   %--------------------------------------
   %&amp;&amp;&amp;&amp;&amp;&amp;&amp;&amp;&amp;&amp;&amp;&amp;( U4 )&amp;&amp;&amp;&amp;&amp;&amp;&amp;&amp;&amp;&amp;&amp;&amp;&amp;&amp;&amp;&amp;&amp;&amp;
   KpS = 80; KdS = 20.0; KiS = 0.08;
   U4 = KpS*( 0 - Psi ) + KdS*( 0 - dPsi );


   %###################( ODE Equations of Quadrotor )###################
   %===================( X )=====================
   ddx = (U1/m)*( Theta*cos(Psi) + Phi*sin(Psi) );

    dx = dx + ddx*dt;
     x =  x +  dx*dt;
   %===================( Y )=====================
   ddy = (U1/m)*( Theta*sin(Psi) - Phi*cos(Psi) );

    dy = dy + ddy*dt;
     y =  y +  dy*dt;

   %===================( Z )=====================
   ddz = (U1/m) - g;

    dz = dz + ddz*dt;
     z =  z +  dz*dt;

   %===================( Phi )=====================
   ddPhi = ( L/Jx )*U2;

    dPhi = dPhi + ddPhi*dt;
     Phi =  Phi +  dPhi*dt;

   %===================( Theta )=====================
   ddTheta =  ( L/Jy )*U3;

    dTheta =  dTheta + ddTheta*dt;
     Theta =   Theta +  dTheta*dt;

   %===================( Psi )=====================
   ddPsi =  (1/Jz)*U4;

    dPsi = dPsi + ddPsi*dt;
     Psi =  Psi +  dPsi*dt;

   %store the erro
     ErrorX(i)   = ( x - DesiredX(i) );
     ErrorY(i)   = ( y - DesiredY(i) );
     ErrorZ(i)   = ( z - DesiredZ(i) );
%    ErrorPhi(i)   = ( Phi - pi/4 );
%    ErrorTheta(i) = ( Theta - pi/4 );
     ErrorPsi(i)   = ( Psi - 0 );


   X(i) = x;
   Y(i) = y;
   Z(i) = z;

   T(i) = t;

%    drawnow 
%    plot3(DesiredX, DesiredY, DesiredZ, 'r')
%    hold on
%    plot3(X, Y, Z, 'b')
   t = t + dt; 


end


Figure1 = figure(1);
set(Figure1,'defaulttextinterpreter','latex');
%set(Figure1,'units','normalized','outerposition',[0 0 1 1]);

subplot(2,2,1)
plot(T, ErrorX, 'LineWidth', 2)
title('Error in $x$-axis Position (m)')
xlabel('time (sec)')
ylabel('$x_{d}(t) - x(t)$', 'LineWidth', 2)

subplot(2,2,2)
plot(T, ErrorY, 'LineWidth', 2)
title('Error in $y$-axis Position (m)')
xlabel('time (sec)')
ylabel('$y_{d}(t) - y(t)$', 'LineWidth', 2)

subplot(2,2,3)
plot(T, ErrorZ, 'LineWidth', 2)
title('Error in $z$-axis Position (m)')
xlabel('time (sec)')
ylabel('$z_{d} - z(t)$', 'LineWidth', 2)


subplot(2,2,4)
plot(T, ErrorPsi, 'LineWidth', 2)
title('Error in $\psi$ (m)')
xlabel('time (sec)')
ylabel('$\psi_{d} - \psi(t)$','FontSize',12);
grid on 


Figure2 = figure(2);
set(Figure2,'units','normalized','outerposition',[0 0 1 1]);

figure(2)
plot3(X,Y,Z, 'b')
grid on

hold on 
plot3(DesiredX, DesiredY, DesiredZ, 'r')

pos = get(Figure2,'Position');
set(Figure2,'PaperPositionMode','Auto','PaperUnits','Inches','PaperSize',[pos(3),pos(4)]);
print(Figure2,'output2','-dpdf','-r0');

legend('actual', 'desired')
</code></pre>

<p>The code of the desired trajectory is </p>

<pre><code>clear all; clc;

fileID = fopen('xyTrajectory.txt','w');

 angle = -pi; radius = 5; z = 0; t = 0;

for i = 1:6000
    if ( z &lt; 2 ) 
        z = z + 0.1;
        x = 0; 
        y = 0;
    end
    if  ( z &gt;= 2 )
        angle = angle + 0.1;
        angle = wrapToPi(angle);
        x = radius * cos(angle);
        y = radius * sin(angle);
        z = 2;
    end

    X(i) = x;
    Y(i) = y;
    Z(i) = z;

    fprintf(fileID,'%f \t %f \t %f\n',x, y, z); end

fclose(fileID); plot3(X,Y,Z) grid on
</code></pre>
","quadcopter matlab microcontroller"
"7242","Underwater ROV Variable Ballast","<p>How would you guys recommend making a variable ballast system for an underwater robot? I was thinking about this problem earlier and I was trying to figure out if there was a way to make one that didn't require a tank of compressed air. </p>
","underwater"
"7243","Fast C++ library for stereo vision/disparity computation","<p>I am looking for a library for disparity map / stereo vision computation. These are my requirements:</p>

<ol>
<li>C++</li>
<li>Multi-platform (Linux, Windows, OSX)</li>
<li>(preferrable but not mandatory) not CUDA based</li>
<li>Suited for robotics (e.g. it should work even if the images are not perfectly rectified and the cameras are not perfectly calibrated)</li>
<li>Suitable for tracking purposes (20fps or more)</li>
<li>Performing even with low-res images (e.g. 320x240px)</li>
<li>Open Source</li>
</ol>
","computer-vision stereo-vision"
"7244","Conceptual problem regarding electronic shutters","<p>I have been looking at CCD and CMOS sensors and cameras to decide which one to use in the process of automatic control of a printing process. By now I am getting the grips on almost all the essential numbers and abbreviations but there remains a problem with shutters.</p>

<p>I understand that there are different types of shutters, both mechanical and electronic, and I can understand how they work. My problem concerns shutter speed. If I use a mechanical shutter, well then the maximum shutter speed depends on that particular element in the assembly, but how does it work for electronic shutters? I have never read ""Max shutter speed"" in any specs. The only thing I usually see floating around are frames per second. But those do usally not pass a limit of about 120 fps. Depending on how the sensor it is built one could think that the maximum shutter speed therefore is 1/120 or 1/240 if it uses half frames.</p>

<p>Can this be right? It seems really slow. I will be faced with the task of recording crisp and clear images of paper which moves at about 17 m/s. That is never possible with shutter speeds that slow. Will I be forced to use a mechanical shutter or am I misunderstanding something?</p>
","computer-vision cameras"
"7245","How many quadcopters would it take to lift a burrito?","<p>I am investigating a possible business opportunity in which quadcopters perform high-precision nutritional delivery via a burrito medium. I have never used a burrito, but I have read on the internet that they typically weigh 600-700 grams (1). This is much too heavy for commercially available platforms.</p>

<p>How many quadcopters would it take to lift a single burrito?</p>

<p>(1): <a href=""https://www.facebook.com/chipotle/posts/390817319252"" rel=""nofollow"">https://www.facebook.com/chipotle/posts/390817319252</a></p>
","quadcopter distributed-systems"
"7249","Quadcopter - is iPhone the ultimate flight controller?","<p>iPhone contains</p>

<ul>
<li>Gyroscope</li>
<li>GPS</li>
<li>Two photo and video cameras</li>
<li>Self-sufficient battery that outlives the motor battery</li>
<li>Wifi</li>
<li>Backup connectivity (cellular, bluetooth)</li>
<li>Programmable computer</li>
<li>Real-time image processing capabilities and face detection</li>
<li>General purpose IO (with something like <a href=""http://redpark.com/lightning-serial-cable-l2-db9v/"" rel=""nofollow"">this</a>)</li>
</ul>

<p>and old models are available very cheap.</p>

<p>What is the main benefit of having a separate dedicated flight controller and camera on hobbyist rotorcraft rather than a general purpose device like the iPhone?</p>
","quadcopter"
"7251","Is there a bug in the Encoder Counts packets 43&44?","<p>I think I have just found another bug - there was one that was mentioned in another post about the <a href=""https://robotics.stackexchange.com/questions/7062/create2-angle-packet-id-20"">angle</a> and <a href=""https://robotics.stackexchange.com/questions/6869/roomba-create-2-problem-reading-distance-traveled"">distance</a>. This one is about reading the encoder's counts. I was using them as a workaround for the other bugs but what I found in one instance is that the counts I was reading from the right encoder were incorrect. I was reading in a loop sleeping for 100msec while turning the create2. Here is part of the counts where it definitely shows a problem:</p>

<pre><code>32767
-32763
32766
-32768
</code></pre>

<p>This kept on going until I stopped. It seems that it has a problem when it reaches the max.</p>

<p>Has anyone else ran into this or can explain or provide another workaround? </p>
","irobot-create"
"7254","Cool robotics projects","<p>I'm a robotics student but very new to this field.</p>

<p>Can you suggest any websites which provide projects/helpful info that I can learn from?</p>

<p>Thanks</p>
","mobile-robot"
"7256","Sensing the level of a liquid in a tube","<p>I'm looking to build a sensor which will detect the level of liquid in a tube.</p>

<p>It does not have to be precisely accurate, just detect whether the level is approximately above a certain height.</p>

<p><img src=""http://i.stack.imgur.com/4USD9.jpg"" alt=""tube""></p>

<p>The liquid level can be seen in the red oval</p>

<p><img src=""http://i.stack.imgur.com/VwvhD.jpg"" alt=""level""></p>

<p>I thought about monitoring this with a webcam and using opencv to detect the liquid level. but this seemed a bit overkill. Especially if I have to have a dedicated PC to process the images.</p>

<p>Surely there's a simpler solution.</p>

<p>Perhaps a component I can attach to a raspberry PI or arduino board ... </p>

<p>I'm not very familiar with laser sensors so I don't know what is suitable. </p>

<p>As long as it's reliable ...</p>

<p><strong>EDIT</strong></p>

<p>I should add that the tube contains toluene which is flammable, and it is vacuum sealed. So we can't just drill into it. Some kind of optical/laser sensor might be  OK, as long as it can recognise a clear liquid. </p>
","arduino sensors raspberry-pi laser"
"7265","PID over another module that implements a PID control?","<p>I'm in charge of a module to control the smoothness with which a platform should move; the platform already implements a closed-loop control on its own but this firmware is closed and I do not have access to source code.
It is therefore requested that a closed loop control should implemented on top of that PID, in a superior layer, above a module that already implements a closed loop control, so i have several question:</p>

<ol>
<li>It's conceptually correct implement a PID control in an upper layer to closed loop control that implements it?</li>
<li>What features may be loose in the lower close loop?</li>
<li>Maybe loop control closed negatively be influenced by the PID that implements the top layer?</li>
<li>Estimate The angular speed, yaw and pitch, based on the position of the motors using Kalman filters can generate values too far from the actual values reported</li>
</ol>
","pid"
"7266","How can I make a quadcopter avoid obstacles using infrared?","<p>I have a quadcopter built, and I need to be able to make it to autonomously follow a route and avoid obstacles where possible.</p>

<p>My general plan is to have an array of sensors on a pre-defined ""front"". The quadcopter will only go forward. Generally I'd like to make  it so that if the sensors pointing at a higher angle detect something getting closer as the bot moves forward, the quadcopter will stop, descend until the distance to that detected object decreases, and then continues forward. Similarly, I'd like the opposite event to happen if the sensors pointing at a lower angle detect something getting closer to the quadcopter.</p>

<p>I'm thinking of having something like 9 small infrared distance detectors (pointing up, forward, down || left, forward, right), basically a 3x3 matrix.</p>

<p>Would anyone have any ideas of the feasibility of this? I'd like to use a raspberry pi, but it will probably also need an additional board to read in the values from its sensors. In addition, I have no idea which sensors to use, or if infrared can even work. Any suggestions are more than welcome.</p>

<p>I was also thinking about ultrasonic sensors, but having 9 of them could get cluttered, and I'd worry about their short range when a crash means death for the quadcopter. I also fear they would cause interference with each other.</p>
","mobile-robot quadcopter sensors"
"7270","What is load current and load speed? Which battery is best suitable for this motor?","<p>These are the specifications of the motor:</p>

<blockquote>
  <p>25000RPM no load speed at 12V</p>
  
  <p>No Load Current - 1A, Stall Current - 10A</p>
  
  <p>0.36Kgcm torque</p>
</blockquote>

<p>What is the definition of load current and load speed? Which battery would be most suitable to power this motor?</p>
","motor"
"7271","How would I implement a following drone with a camera using GPS?","<p>As the title states, is there any way to make a following drone that tracks a GPS unit, and follows/orients camera to that? Similar to <a href=""https://www.lily.camera/"" rel=""nofollow"">this</a></p>
","cameras gps line-following"
"7277","Assuming I have the angle with respect to two beacons, and know the distance between them, can I localize myself?","<p>Let's assume I have the following situation, and need to find (x,y). </p>

<p><img src=""http://i.stack.imgur.com/Iql5h.jpg"" alt=""beacony triangle""></p>

<p>Is it possible? There does not appear to be more than one solution to the system, but my trigonometry is a bit rusty.</p>

<p>I feel like I need one more distance.</p>
","kinematics inverse-kinematics"
"7278","Adding an Actuator or Force to a (Featherstone) Articulated Rigid Body Model","<p>I'm working on a project where I need to model a system that is essentially comprised of a series of ball-and-socket joints attached to a base, which is attached in turn to a prismatic joint (rail). </p>

<p>I've read Roy Featherstone's <em>Rigid Body Dynamics Algorithms</em> cover-to-cover, and I've also read the <em>Dynamics</em> section from the <em>Springer Handbook of Robotics</em> (also written by Featherstone). </p>

<p>It took me a long time to get acclimated to using his ""spatial vector"" and ""spatial matrix"" notation, but after re-creating all of his notation by hand as an exercise it works out to just be a nice way of concatenating 3x3 and 3x1 matrices and vectors into 6x6 and 6x1 matrices and vectors. The maths he invents to perform operations can be a bit tedious to read as he hijacks some standard notation, but overall everything is very compact, very easy to implement in MATLAB. </p>

<p><strong>My problem is this:</strong> How do I add actuators to the model? He walks through explicitly configuring the joint definitions, link definitions, etc., but when it comes to actuators or applied forces he says something like, ""Just add a $\tau_a$ here and Bob's your uncle!"" - it's not discussed at all. In the <em>Handbook of Robotics</em> he suggests introducing a false acceleration to the fixed base to add the gravitational force term, but doesn't show how to add it in local coordinates nor does he mention how to add the actuator input. </p>

<p>Any help would be greatly appreciated. I've considered starting over with a different book, but it's going to be a great expense of my time to re-acclimate myself to a different set of notation. I'd like to move forward with this, but I feel like I'm just a few inches shy of the finish line. </p>
","actuator dynamics joint"
"7287","Odometry vs dead-reckoning","<p>In terms of robotics, what are the differences between odometry and dead-reckoning? I read that odometry uses wheel sensors to estimate position, and dead-reckoning also uses wheel sensors, but ""heading sensors"" as well. Can someone please elaborate on this point for me?</p>

<p>Thanks</p>
","odometry deduced-reckoning"
"7288","Linear Motion Control for quadrotor (clarification)","<p>I've posted a <a href=""http://robotics.stackexchange.com/questions/7235/how-to-implement-and-code-inner-and-outer-pd-controllers-for-quadrotor-for-posit"">question</a> regarding this matter that I couldn't solve. I'm reading this <a href=""ftp://213.176.96.142/ieee8966173d-c305-20140930104826.pdf"" rel=""nofollow"">paper</a>, the authors state </p>

<blockquote>
  <p>Linear $x$ and $y$ Motion Control: From the mathematical model one can see
  that the motion through the axes $x$ and $y$ depends on $U_{1}$. In fact $U_{1}$ is
  the total thrust vector oriented to obtain the desired linear motion.
  If we consider $U_{x}$ and $U_{y}$ the orientations of $U_{1}$ responsible for the
  motion through x and y axis respectively, we can then extract from
  formula (18) the roll and pitch angles necessary to compute the
  controls $U_{x}$ and $U_{y}$ ensuring the Lyapunov function to be negative
  semi-definite ( see Fig. 2).</p>
</blockquote>

<p>The paper is very clear except in the linear motion control. They didn't explicitly state the equations for extracting the angles. The confusing part is when they say </p>

<blockquote>
  <p>we can then extract from
  formula (18) the roll and pitch angles necessary to compute the
  controls $U_{x}$ and $U_{y}$</p>
</blockquote>

<p>where formula (18) is</p>

<p>$$
U_{x} = \frac{m}{U_{1}} (\cos\phi \sin\theta \cos\psi + \sin\phi \sin\psi) \\
U_{y} = \frac{m}{U_{1}} (\cos\phi \sin\theta \sin\psi - \cos\phi \cos\psi) \\
$$</p>

<p>It seems to me that the roll and pitch angles depend on $U_{x}$ and $U_{y}$, therefore we compute the roll and pitch angles based on the $U_{x}$ and $U_{y}$ to control the linear motion. </p>
","control quadcopter"
"7291","Stereo vision in Matlab","<p>I am working on a project about robot soccer vision. </p>

<p>How I utilize two webcams as a stereo vision in matlab for robot soccer matters?</p>
","stereo-vision matlab soccer"
"7294","Dispensing precise quantities of liquid and powder","<p>I've been toying around with the idea of automating the process of testing aquarium water for certain chemicals. Very briefly, salt water aquariums (reefs, specifically) require almost-daily testing for 3-4 chemicals (calcium, alkalinity, ammonia, phosphate). This is typically done by hand, using <a href=""http://www.bulkreefsupply.com/aquarium-monitors-controllers/testing/salifert.html"" rel=""nofollow"">various kits</a>. There are two main types </p>

<ul>
<li>you combine several powders with a fixed amount of aquarium water, and then compare the color the mixture turns with a chart</li>
<li>you combine several liquids together with the aquarium water, and then add another liquid until the mixture turns a color. you then record how much of the final liquid you had to add for the color change to occur (titration).</li>
</ul>

<p>Both methods are straightforward, but tedious. To maintain an aquarium well, you really do need daily readings of all of those metrics, which easily adds up to 30 minutes+ daily.</p>

<p>So - I'd like to be able to automate the process. The biggest question is, how do I reliably dispense the materials needed? We're talking in gram and milliliter UoM here. The kits come with plastic syringes and spoons of correct volume for the powders. I need a way to measure out and dispense both of these, and a way to queue up several days worth (refilling daily defeats the purpose).</p>

<p>Any ideas?</p>

<p><em>Edit</em> this is different from <a href=""http://robotics.stackexchange.com/questions/1259"">How to measure and dispense a finite amount of powder or liquid</a> because of the units of measure involved. I need to be able to reliably dispense ~ 1g +/- 5% of a powder, or 1ml +/- 5% of liquid.</p>
","electronics"
"7295","Help with ultrasonic sensors on obstacles avoiding robot","<p>Well, I will start directly in my problem. I'm working on a project and I only have 10 days left.
The idea is simple, a wheeled robot with 3 ultrasonic sensors to avoid obstacles.
I've developed a code and it's working fine.
I'm using: Arduino Uno, L293D driver for the 2 dc motors, 3 HC-SR04 ultrasonic sensors and the Newping library.</p>

<p>I've made some kind of a shield where I soldered common points for gnd and 5V in order to connect the L293 ic and the sensors pins easily. The problem is that the ultrasonic sensors only functioned once in the expected behavior! After that they were always sending the zero result and sometimes a number is showed when i disconnect the sensor! 
Is it a power problem? I'm using the usb cable to power the arduino and the sensors (motors are powered using 2 Li-po batteries)
kindly provide me with guidance</p>
","arduino power ultrasonic-sensors"
"7297","Arm to disassemble and assemble notebook at home?","<p>Suppose I have perfect AI to control robotic arm. </p>

<p>What characteristics should it fulfill to be able to take such common tools as screwdriver and linesman's and disassemble and then assemble conventional notebook computer?</p>

<p>Are there such models available?</p>

<p>Is seems to me, that such arms as OWI-535 are only toys, i.e. they can just relocate lightweight objects and that's all. Am I right?</p>

<p><strong>UPDATE</strong></p>

<p>Also suppose that my AI can look at assembly area with multiple HD cameras and can perfectly ""understand"" what is sees.</p>
","robotic-arm"
"7308","How to control a dc motor","<p>I have 2 of these 12v motors and a 12v battery
<a href=""http://www.enigmaindustries.com/Motors/Bosch_EV_Warrior.htm"" rel=""nofollow"">http://www.enigmaindustries.com/Motors/Bosch_EV_Warrior.htm</a></p>

<p>I would like to know what the best solution for controlling this motor with an Arduino Uno would be.
Does the motor controller need to have a maximum current of 100A?</p>

<p>I though of a 100A transistor connected to the pwm pin of arduino, and then, control the motor with pwm.</p>

<p>Is voltage regulator better than pwm?</p>
","motor esc pwm microcontroller"
"7311","How can i make a compact soft robot","<p>I want to make a compact (actuators motors and sensors are all in one) soft robot. Actuators can be pneumatic or dielectric. I need suggestions about manufacturating. I'm open to new ideas.</p>
","actuator manufacturing"
"7315","2Nm small motor","<p>Currently designing a spherical wrist, I want to manipulate a 300gr payload.
the design has a 200mm span, so I'm guessing at a 1.1Nm (considering the weight of structure &amp; motors).
<img src=""http://i.stack.imgur.com/WTicB.png"" alt=""spherical wrist""></p>

<p>I've looked at Maxon, Faulhaber, but can't find any motor+gearbox+encoder under a 100gr.
Any suggestion ?</p>
","motor actuator torque"
"7316","How can I make a motion tracking camera?","<p>I'm looking into CCTV, and am interested in minimising costs by having a motion tracking camera cover an area that would otherwise utilise multiple cameras.</p>

<p>There is already something like this on the market manufactured by a company called NightWatcher (I think).  However, it does not track, it merely senses using 3 PIR's and points the camera in 1 of 3 positions. Ping ponging between them if the subject is between sensors.</p>

<p>I like the idea of this, but not the drawbacks, and was wondering if there was anything I could do with an arduino or similar to achieve a better result.</p>

<p>I stumbled across this, but am not entirely sure about it. Also this is for outside application, and that thread is for indoor (if that makes a difference).</p>

<p><a href=""http://robotics.stackexchange.com/a/1397/9751"">http://robotics.stackexchange.com/a/1397/9751</a></p>

<p>Edit...</p>

<p>Just in case I have mislead you, I want to have a unit where sensors detect movement and then a camera to face that position.</p>
","cameras"
"7317","Vacuum Lifter: Moving Playing Cards","<p>I'd like to buy a small Vacuum Lifter so that I can move playing cards around with robotics. But my ""google-fu"" is failing me. I don't really know what search terms to look for... or what webpages to look to find this kind of component.</p>

<p>In essence, I want an electronic version of a <a href=""http://rads.stackoverflow.com/amzn/click/B001U35OJI"" rel=""nofollow"">Vacuum Pen</a>.</p>

<p>I don't really know where to search for this kind of component. I've found pneumatic valves and other complicated machinery... but ideally I'd want a self-contained electronic vacuum pen. Since I'm only planning to move playing cards around.</p>

<p>Anyone have an idea where to look for something like this? Thanks.</p>
","robotic-arm actuator"
"7320","Position estimation from photo fingerprinting","<p>i want to make 3d position and position change estimation from photos taken from flying robot. I need suggestions for fast photo matching. </p>
","localization"
"7327","How to transfer signed integers with libusb?","<p>Folks at programmers stack exchange asked me ask here:</p>

<p>I want to communicate with an arduino and sent integers it. I code this program in C++. I initialy used bulk_transfer(), but it sends only char data.</p>

<p>This in the API reference for libusb:
<a href=""http://libusb.org/static/api-1.0/group__syncio.html"" rel=""nofollow"">http://libusb.org/static/api-1.0/group__syncio.html</a></p>

<p>Here is the prototype of bulk_transfer()</p>

<p>int     libusb_bulk_transfer (struct libusb_device_handle *dev_handle, unsigned char endpoint, unsigned char *data, int length, int *transferred, unsigned int timeout)</p>

<p>As you can see, <em>data</em> is an unsigned char pointer, that is, a pointer to a buffer containing <em>length</em> unsigned chars. I can successfully transcieve strings. How do I transfer integers with sign?</p>

<p>Currently I am thinking about a system in which the arduino asks for the digit by sending a character and my program sends the number as reply followed by the sign, which is requested next. Is this solution viable? Or should I transfer the integer as a string? Is there a better way?</p>
","arduino communication usb c++"
"7331","position control for linear model of quadrotor (problem with tracking task)","<p>Lately, if you notice I have posted some questions regarding position tracking for nonlinear model. I couldn't do it. I've switched to linear model, hope I can do it. For regulation problem, the position control seems working but once I switch to tracking, the system starts oscillating. I don't know why. I have stated what I've done below hope someone guides me to the correct path. </p>

<p>The linear model of the quadrotor is provided <a href=""http://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&amp;arnumber=6915128"" rel=""nofollow"">here</a> which is </p>

<p>$$
\begin{align}
\ddot{x}    &amp;= g \theta \ \ \ \ \ \ \ \ \ \ (1)\\
\ddot{y}    &amp;= - g \phi \ \ \ \ \ \ \ \ \ \ (2)\\
\ddot{z}    &amp;= \frac{U_{1}}{m} - g \\
\ddot{\phi} &amp;= \frac{L}{J_{x}} U_{2} \\
\ddot{\theta} &amp;= \frac{L}{J_{y}} U_{2} \\
\ddot{\psi} &amp;= \frac{1}{J_{z}} U_{2} \\
\end{align}
$$</p>

<p>In this <a href=""http://link.springer.com/article/10.1007%2Fs10846-012-9789-z"" rel=""nofollow"">paper</a>, the position control based on PD is provided. In the aforementioned paper, from (1) and (2) the desired angles $\phi^{d}$ and $\theta^{d}$ are obtained, therefore, </p>

<p>$$
\begin{align}
\theta^{d}  &amp;= \frac{\ddot{x}^{d}}{g}  \\
\phi^{d}    &amp;= - \frac{\ddot{y}^{d}}{g}
\end{align}
$$</p>

<p>where </p>

<p>$$
\begin{align}
\ddot{x}^{d} &amp;= Kp(x^{d} - x) + Kd( \dot{x}^{d} - \dot{x} ) \\
\ddot{y}^{d} &amp;= Kp(y^{d} - y) + Kd( \dot{y}^{d} - \dot{y} ) \\
U_{1} &amp;= Kp(z^{d} - z) + Kd( \dot{z}^{d} - \dot{z} ) \\
U_{2} &amp;= Kp(\phi^{d} - \phi) + Kd( \dot{\phi}^{d} - \dot{\phi} ) \\
U_{3} &amp;= Kp(\theta^{d} - \theta) + Kd( \dot{\theta}^{d} - \dot{\theta} ) \\
U_{4} &amp;= Kp(\psi^{d} - \psi) + Kd( \dot{\psi}^{d} - \dot{\psi} ) \\
\end{align}
$$</p>

<p>with regulation problem where $x^{d} = 2.5 m, \ y^{d} = 3.5 m$ and $z^{d} = 4.5 m$, the results are </p>

<p><img src=""http://i.stack.imgur.com/qBlCa.png"" alt=""enter image description here"">
<img src=""http://i.stack.imgur.com/65V5T.png"" alt=""enter image description here""></p>

<p>Now if I change the problem to the tracking one, the results are messed up. </p>

<p><img src=""http://i.stack.imgur.com/bu5an.png"" alt=""enter image description here"">
<img src=""http://i.stack.imgur.com/kaSZr.png"" alt=""enter image description here""></p>

<p>In the last paper, they state </p>

<blockquote>
  <p>A saturation function is needed to ensure that the reference roll and
  pitch angles are within specified limits</p>
</blockquote>

<p><img src=""http://i.stack.imgur.com/ZdUZJ.png"" alt=""enter image description here""></p>

<p>Unfortunately, the max value for $\phi$ and $\theta$ are not stated in the paper but since they use Euler angles, I believe $\phi$ in this range $(-\frac{\pi}{2},\frac{\pi}{2})$ and $\theta$ in this range $[-\pi, \pi]$
I'm using Euler method as an ODE solver because the step size is fixed. For the derivative, Euler method is used. </p>

<p>This is my code </p>

<pre><code>%######################( PD Controller &amp; Atittude )%%%%%%%%%%%%%%%%%%%%

clear all;
clc;

dt = 0.001;
 t = 0;

% initial values of the system
 x = 0;
dx = 0;
 y = 0;
dy = 0;
 z = 0;
dz = 0;

   Phi = 0;
  dPhi = 0;
 Theta = 0;
dTheta = 0;
   Psi = pi/3;
  dPsi = 0;


%System Parameters:
m = 0.75;      % mass (Kg)
L = 0.25;      % arm length (m)
Jx = 0.019688; % inertia seen at the rotation axis. (Kg.m^2)
Jy = 0.019688; % inertia seen at the rotation axis. (Kg.m^2)
Jz = 0.039380; % inertia seen at the rotation axis. (Kg.m^2)
g  = 9.81;      % acceleration due to gravity m/s^2

errorSumX = 0;
errorSumY = 0;
errorSumZ = 0;

errorSumPhi   = 0;
errorSumTheta = 0;

pose = load('xyTrajectory.txt');

% Set desired position for tracking task
DesiredX = pose(:,1);
DesiredY = pose(:,2);
DesiredZ = pose(:,3);

% Set desired position for regulation task
% DesiredX(:,1) = 2.5;
% DesiredY(:,1) = 5;
% DesiredZ(:,1) = 7.2;


dDesiredX = 0;
dDesiredY = 0;
dDesiredZ = 0;

DesiredXpre = 0;
DesiredYpre = 0;
DesiredZpre = 0;

dDesiredPhi = 0;
dDesiredTheta = 0;
DesiredPhipre = 0;
DesiredThetapre = 0;



for i = 1:6000

   % torque input
   %&amp;&amp;&amp;&amp;&amp;&amp;&amp;&amp;&amp;&amp;&amp;&amp;( Ux )&amp;&amp;&amp;&amp;&amp;&amp;&amp;&amp;&amp;&amp;&amp;&amp;&amp;&amp;&amp;&amp;&amp;&amp;
   Kpx = 90; Kdx = 25; Kix = 0.0001; 


   errorSumX = errorSumX + ( DesiredX(i) - x );

   % Euler Method Derivative
     dDesiredX = ( DesiredX(i) - DesiredXpre ) / dt;
   DesiredXpre = DesiredX(i);


   Ux = Kpx*( DesiredX(i) - x  ) + Kdx*( dDesiredX - dx ) + Kix*errorSumX;
   %&amp;&amp;&amp;&amp;&amp;&amp;&amp;&amp;&amp;&amp;&amp;&amp;( Uy )&amp;&amp;&amp;&amp;&amp;&amp;&amp;&amp;&amp;&amp;&amp;&amp;&amp;&amp;&amp;&amp;&amp;&amp;
   Kpy = 90; Kdy = 25; Kiy = 0.0001; 


   errorSumY = errorSumY + ( DesiredY(i) - y );

   % Euler Method Derivative
   dDesiredY = ( DesiredY(i) - DesiredYpre ) / dt;
   DesiredYpre = DesiredY(i);


   Uy = Kpy*( DesiredY(i) - y  ) + Kdy*( dDesiredY - dy ) + Kiy*errorSumY;
   %&amp;&amp;&amp;&amp;&amp;&amp;&amp;&amp;&amp;&amp;&amp;&amp;( U1 )&amp;&amp;&amp;&amp;&amp;&amp;&amp;&amp;&amp;&amp;&amp;&amp;&amp;&amp;&amp;&amp;&amp;&amp;
   Kpz = 90; Kdz = 25; Kiz = 0; 


   errorSumZ = errorSumZ + ( DesiredZ(i) - z );

      dDesiredZ = ( DesiredZ(i) - DesiredZpre ) / dt;
   DesiredZpre = DesiredZ(i);

   U1 = Kpz*( DesiredZ(i) - z ) + Kdz*( dDesiredZ - dz ) + Kiz*errorSumZ;
   %#######################################################################
   %#######################################################################
   %#######################################################################
   % Desired Phi and Theta


   %disp('before')
   DesiredPhi   = -Uy/g;
   DesiredTheta =  Ux/g;



   %&amp;&amp;&amp;&amp;&amp;&amp;&amp;&amp;&amp;&amp;&amp;&amp;( U2 )&amp;&amp;&amp;&amp;&amp;&amp;&amp;&amp;&amp;&amp;&amp;&amp;&amp;&amp;&amp;&amp;&amp;&amp;
   KpP = 20; KdP = 5; KiP = 0.001;


   errorSumPhi = errorSumPhi + ( DesiredPhi - Phi );


   % Euler Method Derivative
      dDesiredPhi = ( DesiredPhi - DesiredPhipre ) / dt;
   DesiredPhipre  = DesiredPhi;


   U2 = KpP*( DesiredPhi - Phi ) + KdP*( dDesiredPhi - dPhi )  + KiP*errorSumPhi;

   %--------------------------------------
   %&amp;&amp;&amp;&amp;&amp;&amp;&amp;&amp;&amp;&amp;&amp;&amp;( U3 )&amp;&amp;&amp;&amp;&amp;&amp;&amp;&amp;&amp;&amp;&amp;&amp;&amp;&amp;&amp;&amp;&amp;&amp;

   KpT = 90; KdT = 10; KiT = 0.001;
    errorSumTheta = errorSumTheta + ( DesiredTheta - Theta );

   % Euler Method Derivative
      dDesiredTheta = ( DesiredTheta - DesiredThetapre ) / dt;
   DesiredThetapre = DesiredTheta;


   U3 = KpT*( DesiredTheta - Theta ) + KdP*( dDesiredTheta - dTheta ) + KiT*errorSumTheta;
   %--------------------------------------
   %&amp;&amp;&amp;&amp;&amp;&amp;&amp;&amp;&amp;&amp;&amp;&amp;( U4 )&amp;&amp;&amp;&amp;&amp;&amp;&amp;&amp;&amp;&amp;&amp;&amp;&amp;&amp;&amp;&amp;&amp;&amp;
   KpS = 90; KdS = 10; KiS = 0; DesiredPsi = 0; dDesiredPsi = 0;
   U4 = KpS*( DesiredPsi - Psi ) + KdS*( dDesiredPsi - dPsi );


   %###################( ODE Equations of Quadrotor )###################
   ddx = g * Theta;
    dx = dx + ddx*dt;
     x =  x +  dx*dt;
   %=======================================================================  
   ddy = -g * Phi;
    dy = dy + ddy*dt;
     y =  y +  dy*dt;
   %=======================================================================
   ddz = (U1/m) - g;
    dz = dz + ddz*dt;
     z =  z +  dz*dt;
   %=======================================================================  
   ddPhi = ( L/Jx )*U2;
    dPhi = dPhi + ddPhi*dt;
     Phi =  Phi +  dPhi*dt;
   %=======================================================================  
   ddTheta =  ( L/Jy )*U3;
    dTheta =  dTheta + ddTheta*dt;
     Theta =   Theta +  dTheta*dt;
   %=======================================================================  
   ddPsi =  (1/Jz)*U4; 
    dPsi = dPsi + ddPsi*dt;
     Psi =  Psi +  dPsi*dt;
   %=======================================================================  
   %store the erro
   ErrorX(i)   = ( x - DesiredX(i) );
   ErrorY(i)   = ( y - DesiredY(i) );
   ErrorZ(i)   = ( z - DesiredZ(i) );
   ErrorPsi(i)   = ( Psi - 0 );


   X(i) = x;
   Y(i) = y;
   Z(i) = z;

   T(i) = t;

   t = t + dt; 


end


Figure1 = figure(1);
set(Figure1,'defaulttextinterpreter','latex');


subplot(2,2,1)
plot(T, ErrorX, 'LineWidth', 2)
title('Error in $x$-axis Position (m)')
xlabel('time (sec)')
ylabel('$x_{d}(t) - x(t)$', 'LineWidth', 2)

subplot(2,2,2)
plot(T, ErrorY, 'LineWidth', 2)
title('Error in $y$-axis Position (m)')
xlabel('time (sec)')
ylabel('$y_{d}(t) - y(t)$', 'LineWidth', 2)

subplot(2,2,3)
plot(T, ErrorZ, 'LineWidth', 2)
title('Error in $z$-axis Position (m)')
xlabel('time (sec)')
ylabel('$z_{d} - z(t)$', 'LineWidth', 2)


subplot(2,2,4)
plot(T, ErrorPsi, 'LineWidth', 2)
title('Error in $\psi$ (m)')
xlabel('time (sec)')
ylabel('$\psi_{d} - \psi(t)$','FontSize',12);
grid on 


Figure2 = figure(2);
set(Figure2,'units','normalized','outerposition',[0 0 1 1]);

figure(2)
plot3(X,Y,Z, 'b')
grid on

hold on 
plot3(DesiredX, DesiredY, DesiredZ, 'r')

pos = get(Figure2,'Position');
set(Figure2,'PaperPositionMode','Auto','PaperUnits','Inches','PaperSize',[pos(3),pos(4)]);
print(Figure2,'output2','-dpdf','-r0');
</code></pre>

<p>For the trajectory code</p>

<pre><code>clear all;
clc;

fileID = fopen('xyTrajectory.txt','w');

 angle = -pi;
radius = 3;
z = 0;
t = 0;

for i = 1:6000
    if ( z &lt; 2 ) 
        z = z + 0.1;
        x = 0; 
        y = 0;
    end
    if  ( z &gt;= 2 )
        angle = angle + 0.1;
        angle = wrapToPi(angle);
        x = radius * cos(angle);
        y = radius * sin(angle);
        z = 2;
    end

    X(i) = x;
    Y(i) = y;
    Z(i) = z;

    fprintf(fileID,'%f \t %f \t %f\n',x, y, z);
end

fclose(fileID);
plot3(X,Y,Z)
grid on
</code></pre>
","control quadcopter matlab"
"7333","Why do my ESCs stop working?","<p>I'm new to robotics and this is my first time building a quadcopter.  I'm unable to work out why I keep losing ESCs.</p>

<p>Most recently in testing, I've managed to calibrate all 4 ESCs and accurately control the speed of all 4 motors.  But after neatly securing them to the frame, 1 motor didn't work.  I recalibrated the ESCs again and, when running them again, the motor still didn't work.  However, the other 3 motors continued to run at first, but also suddenly just stopped altogether.</p>

<p>Research suggested that ESCs have a cut-off voltage, indicating that my battery might be too flat, so I immediately looked to recharging it. To my surprise, the (still very new) battery appeared to have bulged out, indicating that it had been damaged.</p>

<p>Further research suggested that the size of the battery I was using is insufficient for the amount of current drawn by the motors.  So, without any PWM applied, I reconnected a new fully charged battery in the hope of listening for any beeps to diagnose, and one ESC immediately coughed up a huge puff of smoke.</p>

<p>Before all of this happened, I only managed to get 2 of the ESCs to run their motors.  Despite several attempts at tweaking PWM signals and calibrating them, I ended up replacing the other 2.</p>

<p>Unless there's some obvious reason for my ESCs to keep dying on me, I can only assume that these specific ESCs are badly made and I should ask for my money back.</p>

<p>These are the components I'm using:</p>

<ul>
<li><a href=""https://www.raspberrypi.org/products/raspberry-pi-2-model-b/"" rel=""nofollow"">Raspberry Pi 2 Model B</a></li>
<li><a href=""http://www.adafruit.com/product/815"" rel=""nofollow"">Adafruit 16-Channel 12-bit PWM/Servo Driver - I2C interface - PCA9685</a></li>
<li><a href=""http://rctimer.com/product-1324.html"" rel=""nofollow"">RCTimer Mini ESC 40A OPTO BLHeli Firmware (Oneshot125, Support 2-6S)</a></li>
<li><a href=""http://rctimer.com/product-112.html"" rel=""nofollow"">RCTimer 2208-8 2600kV Outrunner Brushless Motor</a></li>
<li><a href=""http://www.gensace.de/recommend-products/gens-ace-2200mah-11-1v-25c-3s1p-lipo-battery-pack.html"" rel=""nofollow"">Gens ace 2200mAh 11.1V 25C 3S1P Lipo Battery Pack</a></li>
</ul>

<p>The Raspberry Pi is powered through its micro USB interface by a <a href=""https://www.pololu.com/product/2564"" rel=""nofollow"">5V Step-Up Voltage Regulator</a> connected to a <a href=""https://www.robotics.org.za/index.php?route=product/product&amp;path=59_144&amp;product_id=1042"" rel=""nofollow"">5000mAh 3.7V LiPo battery</a>.  The PWM Controller is powered to its Vcc pin by the GPIO1 (3.3V) pin from the Raspberry Pi, that also happens to power other <a href=""https://www.pololu.com/product/2470"" rel=""nofollow"">sensors</a>.</p>

<p>At the time (when all 4 motors worked), I was able to accurately control them at either 50Hz or 400Hz with 1-2 millisecond duty cycles.</p>
","quadcopter power esc pwm"
"7334","Mechanical design for base of robotic arm","<p>I am using DC motors to build a robotic arm.  I want to make the base shoulder (which rotates and lifts) more stable and stronger.  How should I design this using DC motors?</p>

<p>Also I would like to put the motor for the elbow in the base for efficiency.  Which design best suits this? </p>

<p><strong>UPDATE</strong>
I am building a robotic arm for a payload of approx. 1-2 kg and using DC high torque motors. In this model, I am using only a shoulder with a gripper. The gripper is self made by me weighing approximately 400 grams. I want to have a proper design and material choice so that the shoulder part remains less heavy and more stable.</p>

<p>In addition to this I want to operate the movement of the gripper, i.e. the up and down motion, by using the motor in the base part. What should be my design and better alternative?</p>
","robotic-arm design"
"7339","Driving a non-circular timing belt","<p>I'd like to create a camera slider similar to <a href=""http://www.davidhunt.ie/motorised-time-lapse-rail-with-raspberry-pi/2/"" rel=""nofollow"">this one</a>.
<img src=""http://i.stack.imgur.com/9aq9s.jpg"" alt=""enter image description here""></p>

<p>The only part I'm not sure on is how to setup the camera drive.  It looks like I can buy a similar <a href=""https://www.servocity.com/html/xl_timing_belt___foot.html#.VWFNmWRVhHx"" rel=""nofollow"">timing belt here</a>, but I'm not sure how to set up the servo to drive the slider.  Particularly how to keep the belt in contact with the drive pulley. </p>

<p>My fabrication skills are very limited so I need a simple or out of the box solution.</p>
","servos"
"7344","Image based 3d position estimation with one camera","<p>There is too many 2d position estimation with one camera. Is there any 3d position estimation application or technique with one camera? If there is no application or technique why?</p>
","localization"
"7349","Arduino triggers a camera to start recording","<p>I've already made an Arduino device which detects the trigger event, but now I want it to trigger the recording and storage of video when this event occurs.  If the camera could be wirelessly triggered a few feet away from the Arduino unit, that would be optimal, but I can settle for running wires if need be.</p>

<p>I'm looking for suggestions because I'm on a limited budget for this project. I want to avoid reinventing the wheel and ordering parts which I can't get to work with an Arduino.</p>

<p>I'm considering the use of this camera.
<a href=""https://www.sparkfun.com/products/11418"" rel=""nofollow"">https://www.sparkfun.com/products/11418</a></p>

<p>This is my first Arduino project.  Any help is very welcome.</p>
","arduino wireless"
"7358","Suggestion for a camera","<p>Are there good low cost cameras that are frequently used in robotics?</p>

<p>I am assuming there are cameras that are good fit for robotics ...</p>

<ol>
<li>Works well with OpenCV</li>
<li>PC Windows support - USB2/USB3 (GigE, USB3 vision cameras seem pricey)</li>
<li>Good image sensing performance</li>
<li>Adjustable focus - manual or motorized (fine focus control would be great)</li>
</ol>

<p>Do IP cameras make good cameras for Robotic vision projects?</p>
","computer-vision cameras"
"7359","Interfacing GPU image processing with motor control at 30+Hz","<p>I would like to make a robotic system which takes as input a video feed, runs some GPU-based image recognition on the video, and outputs commands to a set of motors. The goal is to have the motors react to the video with as little latency as possible, hopefully of the order of 10s of ms. Currently I have a GTX 770m on a laptop running Ubuntu 14.04, which is connected to the camera and doing the heavy image processing. This takes frames at 30Hz and will output motor commands at the same frequency.</p>

<p>After a few days of looking around on the web for how to design such a system, I'm still at a loose end whether (a) it is even feasible (b) if so, what the best approach is to interface the laptop with the motors? The image processing must run on Linux, so there is no leeway to change that part of things.</p>
","control real-time"
"7361","idea for web application in robotics","<p>I am learning and I am interested robotics, but also I need to update my web development skills so the question is - is there any idea for good web application that could be connected with robotics - service robots, industrial robots etc. Maybe there already is some open source ongoing web application projects for robotics in which I can make contribution.</p>

<p>Thanks!</p>
","design software"
"7363","What is the difference between Multiple robots and swarm robots?","<p>What is the difference between Multiple robots and swarm robots? What is the key point? Also what is multi agent systems? Do multi agent systems works only for computer simulations or games? These terms are used similar applications.</p>
","simulation multi-agent swarm"
"7378","Electric piston (longitudinal electric motor)?","<p>Are there electric motors, which apply force not in rotational motion, but in longitudinal motion?</p>

<p>They should have electromagnetic design and no gears and worms.</p>

<p>Such motors would be great for linear actuators. They could transfer both force and feedback.</p>

<p>What is the name of such devices?</p>
","motor actuator"
"7383","Determine the configuration space for a robotic arm","<p>I'm working with a 4DOF Parallel-Mechanism arm. I'm interested in writing planners for this arm (PRM or RRT) in the configuration space, but I'm not sure how to identify obstacles/collisions. </p>

<p>When writing planners for mobile robots in a 2d workspace, it was easy to define and visualize the workspace and obstacles in which the planner/robot was operating. This website (<a href=""http://www.cs.unc.edu/~jeffi/c-space/robot.xhtml"" rel=""nofollow"">link</a>) shows a great example of visualizing the workspace and configuration space for a 2DOF arm, but how can I do this for higher dimensions?</p>
","robotic-arm motion-planning"
"7384","How can a memory alloy be used as an alternative to a compressor found in a refrigerator?","<p>I'm curious about <a href=""http://www.bbc.com/news/science-environment-32886000"" rel=""nofollow"">this alloy</a> and how they say it can be used as an alternative to a traditional compressor. Can anyone explain how this would work?</p>

<p>My goal is to understand that use case so I can adapt alloys in other robotic projects.  My gut tells me this is perfect for some kinematics, or other mechanisms, but I'm missing some pieces in this puzzle (how would it work?)</p>
","kinematics mechanism"
"7385","3D scanner from Phone Camera","<p><a href=""http://www.123dapp.com/catch"" rel=""nofollow"">123D software</a> can construct a 3D model from photos taken from your phone. It doesn't process the photos in your phone.  Instead, it sends them to the cloud to create 3d model. How can i construct a 3d model like this <strong>(only with one camera)</strong>? I searched it but i can only find information on laser/procetor scanners (simple and desktop use only). I think 123D uses only IMU sensors and camera <strong>why do they use the cloud</strong>?  Can a beaglebone or rasperry pi create 3d models like this?</p>
","computer-vision 3d-printing 3d-reconstruction 3d-model"
"7386","Introduction to Robotics Mechanics & Control, John J Craig., 3rd Ed., Forward transformation problem Examples 2.2 and 2.4","<p>I am reading the book ""Introduction to Robotics Mechanics &amp; Control"", John J Craig., 3rd Ed., Forward transformation problem Examples 2.2 and 2.4.</p>

<p>Ex. 2.2 (Page 29): Frame {B} is rotated relative to frame {A} about X axis by 60 degrees clockwise, translated 20 units along Y axis and 15 units along z axis. Find P in frame {A} where P in frame {b} = [0 8 7]</p>

<p>The book's answer is [0.18.062 3.572].
But my answer is [0 30.062 11.572].</p>

<p>Ex. 2.4 (Page 33): Vector P1 has to be rotated by 60 degrees clockwise, about X axis and translated 20 units along Y axis, and 15 units along Z axis. If P1 is given by [0 8 7], find P2.</p>

<p>Essentially Ex.2.2 and 2.4 are the same problem. However, the Transformation matrix for Ex 2.4, has [0 8 7] as translation vector (The 4th column of T) instead of [0 20 15]. And, the given answer is [0.18.062 3.572].</p>

<p>I am not sure if it is just typo, or I am missing some genuine operation. Please let me know your opinion.</p>

<p>Thanks.</p>
","forward-kinematics books"
"7387","Hector SLAM, Matching algorithm","<p>I'm trying to understand the scan-matching part of <a href=""http://www.sim.informatik.tu-darmstadt.de/publ/download/2011_SSRR_KohlbrecherMeyerStrykKlingauf_Flexible_SLAM_System.pdf"" rel=""nofollow"">Hector SLAM</a> (<a href=""http://www.sim.informatik.tu-darmstadt.de/~kohlbrecher/hector_overview/ROS-Workshop%20Darmstadt%202011.pdf"" rel=""nofollow"">PPT summary</a>). It seems a little difficult to understand, in some cases, how is it possible to actually perform the alignment of the scans. <em>Can anyone explain about it?</em></p>

<p>In my case, I'm working with a simulation. I'm moving my robot in a corridor-like featureless environment (only two walls) and I don't get a map. Nevertheless, if I move in a sinewave motion, I'm able to get a map. Moreover, if I have an additional feature, the algorithm even shows the real path as long as this feature is seen (right part of the image), otherwise it shows a very weird-looking oscillatory path which does not resemble a sinewave at all. Something important to notice is that the width of the map is pretty accurate (real=4m, map's=4.014m), and the length of the movement is also somehow accurate (real=15m, map's= 15.47). I'm using a Hokuyo URG-04LX laser range finder, no odometry, no IMU. I'm running in Ubuntu 14.04 and using ROS Indigo.</p>

<p><img src=""http://i.stack.imgur.com/NhCbG.png"" alt=""Generated map by algorithm""></p>

<p>I more or less understand how Hector works, but I have no idea about why I'm getting this map and specially trajectory.</p>

<p>Thank you.</p>
","localization slam ros mapping rangefinder"
"7388","Fingerprinting/ model matching algorithms for localization","<p>This <a href=""http://www-personal.umich.edu/~johannb/Papers/paper64.pdf"" rel=""nofollow"">paper</a> mentioned the fingerprinting/model matching case. But I could not find an image based algorithm. Any suggestion about image based localization</p>
","mobile-robot localization"
"7389","ROBOTIC arm for playing chess","<p>I wish to build a chess playing robot with robot arm as shown on youtube, can anyone please tell me which robot arm would suit my purpose and whether it can be bought second hand or alternatively anybody willing to sell used chess arm robot? Please help out.</p>
","robotic-arm"
"7395","Simple wireless connection between two circuits","<p>I'm relatively new to robotics, and I'm building a project for which I need a simple wireless connection between two circuits such that when the first circuit is switched on, the other circuit gets switched on too. I'm looking to preferably build something like this on my own, but I have no idea about wireless connections. I only know basic wired robotics. I also know C++ programming if that helps. Apologies if such a question has already been asked.</p>

<p>Regards,
Hanit Banga</p>
","wireless"
"7397","Selecting hardware: stereo camera for beginners","<p>I'm looking for some cheap hardware that would offer me results decent enough to continue my experimentation.</p>

<p>I've been looking into how to obtain hardware for learning about stereo vision and 3D reconstruction, I found two basic ways: - buy 2 cheap webcams and DIY - buy a stereo camera</p>

<p>For what I understood little variations in distance and inclination can easily compromise the diff map and so the DIY version might end up requiring constant calibrations, however on the other end, so buying ""professional"" stereo camera range from 500 euro to infinite.</p>

<p>For the moment I trying something in between, like the minoru 3d, however the overall performance of the camera looks a bit poor also because it's a 2009 product, however I can't find any more recent product offering a similar solution.</p>

<p>Can you suggest me what would be the best way/product/guide to archive decent results without spending a fortune ?</p>

<p>Thank you very much :)</p>
","stereo-vision"
"7398","Matlab toolbox (Windows) for Sick lasers?","<p>Does anybody know where I can get a matlab toolbox or functions to work with a SICK laser-scanner (Windows OS)? I'm using a SICK-LDRS2110 with ethernet cable, but SOPAS software does not allow me to program recording times and other specific tasks. Any tips are more than welcome! </p>

<p>Thanks!</p>
","laser matlab"
"7400","Quadcopter force/torques duty cycle conversion","<p>after having been determined my control loops for my quadcopter project, I'm going to determine the motor commands (PWM duty cycle) from the motor forces/torques. I was following the guidelines of <a href=""http://rwbclasses.groups.et.byu.net/lib/exe/fetch.php?media=quadrotor:beardsquadrotornotes.pdf"" rel=""nofollow"">this document</a> but when I was trying to do the inverse of the matrix M (page 17) it has determinant equal to 0. The procedure is correct? Anyone can suggest me some other link for doing this conversion? I have searched in the Internet but I haven't found so much about that. Thanks
The part of the document that I'm referring is the following:
<img src=""http://i.stack.imgur.com/RsFZb.png"" alt=""enter image description here""></p>
","control quadcopter pwm"
"7403","EKF SLAM C++ code on openslam.org","<p>I have been recently working on code for a robot maze solver using laser sensors and odometry data. I went through the pdf available online $\textbf{'SLAM for Dummies'}$ and understand the process conceptually. I am a master's student in control so that part wasn't hard but writing the code in C++ is the difficult task. The EKF SLAM codes available on the site $\textbf{openslam.org}$ seem a bit advanced since I am a beginner. I couldn't find on the site (maybe missed) the simplest EKF SLAM algorithm for 2D implementation. </p>

<p>Could anyone guide me to an open source code in C++ for 2D implementation so that I can build up on it to suit the robot I am working on?</p>
","slam"
"7406","How do I Program the Create 2","<p>I just un-boxed and set the Create 2 to charge over night.</p>

<p>How do I program it? Where is the software?</p>

<p>Daniel</p>
","irobot-create programming-languages"
"7409","Libusb and arduino communication not working","<p>I am doing a line following robot based on opencv. I have my onboard computer(an old pandaboard) running opencv. It will calculate the offset from the required path and communicate it to the arduino via USB. Then it will do PID optimisation on the data, and adjust the speed of the left and right motors.</p>

<p>To my dismay the communication part is not working, and I've tried hard for a day to fix it with no result. Here is the relavent code running on the pandaboard:</p>

<pre><code>while(1)
    {

        r = libusb_bulk_transfer(dev_handle, 131, recieve, 1, &amp;actual, 0);
        cout&lt;&lt;""r=""&lt;&lt;r&lt;&lt;endl;
        int a;
        cin&gt;&gt;a;
        imgvalue=krish.calc_offset();
        send[0]=imgvalue&amp;0xff; 
        send[1]=imgvalue&gt;&gt;8;

//make write

        cout&lt;&lt;""Data to send-&gt;""&lt;&lt;imgvalue&lt;&lt;""&lt;-""&lt;&lt;endl; //just to see the data we want to write : abcd
        cout&lt;&lt;""Writing Data...""&lt;&lt;endl;
        r = libusb_bulk_transfer(dev_handle, (4 | LIBUSB_ENDPOINT_OUT), send, 2, &amp;actual, 0); //my device's out endpoint was 2, found with trial- the device had 2 endpoints: 2 and 129
        if(r == 0 &amp;&amp; actual == 2) //we wrote the 4 bytes successfully
            cout&lt;&lt;""Writing Successful!""&lt;&lt;endl;
        else
            cout&lt;&lt;""Write Error""&lt;&lt;endl;

    }
</code></pre>

<p>where <code>imgvalue</code> is the data to be send. This is the code running on the Arduino:</p>

<pre><code>void loop()
{
  Serial.write('s');
  if(Serial.available()&gt;0)
    Input_tmp = Serial.read();
  if(Serial.available()&gt;0)
    Input_tmp = Input_tmp | (Serial.read() &lt;&lt; 8);
  Input=Input_tmp;
  myPID.Compute();

// adjust the motor speed

}
</code></pre>

<p>What happens when I run is that it will pause at the libusb read operation as the timeout is zero(infinity). At this point I've tried resetting the arduino, but this doesn't help. So how do I make my program respond to this start byte send my the Arduino? Where did I go wrong?</p>
","arduino communication usb"
"7411","What is the cheapest way to detect and identify vehicles entering a gate in real time?","<p>I want to detect and identify each of the vehicles passing through a gate. </p>

<p>I have the live video feed of the gate which I initially thought to process and detect the number plates with the help of OpenCV or any other graphics library freely available. The problem is, the size of number plates may vary very widely, and the language the number plates are written with(Bengali) does not have a good OCR performance at all.</p>

<p>The next idea was to put a QR code in the windshield of the vehicles. (Yes the vehicles supposed to enter the area are private and enlisted vehicles). But I am not confident that I will be able to detect and identify all the QR codes in real time with 100% accuracy, as the QR codes might get pixelated due to low resolution of video.</p>

<p>So can anyone suggest any other cheap way we can adopt to detect and identify the vehicles? <strong>Can NFC or any other cheap sensors be used</strong> for this purpose?</p>
","sensors design computer-vision"
"7415","Quadcopter PID output and duty cycle conversion","<p>I'm trying to design two PD controllers to control the roll and pitch angle of my quadcopter and a P controller to control the yaw rate. I give to the system the reference roll, pitch and yaw rate from a smartphone controller (with WiFi).In the case of roll and pitch the feedback for the outer 'P' loop is given by my attitude estimation algorithm, while in the inner 'D' loop there is no reference angle rate, and the feedback is provived by a filtered version of the gyroscope data.
As far the yaw rate is concerned, is only a P controller, the reference yaw rate is given by the smartphone, and the feedback of the only loop is provived by the smartphone. This is to illustrate the situation. My sampling frequency is 100hz (imposed by the attitude estimation algorithm, that is a Kalman Filter, that I'm using). I have tuned my controller gains with matlab, imposing a rise time of 0.1 seconds and a maximum percent overshoot of 2% with root locus. Matlab is able to found me a solution, but with very large gains (like 8000 for P and 100 for D). I was doing the tuning, using a quadcopter model (for each euler angle) based on the linearized model for quadcopter or instance : $$\ddot \tau_\Phi = I_x\ddot \Phi    -&gt;  G_\Phi(s) = \frac{I_x }{ s^2} $$ only in order to have a 'reasoned' starting point for my gains, and then re-tune it in the reality. (The transfer function above is continous, in my model I have obliviously used the discrete version at 100hz of sampling rate). 
This is to do a premise of my following questions.
Now, I have to map my controller outputs to duty cycle. Since I'm using a PWM at 25Khz frequency, my period (in the TIM channel configuration) is of 2879.
I have checked the activation threshold (after which the motor starts move) and the threshold after which it stops increasing its speeds, and the first is 202
and the second is 2389.
I was following the very good answer of <a href=""http://robotics.stackexchange.com/questions/2964/quadcopter-pid-output?lq=1"">Quadcopter PID output</a> but I still have some questions.</p>

<p>1) As far the throttle mapping is concerned, I have to map it in such a way that the values coming from my smartphone controller (in the interval [0, 100]) are not
mapped in the whole [202, 2389] interval, but I have to 'reserve' some speed in order to allow the quadcopter to have an angular movement exploiting differences in the 4 motor speeds even with 100% throttle?</p>

<p>2) Coming back to the fact that matlab propose me huge gains for my controllers, this leads to the fact that I cannot directly sum the controller output to the duty cycle as stated in the metioned answer (because I will certainly go out of the [202, 2389] bound of my TIM pulse). Doing a proportion will result in altering the gains of the systems, so placing somewhere else the poles of my systems and the procedure done with matlab will became useless, right? So, what I'm doing wrong? I have tried to enforce matlab to bound the gainsm for instance in the [0,100] interval, but in this case it cannot find gains such that my constraints are verified.
Thank you</p>
","control quadcopter pid matlab"
"7416","Artificial Intelligence Software Packages: Professionals, University education is oft' a step behind. What's actually being used?","<p>Currently using Windows 8, what software packages for artificial intelligence programming (robotics branch) are used in today's professional environment as standard. Lots of internet suggestions, but companies seem to keep this a closely guarded secret. And are the internet rumors true? Would switching to Ubuntu offer me more in terms of depth.</p>

<p>Context: Educational field: Computer Science and Artificial Intelligence, current focus (though obviously experience in others) in programming languages stands at c++, C and Python. Looking to build, program and develop a human-like bot (NOT aiming for singularity at this point ;))and am asking this question in order to build my toolbox a little. </p>
","design software artificial-intelligence programming-languages"
"7423","Many to One Bluetooth Communication Link","<p>I have an application that requires data to be streamed from multiple Bluetooth modules to one host controller. Somewhat like multiple Clients and one Server. </p>

<p>The throughput i am looking at is around 1920-bits per second per module. 
The <a href=""https://www.google.co.in/url?sa=t&amp;rct=j&amp;q=&amp;esrc=s&amp;source=web&amp;cd=1&amp;cad=rja&amp;uact=8&amp;ved=0CCwQFjAA&amp;url=http%3A%2F%2Fwww.st.com%2Fweb%2Fen%2Fcatalog%2Fsense_power%2FFM1968%2FCL1976%2FSC1324%2FPF253470&amp;ei=ivZvVbG3DZafugT99oCIBA&amp;usg=AFQjCNE3rd0Y5qAVPf9wHGOxc3klSjLEfg&amp;sig2=kQg3v-7wIdE7NDGiFL688Q&amp;bvm=bv.94911696,d.c2E"" rel=""nofollow"">SPBT2632C2A.AT2</a> module only supports SPP profile in which i can have a single link (One Client One Server). My application needs multiple modules ( Max 5) to send information to one server.  </p>

<p>Is there a way to have One Receiving Station and have multiple transmitting module using SPP? (All modules being the SPBT2632C2A), or i need a Different higher end module on the server side which supports multiple SPP Links?</p>

<p>It advisable to look into a module like the BCM2070 and have a driver run system?</p>
","electronics"
"7429","Is ROS hard real time safe?","<p>I know that is a question that has been asked too many times, but still its not clear to me. I read online that it isn't but some people say that they control their robots under ROS in applications with hard real time constraints. So, because I need some technical arguments (rather than a plain ""ros is not real time"") I will be more specific (suppose we have ROS under a RTOS):</p>

<ol>
<li>I read that ROS uses a TCP/IP-based communication for ROS topics and I know that TCP/IP is not reliable. That means I cannot use topics in a real time loop? For instance send a control signal to my system publishing it to a topic, and the system sending me some feedback via a topic?</li>
<li>If I have a RTOS (eg Linux+Xenomai) can I build a real time control loop for a robot using ROS, or ROS will be a bottleneck?</li>
</ol>

<p>Maybe the above are naive or I lack some knowledge, so please enlighten me!</p>

<p>Note: I define as a hard real time system (eg in 1KHz), the system that can guarantee that we will not miss a thing (if the control loop fails to run every 1ms the system fails).</p>
","ros real-time"
"7434","What is the achievable stiffness of a impedance/admittance controlled robot (incl. haptic devices), given its structural and control stiffnesses?","<p>EDIT: I realised I missed the point of the paper completely (thanks to very-skim reading ;) ). So, this part of it I'm relating to is about how much damping - not how much stiffness - should we display to obtain stability, given a structural stiffness. I changed the question accordingly - what is achievable stiffness of a impedance/admittance controlled robot, given its structural and control stiffnesses? (Stiffness/compliance is, of course, mathematically just one of the terms in total impedance/admittance)</p>

<p>Let us consider a haptic device with mechanical and control parts, and mechanical part is not infinitely rigid (compliant). Basically, it would be a robot with impedance or admittance control. I thought perceivable stiffness can be just as simple as serial connection of two stiffnesses - and so the stiffer mechanical structure is, the better it can display control stiffness:</p>

<p>$k = \frac{k_e k_c}{k_e + k_c}$</p>

<p>where $k_c$ is stiffness control. Still, I cannot find any confirmation to this, although something very similar is stated in Samur's ""Performance Metrics for Haptic Interfaces"". I would be very grateful if you could refer me to some sources or just plain prove it wrong or right (:</p>

<p>In a paper <a href=""http://web.stanford.edu/class/me327/presentations/6-Shayan-Amin13-WHC-Mechanical.pdf"" rel=""nofollow"">(here, p. 728)</a> I only found stability condition for virtual damping value in relation to virtual stiffness, given structural stiffness.</p>
","control mechanism reference-request"
"7438","How to make a directed graph?","<p>I'm working on an robot that would be able to navigate through a maze, avoid obstacles and identify some of the objects in it. I have a monochromatic bitmap of the maze, that is supposed to be used in the robot navigation. </p>

<p>Up till now, I have converted/read the bitmap image of the maze into a 2D array of bits. However, now I need guidance on how to use that array to plan the path for the robot. I would appreciate if you could share any links as well, because I am new to all this stuff (I am just a 1st year BS electrical engineering student) and would be happy to have a more detailed explanation.</p>

<p>If you need me to elaborate on anything kindly say so.</p>

<p>I would be grateful!</p>

<p>Here's the image of the maze.</p>

<p><img src=""http://i.stack.imgur.com/ahtkQ.png"" alt=""Sample Maze Image""></p>

<p>This is just a sample image; the robot should be able to work with any maze (image) with similar dimensions. And you are welcome!</p>

<p>Thank you Chuck!</p>

<p><strong>UPDATE</strong>
Heres the code for sub2ind in c++. Kindly see if the output is correct:-</p>

<pre><code>ofstream subtoind;
subtoind.open(""sub2ind.txt"");

int sub2ind[96][64] = { 0 };
int ind2subROW[6144] = { 0 };
int ind2subCOL[6144] = { 0 };
int linearIndex=0;
j = 0;
z = 0;

for (j = 1; j &lt;= 64; j++)
    {
        for (z = 1; z &lt;= 96; z++)
            {
                    linearIndex = z + (j - 1) * 96;
                    sub2ind[z-1][j-1] = linearIndex-1;
                    //ind2subROW[linearIndex-1] = j-1;
                    //ind2subCOL[linearIndex-1] = z-1;
            }
    }
for (j = 0; j &lt; 96; j++)
{
    subtoind &lt;&lt; endl; //correction
    cout &lt;&lt; endl;

    for (z = 0; z &lt; 64; z++)
    {
        subtoind &lt;&lt; sub2ind[j][z] &lt;&lt; "" "";
    }
}
subtoind.close();
</code></pre>

<p>Heres the Link to the output file.
<a href=""https://drive.google.com/file/d/0BwUKS98DxycUSk5Fbnk1dDJnQ00/view?usp=sharing"" rel=""nofollow"">https://drive.google.com/file/d/0BwUKS98DxycUSk5Fbnk1dDJnQ00/view?usp=sharing</a></p>
","arduino mobile-robot localization mapping planning"
"7440","Matlab Control Toolbox root locus","<p>I'm using the control system toolbox provided by matlab to estimate the gains of my controller: using root locus design I get a graph like this one <img src=""http://i.stack.imgur.com/jaN4D.png"" alt=""enter image description here"">.</p>

<p>My question is: what is the x on the x-axis? maybe a pole position at a previous iteration of the optimization procedure that I have run to find a gain value that satisfies my requirements? It shouldn't be the open loop pole position, because my system is formed by two integrators multiplied by a constant (1/inertia). Thanks</p>

<p>Edit: I add the requested details: I start from the following simulink diagram:
<img src=""http://i.stack.imgur.com/F3NTm.png"" alt=""enter image description here"">
my trasfer function is $$G_\Theta(s) = \frac{Y(s)}{U(s)} = \frac{\Theta(s)}{\tau_\Theta} =  \frac{1}{I_y s^2}$$ with
Iy = 0.0054 (another little question, the point in which I'm taking out the torque is correct?)
and then I select analysis , control design, compensator design. I select Kp and Kd as the gains to be tuned, and I use the root locus for specifying the constraints. Then I click in SISO Design Task, automated tuning, optimize compensator, which automatically tries to find gain values to satisfy my constraints. The white are is the area that satisfies the constraints, and I think that the pink squares are my poles position after having been completed the optimization procedure. This is correct? But in this case, what is the x(pole) shown? Thanks</p>
","control matlab"
"7445","Why PD controllers for quadcopter angles control?","<p>my question is: in a lot of cases it is possible to find in Internet PD (instead PID) to control the euler angles of quadcopter? Why the integral part is often neglected in this kind of applications? thanks</p>
","control quadcopter pid"
"7449","Changing behaviour Roomba 880","<p>I've seen that it is possible to use some micro controller to send commands to the Roomba through the SCI but i was more interested in changing the behavior of the roomba operation (e.g: change the priority of the behaviours)
Is there some IDE for roomba?</p>

<p>Regards</p>
","roomba"
"7450","Which math course will be most beneficial?","<p>Let me know if this should be on Academia instead, but I posted it here to get responses specifically from people active in robotics development.</p>

<p>I'm currently an undergraduate student completing majors in both mechanical engineering and computer science. I'm still fairly new to the field, but my interest is firmly in electronic and mechanical systems. Next year I can take one of the courses below:</p>

<pre><code>1. Multivariable Calc.
2. Linear Algebra
3. Differential Equations
</code></pre>

<p>I want to take all three and likely will eventually, but for the time being my schedule only allows for one. Therefore, I was wondering if you could explain a little bit about how each is applied to the robotics field and which you believe will be most helpful for me to learn now.Thanks in advance!</p>
","beginner theory"
"7454","How to use an IMU to hover at a fixed location in a quadcopter in the presence of gravity?","<p>There's an accelerometer in the IMU. The output can then be integrated to estimate the position, at least in theory.</p>

<p>But in practice, there's a huge acceleration from gravity, which varies rather randomly across locations. Vibrations etc can be filtered out with low-pass filters, but how do you filter out gravity? Is it simply the case that the vertical vector is ignored when doing any calculations?</p>

<p>My application is, I want to build a quadcopter that could hover in one place even in the presence of (reasonable) winds: the quadcopter ideally would tilt towards random gusts to maintain a certain position. Every single tutorial I could find on the Internet only uses the accelerometer to estimate where down is when stationary, and simply assumes that using the gyroscope to hold the quadcopter level is enough. </p>

<p>I also want to use the IMU to estimate altitude if possible, of course as an input to something like a Kalman filter in conjunction with a sonar system.</p>

<p>Obviously, for my application GPS is far too slow.</p>
","quadcopter imu"
"7456","Human arm inverse kinematics","<p>Hi I want to implement an human arm robot and a task such as moving a glass between two points  using Robotic Toolbox for Matlab  by Peter Coorke. I'm a student and I'm a newbie in this kind of things so I would find a good reference for solving the inverse kinematics of the human arm and  an algorithm that implements some kind of obstacle avoidance exploiting the redundancy of the manipulator (7dof) using null space motion.  Anyone can suggest me a good reference to follow in this implementation with the toolbox? Thanks</p>
","robotic-arm inverse-kinematics manipulator matlab"
"7457","Cognitive Architectures: how do you perform qualitative and quantitative comparisons?","<p>I couldn't find a sub stackexchange for artificial intelligence, but I think robotics comes close, and so I'm posting here.</p>

<p>I recently saw TED talks on <a href=""https://www.ted.com/topics/ai"" rel=""nofollow"">AI</a> and the Google car, with these being the most interesting to me:</p>

<ol>
<li><a href=""https://www.ted.com/talks/hod_lipson_builds_self_aware_robots"" rel=""nofollow"">Hod Lipson - Building ""self-aware"" robots</a> </li>
<li><a href=""https://www.ted.com/talks/juan_enriquez_shares_mindboggling_new_science?language=en"" rel=""nofollow"">Juan Enriquez - The next species of human</a></li>
<li><a href=""https://www.ted.com/talks/ray_kurzweil_get_ready_for_hybrid_thinking"" rel=""nofollow"">Ray Kurzweil - Get ready for hybrid thinking</a></li>
</ol>

<p>The third one led me to the <a href=""http://en.wikipedia.org/wiki/How_to_Create_a_Mind#Analysis"" rel=""nofollow"">'criticism' section</a> (labeled <code>Analysis</code> on that wiki article, though it certainly at least partially reads as a criticism section as well) of Kurzweil 'theory' of the brain, namely <a href=""http://en.wikipedia.org/wiki/How_to_Create_a_Mind#Pattern_Recognition_Theory_of_Mind"" rel=""nofollow"">""Pattern Recognition Theory of Mind"" (PRTM)</a>.  After some link surfing on the people who have performed analysis of PRTM and their respective academic contributions, I came to learn about <a href=""http://en.wikipedia.org/wiki/Cognitive_architecture"" rel=""nofollow"">Cognitive Architecture</a>:</p>

<blockquote>
  <p>""A cognitive architecture can refer to a theory about the structure of
  the human mind. One of the main goals of a cognitive architecture is
  to summarize the various results of cognitive psychology in a
  comprehensive computer model. However, the results need to be in a
  formalized form so far that they can be the basis of a computer
  program. By combining the individual results are so for a
  comprehensive theory of cognition and the other a commercially usable
  model arise. Successful cognitive architectures include ACT-R
  (Adaptive Control of Thought, ACT), SOAR and OpenCog.""</p>
</blockquote>

<p>It appears that there are several interesting architectures, including the 3 mentioned above.  I read a bit about <a href=""http://en.wikipedia.org/wiki/ACT-R"" rel=""nofollow"">ACT-R</a>, <a href=""http://en.wikipedia.org/wiki/Soar_(cognitive_architecture)"" rel=""nofollow"">SOAR</a>, <a href=""http://en.wikipedia.org/wiki/OpenCog"" rel=""nofollow"">OpenCog</a>, <a href=""http://en.wikipedia.org/wiki/DUAL_(cognitive_architecture)"" rel=""nofollow"">DUAL</a>, <a href=""http://en.wikipedia.org/wiki/CHREST"" rel=""nofollow"">CHREST</a>, and <a href=""http://en.wikipedia.org/wiki/CLARION_(cognitive_architecture)"" rel=""nofollow"">CLARION</a>.  The list is not comprehensive.  It also appears that there are two main types of such architectures: <a href=""http://en.wikipedia.org/wiki/Connectionism"" rel=""nofollow"">Connectionism</a> and <a href=""http://ai.eecs.umich.edu/cogarch2/prop/symbolism.html"" rel=""nofollow"">Symbolic</a>.</p>

<p>Though I have many questions, my main question is this:
<strong>What are some quantitative metrics and qualitative properties to measure and compare between the two architecture types?</strong></p>

<p>Other questions</p>

<ul>
<li>Can all architectures be categorized as one, the other, or some
combination of the two, or is there a third, fourth, etc? </li>
<li>How are two main types alike? How are they different?</li>
<li>What are some recommended further readings on this topic. </li>
<li>What centres and organizations are leading development in this?</li>
<li>What are some of the computer programming languages, related skill-sets, and
cross-domain knowledge set utilized in R&amp;D and product offerings of
such systems?</li>
</ul>
","artificial-intelligence"
"7460","How to calculate quadcopter lift capabilities?","<p>I'm looking for an equation (or set of equations) that would allow me to predict (with fair accuracy) how heavy a payload a <a href=""http://en.wikipedia.org/wiki/Quadcopter"" rel=""nofollow"">quadcopter</a> is capable of lifting.</p>

<p>I assume the main variables would be the weight of the copter as well as the size + power of the 4 rotors. What general approach can one use to make such a determination?</p>
","quadcopter"
"7462","How do safety cages around quadcopter rotors/blades affect lift capabilities?","<p>I am interested in building a <a href=""http://en.wikipedia.org/wiki/Quadcopter"" rel=""nofollow"">quadcopter</a> from scratch.</p>

<p>Because I like to err on the side of caution, I'm considering adding ""safety cages"" around each propeller/rotor, to hopefully prevent (at least minimize) the chance of the spinning rotor blades coming into contact with someone. Without knowing much about the physics behind how ""lift"" works, I would have to imagine that cages present two main problems for rotors:</p>

<ol>
<li>They add weight to the copter making it harder to lift the same payload; and</li>
<li>They're sheer presence/surface area makes it harder for the spinning rotor to generate lift and push down away from the ground</li>
</ol>

<p>The former problem should be obvious and self-evident. For the latter problem, what I mean by ""surface area"" is that I <em>imagine</em> that the more caging around a spinning rotor, the more difficult it will be to lift effectively. For instance, a spinning rotor might have the ability to generate enough power to lift, say, 2kg. But if we were to construct an entire <em>box</em> (not cage) around the entire rotors, with 6 sides and no openings, I would imagine its lift capability would drop to 0kg.</p>

<p>So obviously, what I'm interested in is a cage design that provides adequate safety but doesn't ""box in"" the rotor so much that it causes the rotor to be ineffective or incapable of providing lift. So I'm looking for that optimal tradeoff of safety (boxing/caging around the spinning rotor) and lift performance.</p>

<p>I would imagine calculating and designing this is a pretty huge undertaking with <em>a lot</em> of math behind it. I'm just wondering if anyone has already figured all this stuff out, or if anyone knows of a way to model this safety-vs-lift-performance trade off in some way.</p>
","quadcopter"
"7463","Stewart platform as robotic wrist joint","<p>I'm planning the design of a wrist for a humanoid robot. I would like to choose a design that is sturdy while allowing for dexterity comparable to a human wrist.</p>

<p>One option that was presented to me was to use a <a href=""http://en.wikipedia.org/wiki/Stewart_platform"" rel=""nofollow"">Stewart platform</a>. This setup appears to correctly recreate all possible movements of the human hand. My immediate concern is that this platform will use a total of six actuators which will require additional power and computational requirements. I don't want to commit to this design until I am certain that there isn't a better alternative.</p>

<p>Is a Stewart platform a good choice for replicating the dexterousness of the human wrist? If not, what is a better solution?</p>
","robotic-arm design actuator joint humanoid"
"7470","7DOF inverse kinematics spherical wrist","<p>Is it possible to apply kinematic decoupling for a 7 DOF 7R manipulator with spherical wrist?  If it is possible, can anyone suggest a reference on how to apply this approach with a redundant manipulator with spherical wrist, or explain why it is not possible? </p>

<p>I'm working with Robotic Toolbox (matlab) and the numeric algorithm can find the inverse kinematics solution without a problem if I don't specify the orientation.  And I was thinking about solving the problem a second time considering the spherical wrist.  Will this approach work?</p>
","inverse-kinematics manipulator matlab"
"7472","Wiring & driving TowerPro SG90 servos","<p>I got my hands on a few <a href=""http://www.servodatabase.com/servo/towerpro/sg90"" rel=""nofollow"">Tower Pro SG90 9G servos</a> but cannot find their schematics or datasheet anywhere (besides that link).</p>

<p>I have the following concerns:</p>

<ul>
<li>Looks like they're rated for 4.8V, but will they tolerate a 5V supply?</li>
<li>How do I determine the current they require, in amps, mA, etc.?</li>
<li>There's 3 wires: brown, red &amp; yellow-orange, what do each of these guys do?
<ul>
<li>If I had to <em>guess</em> I'd say that red is power, another one is direction, and another one is the position to rotate to</li>
</ul></li>
</ul>
","rcservo wiring"
"7474","How do I accurately calculate the speed of a rotary encoder at a high sample rate?","<p>I'm aiming to control a motorized joint at a specific speed. To do this, I'm planning on attaching a rotary encoder to do this.</p>

<p>I'll be controlling the motor with a PID controller. With this PID controller, I need to control the joints based on their velocity.</p>

<p>Since:</p>

<pre><code>speed = distance / time
</code></pre>

<p>It would make sense to do something like this:</p>

<pre><code>double getCurrentSpeed() {
    return (currentAngle - lastAngle) / samplingRate;
}
</code></pre>

<p>However, there's an issue; the encoder doesn't provide a high enough resolution to accurately calculate the speed (the sample rate is too high). I want to have updated data every 5-15 ms (somewhere in that range as my current motors seem to be able to respond to a change in that range)</p>

<p>Some more information:</p>

<ul>
<li>14 bit precision (roughly 0.0219726562 degrees per ""step"" of encoder</li>
<li>I'd like to be able to calculate as small of speed differences as possible</li>
<li>As the motors will be going fairly fast (120+ degrees/second at highly variable speeds and directions), so the feedback has to be accurate and not delayed at all</li>
</ul>

<p>So, a couple of ideas:</p>

<ul>
<li>I can find encoders that I can sample at a very high rate. I was thinking about sampling the time between the changes of the encoder's value. However, this seems finicky and likely to be noise-prone</li>
<li>I could do some sort of rolling average, but that would cause the data values to ""lag"" because the previous values would ""hold back"" the output of the calculations somewhat and this would play with my PID loop some</li>
<li>Noise filter of some sort, although I don't know if that would work given the rapidly changing values of this application</li>
</ul>

<p>However, none of these seem ideal. Is my only option to get a 16 bit (or higher!) encoder? Or is there another method/combination of methods that I could use to get the data I need?</p>
","motor pid algorithm"
"7475","Can I control more than 18 servo motor with a Raspberry Pi","<p>I'm trying to make an hexapod with 18 servo motors and i'm asking how to control them with a Raspberry Pi. (Never used it). I saw lot's of stuff to control 1, but 18, 20...</p>

<p>Currently I'm working on an <a href=""http://www.arduino.cc/en/Main/arduinoBoardMega"" rel=""nofollow"">Arduino Mega</a>, and a <a href=""http://www.lynxmotion.com/p-395-ssc-32-servo-controller.aspx"" rel=""nofollow"">SSC-32</a> board, but I found the result to slow and jerky.</p>

<p>At this end, I want to add a camera and processing the image, I know an Arduino can't handle that process but a Raspberry Pi can ?</p>

<p>Thank for all information about that subject :) </p>
","arduino raspberry-pi cameras servomotor"
"7483","How to transform x y z coordinates to Tx Ty Tz?","<p>I need to get coordinates of the specific points from 2D CAD file and transform them so that I could use them to move the robotic arm to those points. The problem is that I only get x y z coordinates and the robotic arm needs x y z Tx Ty Tz coordinates to move to the certain position. </p>

<p>Any suggestions?</p>

<p>Edited:</p>

<p>My task: I need robotic arm to go through certain points on PCB board and heat soldering paste. I could do it manually by setting points with pendant. But a much easier way would be to get coordinates of those points from CAD file and write a code using PC.</p>

<pre><code>MOVL MotionSpeedType(0 - linear mm/s, 1 - angular °/s) Speed (0.1 - 1000 mm/s or Max angular 

speed) coordinate X Y Z Tx Ty Tz 

ToolNo [Type] (move robot in a cartesian coordinates in linear motion)
</code></pre>

<p>this is how code for linear motion to a certain point looks like</p>

<p>I only could find this <a href=""http://www.motoman.com/motomedia/manuals/docs/160475-1CD.pdf"" rel=""nofollow"">manual</a>.</p>

<p>This is pendant <a href=""http://www.motoman.com/motomedia/manuals/docs/155490-1CD-R5.pdf"" rel=""nofollow"">manual</a> maybe it will be helpful.</p>

<p>I am second year student in ""Robotics and mechatronics"". I'm currently in a internship at the scientific research institution. I really appreciate your help!</p>
","robotic-arm"
"7485","Matlab: System simulation with dynamic state matrix / input matrix","<p>I have the following system:
$$\dot{x} = A(t)x+B(t)u$$
$$y = x$$</p>

<p>$A(t)$ and $B(t)$ are actually scalar, but time-dependent. If they would be constant, I could simulate the system in Matlab using:</p>

<p><code>sys = ss(A,B,C,0);</code>
<code>lsim(sys,u,t,x0);</code></p>

<p>However, it would be nice to simulate the system with dynamic state and input matrix. The matrices are based on measurement data, this means I would have for each discrete time step $t_i$ another matrix $A(t_i)$. Any suggestions how to do that?</p>
","dynamics matlab simulation"
"7486","How to determine how long a battery will power a robotic circuit for?","<p>Obviously robotic circuits draw different amounts of power/current. So given the same battery, say, a 9V, then connecting it to 2 different circuits will deplete it at two different rates. Robot/Circuit #1 might drain the battery in 5 minutes. Robot/Circuit #2 might drain the battery in 20 minutes.</p>

<p>What ratings do batteries have that allows us to figure out how long it will power a circuit for? <strong>Bonus points:</strong> does this same rating uphold for solar panels and, in deed, all power supplies (not just batteries)?</p>
","power battery circuit"
"7489","Understanding how solar panels can supply power to robotic circuits","<p>Say I have <a href=""http://www.adafruit.com/products/200"" rel=""nofollow"">this solar panel</a> that outputs 6V at 330mA, or ~1.98 Watts. If I connect that to Arduino, which expects a 5V supply at (roughly) 50mA, then the Arduino as a whole requires 5V * .05A = 0.25 Watts to power it. To me, if I understand this correctly, then in perfect weather/sunlight, the solar panel will power Arduino all day long, no problem.</p>

<p>Now let's say we wire up 4 motors to the Arduino, each of which draw 250 Watts. Now the Arduino + 4 motors are drawing ~1.25 Watts. But since the panels are still outputting 1.98 Watts, I would think that (again, under perfect sunlight) the panel would power the Arduino and motors all day long, no problem.</p>

<p>Now we add 4 more motors to the Arduino circuit, for a total of 8 motors. The circuit is now drawing 1.25 Watts + 1 W = 2.25 Watts. I would expect the solar panel to <em>no longer</em> be capable of powering the circuit, at least properly.</p>

<p>My first concern here is: am I understanding these 3 scenarios correctly? If not, where is my understanding going awry?</p>

<p>Assuming I'm more or less on track, my next question is: can solar panels be ""daisy chained"" together to increase total power output? In the third case above, is there a way to add a second solar panel into the mix, effectively making the two panels output 1.98 Watts * 2 = 3.96 Watts, which would then make them capable of powering the Arduino and its 8 motors (yet again, assuming perfect weather/sunlight conditions)?</p>
","power circuit"
"7491","Wiring necessary to route power from any one of several rechargeable batteries","<p>I'm looking for my robotics project to draw its power from one of 3 rechargeable batteries; basically whichever has the most ""juice"" in it. From the initial research I've already done, I believe I could connect each rechargeable battery (probably LiPo) to a diode, and then wire each of the 3 diodes in series.</p>

<p>However, being so new to robotics/electronics, I guess I wanted to bounce this off the community as a sanity check, or to see if there is a better way of achieving this. Again, what I am looking for is a way for the circuit to automagically detect that battery #1 has more power than battery #2, and so it ""<em>decides</em>"" to draw power from #1. The instant #1 is depleted or deemed ""less powerful"" than #2, the #2 battery takes over. Thoughts/criticisms?</p>
","power battery wiring"
"7493","Discover vector/angle between stereo camera pose and vehicle body","<p>I have a calibrated stereo camera system that is mounted in a passenger car which means I am able to retrieve a point cloud from my stereo image. However, I need to find how well is the camera aligned with the vehicle - read: if the camera is perfectly facing forwards or not. I guess it will never perfectly face forwards so I need to get the angle (or rather 3D vector) between ""perfect forwards"" and ""actual camera pose"".</p>

<p>What came to my mind is to drive the vehicle possibly perfectly forwards and use stereo visual odometry to detect the angle of vehicle movement as seen by camera (which is the vector I am looking for). The <a href=""http://www.cvlibs.net/software/libviso/"" rel=""nofollow"">LIBVISO</a> library for visual odometry can output a 3D vector of movement change from one stereo frame to another which could be used to detect the needed vector.</p>

<p>The only problem may be to actually be able to drive perfectly forward with a car. Maybe an RTK GPS could be used to check for this or for correction. Will anyone have a suggestion on how to proceed?</p>

<p>The stereo camera I use consists of 2 separate Point Grey USB cameras. Each camera is mounted on a windshield inside the car with a mount like <a href=""http://drzaky-navigace.heureka.cz/truecam-a3-prisavny-drzak/"" rel=""nofollow"">this one</a>. The cameras were calibrated after mounting. The stereo baseline (distance between the cameras) is about 50 cm.</p>
","stereo-vision odometry"
"7494","Spring with electronically adjustable stiffness","<p>I would like to build a mechanical module that acts like a spring with electronically controllable stiffness (spring rate).</p>

<p>For instance, let's imagine a solid, metallic cube, 0.5 m each side. On the top side of the cube, there is a chair sitting on top of a solid mechanical spring. When you sit on the chair, it would go down proportionally to your weight, and inversely proportional to the spring's rate. </p>

<p>What I want is that this spring's rate be electronically adjustable in real time, for instance a microcontroller system might increase the spring's rate when it detects a larger weight.</p>

<p>I'm using this example to best describe what I want to achieve because I'm not a robotics specialist and I don't know the inside terms.</p>

<p>Is there already an electro-mechanic module as the one I'm describing? (obviously nevermind the cube and the chair, it's the spring I'm interested in).</p>
","actuator"
"7497","Torque of coreless DC micro motor","<p>I would like to build a small two-wheeled robot similar to the one shown <a href=""http://www.instructables.com/id/Mini-2-wheel-robot-with-IR-sensor-weight-14gr-and-/"" rel=""nofollow"">here</a>.</p>

<p>In order to keep the robot small, I intend to use two coreless micro motors like the one shown bellow. The power source would be 2 AAA or AA batteries, in order to reach 3 V. These batteries would represent the bulk of the weight of the robot. The rest of the robot would be virtually weightless.</p>

<p>The specifications of one of such motor are:</p>

<pre><code>Motor diameter: 6 mm
Motor length: 12 mm
Output shaft: 0.8 mm
Output shaft length: 4 mm
Voltage: 3 V 
Current: 17 mA (stall 120 mA) 
Frequency​​: 22000 RPM 
</code></pre>

<p>My question is if small DC motors of this type have enough torque to even make the robot start moving. I have been unable to find torque info on these kind of motors and I suspect the weight of the robot could be too much for them to handle. Do you know the typical torque of such motor? Is there another type of (cheap) motor more appropriate for this project?</p>

<p><img src=""http://i.stack.imgur.com/E7PhAm.jpg"" alt=""enter image description here""></p>
","mobile-robot motor wheeled-robot torque"
"7498","Measurement and physics model fusion","<p>I am combining two position measurements of a ball from two sensors in real time to obtain one triangulated position in x,y,z coordinates. As the data exchange of the measurements carries some latency, the data has to be extrapolated be able to obtain the current position. Due to extrapolation an error appears in the triangulated data.</p>

<p>I know that when the ball is in the air, the velocity of the ball should be constant in x and y directions and the velocity in the z direction should decay with g. The velocities in x and y however oscillate as function of time around a mean value which is the actual x respectively y velocity. The same goes for when I compute the acceleration in the z direction. It oscillates as function of time around g.</p>

<p>Given that I know how the ball should behave, i.e. that vx and vy should be constant and that the acceleration in the z direction should be z, how can I impose these conditions to better estimate the triangulated position? </p>
","sensor-fusion"
"7499","Can I use Bipolar stepper motor driver to drive Unipolar motor in Unipolar configuration?","<p>Can I use Bipolar stepper motor driver to drive Unipolar motor in Unipolar configuration ?</p>
","stepper-motor stepper-driver"
"7502","Control both Velocity and Position (Linear actuator)","<p>I am trying to control the velocity+position of a linear actuator.</p>

<p>At this moment I am able to control the position or the velocity. But I'm trying to control both. What the control has to do: Let the linear actuator drive to a position i.e. 0 to 100 cm with a constant velocity of 1cm/s.</p>

<p>I control the actuator using a PWM signal. And I measure the velocity and position using a position sensor on the shaft.</p>

<p>What kind of control is preferred, PID in cascade?
If so,what would the code look like.
Any other kind of control would function better?</p>

<p>Thanks in advance!</p>

<p>EDIT:
A more describing picture.
<img src=""http://i.stack.imgur.com/jwmYH.png"" alt=""enter image description here""></p>

<p>I want a Velocity controlled Position controller.
Hopefully this will make it clear</p>

<p>EDIT</p>

<p>My first try is with a trapezoid wave. Maybe there is an easy way without to much calculation power to change it a s-curbe. Then the accelartion/jerk will be alot smoother.
        I let the microcontroller calculate 3 different formulas afterwards it will calculate it using loop iteration. This way I can use one PID for the position. The parameters in the following code will fictional:</p>

<pre><code>    AccelerationLoops: 5                            //[Loops]
Velocity:      100                          //[mm/s]
DeltaPosition:     7.5                          //[mm]
Looptime:      5                            //[ms]
Loopfactor:        1000 / Looptime                  //[-]
VelocityLoop:      Velocity  /Loopfactor                //[mm/loop]
VelocityFactor:    VelocityLoop * .5 / AccelerationLoops        //[mm/loop]  (.5 found by integration)
Loops:         DeltaPosition / VelocityLoop / AccelartionLoops  //[Loops]

----Formula---
Formula1:      VelocityFactor * x^2
LastF1:        Last value of Formula1 Formula1(5)

Formula2:      VelocityLoop * x - LastF1

Formula3:      VelocityFactor * (Loops - x)^2 + DeltaPosition)

Using the parameters of above it will generate the following setpoint :
0   0,00
1   0,05
2   0,20
3   0,45
4   0,80
5   1,25
6   1,75
7   2,25
8   2,75
9   3,25
10  3,75
11  4,25
12  4,75
13  5,25
14  5,75
15  6,25
16  6,70
17  7,05
18  7,30
19  7,45
20  7,50
</code></pre>

<p>A big problem with the code above is that the amount of accelartion loops is a constant. It can not be changed except when you already know the amount of loops it will take.</p>

<p>I will be using two separate arduinos, they will be connected using a CAN-bus connection. Anyway, they won't communicate through it unless the load becomes too high. This will make Master/Slave impossible. Also the system has to be modular: adding another actuator to circuit won't be a problem. The actuator is speed controlled by using a PWM signal. The linear sensor will deliver a 0-10v signal which i will reduce to 0-5v by a simple voltage divider. The loop will be around 5 to 10 ms, will depend on the maximum looptime.</p>

<p>Arduino has a 10-bit(1023) ADC but use of oversampling I will probably try to increase it to 12-bit. To not decrease the reading speed I will decrease the prescaler of the ADC.</p>

<p>The PWM output is 8-bit(255), I am trying to find a way to further increase. Because I think 255 steps are too low for my application.</p>

<p>Because the Arduino has limit internal memory, pre calculating all the positions is impossible.</p>

<p>Thank you all for the help so far!</p>
","arduino pid microcontroller"
"7506","Converting a 2D array of bits to a connectivity map (Code Debugging)","<p>I'm working on an robot that would be able to navigate through a maze, avoid obstacles and identify some of the objects (Boxes in which it has to pot the balls)  in it. I have a monochromatic bitmap of the maze, that is supposed to be used in the robot navigation.</p>

<p>Up till now, I have converted/read the bitmap image of the maze into a 2D array of bits. Right now I am writing a code that should convert the 2D array (that represents the maze) into a connectivity map so that I could apply a path planning algorithm on it. Mr. @Chuck has helped me by providing a code in MATLAB. i have converted that code into C++, however the code isn't providing the right output. Kindly see the code and tell me what I am doing wrong.</p>

<p>I am sharing the link to the 2D array that has been made, the MATLAB code, and my code in C++ to convert the array into a connectivity map.</p>

<p><strong>Link to the 2D array:-</strong></p>

<p><a href=""https://drive.google.com/file/d/0BwUKS98DxycUZDZwTVYzY0lueFU/view?usp=sharing"" rel=""nofollow"">https://drive.google.com/file/d/0BwUKS98DxycUZDZwTVYzY0lueFU/view?usp=sharing</a></p>

<p><strong>MATLAB CODE:-</strong></p>

<pre><code>Map = load(map.mat);
nRows = size(Map,1);
nCols = size(Map,2);
mapSize = size(Map);
N = numel(Map);
Digraph = zeros(N, N);

for i = 1:nRows
  for j = 1:nCols
    currentPos = sub2ind(mapSize,i,j);
    % left neighbor, if it exists
    if (j-1)&gt; 0
      destPos = sub2ind (mapSize,i,j-1);
      Digraph(currentPos,destPos) = Map(currentPos)*Map(destPos);
    end
    % right neighbor, if it exists
    if (j+1)&lt;=nCols
      destPos = sub2ind (mapSize,i,j+1);
      Digraph(currentPos,destPos) = Map(currentPos)*Map(destPos);
    end
    % top neighbor, if it exists
    if (i-1)&gt; 0
      destPos = sub2ind (mapSize,i-1,j);
      Digraph(currentPos,destPos) = Map(currentPos)*Map(destPos);
    end
    % bottom neighbor, if it exists
    if (i+1)&lt;=nRows
      destPos = sub2ind (mapSize,i+1,j);
      Digraph(currentPos,destPos) = Map(currentPos)*Map(destPos);
    end
  end
end
</code></pre>

<p><strong>Code in C++:-</strong></p>

<pre><code>int **digraph = NULL;
digraph = new int *[6144];

for (int i = 0; i &lt; 6144; i++)
{
    digraph[i] = new int[6144];
}

for (j = 0; j &lt; 96; j++)
{
    for (z = 0; z &lt; 64; z++)
    {
        currentPos = sub2ind[j][z];
        digraph[currentPos][currentPos] = 0; //------NEW ADDITION-----------

    if ((z - 1) &gt;= 0)
        {
            destPos = sub2ind[j][z - 1];
            digraph[currentPos][destPos] = bitarray[j][z] * bitarray[j][z - 1];
        }

    if ((z + 1) &lt; 64)
        {
            destPos = sub2ind[j][z + 1];
            digraph[currentPos][destPos] = bitarray[j][z] * bitarray[j][z + 1];
        }

    if ((j - 1) &gt;= 0)
        {
            destPos = sub2ind[j - 1][z];
            digraph[currentPos][destPos] = bitarray[j][z] * bitarray[j - 1][z];
        }

    if ((j + 1) &lt; 96)
        {
            destPos = sub2ind[j + 1][z];
            digraph[currentPos][destPos] = bitarray[j][z] * bitarray[j + 1][z];
        }
    }

}

ofstream connectivityMap;
connectivityMap.open(""diGraph.txt"");

for (int l = 0; j &lt; 100; l++) // printing only 100 elements
{
    for (int k = 0; k &lt; 100; k++)
    {
        connectivityMap &lt;&lt; digraph[l][k] &lt;&lt; "" "";
    }
}
</code></pre>
","mobile-robot localization mapping planning"
"7507","Circuit Design and Simulation","<p>I want to design some circuits of my own. My area of expertise is in Computer Science Engineering. I have listed out the components which are essential in the circuit. I want a software which can be used to design and simulate circuits for real time projects. Please suggest me the best among them. Thank you.</p>

<p>@AkhilRajagopal</p>
","software electronics"
"7510","Improving Velocity estimation","<p>I have a sensor reduction model which gives me a velocity estimate of a suspension system(velocity 1) .</p>

<p>This suspension system estimate velocity is used to calculate another velocity(velocity 2) via a transfer function/plant model.</p>

<p>Can I use velocity 2 to improve my velocity estimate (velocity 1) through Kalman filtering or through some feedback system.??</p>

<p><img src=""http://i.stack.imgur.com/fF0tc.jpg"" alt=""enter image description here""></p>

<p>V1 is ""estimated"" using these two sensors.That is fed into a geroter pump (Fs in diagram) which pumps fluid to manupulate the damper viscous fluid thereby applying resistance to the forces applied to the car body. There is no problem did I have an velocity sensor on the spring.I could measure it accurately but now I only have an estimate. I am trying to make the estimate better.Assume I have a model/plant or transfer function already that gives me the V2 given a V1.</p>
","control sensors pid kalman-filter"
"7513","What are the frequencies used for within drones?","<p>What are these frequencies used for within the drone technology, and why these values?</p>

<ul>
<li>35 MHz</li>
<li>433 MHz</li>
<li>868 MHz</li>
<li>2.4 GHz</li>
<li>5.8 GHz</li>
</ul>
","quadcopter wireless radio-control"
"7517","Need help regarding EKF in MonoSLAM","<p>I am trying to understand the implementation of Extended Kalman Filter for <a href=""http://www.doc.ic.ac.uk/~ajd/Publications/davison_iccv2003.pdf"" rel=""nofollow"" title=""Paper describing MonoSLAM"">SLAM using a single, agile RGB camera.</a> </p>

<p>The vector describing the camera pose is 
$$
\begin{pmatrix}
r^W \\
q^W  \\
V^W \\
\omega^R \\
a^W \\
\alpha^R
\end{pmatrix}
$$</p>

<p>where:</p>

<ul>
<li>$r^W$ :   3D coordinates of camera w.r.t world</li>
<li>$q^W$ :   unit quaternion describing camera pose w.r.t world</li>
<li>$V^W$ :   linear velocity along three coordinate frames, w.r.t world</li>
<li>$\omega$ :   angular velocity w.r.t body frame of camera</li>
</ul>

<p>The feature vector set is described as 
$$
\begin{pmatrix}
y_1 \\
y_2  \\
\vdots \\
y_n
\end{pmatrix}
$$
where, each feature point is described using XYZ parameters.</p>

<p>For the EKF acting under an unknown linear and angular acceleration $[A^W,\psi^R] $ , the process model used for predicting the next state is:</p>

<p>$$
\begin{pmatrix}
r^W + V^W\Delta t + \frac{1}{2}\bigl(a^W + A^W\bigr)\Delta t^2 \\
q^W \bigotimes q^W\bigl(\omega^R\Delta t + \frac{1}{2}\bigl(\alpha^R + \psi^R\bigr)\Delta t^2\bigr)   \\
V^W + \bigl(a^W + A^W\bigr)\Delta t\\
\omega^R + \bigl(\alpha^R + \psi^R\bigr)\Delta t \\
a^W + A^W \\
\alpha^R + \psi^R
\end{pmatrix}
$$</p>

<hr>

<p>So far, I'm clear with the EKF steps. Post this prediction step, I'm not clear how to perform the measurement update of the system state.</p>

<p><a href=""http://www.doc.ic.ac.uk/~ajd/Scene/Release/monoslamtutorial.pdf"" rel=""nofollow"">From this slide</a>, I was under the impression that we need to initialize random depth particles between 0.5m to 5m from the camera. But, at this point, both the camera pose and the feature depth is unknown.</p>

<ul>
<li><p>I can understand running a particle filter for estimating feature
depth if camera pose is known. I tried to implement such a concept <a href=""https://github.com/agnivsen/LibMonoSLAM"" rel=""nofollow"">in this project: where I read the camera pose from a ground truth file</a> and keep triangulating the depth of features w.r.t world reference frame</p></li>
<li><p>I can also comprehend running a particle filter for estimating the
camera pose if feature depths are known.</p></li>
</ul>

<p>But both these parameters are unknown. How do I perform the measurement update?</p>

<p>I can understand narrowing down the active search region for feature matching based on the predicted next state of the camera. But after the features are matched using RANSAC (or any other algorithm), how do I find the updated camera pose? We are not estimating homography, are we?</p>

<p>If you have any idea regarding MonoSLAM (or RGB-D SLAM), please help me out with understanding the EKF steps.</p>

<hr>

<p>To be more specific: is there a homography estimation step in the algorithm? how do we project the epipolar line (inverse depth OR XYZ) in the next frame if we do not have any estimate of the camera motion?</p>
","slam ekf"
"7519","Structuring EKF to estimate pose and velocity with odometry inputs","<p>I have a differential drive robot for which I'm building an EKF localization system.  I would like to be able to estimate the state of the robot $\left[ x, y, \theta, v, \omega \right]$ where $x, y, \theta$ represent the pose of the robot in global coordinates, and $v, \omega$ are the translational and rotational velocities. Every mobile robot Kalman filter example I've seen uses these velocities as inputs to prediction phase, and does not provide a filtered estimate of them. </p>

<p><strong>Q:</strong> What is the best way to structure a filter so that I can estimate my velocities and use my measured odometry, gyroscope, and possibly accelerometers (adding $\dot{v}$ and $\dot{\omega}$ to my state) as inputs? </p>

<p><em>My intuition tells me to use a prediction step that is pure feedforward (i.e. just integrates the predicted velocities into the positions), and then have separate updates for odometry, gyro, and accelerometer, but I have never seen anyone do this before. Does this seem like a reasonable approach?</em></p>
","localization kalman-filter gyroscope odometry"
"7521","Calculate required motor torque through Harmonic Drive","<p>I have a term project which is controlling a two-link manipulator with harmonic drive installed at each joint.
To control, i used Computed control method to determine the torque needed for each joints based on the formula: 
 $$\tau_i =M(\theta)(\ddot{\theta_i}+K_d\dot{e}+K_pe)+V+G  $$<br>
To calculate the torque that each motor needs to produce through harmonic drive, i use: 
$$\tau_{motor} =(J_m+J_g)\rho\ddot{\theta_i}+\frac{\tau_i}{\rho\eta_g}$$
where:
 $\rho$ and $\eta_g$ are gear ratio and efficiency of the harmonic drive. $J_m$ and $J_g$ are the motor and gear inertia, respectively. </p>

<p>after these calculation, i can see the effect of harmonic drive in the system by comparing input torque from motor in the model with harmonic drive ($\tau_{motor}$) to that torque in the model without harmonic drive ($\tau_i$) </p>

<p>But my professor doesn't agree the formula $\tau_{motor}$ i used. He want me to include the stiffness $k$ of the harmonic drive.</p>

<p>This is <a href=""https://danangcity45.wordpress.com/2015/06/17/computed-torque-control-of-two-link-manipulator/"" rel=""nofollow"">what i have done</a></p>

<p>P/S: This model which consists of two-link manipulator+harmonic drive at each joint is built in MATLAB.  </p>

<p>Can anyone suggest me the formula about it? </p>

<p>Thank you so much.</p>
","actuator manipulator"
"7522","Highspeed with gearbox or low speed for brushless motor?","<p>I'm attempting to control a small vehicle at relatively slow (.5 m/s - 1 m/s) speeds, but with extreme accuracy (1mm). For the drive system, I'm considering using brushless motors as they have a much greater power / volume ratio than I am able to find with brushed motors, especially at this small size.</p>

<p>I will be using wheels between 1"" and 2"" diameter, so the RPM I will be looking for is between 150 - 500 RPM at max. This would suggest either driving the motors at a low speed directly, or driving them at a high speed and gearing them down. As I understand it, both setups will give high torques, as brushless motors decrease torque with speed. With brushed motors, it's quite obvious that a gearbox is necessary as otherwise there is no torque in the system, but here the choice isn't as clear, which is why I am asking.</p>

<p><strong>tl;dr</strong> Use brushless motors at high speed with gearbox or low speed (ungeared) for high torque / low speed / high precision application?</p>
","motor brushless-motor"
"7533","Which middleware for IPC and multi-threading in a autonomous robot?","<p>Aim: To use multi-threading and inter-process communication(IPC) when coding an autonomous robot.</p>

<p>Platform: Embedded Linux (Yocto)</p>

<p>Constraints : Limited CPU power.</p>

<p>We are building an Autonomous Underwater Vehicle, to compete in the RoboSub competition. This is the first time I am doing something like this. I intent to use a middleware like ROS, MIRA, YART, MOOS etc. The purpose of using one is that I want to modularise tasks, and divide the core components into subsystems, which should be run parallel(by multi-threading). But I have limited computational power (a dual core omap SoC), and the middleware, while robust should also be very efficient.</p>

<p>I need to use a middleware, because I don't want the program to be run on a single thread. My CPU has two cores, and it would be great if I could do some multi-threading to improve performance of the program. The middleware will provide for me the communication layer, so I don't have to worry about data races, or other problems associated with parallel processing. Also I have no prior experience writing multi-threaded programs, and so using parallel processing libraries directly would be difficult. Hence IMO, middlewares are excellent choices.</p>

<p>In your experience, which is the best one suited for the task. I don't really want to use ROS, because it will be having a lot of features, and I wont be using them. I am a computer science student(under graduate freshman, actually) and don't mind getting my hands dirty with one which has not that much features. That's true if only it will take less toll on the CPU.</p>
","ros communication underwater operating-systems"
"7534","On-board monocular odometry for quadcopter stabilization","<p>Has anyone done this with EKF/PID on a small microcontroller? Or know of code snippets to help implementing this?</p>
","quadcopter odometry stability"
"7535","Detecting the presence of a person in a room","<p>I am working on my first hobby project and I'm not very familiar with sensors yet. I am trying to build a system which detects the presence of a person in a small room with a single entrance/exit door.</p>

<p>The idea is when the first person enters the room, the lights turn on and any following person doesn't affect state of the lights. After the last person leaves, the lights should turn off. In a programmatic sense, the lights should turn on when present person count is greater than 0.</p>

<p>I have explored my options and found out that infrared sensors are usually used for this type of problem. What I am not sure is how to detect whether person has entered or left, so I would like to ask for some help with this.</p>
","sensors"
"7537","Inverting a transform (Reading J Craig's book on Robotics)","<p>From Introduction to Robotics by J.J. Craig, chapter 2, Page no. 36:</p>

<p>Could anyone explain how that equation was derived/formed? I am stuck on this page due to failing to understand where the equation came from. Thank you.
<img src=""http://i.stack.imgur.com/gy2wV.jpg"" alt=""""></p>
","design theory books"
"7542","Starter Looking For Advice","<p>I'm no professional. At 29 I just became seriously interested in robotics a few months ago and have been researching everything I can since. Now that I've come to understand how far robotics have truly come I have a desire to try to make my own.
Granted, I know nothing about coding or programming. I have no idea where to begin. And I know it'll probably, the first time at least, be something small rather than a huge life altering project.
Thus, if anyone could suggest to me good resources for a beginner I'd massively appreciate it.</p>
","beginner"
"7547","Lagging sensor data for PID","<p>Let's say a PID is implemented and the errors are calculated using the sensor data, but the sensor data lags by certain amount of time because of the overhead. And the lag time is smaller than the sampling period, how well does PID performs? What I am thinking is that PID will calculate errors based on past data, and use that to control. How will using a Kalman filter to estimate the actual sensor data help? </p>
","pid kalman-filter"
"7549","Question for those who have experience using stereo cameras/module (e.g. ZED, DUO M, Bumblebee, etc.)","<p>This is a question for those of you who have experience using stereo cameras/modules like the ZED, DUO M, Bumblebee cameras, etc. (not TOF cameras). I can't find any sample disparity outputs out there on the internet, and I can't find any information on how they perform. Basically here are a few things I'd like to know to those of you who used any of the cameras mentioned above (and others)</p>

<ol>
<li>What resolution and no. of disparities did you work with? </li>
<li>How was the framerate? </li>
<li>On what hardware?</li>
<li>Did the camera have an ASIC of some sort to produce the disparity maps, or did it require a host?</li>
<li>How was the quality?</li>
</ol>

<p>For those who used the ZED camera, there is a <a href=""http://youtube.com/watch?v=P-QEnnMHYt0"" rel=""nofollow"">promotional video on youtube</a>. Are the disparity maps really that good?</p>
","cameras stereo-vision"
"7552","Pull-down resistor for inter-chip and sensor-to-chip communication","<p>I understand the concept of using a pull-up/pull-down resistor when implementing a button/switch with Arduino to avoid a floating state, and in fact I have implemented this quite often.</p>

<p>But I am not too sure if a pull-down resistor is necessary in chip-chip or chip-sensor communication.</p>

<p>I am connecting a coin acceptor to the Arduino (common ground). The coin acceptor's output pin gives a short pulse each time there is a coin inserted. So far I am connecting the output pin of the coin acceptor directly to an Arduino pin and it works without any problem. Is a pull-down resistor (on this line) usually required as precaution in this case?</p>

<p>Also I have the same question when connecting 2 pins of 2 separate Arduino's (also common ground) so that one Arduino can read pulses from the other.</p>

<p>Thanks in advance for any experience shared!</p>

<p>Dave</p>
","arduino"
"7556","Mini Recorder from RC Heli Parts","<p>I had a RC Helicopter (with video,picture, and audio taking capabilities) that recently ""died"" (unrelated to short circuit). The reciever board short circuted, but the board that sent data to micro-sd card and had camera+mic was fine. I can access the data on the micro-sd card through the circuit, with a USB cable. The reciever board sent data via a 4 wire bundle to the camera board to make it take pictures/record audio. Is there any way to still do this from my computer (from the USB), and turn it into a mini spy camera? (Not remotely, jst through a cable)</p>

<p>I got this heli a while back so I don't have the heli number but the camera board number is <code>TX6473 R1</code>, and the reciever board number is <code>3319B rev.a</code></p>

<p><strong>Reciever Board Image</strong>
<img src=""http://i.stack.imgur.com/8xeKS.jpg"" alt=""Reciever Board Image"">
<strong>Camera/Data Board Image</strong>
<img src=""http://i.stack.imgur.com/eUTFN.jpg"" alt=""Camera/Data Board Image""></p>
","control cameras circuit"
"7557","Troubleshooting Xbox Kinect 360","<p>I recently got <strong>libfreenect</strong> running on my mac and was able to test out <code>freenect-glpclview</code> which uses some of the 3D capabilities of the depth sensor.</p>

<p>I noticed that the Kinect would only respond / pick up movement that happened within a range of about 3-6 inches in front of the sensor.</p>

<p>I thought this may be because the lights where on so I turned them off. It seemed to get a little better but it still only ""works"" if something is block the sensor almost completely.</p>

<p>Does anyone know if this is something that can be solved? I know it's an old sensor but I got it for $20 so I could do some prototyping with it.</p>

<p>Notes:</p>

<ul>
<li>laser project is ON</li>
<li>light starts out blinking then goes solid green</li>
<li>when not level light goes red</li>
<li>RGB camera works but is a little choppy and sometimes shows tears in the picture.</li>
</ul>

<p>freenect-glcplview output (snippet):</p>

<pre><code>[Stream 80] Expected 1908 data bytes, but got 948
[Stream 80] Expected 1908 data bytes, but got 948
[Stream 80] Expected 1908 data bytes, but got 948
[Stream 80] Expected 1908 data bytes, but got 948
[Stream 80] Expected 1908 data bytes, but got 948
[Stream 80] Expected 1908 data bytes, but got 948
[Stream 70] Expected 1748 data bytes, but got 948
[Stream 70] Expected max 1748 data bytes, but got 1908. Dropping...
[Stream 70] Expected max 1748 data bytes, but got 1908. Dropping...
[Stream 80] Expected 1908 data bytes, but got 948
[Stream 80] Expected 1908 data bytes, but got 948
[Stream 80] Expected 1908 data bytes, but got 948
[Stream 80] Expected 1908 data bytes, but got 948
[Stream 80] Expected 1908 data bytes, but got 948
[Stream 80] Expected 1908 data bytes, but got 948
[Stream 80] Expected 1908 data bytes, but got 948
[Stream 80] Expected 1908 data bytes, but got 948
[Stream 80] Expected 1908 data bytes, but got 948
[Stream 80] Expected 1908 data bytes, but got 948
[Stream 80] Expected 1908 data bytes, but got 948
[Stream 80] Expected 1908 data bytes, but got 948
[Stream 80] Expected 1908 data bytes, but got 948
</code></pre>

<p>freenect-regview output (snippet)</p>

<pre><code>[Stream 70] Invalid magic 2dc5
[Stream 70] Invalid magic aaf5
[Stream 70] Invalid magic dddb
[Stream 70] Invalid magic 9272
[Stream 70] Invalid magic 9873
[Stream 70] Invalid magic 9b8b
[Stream 70] Invalid magic 59eb
[Stream 70] Invalid magic 88f1
[Stream 70] Invalid magic 75ee
[Stream 70] Invalid magic ffff
[Stream 80] Expected 1908 data bytes, but got 948
[Stream 80] Expected 1908 data bytes, but got 948
[Stream 80] Expected 1908 data bytes, but got 948
[Stream 80] Lost 1 packets
[Stream 80] Lost 15244 total packets in 514 frames (29.657587 lppf)
[Stream 80] Expected 1908 data bytes, but got 948
[Stream 80] Expected 1908 data bytes, but got 948
[Stream 80] Expected 1908 data bytes, but got 948
[Stream 80] Expected 1908 data bytes, but got 948
[Stream 80] Expected 1908 data bytes, but got 948
[Stream 80] Invalid magic 3b46
[Stream 80] Lost 1 packets
</code></pre>

<p>Found this which gives me the idea that this may be a USB issue: <a href=""https://github.com/OpenKinect/libfreenect/issues/84"" rel=""nofollow"">Regular receipt of undersized packet</a>.</p>
","kinect"
"7558","Problems about Complementary Filter IMU tuning","<p>I'm developing a project consists of an IMU controlled by Arduino through which you can send via a radio module, the data to the PC of the three Euler angles and raw data from the sensors.
For filtering I used the code made available by SparkFun: Razor AHRS 9 dof</p>

<p><a href=""https://github.com/ptrbrtz/razor-9dof-ahrs/tree/master/Arduino/Razor_AHRS"" rel=""nofollow"">https://github.com/ptrbrtz/razor-9dof-ahrs/tree/master/Arduino/Razor_AHRS</a></p>

<p>The code does not provide radio transmissions and is tuned for 50 Hz sampling rate, in fact its parameters are:</p>

<pre><code>// DCM parameters
#define Kp_ROLLPITCH (0.02f)
#define Ki_ROLLPITCH (0.00002f)
#define Kp_YAW (1.2f)
#define Ki_YAW (0.00002f)
</code></pre>

<p>in this project data is read every 20ms (50Hz) and records of the sensors are set to the accelerometer odr 50hz and 25 bandwidth. with the gyroscope 50 Hz odr.
In my project I used a gyroscope different, namely that I used L3G4200D frequency odr starting at 100Hz, I set then registers with the 100Hz. My global data rate is 33Hz max, beacouse the use of a radio, i read the complete date with a frequency of 33Hz.
How can i tune the <code>Ki</code> and <code>Kp</code> of my setup? the <code>Kp</code> is the period, I have to consider the frequency odr that I set to register in the individual sensors or i have to set the global system sample rate limited to 33Hz by the radio transmission?</p>
","arduino imu gyroscope sensor-fusion"
"7560","Digital Controller Design for System with variable sample time","<p>Basically I got system with a sensor and an output. I want to apply a digital implemented feedback controller. The problem in this setup is the sensor. The specifications of the module says that the sampletime of the sensor does change in wide range, depending on the usecase; from 1.3 second to 10 second. But it stays constant until the system is disabled.</p>

<p>My first approach was tuning a digital PID-Controller for the longest sampletime. This works fine. Even if I change the sampletime to the shortest the system stays stable, which was expected because I'm still in ROC.
The problem now is that the system's response is pretty slow.</p>

<p>If I design the controller for my fastest samplingrate the results are satisfying but become instable for the slowest samplerate, which can be explained again by the ROC</p>

<p>I could use some kind of adaptive predefined gains which I change depending on the samplerate but I was wondering if there are  control strategies which are able to handle the sampletime changes?</p>

<p><strong>EDIT:</strong> <br>
To give a better overview I will add some details: <br>
I'm talking about a heating system which heats with radiation. As a sensor I use a pyrometer module with a samplingrate of up 1kHz. The problem is, that the pyrometer is not able to produce reasonable readings whenever the radiator is turned on. (Yes there are other alternatives to the pyrometer, but they start at $50k and are too expensive). The radiator has to be pulsed to operate it. So to maintain a decent heat up time and steady-state temperature the ""duty-cycle"" has to be at a decent rate(target is 95%). The minimum ""off-time"" of the radiator is 0.2 seconds before the measured values are reasonable. So at the end my sensor got an effective sampletime of 1-10seconds (by varying the duty cycle).</p>

<p>The hardware is hard too change, radiator and sensor have been evaluted for months right now. Therefore I try to improve the results by ""just"" changing the control algorithm.</p>
","control"
"7564","How to localise a underwater robot?","<p>I am building an autonomous underwater robot. It will be used in swimming pools. It should be capable of running in any normal sized pool, not just the pool in which I test. So I cannot rely on a particular design or feature. It has to know it's position in the pool, either with respect to the initial position or with respect to the pool. I have a IMU, which is a <a href=""https://www.pololu.com/product/2468"" rel=""nofollow"">Pololu MiniIMU</a> but finding the displacement with an IMU is a near impossible task. </p>

<p>What sensor can I use for this task? It should not be very expensive. (below 200$)</p>

<p>Tank size: 25x20x2.5 meters</p>
","sensors localization sensor-fusion underwater"
"7565","How to determine the trajectory reference on the real robot trajectory tracking","<p>I know that we can use some algorithms like LQR, MPC, or even PID to make the robot follows the trajectory references. In the simulation like MATLAB, I usually specify the trajectory reference by a function. Let say, given a sequence of points generated by a path planning algorithm, then I want to do a real experiment of trajectory tracking over those sequence of points. My question is:
- How to specify the errors towards the path in real situation. My impression is the generated path by path planning algorithm is uncertain due to the error of the robot sensing. And unlike the line following robot which has a real physical line for the reference, the generated path from path planning is virtual, e.g. it does not exist in the real world. I am really confused about these matter.</p>
","mobile-robot control"
"7570","Homogenous Transformation Matrix for DH parameters","<p>I'm studying Introduction to robotic and found there is different equations to determine the position and orientation for the end effector of a robot using DH parameters transformation matrix, they are :</p>

<p><img src=""http://i.stack.imgur.com/uVJMt.gif"" alt=""1-""></p>

<p><img src=""http://i.stack.imgur.com/Z1hv9.gif"" alt=""enter image description here""></p>

<p><img src=""http://i.stack.imgur.com/voRGM.gif"" alt=""enter image description here""></p>

<p>Example: Puma 560, All joints are revolute</p>

<p>Forward Kinematics:</p>

<p>Given :The manipulator geometrical parameters.</p>

<p>Specify: The position and orientation of manipulator.</p>

<p>Solution:</p>

<p><img src=""http://i.stack.imgur.com/1sRQj.png"" alt=""enter image description here""></p>

<p>For Step 4:</p>

<p><img src=""http://i.stack.imgur.com/z8Ha7.jpg"" alt=""enter image description here""></p>

<p>for step 3 :Here I'm confused </p>

<p>Here we should calculate the transformation matrix  for each link  and then multiply them to get the position and orientation for the end effector.</p>

<p>I've seen different articles using one of these equations when they get to this step for the same robot(puma 560)</p>

<p>What is the difference between them? Will the result be different? Which one should I use when calculating the position and orientation?</p>
","dh-parameters"
"7575","DH-Parameters for Forward Kinematics for Translatory Motion only","<p>I am fairly new to the DH-transformation and I have difficulties to understand how it works. Why are not all coordinates (X+Y+Z) incorporated into the parameters? It seems to me that at least one information is useless/goes to the trash, since there is only a, d (translatory information) and alpha, theta(rotatory information). </p>

<p>Example: 
The transition between two coordinate systems with identical orientation(alpha=0, theta=0) but with different coordinates(x1!=x2, y1!=y2, z1!=z2). 
DH only makes use of a maximum of two of these information.</p>

<p>Please enlighten me! </p>

<p>Greetings</p>

<p>:EDIT: </p>

<p>To clarify which part of the DH-Transform I don't understand, here is an example. </p>

<p>Imagine a CNC-Mill(COS1) on a stand(COS0) without any variable length(=no motion) between COS0-COS1. For some reason I need to incorporate the transformation from COS0-COS1(=T0-1) into the forward transformation of my CNC-Mill. 
<img src=""http://i.stack.imgur.com/csRPG.png"" alt=""DH1""></p>

<p>DH-Parameters for T0-1 would be a=5mm, alpha=90°, d=2mm and theta=90°. Assuming this is correct, the dX=10mm information is lost during this process?
If I recreate the relation between COS0 and COS1 according to the DH-Parameters, I end up like this: 
<img src=""http://i.stack.imgur.com/EAbhr.png"" alt=""DH2""></p>

<p>As far as I understand, on non parallel axis the information is not lost because the measurement of a/d would be diagonal, therefore include either dX/dY, dX/dZ or dY/dZ(pythagorean theorem) in one parameter. </p>

<p>Where is the flaw in my logic?</p>
","forward-kinematics dh-parameters"
"7578","Orientation parameter for quadcopter with madgwick fusion algorithm","<p>I recently decided to build a quadricopter from scratch using Arduino and now I'm faced with an orientation estimation problem.</p>

<p>I bought a cheap 10DOF sensor with 3 axis magnetometer, 3 axis accelerometer, 3 axis gyro and a barometer and the complementary filter that I use to get orientation returns usable but noisy values.</p>

<p>I tried the Madgwick fusion filter too, but it returns unstable values that diverges from the ones I get with complementary filter. Given that the Madgwick filter implementation is correct, I pass acceleration values measured in Gs, gyro values measured in rps (radians per second) and Magnetometer values measured in uT, while sampling time is the same of my loop cycle. Is there anything I have missed?</p>

<p>Is there any advantage using Kalman filter?</p>

<p><strong>EDIT1:</strong></p>

<p>My problem was due to an wrong choice of sampling time and now seems to work, but convergence is very very slow (i.e. it takes about 3 seconds to reach the right value after a quick flip of the IMU). Rising value of Kp adds to much noise. I also tried to repeat filter update step more than once per cycle but it requires too much time exceeding the sampling time.</p>

<p>Here some graphs, from top to bottom Complementary filter, Madgwick filter and Madgwick filter with high Kp:</p>

<p><img src=""http://i.stack.imgur.com/Rfyh4.png"" alt=""Complementary filter: fast but a bit noisy""></p>

<p><img src=""http://i.stack.imgur.com/85NdG.png"" alt=""Madgwick filter""></p>

<p><img src=""http://i.stack.imgur.com/MPTdg.png"" alt=""Madgwick filter with high kp value""></p>

<p><strong>EDIT2:</strong></p>

<p>Different values probably are caused by cable plug and unplug. Anyway raw data example from my sensor can be downloaded <a href=""https://drive.google.com/file/d/0B59xb_cEOCa7NUl2OS1UME16SUU/view?usp=sharing"" rel=""nofollow"">here</a></p>
","arduino quadcopter kalman-filter imu"
"7580","Sensors' field of view in car driving","<p>I want to develop an autonomous driving RC car. For detecting obstacles, I plan to mount 3-5 ultrasonic sensors in the front and in the back the car. What is the  minimum necessary combined field of view of the sensors so the car never hits an obstacle? I.e. what is the minimum angle of detection of the combined sensors the car should have to detect any obstacle in its path?</p>

<p>Some data about the car: (I don't know whether all the data is relevant)</p>

<ul>
<li>Separation between right and left wheel : 19,5 cm</li>
<li>Wheelbase (distance between the front and the back wheels):  31,3cm</li>
<li>Steering axle: front.</li>
<li>Maximum angle of steering: around 30 degrees. The car uses <a href=""https://en.wikipedia.org/wiki/Ackermann_steering_geometry"" rel=""nofollow"">Ackermann steering</a></li>
</ul>
","mobile-robot sensors wheeled-robot"
"7588","Is Lego Mindstorm a good start?","<p>I would like to start experimenting with Robots. Is <a href=""http://www.lego.com/nl-be/mindstorms/?domainredir=mindstorms.lego.com&amp;showlanguageselector=true"" rel=""nofollow"">Lego Mindstorm</a> a good start? Should I consider other platforms?</p>
","platform"
"7592","IMU rotate about one axis, other two angles change too","<p>I am trying to use Invensense's MPU9250. I am using provided library to read euler angle. When the IMU rotates about one axis, angles about other two axes change too. What could be potential cause to it? </p>
","imu"
"7595","STM32_OC_Timing and IRQHandler","<p>I created a program to simple time base delay (in seconds). I have problem:</p>

<p>How to read a interrupt flag from channel 1 etc.?</p>

<p>When I use <code>if(__HAL_TIM_GET_FLAG(&amp;htim2, TIM_FLAG_CC1) != RESET)</code> an error occurs.</p>

<p>When the interrupt occurred , uC should clear flag and set Blue LED in Discovery Board.</p>

<p>Here is my program:</p>

<p><strong>Main.c</strong></p>

<pre><code>/* Includes 
#include ""stm32f3xx_hal.h""


/* Private variables &lt;br /&gt;
**TIM_HandleTypeDef htim2;**



/* Private function prototypes &lt;br /&gt;
void SystemClock_Config(void);&lt;br /&gt;
static void MX_GPIO_Init(void);&lt;br /&gt;
static void MX_TIM2_Init(void);&lt;br /&gt;



int main(void)
{


  /* MCU Configuration----------------------------------------------------------*/&lt;br /&gt;

  /* Reset of all peripherals, Initializes the Flash interface and the Systick. */&lt;br /&gt;
 HAL_Init();

  /* Configure the system clock */&lt;br /&gt;
  SystemClock_Config();&lt;br /&gt;

  /* Initialize all configured peripherals */&lt;br /&gt;
  MX_GPIO_Init();&lt;br /&gt;
  MX_TIM2_Init();&lt;br /&gt;


  /* Infinite loop */

  while (1)
  {
      HAL_GPIO_WritePin(GPIOE,GPIO_PIN_11,GPIO_PIN_RESET);
  }
}

/** System Clock Configuration*/

void SystemClock_Config(void)
{

  RCC_OscInitTypeDef RCC_OscInitStruct;&lt;br /&gt;
  RCC_ClkInitTypeDef RCC_ClkInitStruct;&lt;br /&gt;

  RCC_OscInitStruct.OscillatorType = RCC_OSCILLATORTYPE_HSE;&lt;br /&gt;
  RCC_OscInitStruct.HSEState = RCC_HSE_ON;&lt;br /&gt;
  RCC_OscInitStruct.HSEPredivValue = RCC_HSE_PREDIV_DIV1;&lt;br /&gt;
  RCC_OscInitStruct.PLL.PLLState = RCC_PLL_ON;&lt;br /&gt;
  RCC_OscInitStruct.PLL.PLLSource = RCC_PLLSOURCE_HSE;&lt;br /&gt;
  RCC_OscInitStruct.PLL.PLLMUL = RCC_PLL_MUL9;&lt;br /&gt;
  HAL_RCC_OscConfig(&amp;RCC_OscInitStruct);&lt;br /&gt;

  RCC_ClkInitStruct.ClockType = RCC_CLOCKTYPE_SYSCLK|RCC_CLOCKTYPE_PCLK1;&lt;br /&gt;
  RCC_ClkInitStruct.SYSCLKSource = RCC_SYSCLKSOURCE_PLLCLK;&lt;br /&gt;
  RCC_ClkInitStruct.AHBCLKDivider = RCC_SYSCLK_DIV1;&lt;br /&gt;
  RCC_ClkInitStruct.APB1CLKDivider = RCC_HCLK_DIV2;&lt;br /&gt;
  RCC_ClkInitStruct.APB2CLKDivider = RCC_HCLK_DIV1;&lt;br /&gt;
  HAL_RCC_ClockConfig(&amp;RCC_ClkInitStruct, FLASH_LATENCY_2);&lt;br /&gt;

  HAL_SYSTICK_Config(HAL_RCC_GetHCLKFreq()/1000);&lt;br /&gt;

  HAL_SYSTICK_CLKSourceConfig(SYSTICK_CLKSOURCE_HCLK);&lt;br /&gt;

}

/* TIM2 init function */&lt;br /&gt;
void MX_TIM2_Init(void)
{

  TIM_ClockConfigTypeDef sClockSourceConfig;&lt;br /&gt;
  TIM_MasterConfigTypeDef sMasterConfig;&lt;br /&gt;
  TIM_OC_InitTypeDef sConfigOC;&lt;br /&gt;

  htim2.Instance = TIM2;&lt;br /&gt;
  htim2.Init.Prescaler = 7199; //72Mhz/7200 &lt;br /&gt;
  htim2.Init.CounterMode = TIM_COUNTERMODE_UP;&lt;br /&gt;
  htim2.Init.Period = 65535;&lt;br /&gt;
  htim2.Init.ClockDivision = TIM_CLOCKDIVISION_DIV1;&lt;br /&gt;
  HAL_TIM_Base_Init(&amp;htim2);&lt;br /&gt;

  sClockSourceConfig.ClockSource = TIM_CLOCKSOURCE_INTERNAL;&lt;br /&gt;
  HAL_TIM_ConfigClockSource(&amp;htim2, &amp;sClockSourceConfig);&lt;br /&gt;

  HAL_TIM_OC_Init(&amp;htim2);&lt;br /&gt;

  sMasterConfig.MasterOutputTrigger = TIM_TRGO_RESET;&lt;br /&gt;
  sMasterConfig.MasterSlaveMode = TIM_MASTERSLAVEMODE_DISABLE;&lt;br /&gt;
  HAL_TIMEx_MasterConfigSynchronization(&amp;htim2, &amp;sMasterConfig);&lt;br /&gt;

  sConfigOC.OCMode = TIM_OCMODE_TIMING;&lt;br /&gt;
  sConfigOC.Pulse = 20000; //0.0001[s] * 20000 = 2 [s] DELAY &lt;br /&gt; 
  sConfigOC.OCPolarity = TIM_OCPOLARITY_HIGH;&lt;br /&gt;
  sConfigOC.OCFastMode = TIM_OCFAST_DISABLE;&lt;br /&gt;
  HAL_TIM_OC_ConfigChannel(&amp;htim2, &amp;sConfigOC, TIM_CHANNEL_1);&lt;br /&gt;

  sConfigOC.OCMode = TIM_OCMODE_TIMING;&lt;br /&gt;
   sConfigOC.Pulse = 30000;&lt;br /&gt;
   sConfigOC.OCPolarity = TIM_OCPOLARITY_HIGH;&lt;br /&gt;
   sConfigOC.OCFastMode = TIM_OCFAST_DISABLE;&lt;br /&gt;
   HAL_TIM_OC_ConfigChannel(&amp;htim2, &amp;sConfigOC, TIM_CHANNEL_2);&lt;br /&gt;
  HAL_TIM_Base_Start_IT(&amp;htim2);&lt;br /&gt;

   HAL_TIM_OC_Start_IT(&amp;htim2,TIM_CHANNEL_1 );&lt;br /&gt;
   //HAL_TIM_OC_Start_IT(&amp;htim2,TIM_CHANNEL_2 );&lt;br /&gt;
}

/** Configure pins as &lt;br /&gt;
        * Analog &lt;br /&gt;
        * Input &lt;br /&gt;
        * Output&lt;br /&gt;
        * EVENT_OUT&lt;br /&gt;
        * EXTI&lt;br /&gt;
     PC9   ------&gt; I2S_CKIN&lt;br /&gt;
*/
void MX_GPIO_Init(void)&lt;br /&gt;
{
&lt;br /&gt;
  GPIO_InitTypeDef GPIO_InitStruct;&lt;br /&gt;

  /* GPIO Ports Clock Enable */&lt;br /&gt;
  __GPIOF_CLK_ENABLE();&lt;br /&gt;
  __GPIOC_CLK_ENABLE();&lt;br /&gt;
  __GPIOE_CLK_ENABLE();&lt;br /&gt;

  /*Configure GPIO pin : PC9 */&lt;br /&gt;
  GPIO_InitStruct.Pin = GPIO_PIN_9;&lt;br /&gt;
  GPIO_InitStruct.Mode = GPIO_MODE_AF_PP;&lt;br /&gt;
  GPIO_InitStruct.Pull = GPIO_NOPULL;&lt;br /&gt;
  GPIO_InitStruct.Speed = GPIO_SPEED_HIGH;&lt;br /&gt;
  GPIO_InitStruct.Alternate = GPIO_AF5_SPI1;&lt;br /&gt;
  HAL_GPIO_Init(GPIOC, &amp;GPIO_InitStruct);&lt;br /&gt;
&lt;br /&gt;
 /*
  * Configure GPIO pin : PE8 BLUE LED
  */
&lt;br /&gt;
  GPIO_InitStruct.Pin=GPIO_PIN_8;&lt;br /&gt;
  GPIO_InitStruct.Mode=GPIO_MODE_OUTPUT_PP;&lt;br /&gt;
  GPIO_InitStruct.Pull=GPIO_NOPULL;&lt;br /&gt;
  GPIO_InitStruct.Speed=GPIO_SPEED_HIGH;&lt;br /&gt;
  HAL_GPIO_Init(GPIOE,&amp;GPIO_InitStruct);&lt;br /&gt;

  GPIO_InitStruct.Pin=GPIO_PIN_12;&lt;br /&gt;
  GPIO_InitStruct.Mode=GPIO_MODE_OUTPUT_PP;&lt;br /&gt;
  GPIO_InitStruct.Pull=GPIO_NOPULL;&lt;br /&gt;
  GPIO_InitStruct.Speed=GPIO_SPEED_HIGH;&lt;br /&gt;
  HAL_GPIO_Init(GPIOE,&amp;GPIO_InitStruct);&lt;br /&gt;
&lt;br /&gt;
/*
 * COnfigure GPIO pin : PE11 GREEN LED
 */
&lt;br /&gt;
  GPIO_InitStruct.Pin=GPIO_PIN_11;&lt;br /&gt;
    GPIO_InitStruct.Mode=GPIO_MODE_OUTPUT_PP;&lt;br /&gt;
    GPIO_InitStruct.Pull=GPIO_NOPULL;&lt;br /&gt;
    GPIO_InitStruct.Speed=GPIO_SPEED_HIGH;&lt;br /&gt;
    HAL_GPIO_Init(GPIOE,&amp;GPIO_InitStruct);
}

low level implementation :&lt;br /&gt;
void HAL_TIM_Base_MspInit(TIM_HandleTypeDef* htim_base)&lt;br /&gt;
{
&lt;br /&gt;
  if(htim_base-&gt;Instance==TIM2)&lt;br /&gt;
  {
 &lt;br /&gt;
    /* Peripheral clock enable */&lt;br /&gt;
    __TIM2_CLK_ENABLE();&lt;br /&gt;
  /* Peripheral interrupt init*/&lt;br /&gt;
    HAL_NVIC_SetPriority(TIM2_IRQn, 0, 0);&lt;br /&gt;
    HAL_NVIC_EnableIRQ(TIM2_IRQn);&lt;br /&gt;

  }

}

 void TIM2_IRQHandler(void)
{
&lt;br /&gt;
  /* USER CODE BEGIN TIM2_IRQn 0 */&lt;br /&gt;
     HAL_GPIO_WritePin(GPIOE,GPIO_PIN_8,GPIO_PIN_SET);&lt;br /&gt;

    // HAL_GPIO_TogglePin(GPIOE,GPIO_PIN_12);&lt;br /&gt;
  HAL_TIM_IRQHandler(&amp;htim2);&lt;br /&gt; //THis function is implemented by StmCubeMX , WHAT IS THIS?
}
</code></pre>

<p>So how should my <code>TIM2_IRQHandler</code> look like? Each channel generate delay in +1 sec. When I am debugging this program, when LED is set the period is equal to 1s (time for set LED). </p>
","microcontroller"
"7598","Extend robotic arm with wrist rotation","<p>I got an <a href=""http://www.owirobots.com/store/catalog/robotic-arm-kits-and-accessories/owi-535pc-robotic-arm-kit-with-usb-pc-interface-138.html"" rel=""nofollow"">OWI Robotic arm</a>, but was slightly disappointed at it having only horizontal position for gripper. What would be the easiest way to extend with gripper/wrist rotation, i.e. 6th degree of freedom? </p>

<p><img src=""http://i.stack.imgur.com/9uGgf.jpg"" alt=""OWI Robotic arm""></p>
","robotic-arm"
"7600","KUKA Robotics API IDE","<p>I've got Robotics API library, demo-program and a robot. I want to develop app for it. The best solution is offline development on some kind of simulator. I'm completely new in such tasks - is there any IDE for this? Or a way do deliver byte-code to machine? Thanks in advance!</p>
","robotic-arm dynamic-programming"
"7607","Can ESC be programmed to run full throttle only on one side of a quadcopter?","<p>Can ESC in quads be programmed in such a way that only one side has throttle and no throttle at all on the other? This would cause the quad to flip I suppose? </p>

<p>With that, is there a way we can program the controller to like trigger a switch when we want the quad to flip? Because I was thinking of doing a waterproof quad. So initially, it flies in the air normally with the 4 channel, and then I set it to float on water. After that, I was thinking of maybe triggering a switch on the controller so that this time it's just going to flip and nothing else. After it flips, I would trigger the switch back to normal operation. Is that possible?</p>
","quadcopter"
"7612","Raspberry Pi Hexapod 18DOF, Best servo control board?","<p>Recently I've bought a <a href=""http://www.ebay.co.uk/itm/231583577928"" rel=""nofollow"">hexapod kit</a> and 18 TowerPro MG995 servos.</p>

<p>My objective is to apply also the Pi camera, sensors and perhaps a claw...
So I've been researching and I haven't found a clear answer when comes to the servo control board.</p>

<p>Which servo controller board shall I choose to complete my project?</p>
","mobile-robot raspberry-pi servomotor rcservo hexapod"
"7613","How to efficiently do 3D mapping of an area on a MAV?","<p>I have been researching on a cost-effective way to scan an area on a MAV (exploraton) and later use it for CAD/civil purposes(use the point cloud data for CAD) but the major sensors available have their own problems.</p>

<p><strong>kinect</strong> - can't use outside,high computation power<br>
<strong>stereo</strong> - high computation power,somewhat expensive<br>
<strong>lidar</strong> - very expensive + not real time + heavy</p>

<p>I need a system(on the MAV/quadrotor) that can work over wifi/wireless, can scan outdoors , not very expensive and that gives data real-time.Please suggest a system that can be as close to the above requirements.</p>

<p>Also can stereo be operated over wifi? </p>
","kinect mapping stereo-vision 3d-reconstruction"
"7615","Problem with acceleration sensor","<p>I’m using the BMA020 (<a href=""http://www.elv.de/3-achsen-beschleunigungssensor-3d-bs-komplettbausatz.html"" rel=""nofollow"">from ELV</a>) with my Arduino Mega2560 and trying to read acceleration values that doesn’t confuse me.
First I connected the sensor in SPI-4 mode. Means</p>

<p>CSB &lt;-> PB0 (SS)</p>

<p>SCK &lt;-> PB1 (SCK)</p>

<p>SDI &lt;-> PB2 (MOSI)</p>

<p>SDO &lt;-> PB3 (MISO)</p>

<p>Also GND and UIN are connected with the GND and 5V Pins of the Arduino board.</p>

<p>Here is the self-written code I use</p>

<pre><code>#include &lt;avr/io.h&gt;
#include &lt;util/delay.h&gt;

#define sensor1     0
typedef int int10_t;

int TBM(uint8_t high, uint8_t low)
{   
    int buffer = 0;
    if(high &amp; (1&lt;&lt;7)) {
        uint8_t high_new = (high &amp; 0x7F);
        buffer = (high_new&lt;&lt;2) | (low&gt;&gt;6);
        buffer = buffer - 512;
    }
    else
        buffer = (high&lt;&lt;2) | (low&gt;&gt;6);


    return buffer;
}

void InitSPI(void);
void AccSensConfig(void);
void WriteByteSPI(uint8_t addr, uint8_t Data, int sensor_select);
uint8_t ReadByteSPI(int8_t addr, int sensor_select);
void Read_all_acceleration(int10_t  *acc_x, int10_t *acc_y, int10_t *acc_z, int sensor_select);


int main(void)
{
    int10_t S1_x_acc = 0, S1_y_acc = 0, S1_z_acc = 0;
    InitSPI();
    AccSensConfig();
    while(1) {
        Read_all_acceleration(&amp;S1_x_acc, &amp;S1_y_acc, &amp;S1_z_acc, sensor1);
    }
}

void InitSPI(void) {

    DDRB |= (1&lt;&lt;DDB2)|(1&lt;&lt;DDB1)|(1&lt;&lt;DDB0);

    PORTB |= (1&lt;&lt;PB0);

    SPCR |= (1&lt;&lt;SPE);
    SPCR |= (1&lt;&lt;MSTR);  
    SPCR |= (0&lt;&lt;SPR0) | (1&lt;&lt;SPR1);  
    SPCR |= (1&lt;&lt;CPOL) | (1&lt;&lt;CPHA);  
}

void AccSensConfig(void) {

    WriteByteSPI(0x0A, 0x02, sensor1);
    _delay_ms(100);

    WriteByteSPI(0x15,0x80,sensor1);    //nur SPI4 einstellen
}

void WriteByteSPI(uint8_t addr, uint8_t Data, int sensor_select) {

    PORTB &amp;= ~(1&lt;&lt;sensor_select);   
    SPDR = addr;                    
    while(!(SPSR &amp; (1&lt;&lt;SPIF)));     

    SPDR = Data;                    
    while(!(SPSR &amp; (1&lt;&lt;SPIF)));     

    PORTB |= (1&lt;&lt;sensor_select);    
}

uint8_t ReadByteSPI(int8_t addr, int sensor_select)
{
    int8_t dummy = 0xAA;

    PORTB &amp;= ~(1&lt;&lt;sensor_select);   

    SPDR = addr;                    
    while(!(SPSR &amp; (1&lt;&lt;SPIF)));     
    SPDR = dummy;                   
    while(!(SPSR &amp; (1&lt;&lt;SPIF)));     

    PORTB |= (1&lt;&lt;sensor_select);    

    addr=SPDR;
    return addr;
}

void Read_all_acceleration(int10_t  *acc_x, int10_t *acc_y, int10_t *acc_z, int sensor_select)
{
    uint8_t addr = 0x82;
    uint8_t dummy = 0xAA;
    uint8_t high = 0;
    uint8_t low = 0;

    PORTB &amp;= ~(1&lt;&lt;sensor_select);   

    SPDR = addr;
    while(!(SPSR &amp; (1&lt;&lt;SPIF)));

    SPDR = dummy;           
    while(!(SPSR &amp; (1&lt;&lt;SPIF)));
    low = SPDR;     
    SPDR = dummy;   
    while(!(SPSR &amp; (1&lt;&lt;SPIF)));
    high = SPDR;                    
    *acc_x = TBM(high, low);

    SPDR = dummy;       
    while(!(SPSR &amp; (1&lt;&lt;SPIF)));
    low = SPDR;     
    SPDR = dummy;   
    while(!(SPSR &amp; (1&lt;&lt;SPIF)));
    high = SPDR;    
    *acc_y = TBM(high, low);

    SPDR = dummy;       
    while(!(SPSR &amp; (1&lt;&lt;SPIF)));
    low = SPDR; 
    SPDR = dummy;   
    while(!(SPSR &amp; (1&lt;&lt;SPIF)));
    high = SPDR;    
    *acc_z = TBM(high, low);

    PORTB |= (1&lt;&lt;sensor_select);    
}
</code></pre>

<p>And now here is what really confuses me. I got 5 of this sensors. One is working with this code perfectly fine. The Data I get is what I expect. I measure earth gravity in z-component if Iay the sensor on the table, if I start turning it I measure the earth gravity component wise in x-, y- and z- direction depending on the angle I turn the sensor.</p>

<p>From the other 4 sensors I receive data that is different. The values jump from -314 (about -1.2 g) to +160 (about 0.5g). With the same code, the same wires and the same Arduino.</p>

<p>I checked the register settings of all sensors, they are all the same. I checked the wire connection to the first component at the sensors, they are all around 0.3 Ohm. I used an Oscilloscope and made sure CSB, SCK and MOSI work properly.</p>

<p>Am I missing something? What causes this similar but wrong behavior of 4 out of 5 sensors?</p>
","arduino accelerometer"
"7617","Using 2x UARTs on STM32F072RB","<p>I am trying to use 2x UARTs with ChibiOS on the STM32F072RB Nucleo Board. </p>

<p>I initialized UART2 but I am still getting output on UART1 pins, which is totally weird.</p>

<pre><code>#include ""ch.h""
#include ""hal.h""


/*
 * UART driver configuration structure.
 */
static UARTConfig uart_cfg_1 = {
    NULL,   //txend1,
    NULL,   //txend2,
    NULL,   //rxend,
    NULL,   //rxchar,
    NULL,   //rxerr,
    800000,
    0,
    0,      //USART_CR2_LINEN,
    0
};

static UARTConfig uart_cfg_2 = {
    NULL,   //txend1,
    NULL,   //txend2,
    NULL,   //rxend,
    NULL,   //rxchar,
    NULL,   //rxerr,
    800000,
    0,
    0,
    0
};

/*
 * Application entry point.
 */
int main(void) {

  /*
   * System initializations.
   * - HAL initialization, this also initializes the configured device      drivers
   *   and performs the board-specific initializations.
   * - Kernel initialization, the main() function becomes a thread and the
   *   RTOS is active.
   */
  halInit();
  chSysInit();

  /*
   * Activates the serial driver 1, PA9 and PA10 are routed to USART1.
   */
  //uartStart(&amp;UARTD1, &amp;uart_cfg_1);
  uartStart(&amp;UARTD2, &amp;uart_cfg_2);

  palSetPadMode(GPIOA, 9, PAL_MODE_ALTERNATE(1));  // USART1 TX.
  palSetPadMode(GPIOA, 10, PAL_MODE_ALTERNATE(1)); // USART1 RX.
  palSetPadMode(GPIOA, 2, PAL_MODE_ALTERNATE(1));  // USART2 TX.
  palSetPadMode(GPIOA, 3, PAL_MODE_ALTERNATE(1));  // USART2 RX.

  /*
   * Starts the transmission, it will be handled entirely in background.
   */
  //uartStartSend(&amp;UARTD1, 13, ""Starting...\r\n"");
  uartStartSend(&amp;UARTD2, 13, ""Starting...\r\n"");

  /*
   * Normal main() thread activity, in this demo it does nothing.
   */
  while (true) {
    chThdSleepMilliseconds(500);
    uartStartSend(&amp;UARTD2, 7, ""Soom!\r\n"");
    //uartStartSend(&amp;UARTD1, 7, ""Boom!\r\n"");
  }
}
</code></pre>

<p>The line <code>uartStartSend(&amp;UARTD2, 7, ""Soom!\r\n"");</code> gives output on UART1. </p>

<p>Is there anything else I need to do?</p>

<p>mcuconfig.h reads</p>

<pre><code>#define STM32_UART_USE_USART1               TRUE
#define STM32_UART_USE_USART2               TRUE
#define STM32_UART_USART1_IRQ_PRIORITY      3
#define STM32_UART_USART2_IRQ_PRIORITY      3
#define STM32_UART_USART1_DMA_PRIORITY      0
#define STM32_UART_USART2_DMA_PRIORITY      0
</code></pre>
","microcontroller serial c"
"7622","Virtual model in PLC discrete / continuous","<p>How does one implement virtual model (continuous) while control system itself is discrete (PLC)?</p>

<p>I've done this in practice but what about theory, how does one explain this topic to a stranger? (lets say myself)</p>
","control"
"7626","what are methods to compare PID controller performance?","<p>If there are input and the sensor measured outputs. What are the objective methods to compare performance besides looking at inputs and outputs matching or not?</p>
","pid"
"7632","Project - building a Electric skateboard","<p>During the vacation here i wa thinking it could be fun to make an electric skateboard/longboard... 
I haven't decided on which Longboard i want to use, but is nearly done with selecting the parts. </p>

<p>The board will be driven by one out runner [Turnigy Aerodrive SK3 - 6374-149kv][1] 
And to mount the motor i use this [kit][2] 
As for ESC i was recommended to [this][3], and [this][4] as battery. </p>

<p>Which seems like be good seem to be an ok setup for my application.  The problem is a bit that I don't have any reason for why i have chosen the ESC and the battery pack. Surely they deliver the needed amps and the ESC can withstand the max current but other than that, I don't see why i couldn't buy an other set.. especially if it is cheaper... </p>

<p>what cheaper alternative do i have, and what the drawbacks do they have </p>
","brushless-motor battery esc"
"7633","Brushless DC motor - Electronic Speed Control - Quadcopter","<p>I'm a student who is doing electrical and electronics engineering. I'm currently doing my final project which is a quadcopter. One of my objectives in that is to make a <strong>Electronic Speed Controller (ESC) for the brushless motors</strong> that are being used. </p>

<p>I made a design for the ESC using proteus and I made the PCB also. I have attached the schematic. I used <strong>PIC16F628A</strong> for the ESC and wrote a small code in <strong>mikroC</strong> for the ESC to work when powered up. Unfortunately it didn't work properly. I tried <strong>sensorless control</strong> of brushless motors without getting any feedback.</p>

<p>Can I know how much of current that I should provide for the motor? According to some articles that I read the brushless DC (BLDC) motor requires around 10A at the startup for around 20 ms. I have posted the code also. I used two codes to run the motor. One with PWM and other without PWM (100% duty cycle). </p>

<p>I am a rookie to the subject of BLDC motor controlling. I am very grateful if anybody can help me to clear out the doubts and figure out the mistakes in my design to make it work properly. </p>

<p>Below given is the code that I tried. Please help me to figure out the right way to program the chip.</p>

<pre><code>    const delay = 7000;

void main() {
 TRISB = 0x00;
 PORTB = 0x00;

 while(1)
 {
   PORTB = 0x24;
  delay_us(delay);

  PORTB = 0x36;
  delay_us(delay);

  PORTB = 0x12;
  delay_us(delay);

  PORTB = 0x1B;
  delay_us(delay);

  PORTB = 0x09;
  delay_us(delay);

  PORTB = 0x2D;
  delay_us(delay);
 }


}
</code></pre>

<p>When I uploaded the above given code and when I set the delay to around 3000 &mu;s, the motor spun but at each time one of the MOSFETs got heated up until I cannot touch it anymore. Here is the <a href=""https://drive.google.com/open?id=0B1...jBROGk4QXFJR3c"" rel=""nofollow"">video of this scenario</a>.</p>

<p>This is the other code (PWM);</p>

<pre><code>const delay1 = 2000;
const delay2 = 1000;
int count = 0;
int cnt;
int arr[6] = {0x24, 0x36, 0x12, 0x1B, 0x09, 0x2D};
int i = 0;
int x =  0x32;


 void init(void)
 {
  TRISB = 0x00;
 PORTB = 0x00;
 //OPTION_REG = 0x87;
 //INTCON = 0xA0;
 CCP1CON = 0;
 CMCON = 0x07;
 }



void main() {
   init();
   while(1){

   for (cnt = 0; cnt &lt; 10; cnt++)
       {
         PORTB = arr[i];
         delay_us(2);
         PORTB = 0x07;
         delay_us(2);

        }

     i++;

         if (i == 6)
         {
         i =0;
         }

   };


 }
</code></pre>
","quadcopter brushless-motor esc"
"7634","Arduino original or generic for a beginner?","<p>I'm new to the robotics and electronics world, but I'm willing to dive into it. I'm a software developer and I want to create a project that uses GPS and Accelerometer data to show as a layer on Google Maps after transferred to PC.</p>

<p>My doubt is about which controller to get. In my country, there are generic controllers based on the Atmega328 that are being sold with a massive difference of price from the original Arduino (talking about the UNO model). </p>

<p>Should I start with an original model? </p>

<p>Should I expect to break the controller, fry it, or break any components by connecting them wrong? </p>

<p>Would the experience with a generic controller be less exciting than with the original Arduino one?</p>
","arduino beginner"
"7640","PID gains for motor position and velocity control","<p>I have a servo motor with quad optical encoder and I'm trying to control its position and velocity. By controlling both I meant that if I input that the motor should reach 90° at 200rpm then it should. How can I do that? I am using an Arduino Uno. Kindly share some code if possible. </p>

<p>Though I have implemented the PID, I don't think it is correct because I didn't implement the feedforward controller (because I have no idea what that is) and I have not been able to find suitable gains for PID. The gains I find for small steps (or say degree rotation) do not work out well for large steps and vice versa. I have also not used a limit for integral sum (because I don't how much it should be).</p>

<p>I am using a Pittman motor.</p>
","robotic-arm"
"7641","Mapping formats for small autonomous robots","<p>I have some robot software I'm working on (Java on Android) which needs to store a pre-designed map of a playing field to be able to navigate around. The field's not got any fancy 3d structure, the map can be 2d.</p>

<p>I've been trying to find a good format to store the maps in.
I've looked into SVGs and DXFs, but neither one is really designed for the purpose.</p>

<p>Is there any file format specifically designed for small, geometric, robotics-oriented maps?</p>

<p>The field I'd be modelling is this one:
<img src=""http://i.stack.imgur.com/AdJTy.jpg"" alt=""FTC playing field""></p>
","mapping"
"7644","How to use a POMDP-based planner on top of a probabilistic filter","<p>POMDPs extend MDPs by conceiling state and adding an observation model. A POMDP controller processes either</p>

<ul>
<li>action/observation histories or</li>
<li>a bayesian belief state, computed from the observations (<em>belief-MDP</em> transformation)</li>
</ul>

<p>In a complex, real-world system like a robot, one usually preprocesses sensory readings using filters (Kalmann, HMM, whatever). The result of which is a belief-state.</p>

<p>I am looking for publications that discuss the problem of fitting a (probably more abstract) POMDP model on top of an existing filter-bank. </p>

<ol>
<li>Do you have to stick to the belief-MDP, and hand over the filtered belief-state to the controller?</li>
<li>Is there any way of using history-based POMDP controllers, like MCTS?</li>
<li>How do you construct/find the abstract observations you need to formulate the POMDP model?</li>
</ol>
","kalman-filter particle-filter planning filter"
"7645","How do I choose the best filter for dead reckoning with an IMU?","<p>I'm searching filter to reduce noise and smooth the signal while dead reckoning with an IMU (6dof gyro+accelerometer). What are the differences/advantages/disadvantages of the following filters:</p>

<ul>
<li>Kalman</li>
<li>Complementary</li>
<li>moving average</li>
<li>Mahony </li>
</ul>

<p>I applied kalman and complementary filters to an IMU and both of them gives time lag to actions with respect to filter parameters. Also kalman filter works slower than moving average and complementary. How can I choose right filter and filter parameters?</p>
","mobile-robot localization kalman-filter imu"
"7647","ROS Calibration Camera Problems","<p>I am trying to calibrate a monocular camera using ROS with the help of this website: <a href=""http://wiki.ros.org/camera_calibration/Tutorials/MonocularCalibration"" rel=""nofollow"">How to Calibrate a Monocular Camera</a>. When I run <code>rostopic list</code>, I get:</p>

<pre><code>/left
/right
/rosout
/rosout_agg
/usb_cam/image
</code></pre>

<p>When I run <code>rosservice list</code>, I get:</p>

<pre><code>/cameracalibrator/get_loggers
/cameracalibrator/set_logger_level
/rosout/get_loggers
/rosout/set_logger_level
</code></pre>

<p>Finally, when I run:</p>

<pre><code>rosrun camera_calibration cameracalibrator.py --size 10x7 --square 0.025 image:=/usb_cam/image camera:=/usb_cam 
</code></pre>

<p>It says:</p>

<pre><code>('Waiting for service', '/usb_cam/set_camera_info', '...')
Service not found
</code></pre>

<p>I even added the parameter at the end, <code>--no-service-check</code>, but that just makes the terminal stall indefinitely. </p>

<p>Could someone please help me figure out what is going wrong and how I can fix it? Also if it is important, <code>usb_cam</code> is saved at <code>catkin_ws/src/usb_cam</code>.</p>
","ros cameras calibration"
"7648","iRobot Create 2 sensors","<p>Can someone please provide me with a list of sensors on the create 2?  I am hoping to get one soon, but want to be sure it has ultrasonic sensors and not just bump sensors before I do.</p>
","irobot-create roomba"
"7652","How do I control the robotic arm motion?","<p>I have a Robotic arm mounted on a car. There's a camera attached to it. Suppose the camera takes the image of a room, and finds that there's something, say an object, that has to be picked up. Say it's 50 feet away from the robot. My question is that how will the robot reach the object in the first place, and secondly, when it has reached the object, how will it know the real world co-ordinates of the object, to pick the object up, using inverse kinematic equations. Any help would be appreciated. Thanks</p>
","mobile-robot robotic-arm"
"7656","How does the cliff sensors on ROOMBA work through a glass wall?","<p>I want to use IR sensors to detect whether my dustbin is full but I want to protect it from outside dust. I am planning to use the IR sensors on Roomba.</p>

<ol>
<li>How are they working despite a plastic wall?</li>
<li>Also, what is the range of the sensor?</li>
<li>Can they detect obstacle at about 25 cm?</li>
<li>Why is there a wall between the IR sensors?</li>
<li>Is there a reason they are positioned at certain angle?</li>
</ol>
","sensors roomba"
"7659","PTAM CameraCalibrator error","<p>I am trying to run the <code>cameracalibrator.launch</code> using PTAM according to this <a href=""http://wiki.ros.org/ethzasl_ptam/Tutorials/camera_calibration"" rel=""nofollow"">Camera Calibration tutorial</a>. However, when I do so, I get the following error:</p>

<pre><code>ERROR: cannot launch node of type [ptam/cameracalibrator]: can't locate node [cameracalibrator] in package [ptam]
</code></pre>

<p>I source my <code>devel/setup.bash</code> before I run the code as well and it still does not work. Here is my launch file:</p>

<pre><code>&lt;launch&gt;
    &lt;node name=""cameracalibrator"" pkg=""ptam"" type=""cameracalibrator"" clear_params=""true"" output=""screen""&gt;
        &lt;remap from=""image_raw"" to usb_cam/image_raw"" /&gt;
        &lt;remap from=""pose"" to=""pose""/&gt;
        &lt;rosparam file=""$(find ptam)/PtamFixParams.yaml""/&gt;
    &lt;/node&gt;
&lt;/launch&gt;
</code></pre>

<p>Here is what I get for <code>rostopic list</code>:</p>

<pre><code>/rosout
/rosout_agg
/svo/dense_input
/svo/image
/svo/image/compressed
/svo/image/compressed/parameter_descriptions
...
/tf
/usb_cam/camera_info
/usb_cam/image_raw
/usb_cam/image_raw/compressed
...
/usb_cam/image_raw/theora
/usb_cam/image_raw/parameter_descriptions
/usb_cam/image_raw/parameter_updates
</code></pre>

<p>The path where the <code>cameracalibration.launch</code> file is <code>catkin_ws/src/ethzasl_ptam/ptam/launch</code>. </p>

<p>I am not sure why this error keeps coming up because when I run <code>roslaunch ptam cameracalibrator.launch</code>, it says:</p>

<pre><code>NODES
  /
    cameracalibrator (ptam/cameracalibrator)
</code></pre>

<p>So I'm thinking that PTAM does include <code>cameracalibrator</code>. If someone could please point out my error, that would be really helpful. I've been using this post as a guide, but it's not been helping me much: <a href=""http://stackoverflow.com/questions/16759356/ros-dynamic-config-file"">Ros Dynamic Config file</a>.</p>

<p>As it says in the above link, I tried <code>find . -executable</code> and I could not find <code>cameracalibrator</code>. I could only find the below. How do I proceed?</p>

<pre><code>./include
./include/ptam
./cfg
...
./launch
./src
./src/ptam
./src/ptam/cfg
...
</code></pre>
","ros cameras calibration"
"7660","Implementing a boustrophedon algorithm in a given room with obstacles","<p>I have a mobile robot which is navigating around a room, I already have the map of the room. I am using the <a href=""http://wiki.ros.org/navigation"" rel=""nofollow"">navigation_stack</a> of ROS. I am using rotary encoders for odometry. I am fusing the data from Rotary encoders and IMU using robot_pose_ekf. I am using amcl for localization and move_base for planning. </p>

<p>Now, I have to write a Complete coverage Path planning algorithm and I am following <a href=""http://www.ri.cmu.edu/pub_files/pub4/choset_howie_1997_3/choset_howie_1997_3.ps.gz"" rel=""nofollow"">this</a> paper and I would like to ask what is the best way to generate the Boustrophedon path (simple forward and backward motions) in a cell (can be rectangular, trapezium, etc.) with no obstacles? I read a paper where they use different templates and combine them in a certain way to come up with the Boustrophedon path. Is there any other way by which we can generate the boustrophedon path? If someone can suggest how to implement it in ROS, that will be great.</p>

<p>Please let me know if you need more information from me. Any help will be appreciated.</p>
","ros navigation motion-planning coverage"
"7662","How do I work out the specifications of motors and propellers for a quadcopter?","<p>What will be the specifications of motors and propellers that can approx produce a thrust of 100kg in a quadcopter?</p>

<p>We are planning to lift a total weight of 50 Kg along with the 20 Kg weight of the quadcopter itself. So at 50% throttle the total thrust produced by it should be 150 Kg with a per motor total thrust of 37.5 kg.</p>

<p>I have looked at <a href=""http://robotics.stackexchange.com/a/7468/37"">this answer</a> to <a href=""http://robotics.stackexchange.com/q/7662/37"">How to calculate quadcopter lift capabilities?</a> but don't understand how to use this information to work out the specifications of motor and propeller required for my application.</p>

<p>The answer given in previous question is limited for small quad &amp; I require the specifications of BLDC motor such as Kv,torque,Imax,V,Power,etc and of Propeller suitable for such motor.</p>
","quadcopter"
"7664","What actuator types exist that remain locked in their last position like hydraulic piston?","<p>I would like to find an electronic actuator that mimics the characteristics of a hydraulic actuator, in that the position remains fixed without power drain when the actuator is not moving. Which actuators exist that match these criteria?</p>
","electronics actuator"
"7666","where to get this reference about Kalman filter, technical report","<p>I'm sorry for this question that might not fit in here however, I would like to give it a shot. I've chosen this stack since the question is somehow related to mobile robots. I've came across a paper in Mobile Robot Localization that has cited the following reference, </p>

<blockquote>
  <p>C. Brown, H. Durrant-Whyte, J. Leonard, B. Rao, and B. Steer.  Kalman
  filter algorithms, applications, and utilities. Technical Report
  OUEL-1765/89, Oxford U. Robotics Research Group, 1989.</p>
</blockquote>

<p>I couldn't find this reference. Nothing show up in Google not even in Google Scholar. In my university which allows me to access to a massive database, also nothing show up. Since this is a technical report, I'm interested to read it to have more appreciation about Kalman Filter. Has anyone came across this reference?</p>
","mobile-robot localization kalman-filter"
"7670","Kinect v1 VS Kinect v2","<p>From a technical standpoint what are the differences between the Kinect v1 and the Kinect v2 ?</p>

<p>I'm interested both in the hardware equipment and the format of the data.</p>
","kinect"
"7671","Kinect VS Stereo cameras","<p>As I'm advancing in my project I realized I need better hardware, particularly for video input and processing.</p>

<p>From an intuitive feeling sounds like stereo cameras offers a more powerful and flexible solution, on the other hand the Kinect looks like a great out-of-the-box solution for depth sensing and it also takes away a lot of computational complexity as it output directly the depth.</p>

<p>So I would like to know what are the upsides and downsides of the 2 solutions and if they have any well known limitation and/or field of application and why.</p>

<p>Thank you </p>
","kinect cameras stereo-vision"
"7672","Extending iCreate battery power for auxilliary equipment","<p>I plan to use the icreate as a platform to carry a tablet, or notebook PC and want to have power for some time so I need more than the 3000 mAh battery. I want all to be powered from same battery system and use same charging source. So I need info as to how to wire in additional 14.4V NiMH batteries in parallel with the existing and how to deal with the additional temperature sensors (I could ignore of course but...). Can the built in power control deal with this? Do I need to upgrade it somehow? I would appreciate suggestions as I do not want a completely separate power system for aux devices. Charging all from standard home base is the goal even though it will take longer. I can deal with adapting the 14.4V to whatever aux devices I add. Thanks.</p>
","power battery roomba"
"7678","How to reduce battery power 10v 1.5A to 6v 1.5A","<p>What's least complex way to reduce power from a 10V 1.5A battery to 6V 1.5A</p>

<p>Thank you!</p>
","battery"
"7680","Arduino compatible sensor for motion detection and positioning","<p>I am working on a project that requires motion detection and positioning. I've worked substantially with a camera but the issue with this is that I need something sleek, small and not heavy at all. Cameras also tend to rely on luminosity and they don't work well in poorly lit spaces.</p>

<p>I need someone who's worked on something like this or who knows the best sensor for this purpose.</p>
","arduino"
"7682","Object Grasping Robot Arm Control","<p>I have a 2 DOF Robot Arm with a camera attached to it. It takes an Image and there's an object in that image, say a glass. Of course, in order to move the arm to the required position to grasp the object, I have to solve the inverse kinematic equations. In order to solve them, I need the x and y, the coordinates where the arm has to reach to grasp the object. My question is how can I find the x and y of say the midpoint of the object from the image. Thanks </p>
","mobile-robot robotic-arm"
"7685","What is required to build a simple XY-stage?","<p>In the scope of my PhD, I would like to build an automated microscopy set-up that should take images of a sample of 2cm by 2cm. This should be done by taking pictures of 500 micrometers by 500 micrometers.</p>

<p>Therefore I need to design an XY-stage moving my sample over the optical setup. I would use a Raspberry Pi to steer all the hardware. </p>

<p>Could you direct me to material about how to best make an XY-stage ? My questions are about what types of motors to use (stepper?), how many, how to create a good sliding mechanism to avoid jerky steps, etc. </p>

<p>Simple links to basic engineering of such set-ups would be more than enough for me to start, as I am a complete layman in this field.</p>

<p>EDIT: I have found this <a href=""http://letsmakerobots.com/node/31381"" rel=""nofollow"">blogpost</a>. It does what I require, if I get small enough angle step stepper motors.</p>

<p>EDIT2: I need a maximal range of motion of 10 cm in both directions. The overall size should not exceed 30x30 cm^2. Step sizes should not exceed 10 microns. I do not care about moving speed. Based upon the design in the link, buying a stepper motor with a 100:1 gear box could allow my very small radial steps (&lt;0.05 deg) which would result in about 5 micron steps, assuming a rotor radius of about 1cm.
As far as price goes, it should not exceed commercially available options which start at about 5k USD</p>
","raspberry-pi stepper-motor"
"7691","Is anybody using robot simulators?","<p>Do you use simulators for developing your robot algorithms or do you test directly in your robot?
I would like to get introduced into the simulators world, but don't know from where to start... can you recommend me one?</p>

<p>Regards</p>
","simulator"
"7694","Degree of Freedom","<p>A robotic arm should pick a cuboid up of a table, rotate it around its vertical axis and put it down on all possible positions. How many degrees of freedom are at least necessary?
(All coordinates, that should be reached by the robotic arm, are in its workspace. It is not allowed to put the cuboid down and pick it up, once the robot has it )</p>

<ul>
<li>The answer is  4 (3 translatory and 1 rotatory).</li>
</ul>

<p>But I don’t understand why. I thouhgt that it should be 3.
2 prismatic joints:  1 to pick the cuboid up,  and another one to move it anywhere on the table.
1 revolute joint to rotate the cuboid around its vertical axis. => 2 translatory and 1 rotatory. </p>
","robotic-arm"
"7698","Voltage rpm relation","<p>I measure the voltage ESC drawing while increasing the dc motor speed. Multimeter shows that as long as the speed increases the voltage value decreases. Can anybody explain why this is happening? </p>
","brushless-motor electronics esc"
"7703","increase PID sampling rate on embedded system","<p>my robotic project is running at every 1ms and the processes are taking about 0.9ms. I am running PID so my max clock rate is 1kHz. About half of the processing time are taken by SPI peripherals, IMU and encoders. Is there any recommendation on how I can run faster PID sampling rate?</p>
","pid embedded-systems"
"7705","Low variance resampling algorithm for particle filter","<p>For my particle filter, I decided to try using the low variance resampling algorithm as suggested in Probabilistic Robotics. The algorithm implements systematic resampling while still considering relative particle weights. I implemented the algorithm in Matlab, almost word-for-word from the text:</p>

<pre><code>function [state] = lowVarianceRS(prev_state, weight, state_size)
    state = zeros(1,state_size);    % Initialize empty final state
    r = rand;                       % Select random number between 0-1
    w = weight(1);                  % Initial weight
    i = 1;
    j = 1;

    for m = 1:state_size
        U = r + (m - 1)/state_size; % Index of original sample + size^-1
        while U &gt; w                 % I'm not sure what this loop is doing
            i = i + 1;
            w = w + weight(i);
        end
        state(j) = prev_state(i);   % Add selected sample to resampled array
        j = j + 1;
    end
end
</code></pre>

<p>As would be expected given the while loop structure, I am getting an error for accessing weight(i), where i exceeds the array dimensions.</p>

<p>To solve this, I was considering circularly shifting my weight array (putting the first index used as the first value in weight, so that I never exceed matrix dimensions). However, I wasn't sure if this would negatively impact the rest of the algorithm, seeing as I'm having trouble understanding the purpose of the U calculation and while loop.</p>

<p>Could anyone help clarify the purpose of U and the while loop, and whether or not a circular shift is an acceptable fix?</p>
","mobile-robot algorithm particle-filter probability"
"7708","How to make door opening to the top","<p>I need to make this construction (door is closed by default, door is opened to the top). This is the scheme:</p>

<p><img src=""http://i.stack.imgur.com/BL5iw.jpg"" alt=""enter image description here""></p>

<p>Red rectangle on the picture is the aperture, blue rectangle is the door (weight is about 0.5 kg), which moves top when door need to be opened. Green stripe on the picture is the rail for the door.</p>

<p>Which electrical engine should I use?</p>

<p>Estimated time of door opening is about 10 seconds, I want to send signal to up the door, it should be drop down when power is lost or I should send a signal to drop it down.</p>
","design electronics actuator"
"7710","Formationing Algorithm for Multiple Robots","<p>I'm looking for an algorithm for formationing multiple robots in 2D simulation. Can you suggest resources about this topic. Also I need suggestions and comments about these topics:</p>

<ul>
<li>Can I recruit algorithm from optimization algorithms like particle or ant?</li>
<li>Is there any way except ""go to goal"" for each robot</li>
<li>Is patter formationing algorithms feasible?</li>
<li>Suggestions about a fast way of formationing/ aligning</li>
</ul>

<p>Notes:</p>

<ul>
<li>Im not using a robotics simulator or physics engine for this. </li>
<li>Robots are represented as dots.</li>
<li>multi robot system is homogeneous</li>
<li>every robot can sense obstacles and other robots in a sense range circle around the robot.  </li>
<li>number of obstacles and robots can vary from 2 to 100 </li>
<li>multi robot system is not a central </li>
</ul>
","mobile-robot multi-agent swarm"
"7712","Prototyping with IRobot roomba","<p>For a project I am building a Tele-Op Robot using the IRobot's Roomba as my drivetrain. In order for my robot to work, I need an extra castor. IRobot provides .stl and .stp files for me to use and I used them and printed the files. (The file I printed was from this link: <a href=""http://www.irobot.com/~/media/MainSite/PDFs/About/STEM/Create/Create_2_Bin_Modification.pdf"" rel=""nofollow"">Create® 2 Bin Modification</a>.</p>

<p>This file is a new part to the drivetrain to allow another caster.</p>

<p>And I downloaded the first link called ""<a href=""http://www.irobot.com/~/media/MainSite/Files/About/STEM/Create/c2binbottom.stl"" rel=""nofollow"">Full bin bottom with caster mount</a>""</p>

<p>The piece was great but it made the castor a different height then the wheels. I was wondering if anyone had this file but saved as something different so I can edit it in preferably Solidworks. I was on the phone with IRobot for over 2 hours today and they told me to post here. So please help!!!! :)</p>
","irobot-create roomba"
"7718","Optimal hardware for linear algebra operations","<p>I've been working lately on SLAM algorithms implementing extended kalman filtering to brush up on some localisation techniques and I have been thinking forward to the hardware side of things. Are there embedded chips such a microcontroller that are optimised for large linear algebra operations? What sort of embedded options are the best for processing these sorts of operations?</p>
","slam kalman-filter"
"7720","Reading crazy large numbers from Naze32","<p>I hope someone can help me here.  I am reading very large numbers from my naze32.  What is the max and min values from pitch, roll and yaw? how would I then convert them to degrees or radians?</p>
","quadcopter"
"7722","What type of control law is used in ""Reaction Control System"" of Apollo Lunar Module or Space Shuttle?","<p>Reaction Control Systems (RCS) on these vehicles are implemented by using small rocket thrusters. For me it looks like these thrusters work in some kind of ""pulse"" mode. And I can't understand - do they use some optimal control to calculate in advance the required impulse to reach the new desired state of the system OR they use ""pulse"" mode just for precise magnitude variation of provided thrust (like average voltage in PWM(pulse-width modulation)) in a classic PID control loop?</p>
","control automatic rocket"
"7723","MATLAB 3D Simulation with SOLIDWORKS model","<p>I'm learning  to make a 3D simulation in MATLAB based on a model designed from SOLIDWORKS.
There is an example: <a href=""https://youtu.be/1nSafEJlySs"" rel=""nofollow"">SIMULINK+SOLIDWORKS</a></p>

<p>The way used here is: </p>

<ol>
<li><p>Create a 3D model in SOLIDWORKS</p></li>
<li><p>Create a xml file applicable to import to MATLAB via SimMechanics Link</p></li>
<li><p>Import the model to MATLAB/SIMULINK. A simulink system is created.</p></li>
</ol>

<p>After these steps, controlling the system will be implemented in <strong>SIMULINK</strong>.
But I feel simulink is kind of strict to control. I want to be more flexible, apply any algorithm to the model. And using matlab *.m file to control is more efficient way. </p>

<p>So my question is this: Is there any way to do 3D simulation (MATLAB+SOLIDWORKS) by using only *.m file to control, no <strong>SIMULINK</strong> anymore? </p>

<p>All model information will be contained in the *m.file. Maybe the step 1 and 2 are inherited, but step 3 is different.</p>
","matlab simulation"
"7724","Determine what the rotation axis is given a rotation matrix","<p>How do I find out around which axis the coordinate system has to rotate, if the rotation matrix is given?</p>

<p>$ {^{a}R_{b} } $ = $ \left(\begin{matrix} 0 &amp; 1 &amp; 0 \\ -1 &amp; 0 &amp; 0 \\ 0 &amp; 0 &amp; 1 \\\end{matrix}\right)$ </p>

<p>$ {^{a}R_{c} } $ = $ \left(\begin{matrix} 0 &amp; 0 &amp; 1 \\ 1 &amp; 0 &amp; 0 \\ 0 &amp; 1 &amp; 0 \\\end{matrix}\right)$ </p>

<p>For $ {^{a}R_{b} } $ I thought, that it has to be a rotation around the z-axis, because 
$R(z,\theta) =  \left(\begin{matrix} cos(\theta) &amp; -sin(\theta) &amp; 0 \\ sin(\theta) &amp; cos(\theta) &amp; 0 \\ 0 &amp; 0 &amp; 1 \\\end{matrix}\right)$ </p>

<p>the values at the positions $a_{13}, a_{23},a_{33},a_{32},a_{31}$ of $ {^{a}R_{b} } $ and $R(z,\theta)$ are identical.</p>

<p>So I solved $cos(\theta) = 0$ =>$\theta = 90° $ => 90° rotation around z-axis.</p>

<p>But how do I solve it, if there is more than 1 rotation, like for $ {^{a}R_{c} } $?  </p>
","joint"
"7728","Where to make changes for simulation project Bullet/Gazebo/ROS/Orocos","<p>I am starting to develop robotics project which involves simulation (and maybe real world programs) of soft-body dynamics (for food processing) and clothes/garment handling (for textile industry or home service robots). It is known that soft-body dynamics and garment handling are two less explored areas of robotics and simulation, therefore I hope to make some development of (contribution to) projects that are involved. The following projects are involed:</p>

<ul>
<li>Bullet physics engine - for dynamics</li>
<li>Gazebo - simulation environment</li>
<li>ROS - robot OS, I hope to use Universal Robot UR5 or UR10 arms and some grippers (not decided yet)</li>
<li>Orocos - for control algorithms</li>
</ul>

<p>Initially I hope use ""ROS INDIGO IGLOO PREINSTALLED VIRTUAL MACHINE"" (from nootrix.com), but apparently I will have to make updates to the Bullet, Gazeboo, add new ROS stacks and so on.</p>

<p>The question is - how to organize such project? E.g. If I am updating Bullet physics engine with the new soft-body dynamics algorithm then what executable (so) files should I produce and where to put them into virtual machine? The similar question can be asked if I need to update Gazebo. </p>

<p>There seems to be incredibly large number of files. Is it right to change only some of them.</p>

<p>Sorry about such questions, but the sofware stack seems to be more complex than the robotics itself.</p>
","ros software programming-languages dynamics gazebo"
"7729","Is it possible to simulate vision (perception) in Gazebo (or other simulators)","<p>Vision is important part of robotics and frequently it is unavoidable component of control loop. E.g. many clothes/garment handling algorithms rely on visual cues in deciding how to proceed. The question is - does simulation environments (Gazebo or some others) allow one to design world with robot and garment and simulate not only garment dynamics but <strong>simulate also what robot sees, how robot perceives garment in each simulation step</strong>? If it is not possible to simulate vision then how to simulate algorithms with vision as component of control loop?</p>

<p>Maybe simulation of vision can be good research theme? Are here some trend or good articles about it? Some initial projects that could be expanded?</p>

<p>Actually - it can be stated as more general question - <strong>is it possible to simulate sensors in Gazebo?</strong> E.g. food handling (soft-body handling) can involve tactile sensors. In principle Gazebo can calculate deformation and forces of soft-body and format these data as the simulated values of sensor readings. Maybe similar mechanism can be used for simulation of vision as well?</p>
","computer-vision simulator research simulation gazebo"
"7731","Comparing industrial robot arms","<p>I'd like to study the capabilities of industrial robot arms. For example, to answer the question how does price vary with precision, speed, reach and strength?</p>

<p>Is there a database of industrial robot arms including information like the price, precision, speed, reach and strength of each model?</p>
","robotic-arm industrial-robot"
"7735","Estimation of Battery Life Time From PWM Signals in a Quadrotor","<p>Is there any way of estimation the battery life from pwm outputs which goes to motors in <strong>microcontroller level</strong>. I'm planning to estimate path range with this. <em>Microcontroller, sensor and other electronic device should be neglected.</em> </p>
","quadcopter battery"
"7737","Building parts, and keeping laser alignment steady","<p>I am building a laser gun for pentathlon targets (also doing one).
I would like to know how build a part of the gun and if I can count on a steady laser if it is attached to a motor.</p>

<p>The question is about the laser. I want it maybe attached to a small-(servo)motor to try to implement some cheat just for fun. Assuming the motor has a good torque, can I assume that the laser will not move (not the sightliest bit) when the motor is turned off? (I don't have any to test)
This is for precision shooting, so small vibrations and a moving pointer would be really prejudicial.
In case it does, what can I do to minimize the problem? Is it all about ordering the motor with the highest torque?</p>

<p>I also have a second question which is slightly off-topic, yet related, and robotics people usually have solutions for such problems.
I also need to build the sights. Here's a gun:
<a href=""http://i.stack.imgur.com/7iKYW.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/7iKYW.png"" alt=""enter image description here""></a></p>

<p>As you can tell, its sights are a fixed plastic point in the front, and an adjustable large back. There are two bolts, one on each side. One makes the sight higher or lower, and the other makes it point more to the right or left.
How can such part be built with simple tools?</p>

<p>Thanls</p>
","motor stability laser"
"7739","Navigating through a Maze using path-planning (Dijkstra)","<p>I'm working on an robot that would be able to navigate through a maze, avoid obstacles and identify some of the objects in it. I have a monochromatic bitmap of the maze, that is supposed to be used in the robot navigation.</p>

<p>Up till now I have processed the bitmap image, and converted it into an adjacency list. I will now use the dijkstra's algorithm to plan the path.</p>

<p>However the problem is that I have to extract the entrance point/node and exit node from the bmp image itself for dijkstra's algorithm to plan the path.</p>

<p>The robots starting position will be slightly different (inch or two before the entrance point) from the entrance point of maze, and I am supposed to move to the entrance point using any ""arbitrary method"" and then apply dijkstra algorithm to plan path from maze's entrance to exit.</p>

<p>On the way I have to also stop at the ""X's"" marked in the bmp file I have attached below. These X's are basically boxes in which I have to pot balls. I will plan the path from entrance point to exit point , and not from the entrance to 1st box, then to second, and then to the exit point; because I think the boxes will always be placed at the shortest path.</p>

<p>Since the starting position is different from the entrance point, how will I match my robot's physical location with the coordinates in the program and move it accordingly. Even if the entrance position would have been same as starting position there may have been an error. How should I deal with it? Should I navigate only on the bases of the coordinates provided by dijkstra or use ultrasonics as well to prevent collisions? And if we yes, can you give me an idea how should I use the both (ultrasonics, and coordinates)?</p>

<p><a href=""http://i.stack.imgur.com/DcBwl.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/DcBwl.png"" alt=""Sample Maze BMP image""></a></p>
","arduino mobile-robot localization motion-planning mapping"
"7741","Mathematical Moddeling of Elastic Robots","<p>We can easily compute the rigid robot kinematics and dynamics. There is many resources, simulators and modelling tools about it. But i couldnt find any of these for elastic robots. Can you suggest resources and modelling tools?</p>
","kinematics simulator dynamics"
"7757","Calculus in robotics","<p><br>
I am still in high school and am a part of the robotics club that competes in the FTC (First Tech Challenge). I am just about finishing my first Calculus class (Calc 1), and would be ecstatic to be able to apply this someway in a real world example such as robotics. [Besides PID. It seems like only approximations anyways] <br><br>
So far, I've only been working with ""fabricated"" math problems. Would deriving an equation from real life situations be too complicated?
<br>
Thank you!</p>
","control software"
"7760","Create a simple C++ client Application to control KUKA's Robot-arm LBR iiwa via FRI","<p>Until now I have been programming the robot using Java on KUKA's IDE ""KUKA Sunrise.Workbench"", what I want to do is control the robot arm via my C++.Net application (I would use a camera or Kinect to get commands). 
I'm reading the documents provided by Kuka, but as I'm a bit in hurry, I want to understand how a C++ client application (running on my laptop) can send/receive information to/from the robot's controller ""KUKA Sunrise Cabinet"" (running the server application) via FRI. I still have issues grasping the whole mechanism.</p>

<p>A simple application (Server/Client) source code with explanation (or a schematic) would be more than helpful .</p>
","robotic-arm c++"
"7763","Why do we generally prefer DH parameters over other kinematic representations of robot arms?","<p>I am specifically interested in DH parameters versus other representations in terms of kinematic calibration.  The best (clearest) source of information I could find on kinematic calibration is in the book ""<a href=""https://www.google.com/shopping/product/18328483085519729280"" rel=""nofollow"">Robotics: Modelling, Planning and Control</a>"" by Bruno Siciliano, Lorenzo Sciavicco, Luigi Villani, Giuseppe Oriolo, chapter 2.11.  Which requires a description of the arm in DH parameters, multiplying out the kinematics equation, partial differentiation w.r.t. each DH parameter, then a least-squares fit (with the left pseudo-inverse), then iterate.  </p>

<p>Is there some fundamental reason why DH parameters are used instead of a different representation (like xyz + euler angles).  I understand that there are fewer parameters (4 versus 6 or more), but for a calibration procedure like this I will be taking much more data than unknowns anyway.  All the robotics textbooks i have read just present DH parameters and say ""this is what you should use"", but don't really go into <em>why</em>.  Presumably this argument can be found in the original paper by Denavit, but I can't track it down.</p>
","robotic-arm kinematics calibration dh-parameters"
"7764","Blade 180QX :What does this red wire do?","<p>I have a Blade 180QX quadrotor / quadcopter and I had to move a red wire shoved under the circuit board when I fixed a broken power wire. Now the red wire shown out straight from the circuit board is not in the right configuration - or at least as it was. If I understood what it was for I might know how to place it. Is this a horizon sensor (temperature)?
Ever since I had to move this wire, the quadrotor goes unstable when flying. The only appreciable change is the wire POSITION.
The red wire was not attached anywhere else on the board. It was shoved under the circuit board inside the battery holder.
<a href=""http://i.stack.imgur.com/81yCR.jpg"" rel=""nofollow""><img src=""http://i.stack.imgur.com/81yCR.jpg"" alt=""Updated Blade 180QX image with red wire highlighted""></a></p>
","quadcopter"
"7765","Drone Battery Question","<p><strong>Background:</strong></p>

<ol>
<li>6 propeller drone w 20C 3s 6400 mAh 11.1 liPO battery</li>
<li>4 propeller drone w 25C 2s 5000 mAh 7.40 liPO battery</li>
</ol>

<p><strong>Behavior:</strong></p>

<ul>
<li><p>Drone 1 flies with ease</p></li>
<li><p>Drone 2 struggles hover 2-3 inches above ground</p></li>
</ul>

<p><strong>Question:</strong></p>

<p>The microcontroller, all props, ESCs, and motors are the same. I'm thinking the reason the drones are flying so differently is because of the difference in batteries. IF the batteries are the reason, what would be the property that is most responsible for the difference in flight?</p>
","battery"
"7766","Duty cycle mapping","<p>I need to build a conversion/mapping algorithm from a controller (PID etc.) output to the duty cycle in order to command my bldc motor via esc. I couldn't do it yet because l think l dont know the meaning of controller output. Anybody highlights my way?</p>
","arduino motor esc microcontroller"
"7772","How to cut throttle signal to ESC properly?","<p>I have a 16 Channel Servo Driver board from Adafruit (see <a href=""https://www.adafruit.com/products/815"" rel=""nofollow"">here</a>), and I communicate to it via I2C using a Raspberry Pi. The servo board is controlling a <a href=""http://www.hobbyking.com/hobbyking/store/__36674__q_brain_4_x_20a_brushless_quadcopter_esc_2_4s_3a_sbec.html"" rel=""nofollow"">Qbrain</a> by sending a PWM pulse between 1ms to 2ms and it works great.</p>

<p>Problem is, I'm trying to create a kill switch such that the signal from the servo board would cease, and the ESC would stop because it detects no PWM signal. I have placed a toggle switch that cuts the VCC to the servo board, so technically it should no longer produce any PWM signal, however when the power is cut, the ESC jumps to 100% throttle, I can only assume this is because the ESC believes the signal is 100% duty cycle, but how do I solve this?</p>
","raspberry-pi esc"
"7774","Modeling a robot to find its position","<p>The task of the robot is as follows.
My robot should catch another robot in the arena, which is trying to escape. The exact position of that robot is sent to my robot at 5Hz. Other than that I can use sonsor to identify that robot.
Is that possible to estimate the next position of other robot using a mathematical model. If so, can anyone recommend tutorials or books to refer..?</p>
","arduino kalman-filter automatic probability"
"7778","Electric Motor Speed Control - PWM vs analog voltage?","<p>I'm working on a 2-wheeled robot and have connected up a raspberry pi to an L298N motor driver.</p>

<p>I'm sending the enable pin of a particular motor a software-generated PWM signal at 100Hz with a 50% duty cycle.  I observe with an osciloscope:</p>

<ul>
<li>a fairly clean square wave going into the enable pin as expected.</li>
<li>a fairly dirty square wave across the output motor terminals.</li>
</ul>

<p>The motor turns at about 50% speed/torque as expected.</p>

<p>I find myself wondering if it would be better to control the speed of the motor by placing a flat lower constant voltage across its terminals, rather than oscillating a square wave.  ie to do 50% speed/torque - instead of oscilating between 0V and 5V - just put a constant 2.5V across the motor terminals.  I wonder if the oscillation is a waste of power/energy.</p>

<p>Is this true?  Or doesn't it make any difference?  Do high-end motor drivers use a variable flat analog voltage to control speed/torque, or do they use a PWM?  If a PWM, does the frequency make any difference?</p>
","motor"
"7781","How do I decide the size of the time steps between sensing and control actuation?","<p><strong>My Background:</strong></p>

<p>My experience is in solid mechanics and FEA.  So I have zero experience in robotics/controls.  </p>

<p><strong>Problem Description</strong></p>

<p>I'm developing a control strategy to stabilize a complicated 6-legged dynamical system.  Torques <strong><em>Ti</em></strong> from each leg's joints will be used to create a net moment <strong><em>M</em></strong> on the body, stabilizing the system.  This moment <strong><em>M</em></strong> is known from the pre-determined control strategy.  (Side note: the dynamical solver is of the nonlinear computational type)</p>

<p>Due to my lack of background, I have a fundamental confusion with the dynamical system.  I want to use joint torques <strong><em>Ti</em></strong> to create this known net moment <strong><em>M</em></strong> on the body.  This moment <strong><em>M</em></strong> is a function of the</p>

<ol>
<li>current positions/angles of all the leg segments</li>
<li>reaction forces and moments (that cannot be controlled) of each leg</li>
<li>controllable joint torques <strong><em>Ti</em></strong> of each leg</li>
<li>time</li>
</ol>

<p><strong>$(*)$ At a given time $(n-1)\Delta$t:</strong> </p>

<blockquote>
  <p>--From the control strategy, the desired net moment <strong><em>M</em></strong> is computed/known</p>
  
  <p>--One can read/sense the legs' positions, angles, reaction forces, and reaction moments (say, from well placed sensors), at this time $t = (n-1)\Delta$t.  </p>
  
  <p>--From this information, vector algebra easily yields the desired joint torques <strong><em>Ti</em></strong> required to create the net moment <strong><em>M</em></strong></p>
</blockquote>

<p><strong>$(**)$ At the time $(n)\Delta$t:</strong></p>

<blockquote>
  <p>--one applies the previously determined joint torques <strong><em>Ti</em></strong> (determined at $t=(n-1)\Delta$t) to create the desired moment <strong><em>M</em></strong> </p>
  
  <p>--of course these torques <strong><em>Ti</em></strong> are applied at the immediate proceeding time step because they cannot be applied instantaneously</p>
</blockquote>

<p>So this is exactly where my fundamental confusion exists.  The torques <strong><em>Ti</em></strong> were calculated in $(*)$, based on data of angles/positions/reactions in $(*)$, with the objective to create moment <strong><em>M</em></strong>.  However, these torques <strong><em>Ti</em></strong> are applied in $(**)$, where the data (angles/positions/reactions) are <strong>now different</strong> - thus the desired net moment <strong><em>M</em></strong> can never be created (unless you an magically apply actuation at the instantaneous time of sensing).  Am I understanding the controls problem correctly?  </p>

<p><strong>Questions</strong></p>

<ol>
<li>Am I understanding the robotics problem correctly?  What are the terms and strategies around this dilemma?</li>
<li>Of course I could create the time steps between the sensing and the actuation to be infinitely small, but this would be unrealistic/dishonest.  What is the balance between a realistic time step, but also performs the task well?</li>
</ol>
","control actuator stability legged"
"7782","Is it possible to do SLAM with few IR sensors like Buddy?","<p>I saw <a href=""https://www.indiegogo.com/projects/buddy-your-family-s-companion-robot/x/11637197#/story"" rel=""nofollow"">Buddy</a>'s page and want to purchase for my SLAM research. However, I wonder is it possible to program Buddy for SLAM? </p>

<p>According to Buddy's spec, they're only few IR's, sonars and a camera. As I know, most SLAM algorithms are implemented with powerful sensors such as RGBD/stereo camera, or even laser range finder.</p>

<p>Are there any pepers mention about IR-based SLAM?</p>
","mobile-robot localization slam mapping rangefinder"
"7786","Which joints to discretize for IK","<p>I am using <a href=""http://openrave.org/docs/latest_stable/openravepy/ikfast/"" rel=""nofollow"">ikfast</a> in <a href=""http://openrave.org/"" rel=""nofollow"">OpenRave</a> for my inverse kinematics.  This is an analytical solver, so if your robot's DOF matches the IK type's DOF, then you get all possible solutions.  But if your robot has more DOFs, then you need to pick some joints to have a constant value.  (However, if you use OpenRave's Python interface it will discretize that joint for you.  i.e. give you a set of solutions for every 0.1 radians of that joint.  But my question holds for either interface.)  I have a 7 DOF anthropomorphic arm with joints: Roll-Pitch-Roll-Pitch-Roll-Pitch-Yaw as seen in this image:</p>

<p><a href=""http://i.stack.imgur.com/QIfLq.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/QIfLq.png"" alt=""7 DOF arm""></a></p>

<p>The discretized joints are call ""free joints"" in OpenRave's terminology.  If I let ikfast decide, it picks joint 3 (upper arm roll) to be the free joint.  However, I have been using joint 4 (elbow) to be the free joint because it is easier for me to think about.  But then I realized that perhaps joint 5, 6, or 7 would be better to discretize because they are closer to the end of the chain.  Won't the IK solutions suffer if joints closer to the start of the chain have a large discretization?  Or is OpenRave picking the optimal joint to discretize?</p>

<p>I was just wondering if there is some standard practices or known conventions for this sort of thing.</p>

<p>Put simply: I want a set of IK solutions for the end-effector at some pose.  I will fix a joint either near the start or end of the kinematic chain.  And what i set it to isn't going to be perfect.  Lets say it is off from some ""ideal"" position by some epsilon.  Now you can imagine that if i want the hand in-front of the robot, and I pick a bad angle for the shoulder (like straight up for example), the rest of the joints will have a hard time getting the end-effector to the target pose, if at all.  But If I fix the wrist to be at some awkward angle, there is still a good chance of getting the end-effector there, or at lease close.  What kind of trade-offs are there?  Which will have a ""better"" set of solutions?</p>
","inverse-kinematics"
"7790","How to control motor speed via neural network on a dsPIC?","<p>I would like to seek some advise on how I should go about implementing a neural network on a dsPIC to control the motor speed of a robot (if it is at all feasible). Currently, I am deploying a typical PID control loop to control the speed of the motors that have encoders attached to them. </p>

<p>Here are some thoughts on what I think the set-up should be like:</p>

<p>Inputs -> Desired Speed, Current Speed
Output -> PWM
Hidden -> Not sure how many nodes are needed and how many layers</p>

<p>I also face a contradiction on how to do supervised learning for this. Assuming I pass in a desired speed and the current speed, the output,if using sigmoid function, would be from 0-1. I would take this an multiply by the maximum PWM value then compare to the PWM value required to generate that speed. Is this the correct way to determine the error? I am a little unclear on this part. Technically, I could manually determine the PWM values required for certain speeds, but that would result in a very small data set for training. </p>

<p>Alternatively, I considered passing the PWM values into the motor function, wait short period, then capture the current speed of the motor then compare that with the desired speed to get an error. </p>

<p>I only just started coding some basic neural nets and I hope to get some ideas. </p>

<p>Thanks!</p>
","motor pid pwm"
"7791","thrust measurement","<p>I try to find out the relation between rpm vs. thrust for a battery+motor+propeller combination. the image shows my setup and also the measurement result. Can anyone explain how l should use this datas (I know Kv.v gives the rpm but my voltage values decreasing because of P=V.I relation etc.) </p>
","quadcopter brushless-motor esc"
"7794","why is my robot not working","<p>I have a spektrum dx5e transmitter and a ar610 receiver. I believe that they are paired because when I turn on the transmitter the receiver's light turns on. However I lost the bind plug but I somehow managed to bind them. I plugged in my servos and batteries into the correct slots but nothing happens when I move the sticks. What am I doing wrong? I have a Y splitter for two power hd servos (contentious rotation) and another power hd micro servo for the steering but none of them are responding.</p>
","wheeled-robot"
"7795","Can I connect a UDOO to a PC using a straight-through ethernet cable or do I need a cross over?","<p>Can I connect a UDOO board to a PC using a straight-through ethernet cable? Or do I need a cross-over cable?</p>

<p>As far as I know, most modern devices can use the two interchangeably. However, I am not sure if a UDOO can do that. Anyone with any experience?</p>

<p>Thank you for your help.</p>

<p>(PS: I don't have a UDOO on me at the moment, so I can't test it myself. Couldn't find any information in the documentation either).</p>
","embedded-systems"
"7796","Calculating Required Torque","<p>Say I had an object with 4 motors/wheels attached (in a fairly standard arrangement).</p>

<p>I need to calculate the amount of torque required from the motors to be able to move the object of x kilograms consistently (without skipping any steps) at a velocity of y, travelling up a slope of angle z.</p>

<p>I'm guessing this would also depend on factors like the grip of the tyre and such?</p>

<p>Fairly straightforward question (I hope the answer is that way too).
Thanks in advance.</p>
","motor motion torque wheel"
"7802","EKF-SLAM initialize new landmark in covariance matrix","<p>I am trying to implement an EKF-SLAM using the algorithm for unknown correspondences proposed in the book ""Probalistic Robotics"" by Sebastian Thrun in Table 10.2 . </p>

<p>By now I understand actually all of the algorithm except of the initialization of new landmarks in the covariance matrix $ P_{new} $. </p>

<p>In that algorithm when a new landmark is detected the procedure is just the same as if a normal measurment update for an already observed landmark is done: the Kalman gain $ K $ is calculated for the new landmark and then the covariance is updated with that Kalman gain and the jacobian $ H $ of that new landmark like this $ P_{new}=  (I  -  K * H) * P$ . </p>

<p>In my understanding a just new observed landmark would not have any effect on the rows and columns that correspond to already mapped landmarks or the robot pose in the covariance matrix. Instead I think that just two rows and columns for x and y should be created with some uncertainity like proposed here: <a href=""http://robotics.stackexchange.com/questions/4599/the-uncertainty-of-initializing-new-landmark-in-ekf-slam"">the uncertainty of initializing new landmark in EKF-SLAM</a> .</p>

<p>I tried to split down the calculation of $ P_{new}$ via claculating it blockwise to see if I could somehow come to the same initialization as shown in the link above. But I end up having a different covariance matrix where apparently the new landmark is effecting the rows and columns of the parts of the old covariance, which in my view can't be right.</p>

<p>I hope I don't understand the pseudo code of the book wrong or I did a mistake in my try to come to the same initialization. Any advice how the initialization of new lnndmarks work in that code or if it actually is the same as in the link will be appreciated.</p>

<p><strong>Edit</strong></p>

<p>So basically what I am asking is: why would they do a normal Kalman update of the covariance matrix in line 24 of table 10.2 for a new observed landmark? Why is there no explicit case for the initialization of new rows/columns of new observed landmarks in the covariance matrix? It seems to me like they just do a normal measurement update even for a just newly observed landmark.</p>
","mobile-robot slam ekf"
"7803","Gyroscope - How can I remove low frequency component with a high pass filter only?","<p>I'm using Matlab to suppress low frequency components with a high pass filter. </p>

<p><strong>Objective</strong></p>

<ul>
<li>Filter angular velocity measurements affected by high frequency noise and bias in order to get the best estimate of the angular position.</li>
</ul>

<p>The output when the gyroscope is still looks like this.</p>

<p><a href=""http://i.stack.imgur.com/y8isi.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/y8isi.png"" alt=""Simulated angular velocity""></a></p>

<p><strong>First Approach</strong></p>

<p>The easiest way to remove baseline is to remove the average and can be achieved with Matlab using one line of code.</p>

<pre><code>yFilt = y - mean(y)
</code></pre>

<p><strong>Second Approach</strong></p>

<p>We can design a high pass filter to attenuate low frequency components. If we analyze the frequency components of the signal we will see one peak at low frequency and ""infinite"" small components in all frequencies due to Noise. With a second order ButterWorth filter with normalized cutoff freq <code>Wn = 0.2</code> we will get what we are looking for.</p>

<p><a href=""http://i.stack.imgur.com/FVmKM.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/FVmKM.png"" alt=""First half of DFT in normalized scale""></a></p>

<p><strong>Filtered data</strong></p>

<p><a href=""http://i.stack.imgur.com/ELXzb.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/ELXzb.png"" alt=""Filtered data""></a></p>

<p><strong>Tilting the Gyro</strong></p>

<p>When we tilt the gyroscope the situation changes. With a sampling frequency of 300Hz we get the following plot.</p>

<p><a href=""http://i.stack.imgur.com/m1XaJ.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/m1XaJ.png"" alt=""Angular Velocity Wx [deg/sec]""></a></p>

<p>The first half of the dft is shown below in a normalized scale.</p>

<p><a href=""http://i.stack.imgur.com/w8mvU.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/w8mvU.png"" alt=""First half of DFT in normalized scale""></a></p>

<p>You can find the sample.mat file <a href=""https://drive.google.com/file/d/0BwU34T-_OF2saFVGS0hqMUk0cVE/view?usp=sharing"" rel=""nofollow"">here</a></p>

<p>The first approach works great. I would like to apply the second one to this particular case but here there are other low frequency components that make to job harder.</p>

<p><strong>How can I apply the second approach based on the High Pass filter to remove the bias?</strong></p>

<p><strong>EDIT 1</strong> 
You can find more information <a href=""http://userk.co.uk/filtering-matlab-gyroscope-bias/"" rel=""nofollow"">here</a></p>

<p><strong>EDIT 2</strong>
How can we filter this signal to remove bias while keeping the angular velocity information (from 110-th to 300-th sample) intact?</p>

<p>If gyroscopes have the bias problem only when they are not experiencing any rotation, then the offset is present only in the first ~110 samples.</p>

<p>If the above hypothesis is correct, maybe if we apply high pass filtering only in the first 110 samples and desactivate the filter during rotations of the gyro, the estimated angular position will be more accurate.</p>
","gyroscope matlab filter"
"7809","Robot structure kit or materials","<p>I have an arduino, wires, resistors, all of that good stuff. However, I don't have materials to build the structure of the robot. What do you guys recommend? I don't have a place to solder yet so I can't solder but is there a kit or material that you guys recommend? Will it work well with motors and other stuff? Thanks! </p>

<p>P.S. I plan on building a standard driving robot, but I want to be able to make other robots with the same materials/kit. I don't want a kit that only makes one robot, I want a Lego-esque approach to building the structure where I can build whatever I want with it. (Bump2)</p>
","arduino beginner"
"7815","Are there off the shelf solutions for GPS+INS (accelerometer,gyro,magneto) sensor fusion for getting filtered/fused location and speed output?","<p>I am working on a project that needs tracking location and speed of pedestrians/runners/athletes (so not really robotics, but I see a lot of related usage and posts in the robotics domain, and an answer to this question could help with <em>follower</em> robots). I'm interested in just the 2D location (latitude-longitude).</p>

<p>Using just the GPS position has noisy/jump samples and also the degradation due to multi-path near trees etc. From reading about filtering solutions, I understand that sensor fusion that fuses GPS with the data from inertial sensors (INS) helps improve a lot of these issues. Also, this kind of sensor fusion seems to be used in a lot of places -- robotics, wearables, drones etc. Hence I think there might be off the shelf chips/modules/solutions for this, but I couldn't find any.</p>

<p>I found a sensor hub from <a href=""http://www.invensense.com/products/motion-tracking/9-axis/mpu-9250/"" rel=""nofollow"">Invensense</a> that integrates the 9 dof inertial sensors and comes with the fusion firmware, but it doesn't seem to have hookups and firmware for fusing GPS and providing filtered latitude-logitude. </p>

<p>So, what should I be looking for? Are there any off the shelf chips/modules/solutions that come with the built in sensor fusion Software/firmware for doing GPS+INS fusion? </p>

<p>I understand that it will still need tuning some params as well as some calibration.</p>
","localization kalman-filter sensor-fusion gps"
"7816","Displacement with accelerometer","<p>I want to use a sensor to find displacement with accelerometer.
How can I use accelerometer to find displacement? I want to use this for a quadcopter.</p>
","quadcopter imu accelerometer uav"
"7819","Deciding length of quadcopter arms","<p>How quadcopter's arm length affect stability?</p>

<p>As per my view I'll have better control on copter with longer arms but with stresses in arms and also it doesn't affect lift capabilities.</p>
","control quadcopter stability"
"7823","Which sensor type most accurately measures position?","<p>We're building an 6dof joystick, and we need to accurately measure the displacement of our central device. We can easily use a mechanical connection to the edges, but there has been some discussion about what the best way to achieve this is. The range of motion will be fairly small, but accuracy is incredibly important for us.</p>

<p>Which sensors are most easily and accurately measured?</p>

<p>My impulse response is that rotational and linear potentiometers are the most reliable, but others have been arguing for using gyros/accelerometers. I've also heard that hall effect sensors can be used to great effect.</p>
","control sensors accelerometer"
"7825","What device do I need to project a laser to point at a specific location?","<p>I'm searching for a (commercial) projector that just projects a single laser point into the world (e.g. using two moving mirrors). However, I'm struggling because I'm not sure what such a thing is called. I either find area projectors that use lasers, party equipment or laser pointers. </p>

<p>What is the name for such a device? </p>
","laser"
"7828","How is a brushless gimbal motor different from a regular brushless motor?","<p>How are the brushless motors in a gimbal assembly designed?</p>

<p>Obviously it doesn't need continual rotation, but it does need accurate control of precise position. I've noticed that the motors in my gimbal don't have the usual magnetic 'snap' positions that my other motors do. </p>

<p>What are the primary design differences in these kinds of motor, if any?</p>
","motor"
"7829","How To Program Three Wheel Omni","<p>I have created a three wheeled omni robot like the diagram below. Now I am unsure of how to program it. I want to use a single joystick so one x and one y value. The values for x and y are between -1 and 1, also the motors can be set anywhere from -1 to 1. How do I use this data to make the robot move based on the joystick without changing orientations? After doing some initial research this seems like a complex problem, but I am hoping there is a formula that I can.</p>

<p><a href=""http://i.stack.imgur.com/xDIs0.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/xDIs0.png"" alt=""enter image description here""></a></p>
","wheeled-robot"
"7831","Complementary and Kalman filter don't work for Y angle","<p>I'm working on a Python script which reads the data from the <strong>MPU6050</strong> IMU and returns the angles using sensor fusion algorithms: <strong>Kalman</strong> and <strong>Complementary</strong> <strong>filter</strong>. Here is the implementation:
Class MPU6050 reads the data from the sensor, processes it. Class Kalman is the implementation of the Kalman filter. The problem is the next: None of the Kalman, neither the Complementary filter returns appropriate angle values from the <strong>Y</strong> angle. The filters work fine on the <strong>X</strong> angle, but the <strong>Y</strong> angle values make no sense. See the graphs below. I've checked the code million times, but still can't figure out where the problem is. <a href=""http://i.stack.imgur.com/1hgV8.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/1hgV8.png"" alt=""X angle""></a> <a href=""http://i.stack.imgur.com/Hugl7.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/Hugl7.png"" alt=""Y angle""></a></p>

<pre><code>class MPU6050():
    def __init__(self):
        self.bus = smbus.SMBus(1)
        self.address = 0x68

        self.gyro_scale = 131.072 # 65535 / full scale range (2*250deg/s)
        self.accel_scale = 16384.0 #65535 / full scale range (2*2g)

        self.iterations = 2000                    

        self.data_list = array('B', [0,0,0,0,0,0,0,0,0,0,0,0,0,0])
        self.result_list = array('h', [0,0,0,0,0,0,0])       

        self.gyro_x_angle = 0.0
        self.gyro_y_angle = 0.0
        self.gyro_z_angle = 0.0         

        self.kalman_x = Kalman()
        self.kalman_y = Kalman()

    def init_sensor()...

    def calculate_angles(self):
        dt = 0.01

        comp_y = 0.0
        comp_x = 0.0
        print(""Reading data..."")

        while True:             
            self.read_sensor_raw()

            gyro_x_scaled = (self.result_list[4] / self.gyro_scale)
            gyro_y_scaled = (self.result_list[5] / self.gyro_scale)
            gyro_z_scaled = (self.result_list[6] / self.gyro_scale)

            acc_x_scaled = (self.result_list[0] / self.accel_scale)
            acc_y_scaled = (self.result_list[1] / self.accel_scale)
            acc_z_scaled = (self.result_list[2] / self.accel_scale)

            acc_x_angle = math.degrees(math.atan2(acc_y_scaled, self.dist(acc_x_scaled,acc_z_scaled)))
            acc_y_angle = math.degrees(math.atan2(acc_x_scaled, self.dist(acc_y_scaled,acc_z_scaled)))

            comp_x = 0.95 * (comp_x + (gyro_x_scaled * dt)) + 0.05 * acc_x_angle
            comp_y = 0.95 * (comp_y + (gyro_y_scaled * dt)) + 0.05 * acc_y_angle

            kalman_y_angle = self.kalman_y.filter(acc_y_angle, gyro_y_scaled, dt)
            kalman_x_angle = self.kalman_x.filter(acc_x_angle, gyro_x_scaled, dt)

            self.gyro_x_angle += gyro_x_scaled * dt
            self.gyro_y_angle -= gyro_y_scaled * dt
            self.gyro_z_angle -= gyro_z_scaled * dt   

            time.sleep(dt) 

    def read_sensor_raw(self):
        self.data_list = self.bus.read_i2c_block_data(self.address, 0x3B, 14)

        for i in range(0, 14, 2):
            if(self.data_list[i] &gt; 127):
                self.data_list[i] -= 256

            self.result_list[int(i/2)] = (self.data_list[i] &lt;&lt; 8) + self.data_list[i+1]

    def dist(self, a,b):
        return math.sqrt((a*a)+(b*b))

class Kalman():
    def __init__(self):
     self.Q_angle = float(0.001)
     self.Q_bias = float(0.003)
    self.R_measure = float(0.03)

    self.angle = float(0.0)
    self.bias = float(0.0)
    self.rate = float(0.0)

    self.P00 = float(0.0)
    self.P01 = float(0.0)
    self.P10 = float(0.0)
    self.P11 = float(0.0)

def filter(self, angle, rate, dt):
    self.rate = rate - self.bias
    self.angle += dt * self.rate

    self.P00 += dt * (dt * self.P11 - self.P01 - self.P10 + self.Q_angle)
    self.P01 -= dt * self.P11
    self.P10 -= dt * self.P11
    self.P11 += self.Q_bias * dt

    S = float(self.P00 + self.R_measure)

    K0 = float(0.0)
    K1 = float(0.0)
    K0 = self.P00 / S
    K1 = self.P10 / S

    y = float(angle - self.angle)

    self.angle += K0 * y
    self.bias += K1 * y

    P00_temp = self.P00
    P01_temp = self.P01

    self.P00 -= K0 * P00_temp
    self.P01 -= K0 * P01_temp
    self.P10 -= K1 * P00_temp
    self.P11 -= K1 * P01_temp

    return self.angle
</code></pre>

<p><strong>EDIT:</strong>
I've added some information based on @Chuck's answer:</p>

<ul>
<li><code>self.result_list[3]</code> contains the temperature</li>
<li>In my opinion the compl. filter is implemented correctly: <code>gyro_x_scaled</code> and <code>gyro_y_scaled</code> are angular velocities, but they are multiplied by <code>dt</code>, so they give <strong>angle</strong>. <code>acc_?_scaled</code> are accelerations, but <code>acc_x_angle</code> and <code>acc_x_angle</code> are <strong>angles</strong>. Check my comment, where the Complementary filter tutorial is.</li>
<li>Yes, there was something missing in the Kalman filer, I've corrected it.</li>
<li>I totally agree with you, <code>sleep(dt)</code> is not the best solution. I've measured how much time the calculation takes, and it is about 0.003 seconds. The Y angle filters return incorrect values, even if <code>sleep(0.007)</code> or <code>sleep(calculatedTimeDifference)</code> is used.</li>
</ul>

<p>The Y angle filters still return incorrect values.</p>
","kalman-filter"
"7832","Kalman filter for estimating position with “direction” measurements","<p>I am currently working on a pose estimation problem for which I would like to use filtering. To explain the system briefly, it consists of two cameras and each has its own GPS/IMU module. The main assumption is that Camera1 is fixed and stable, whereas camera2 has a noisy pose in 3D. I am using computer vision to obtain the pose (metric translation and rotation) of camera2 w.r.t. camera1, so that I can improve upon the inherent noise of GPS/IMU modules.</p>

<p>The problem here is that the translation obtained through the vision method is only up to an arbitrary scale, i.e. at any given instant, I can only obtain a unit vector that specifies the ""direction"" of the translation and not absolute metric translation. The camera based estimation, although accurate, has no idea about how much actual distance is between the cameras, which is why I have the GPS, which gives me position data with some noise.</p>

<p>Example: camera 2 is 5 m to the east of camera 1, the pose from my vision algorithm would say [1, 0, 0] ; 1 m north-east to camera 1, it would be something like [0.7, 0.7, 0]</p>

<p>Hence, would it be possible to consider the GPS estimate of the metric translation as well as its covariance ellipse, and somehow link it with the normalized camera measurements to obtain a final, more accurate estimate of metric translation? I am not sure what kind of filters would be happy to use a measurement that has no absolute value in it.</p>

<p>Thanks!</p>
","kalman-filter cameras pose"
"7837","Type of servo and torque calculation required for a 2axis robot arm","<p>I am trying to build a 2-axis robot arm with pan and tilt mechanism. The gripper/holder will hold an object weighing 300 grams. The total weight of the arm including the motors will be around 2 kg. I have decided to use 180 degree servo motors. The maximum arm reach will be 340 mm. 
what I want to ask is:</p>

<ol>
<li>What kind of servos (analog/digital) will be suitable to support the
total weight (2 kg) and the object weight (300 g)?</li>
<li>How do I calculate the required torque?</li>
<li>How many servos should I use to make sure that my arm doesn't flip
over?</li>
</ol>

<p>Please suggest me if there is a better approach to designing the robot. I am fairly new to electronics and this is the first time I am building a robot.
Thanks in advance. </p>
","robotic-arm design servos"
"7838","MCU architecture design","<p>Are there any standards regarding single vs multiple MCU in a robotic system? More specifically, if a single MCU can handle all of the sensor data and actuator controls, is it better to use a single MCU or multiple MCUs in a hierarchical manner? Are there any references/papers regarding this topic? What are the arguments towards one or the other? I am looking for facts, not personal opinions, so pros, cons, standards and such.</p>
","design"
"7841","Real-time camera localisation in known environment","<p>I am young researcher/developer coming from different (non-robotic) background and I did some research on camera localisation and I came to the point, where I can say that I am lost and I would need some of your help.</p>

<p>I have discovered that there is a lot of SLAM algorithms which are used for robots etc. As far as I know they are all in unknown environments. But my situation is different.</p>

<p>My problems and idea at the same time is:</p>

<ol>
<li>I will be placed in an known room/indoor environment (dimensions would be known)</li>
<li>I would like to use handheld camera</li>
<li>I can use predefined landmarks if they would help. In my case, I can put some "" unique stickers"" on the walls at predefined positions if that would help in any way for faster localisation.</li>
<li>I would like to get my camera position (with its orientation etc) in realtime(30 Hz or faster).</li>
</ol>

<p>For beginning I would like to ask which SLAM algorithm is the right one for my situation or where to start. Or do you have any other suggestions how to get real time camera positions inside of the known room/environment. It must be really fast and must allow fast camera movements. Camera would be on person and not on robot.</p>

<p>thank you in advance.</p>
","localization slam real-time"
"7850","EMAX ESC Simon Series with arduino","<p>I want to control a brushless motor with the ""EMAX Simon Series 30amp ESC"" and Arduino (Leonardo) board. I am really confused how to do that. I can't understand which beep sounds mean what. I have tested many code examples but they weren't useful.  </p>
","arduino brushless-motor esc"
"7852","Olf Futaba Rx & Tx with APM micro 2.7.2?","<p>Hi there I just found this old Rx &amp; Tx in my loft and need to know weather it is compatible with my APM micro 2.7.2. I already have telemetry but that does not give me manual control. My guess is I need a new Rx because the current one will make a hash of the electronics on the APM. Thanks in advance[![enter image description here][1]][1]</p>

<p>[![enter image descriptere][2]][2]</p>

<p><a href=""http://i.stack.imgur.com/IjEby.jpg"" rel=""nofollow""><img src=""http://i.stack.imgur.com/IjEby.jpg"" alt=""enter image description here""></a></p>

<p><a href=""http://i.stack.imgur.com/dDXch.jpg"" rel=""nofollow""><img src=""http://i.stack.imgur.com/dDXch.jpg"" alt=""enter image description here""></a></p>
","quadcopter"
"7853","How to tune PID for a Y(t) = k*X(t) system?","<p>Could I have your opinions on PID type selection?</p>

<p><a href=""http://i.stack.imgur.com/rrB8u.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/rrB8u.png"" alt=""Plant with a PID controller""></a>
<strong>System description</strong></p>

<ol>
<li>Here comes a very simple system: $\mbox{Output}(t) = k * (\mbox{Input}(t) + \mbox{systemVariable}(t))$. $k$ is constant and $\mbox{systemVariable}(t)$ is a system variable which may change according to time.</li>
<li>The goal of the whole system is to maintain system output at $0$. It has to be as close to zero as possible. The controller has to compensate the $\mbox{systemVariable}$.</li>
<li>The change ($\mbox{systemVariable}$ ) is modeled by a very slow ramp.</li>
</ol>

<p><strong>Controller description</strong></p>

<ol>
<li><p>The controller's input is the output of the system. However, the measurements are always noisy, and I modeled Band-Limited White Noise into the measurements.</p></li>
<li><p>After PID controller, the output goes into an integrator, since the PID controller always calculates the ""change"" of the plant input. </p></li>
</ol>

<p><strong>Questions</strong></p>

<ol>
<li><p>My original thoughts: Add a PID controller with P=1/k is enough. Since every time the controller gets an error $e$, it can be calculated back that the compensation on controller output shall be $e/k$. However, Matlab auto-tuning always give me a PID. Why is that?</p></li>
<li><p>What is the relation between P of PID and measurement noises? If P is large, the system will tend to be rambling largely, due to the noises. If P is small, the system will tend not to converge to the correct value or very slow. How to make the trade-offs? Or how to prevent system from rambling largely and get quick system responses? </p></li>
</ol>

<p>Thanks a lot!</p>
","control pid"
"7855","How is the absolute flash size calculated in a microcontroller?","<p>I am working with an STM32F103C8 which has a flash size of 64kBytes.</p>

<p>Now i am using ChibiOS 2.6 and the build file is a binary file of 82kBytes.</p>

<p>Using ST-Link Utility, the program is getting dumped into the microcontroller's flash. </p>

<p>My question is how come a 82kB code fits in the 64kB Flash?</p>

<p>How is the size of that .bin calculated? I am attaching a picture of the display. </p>

<p>I did a compare ch.bin with device memory and it doesn't report any errors found.</p>

<p>All parts of the code work just fine, i don't see any problems anywhere tried all the features of the code, nothing breaks or behaves abnormally.</p>

<p>Could someone please explain this?</p>

<p>Thanks!</p>

<p><a href=""http://i.stack.imgur.com/IbB5c.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/IbB5c.png"" alt=""STM32 ST-Link Image""></a></p>
","microcontroller"
"7860","iRobot Create Serial Cable for Turtlebot I","<p>I need an iRobot Create Serial Cable (one end 7-pin Mini-DIN Connector and the other end is USB) for Turtlebot I.  How can I connect my bot to my PC?</p>
","mobile-robot irobot-create serial roomba"
"7864","Can RC servo motors continually rotate?","<p>I know that RC servo motors are designed for precise movement, rather than a D.C. motors' continual rotation. Are most RC servo motors limited to movement within one rotation or can they actually be made to continually rotate? That is to say, is their movement limited to a specific arc? Or does it depend on the type of RC servo motor?</p>

<p>I have seen videos of <a href=""https://www.youtube.com/watch?v=EAUcQM5KChc"" rel=""nofollow"">industrial size steppers rotating constantly</a>, but, more specifically, I was wondering whether a MG995 can.</p>

<p><a href=""http://i.stack.imgur.com/vmpb3m.jpg"" rel=""nofollow""><img src=""http://i.stack.imgur.com/vmpb3m.jpg"" alt=""MG995 Stepper motor""></a></p>

<p>I don't own any RC servo motors yet, so I can't actually test it myself. I just want to make sure before I make a purchase. I keep seeing conflicting information, for example the instructable, <a href=""http://www.instructables.com/id/How-to-modify-a-servo-motor-for-continuous-rotatio/"" rel=""nofollow"">How to modify a RC servo motor for continuous rotation (One motor walker robot)</a>, implies that a RC servo motor will not continually rotate, else otherwise, why would there be a need to modify it? </p>

<h3>Addendum</h3>

<p>I have just realised, after <a href=""https://learn.adafruit.com/all-about-stepper-motors/what-is-a-stepper-motor"" rel=""nofollow"">further digging about</a> on google, and as HighVoltage points out in <a href=""http://robotics.stackexchange.com/questions/7864/can-stepper-motors-continually-rotate#answer-7865"">their answer</a>, that I have confused steppers and servos.</p>

<p>In addition, I found out <a href=""https://www.youtube.com/watch?v=cnOKG0fvZ4w"" rel=""nofollow"">how to hack the TowerPro MG995 Servo for continuous rotation</a>.</p>
","stepper-motor rcservo"
"7870","Simple Sensor Fusion for pose estimation","<p>I am currently working on a <a href=""https://www.youtube.com/watch?v=oVgBeTJ4OwY"" rel=""nofollow"">balancing robot project</a>, which features fairly low-cost sensors such as an 9-Dof IMU with the measurement states</p>

<p>$\textbf{x}_\text{IMU} = \left[a_x, a_y, a_z, g_x, g_y, g_z, m_x,m_y,m_z \right]^\text{T}$.</p>

<p>Currently I use the accelerometer and gyroscope readings, fused by a complimentary filter to get the angular deviation of the robot's upright (stable) position. The magnetometer values are tilt-compensated and yield the robots orientation with respect to the earth-magnetic field (awful when close to magnetic distortion). Furthermore I have pretty decent rotational encoders mounted on the wheels which deliver information on a wheel's velocity.</p>

<p>$\textbf{x}_\text{ENC} = \left[v_l,v_r\right]^\text{T}$.</p>

<p>Given these measurements i want to try to get the robots pose (position + heading).</p>

<p>$\textbf{x}_\text{ROB} = \left[x,y,\theta\right]^\text{T}$</p>

<p>I do have minor theoretical knowledge on EKF or KF, but it is not sufficient for me to actually derive a practical implementation. Note that my computational resources are fairly limited (Raspberry Pi B+ with RTOS) and that I want to avoid using ROS or any other non-std libs. Can anybody help me on how to actually approach this kind of problem?
<a href=""http://i.stack.imgur.com/g0imv.jpg"" rel=""nofollow""><img src=""http://i.stack.imgur.com/g0imv.jpg"" alt=""overview""></a></p>
","sensors kalman-filter imu sensor-fusion odometry"
"7872","Simulation environment for conducting visual servoing experiment","<p>I want to conduct the following experiment:
I want to set up a scene with a kuka lwr4+ arm, a 3D model of an object and a camera overlooking them. I want to find the pose of the object using some pose estimation algorithm and move the arm towards the object. 
In general I want a piece of software or a combination of cooperating software that can do all that without having to reinvent the wheel. Is there anything available?</p>
","software simulation visual-servoing"
"7874","What commands make the irobot create 2 go left and right not just forwards and backwards?","<p>I am new to the create 2 and I downloaded real term to program, opened an interface to the robot and send numbers with it to the robot.</p>

<p>I can only get the drive command to work. I only know how to make the robot go faster, turning around or slower.</p>

<p>I would like to know how to make the other commands work along with making it go left and right.</p>
","irobot-create"
"7877","Original paper of Kalman filter","<p>Recently we've encountered Kalman filter algorithm for state estimation in a course of Probabilistic Robotics.</p>

<p>After taking several days to try to read Kalman's original paper published in 1960, ""A New Approach to Linear Filtering and Prediction Problems"", it firstly feel a bit difficulty to read, and it seems the majority is to show the orthogonal projection is the optimal estimation under certain conditions and solutions to Wiener's problem.</p>

<p>But I did not find the exact algorithm in this original paper as the one in the textbook. </p>

<ul>
<li>For example, is there an explanation of ""Kalman gain"" in this paper ?</li>
<li>Does Kalman's paper provide a mathematical derivation of Kalman
filter algorithm?</li>
</ul>
","kalman-filter"
"7879","Forward kinematics: Why ω should remain same?","<p>Can anyone please explain me these lines found on <a href=""http://www.diva-portal.org/smash/get/diva2:796235/FULLTEXT01.pdf"" rel=""nofollow"">page 5 of 
<em>Kinematics Equations for Differential Drive and Articulated Steering</em>
by <em>Thomas Hellström</em></a>?</p>

<blockquote>
  <p>Note that plugging in $r$ and $v$ for both left and right wheel result in the same $\omega $ (otherwise the wheels would move relative to each other). Hence, the following equations hold:</p>
  
  <p>$$
\begin{align}
\omega~ \left(R+\frac{l}{2}\right) &amp;= v_r\\
\omega~ \left(R-\frac{l}{2}\right) &amp;= v_l\\
\end{align}$$</p>
  
  <p>where $R$ is the distance between ICC and the midpoint of the wheel axis, and $l$ is the
  length of the wheel axis (see Figure 6). </p>
  
  <p><a href=""http://i.stack.imgur.com/M9Y7g.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/M9Y7g.png"" alt=""Figure 6""></a></p>
  
  <p>Figure 
  6
  .
  When left and right wheel rotate with different speeds, the robot rotates around
  a common point denoted ICC
  My questions are:</p>
</blockquote>

<ol>
<li><p>How do these equations come to be?</p></li>
<li><p>Why does $\omega$ have to be same if we want to analyse the behavior after changing the wheel velocity relative to other?</p></li>
<li><p>How do we know about the circle, in which the robot rotates by doing variations in one wheel velocity, surely passes through the center point between two wheels.</p></li>
</ol>
","mobile-robot"
"7881","How do I dispense a greasy fluid?","<p>I'm a agricultural engineering student and complete newbie trying to build a simple mechanism attached to a drone that dispenses a <a href=""http://ocp.com.au/wp-content/uploads/2014/07/ISCA+SPLAT+Bloom+MSDS.pdf"" rel=""nofollow"">grease-type fluid</a>. However, since I'm not familiar with the field, I'm having a hard time googling because I don't know the correct terms to search for. </p>

<p>I'm looking for a mechanism that will remotely push the grease out. The problem is carrying the necessary weight for an hectare (300g to 1,5kg of fluid) and the dispenser mechanism within the drone. So I'm looking for a lightweight dispenser mechanism capable of deliver small amounts of this fluid (3g) distributed on the trees canopy. The grease do not need to be heated as it flows naturally in normal temperatures (like a toothpaste). Both pump or syringe-type arrangement would be fine as long as I can control it remotely.</p>
","motor quadcopter design battery actuator"
"7882","How to determine x-axis if the two z-axis are intersecting in Denavit Hartenberg representation","<p>Suppose I have a 3 link(1 dimensional) chain in which all the joints are revolutes, the axis of first revolute joint is along Z-axis(global) and axis of second joint is along X-axis(global). The first link is along X-axis(global) and second link is along Z-axis(global).</p>

<p>Now in order to use DH representation I introduced a local frame for link 1 at joint 1(z axis along Z and x axis along X) and another frame at joint 2.Here z-axis is along axis of rotation(global X) and here I am clueless how to determine x-axis for joint 2 because the two z axis are intersecting.(standard procedure is to find common normal between two z axis)<a href=""http://i.stack.imgur.com/0EL6h.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/0EL6h.png"" alt=""3-link chain""></a>
Thanks for your time.</p>
","robotic-arm dh-parameters"
"7883","How can I get the values of a IMU from the serial message received in Simulink via UART?","<p><em>I try to read IMU sensor data from an Arduino mega 2560 UART with serial receive block of Arduino support package for simulink. The IMU can send binary packets and also nmea packets and I can configure it to any output. When the serial recieve block output is directly used, it displays just the numbers between 0-255. l need help about how to parse the coming data which contains the euler angles that I want to use.</em></p>

<p>Here is <strong>binary structure ;</strong></p>

<p>""<strong>s</strong>"",""<strong>n</strong>"",""<strong>p</strong>"",<strong>packet type(PT)</strong>,<strong>Address</strong>,<strong>Data Bytes (D0...DN-1)</strong>,<strong>Checksum 1</strong>,<strong>Checksum 0</strong></p>

<p>The PT byte specifies whether the packet is a read or a write operation, whether it is a batch operation, and the length of the batch operation (when applicable). The PT byte is also used by the UM7 to respond to commands. The specific meaning of each bit in the PT byte is given below.</p>

<p>Packet Type (PT) byte;</p>

<p>7 Has Data,
6 Is Batch,
5 BL3,
4 BL2,
3 BL1,
2 BL0,
1 Hidden,
0 CF</p>

<p>Packet Type (PT) Bit Descriptions;</p>

<p>7...Has Data: If the packet contains data, this bit is set (1). If not, this bit is cleared (0). </p>

<p>6...Is Batch: If the packet is a batch operation, this bit is set (1). If not, this bit is cleared (0) </p>

<p>5:2..Batch Length (BL): Four bits specifying the length of the batch operation. Unused if bit 7 is cleared. The maximum batch length is therefore 2^4 = 16 </p>

<p>1...Hidden: If set, then the packet address specified in the “Address” field is a “hidden” address. Hidden registers are used to store factory calibration and filter tuning coefficients that do not typically need to be viewed or modified by the user. This bit should always be set to 0 to avoid altering factory configuration.</p>

<p>0...Command Failed (CF): Used by the autopilot to report when a command has failed. Must be set to zero for all packets written to the UM7.</p>

<p>The address byte specifies which register will be involved in the operation. During a read operation (Has Data = 0), the address specifies which register to read. During a write operation (Has Data = 1), the address specifies where to place the data contained in the data section of the packet. For a batch read/write operation, the address byte specifies the starting address of the operation.
The ""Data Bytes"" section of the packet contains data to be written to one or more registers. There is no byte in the packet that explicitly states how many bytes are in this section because it is possible to determine the number of data bytes that should be in the packet by evaluating the PT byte.
If the Has Data bit in the PT byte is cleared (Has Data = 0), then there are no data bytes in the packet and the Checksum immediately follows the address. If, on the other hand, the Has Data bit is set (Has Data = 1) then the number of bytes in the data section depends on the value of the Is Batch and Batch Length portions of the PT byte.
For a batch operation (Is Batch = 1), the length of the packet data section is equal to 4*(Batch Length). Note that the batch length refers to the number of registers in the batch, NOT the number of bytes. Registers are 4 bytes long.
For a non-batch operation (Is Batch = 0), the length of the data section is equal to 4 bytes (one register). The data section lengths and total packet lengths for different PT configurations are shown below.
The two checksum bytes consist of the unsigned 16-bit sum of all preceding bytes in the packet, including the packet header.</p>

<p><strong>Read Operations;</strong></p>

<p><strong>To initiate a serial read of one or more registers aboard the sensor, a packet should be sent to the UM7 with the ""Has Data"" bit cleared. This tells the device that this will be a read operation from the address specified in the packet's ""Address"" byte. If the ""Is Batch"" bit is set, then the packet will trigger a batch read in which the ""Address"" byte specifies the address of the first register to be read.
In response to a read packet, the UM7 will send a packet in which the ""Has Data"" bit is set, and the ""Is Batch"" and ""Batch Length"" bits are equivalent to those of the packet that triggered the read operation. The register data will be contained in the ""Data Bytes"" section of the packet.</strong></p>

<p>here is an Example Binary Communication Code;</p>

<pre><code>{
uint8_t Address;
uint8_t PT;
uint16_t Checksum;
uint8_t data_length;
uint8_t data[30];
} 
UM7_packet;
// parse_serial_data.This function parses the data in ‘rx_data’ with length ‘rx_length’ and attempts to find a packet in the data. If a packet is found, the structure ‘packet’ is filled with the packet data.If there is not enough data for a full packet in the provided array, parse_serial_data returns 1. If there is enough data, but no packet header was found, parse_serial_data returns 2.If a packet header was found, but there was insufficient data to parse the whole packet,then parse_serial_data returns 3. This could happen if not all of the serial data has been received when parse_serial_data is called.If a packet was received, but the checksum was bad, parse_serial_data returns 4. If a good packet was received, parse_serial_data fills the UM7_packet structure and returns 0.

uint8_t parse_serial_data( uint8_t* rx_data, uint8_t rx_length, UM7_packet* packet )
{
uint8_t index;
// Make sure that the data buffer provided is long enough to contain a full packet The minimum packet length is 7 bytes
if( rx_length &lt; 7 )
  {
  return 1;
  }
// Try to find the ‘snp’ start sequence for the packet
for( index = 0; index &lt; (rx_length – 2); index++ )
  {
  // Check for ‘snp’. If found, immediately exit the loop
  if( rx_data[index] == ‘s’ &amp;&amp; rx_data[index+1] == ‘n’ &amp;&amp; rx_data[index+2] == ‘p’ )
    {
    break;
    }
  }
uint8_t packet_index = index;
// Check to see if the variable ‘packet_index’ is equal to (rx_length - 2). If it is, then the above loop executed to completion and never found a packet header.
if( packet_index == (rx_length – 2) )
  {
  return 2;
  }
// If we get here, a packet header was found. Now check to see if we have enough room left in the buffer to contain a full packet. Note that at this point, the variable ‘packet_index’contains the location of the ‘s’ character in the buffer (the first byte in the header)
if( (rx_length – packet_index) &lt; 7 )
  {
  return 3;
  }
// We’ve found a packet header, and there is enough space left in the buffer for at least the smallest allowable packet length (7 bytes). Pull out the packet type byte to determine the actual length of this packet
uint8_t PT = rx_data[packet_index + 3];
// Do some bit-level manipulation to determine if the packet contains data and if it is a batch.We have to do this because the individual bits in the PT byte specify the contents of the packet.
uint8_t packet_has_data = (PT &gt;&gt; 7) &amp; 0x01; // Check bit 7 (HAS_DATA)
uint8_t packet_is_batch = (PT &gt;&gt; 6) &amp; 0x01; // Check bit 6 (IS_BATCH)
uint8_t batch_length = (PT &gt;&gt; 2) &amp; 0x0F; // Extract the batch length (bits 2 through 5)
// Now finally figure out the actual packet length
uint8_t data_length = 0;
if( packet_has_data )
  {
  if( packet_is_batch )
    {
    // Packet has data and is a batch. This means it contains ‘batch_length' registers, each // of which has a length of 4 bytes
    data_length = 4*batch_length;
    }
  else // Packet has data but is not a batch. This means it contains one register (4 bytes)
    {
    data_length = 4;
    }
  }
else // Packet has no data
  {
  data_length = 0;
  }
// At this point, we know exactly how long the packet is. Now we can check to make sure we have enough data for the full packet.
if( (rx_length – packet_index) &lt; (data_length + 5) )
  {
  return 3;
  }
// If we get here, we know that we have a full packet in the buffer. All that remains is to pullout the data and make sure the checksum is good. Start by extracting all the data
packet-&gt;Address = rx_data[packet_index + 4];
packet-&gt;PT = PT;
// Get the data bytes and compute the checksum all in one step
packet-&gt;data_length = data_length;
uint16_t computed_checksum = ‘s’ + ‘n’ + ‘p’ + packet_data-&gt;PT + packet_data-&gt;Address;
for( index = 0; index &lt; data_length; index++ )
  {
  // Copy the data into the packet structure’s data array
  packet-&gt;data[index] = rx_data[packet_index + 5 + index];
  // Add the new byte to the checksum
  computed_checksum += packet-&gt;data[index];
  }
// Now see if our computed checksum matches the received checksum 
// First extract the checksum from the packet
uint16_t received_checksum = (rx_data[packet_index + 5 + data_length] &lt;&lt; 8);
received_checksum |= rx_data[packet_index + 6 + data_length];
// Now check to see if they don’t match
if( received_checksum != computed_checksum )
  {
  return 4;
  }
// At this point, we’ve received a full packet with a good checksum. It is already fully parsed and copied to the ‘packet’ structure, so return 0 to indicate that a packet was processed.
return 0;
}
</code></pre>
","arduino electronics embedded-systems matlab"
"7884","Using Blob detection in V-Rep","<p>I was trying to reproduce <a href=""https://www.youtube.com/watch?v=kOjQRYmeX_o"" rel=""nofollow"">this</a> youtube tutorial in V-rep and I came across some problems concerning blob detection. There are some complaints on this matter under the video. I don't believe that blob detection stopped working in recent v-rep versions, but I was unable to make it work (as a new v-rep user myself). Has anyone any idea how to properly implement it? </p>

<p>More specifically, I have a vision sensor named <code>cam</code> and I want it to follow a red ball. The vision sensor will detect the position of the ball and I will use it to control the joints that steer the sensor (<code>yaw</code> and <code>pitch</code>). My script follows</p>

<pre><code>threadFunction=function()
    yaw=simGetObjectHandle(""yaw"")
    pitch=simGetObjectHandle(""pitch"")
    cam=simGetObjectHandle(""cam"")
    while simGetSimulationState()~=sim_simulation_advancing_abouttostop do
        result,pack1,pack2=simReadVisionSensor(cam)
        if result&gt;0 then
            xtarget=pack2[5]
            ytarget=pack2[6]
            simAuxiliaryConsolePrint(out,string.format(""\n x: %0.2f, y: %0.2f"",xtarget,ytarget))
            simSetJointTargetVelocity(yaw,1*(0.5-xtarget))
            simSetJointTargetVelocity(pitch,1*(0.5-ytarget))
        end
    end
end

simSetThreadSwitchTiming(2)
out = simAuxiliaryConsoleOpen(""Debug"",8,1)
res,err=xpcall(threadFunction,function(err) return debug.traceback(err) end)
if not res then
    simAddStatusbarMessage('Lua runtime error: '..err)
end
</code></pre>

<p>When I run the simulation I can see that the sensor sees the red ball at some point but <code>result</code> is always 0 meaning that no detection takes place. <a href=""http://i.stack.imgur.com/tEQhF.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/tEQhF.png"" alt=""image taken during simulation""></a> </p>

<p>Here is <a href=""https://www.dropbox.com/s/8hchlzjzabjrmoy/visualServoing.ttt?dl=0"" rel=""nofollow"">my scene</a></p>
","simulator visual-servoing"
"7886","How to numerically calculate the Jacobian?","<p>I'm trying to calculate the Jacobian for days now. But first some details. Within my Master's Thesis I have to numerically calculate the Jacobian for a tendon-driven continuum Robot. I have all homogeneous transformation matrices as I already implemented the kinematics for this Robot. Due to it's new structure there are no discrete joint variables anymore but rather continuous parameters. Therefore I want to compute the Jacobian numerically.
It'd be awesome if someone could provide a detailed way how to compute the numerical Jacobian for a 6-DoF rigid-link robot (only rotational joints => RRRRRR). From that I can transfer it to the continuum robot.</p>

<p>I've already started computing it. Let T be the homogeneous transformation matrix for the Endeffector (Tip)  with</p>

<p>$$T=\begin{bmatrix}R &amp; r \\ 0 &amp; 1 \end{bmatrix} $$</p>

<p>with R = rotational matrix (contains orientation) and $ r = \begin{bmatrix} x &amp; y &amp; z \end{bmatrix}^T$ endeffector position.
My approach is to compute the first three rows of J by successively increasing the joints, computing the difference to the ""original"" joint values and dividing it by the increment delta, the joint-space is $ q = \begin{bmatrix} q_1 &amp; q_2 &amp;  q_3 &amp; ... &amp;q_6 \end{bmatrix}T $</p>

<p>$q_1 = q_1 + \delta$ => $J(1,1) = (X_{increment} - X_{orig})/\delta$ </p>

<p>$q_2 = q_2 + \delta$ => $J(1,2) = (X_{increment} - X_{orig})/\delta$   </p>

<p>and so on. I do the same for the y and z coordinates. So I get the first 3 rows of J. </p>

<p>Now I don't know how to compute the last three rows as they refer to the rotational Matrix R. Since it's a 3x3 matrix and no scalar value I don't know how to handle it. </p>
","jacobian"
"7889","Forward kinematics of constrained double pendulum","<p>I was wondering whether maybe you could help me with this problem. I have a double pendulum. I have set the origin of cartesian coordinates to be the ""head"" of the first arm, which is fixed. The end of the second arm is attached to a block that slides along the x-axis. What I want to do is to derive the equations relating the pendulum's angles with the distance from the origin to the block. </p>

<p>Now, I know how I could go about deriving the equations without the constraint. </p>

<p>$$x_1 = L_1cos(a_1)$$
$$y_1 = L_1sin(a_1)$$</p>

<p>Where $x_1$ and $y_1$ is where the first arm joins the second arm and $a_1$ is the angle between the horizontal and the first arm. </p>

<p>Similarly, I can derive the equations for the end of the second arm 
$x_2 = x_1 + L_2 cos(a_2)$ and $y_2 = y_1 - L_2 sin(a_2)$</p>

<p>Now then, if I attach a sliding block to the end of my second arm, I don't know whether my equation for $x_2$ would change at all. I don't think it would but  would I have to somehow restrict the swing angles so that the block only moves along the x direction? </p>

<p>Well, basically the problem is finding the equation of $x_2$ if it's attached to a block that only moves along the x- direction. </p>
","forward-kinematics"
"7894","Beginner Soldering question","<p>So, I need to know a couple of things about soldering. My primary workspace, robotics and otherwise is a desk with a computer and only a little bit of free space (4ft. by 6 in.). I am wondering if it is safe to solder in such a small area. Also, what level of ventilation do I need to solder safely? My desk is in a normal house room and my desk is write next to an air vent. My house has heating and A/C? Do I need a fan or a fume sucker thing? I plan to only solder a little to get things to stay in my solder less bread board (soldering header pins onto wires and such). So, basically, what are the minimum requirements for soldering safely (space and ventilation). Also, if anyone could point me to some hobby/beginner level soldering must-haves on amazon that would be great, thanks.</p>
","beginner"
"7895","IRobot Create 2: Powering Up after Sleep","<p>I've notice the IRobot Create 2 does not respond to the app's commands when it has been sleeping.  If I press the <em>Clean</em> button and re-run the app then the robot is responsive to the commands.</p>

<p>My initialization sequence (Android/Java) using <a href=""https://github.com/mik3y/usb-serial-for-android"" rel=""nofollow"">usb-serial-for-android</a>:</p>

<pre><code>port.open(connection);
port.setParameters(115200, 8, UsbSerialPort.STOPBITS_1,UsbSerialPort.PARITY_NONE);
command(Opcode.START);
command(Opcode.SAFE);
</code></pre>

<p>The physical architecture is IRobot Create 2 connected by IRobot Serial Cable to Google Project Tango Tablet.</p>

<p>How can my app wake up the Roomba from it's sleep?</p>
","irobot-create"
"7896","How to rotate a dc motor at a fixed rpm","<p>I am using 8051 microcontroller and a dc motor.What to do if i have to rotate the motor at any fixed rpm. Let's say 120rpm.</p>

<p>And if it is possible by generating pwm,how to do the calculations for the relation between duty cycle and rpm?</p>
","control motor microcontroller"
"7903","How can I tell if a servo motor is capable of being controlled degree by degree?","<p>I want to create a rotating control mechanism that can turn a surface to face any direction in a sphere. My dad (an electrical engineer) said I can probably do it by connecting two servo motors together. </p>

<p>I am looking for a servo motor that can do what I want to do, which is moving the sphere with decent precision (within ~1 degree) but I don't know which kinds of motors are able to handle such precision. </p>

<p>Another challenge is that one servo will have to hold the second servo on top. As I understand it, the torque rating determines the maximum amount of force the servo can exert on its load so I can figure out if the servo is strong enough through some math?</p>
","control servos"
"7910","How can I recognize animals in a video stream or static images with openCV or other library/software?","<p>I'm a software developer not experienced in AI or machine learning, but I'm now interested in developing this kind of software. I want to develop software that recognizes some specific objects, specifically, animals from a video stream (or a sequence of static images).</p>

<p>I saw there's a library called openCV which is often commented in this forum, but what I saw so far is this library is a helper for working with images, I didn't find the <em>object recognition</em> or <em>self learning</em> part.</p>

<p>Is <em>openCV</em> a good starting point? better go for some theory first? or there are other already developed libraries or frameworks aimed for object recognition?</p>

<p><strong>EDIT</strong>
To give some context: I will have ona camera checking a landscape, mostly static but some leaves may move with the wind or some person may step in, and I want to get an alert when some animal is into view, I can reduce the ""animals"" to only birds (not always I will have a nice bird/sky contrast).</p>

<p>I did some work with supervised neural networks some 15 years ago and studied some AI and machine learning theory, but I guess things have improved way too much since then, that's why I was asking for some more practical first steps.</p>

<p>Thank you</p>
","computer-vision"
"7912","Powering a Project Tango Tablet with iRobot Create 2","<p>Project Tango Development Kits come with a mini-dock (see picture below).  </p>

<p>I am controlling the iRobot Create 2 by the mounted Tablet using the USB cable provided plugged into the mini-dock. (see <a href=""https://developers.google.com/project-tango/hardware/tablet#get_started"" rel=""nofollow"">docs</a>).</p>

<blockquote>
  <p>The USB 3.0 port on the mini-dock is only functional when the tablet is docked. The port can be used to attach an external memory drive or standard peripherals to the tablet.</p>
</blockquote>

<p>I wish to recharge the tablet using the power from the iRobot. The mini dock comes with a port for external charging:</p>

<blockquote>
  <p>The mini-dock accepts a power adapter for faster charging (not provided). The power adapter output must be 12V, 2A, and the connector must be a barrel plug with 5.5mm outer diameter, 2.1mm inner diameter, center positive.</p>
</blockquote>

<p>Ideally the charging would happen only when the iRobot is also charging, but charging all the time is acceptable. </p>

<p>Is this possible?  If so, how?</p>

<p><a href=""http://i.stack.imgur.com/595xX.jpg"" rel=""nofollow""><img src=""http://i.stack.imgur.com/595xX.jpg"" alt=""enter image description here""></a></p>
","irobot-create"
"7913","Non linear control system?","<p>I have a dual (sequential) loop control system controlling the angle of a rotational joint on a robot using an absolute encoder. I have tuned the inner control loop (for the motor) and am now working on tuning the outer loop (for the joint).</p>

<p><em>Example of a dual loop controller</em>
<a href=""http://i.stack.imgur.com/IuDx0.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/IuDx0.png"" alt=""dual loop""></a></p>

<p>When I disturb the system the response isn't what I would expect.</p>

<p><em>Kp = 0.4</em>
<a href=""http://i.stack.imgur.com/fmBnB.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/fmBnB.png"" alt=""enter image description here""></a></p>

<p><em>Kp = 0.1 Kd = 0.001</em>
<a href=""http://i.stack.imgur.com/PBz8Q.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/PBz8Q.png"" alt=""enter image description here""></a></p>

<p>I didn't add a <em>Ki</em> term because I don't have any steady state error.</p>

<p>I'm confused by the fact that the second overshoot in the first plot is larger than the first one. No matter how I adjust the parameters I can't seem to get rid of the oscillation in the velocity of the joint (seen in the second plot). One limitation I have is if I increase both <em>Kp</em> and <em>Kd</em> too high the gearbox of the becomes very noisy because the noise in the encoder signal creates larger adjustments in the position of the motor. I'm working on adding a filter to the output using the method described <a href=""http://controlguru.com/pid-with-controller-output-co-filter/"" rel=""nofollow"">here</a>.</p>

<p>The code I'm using for the outer loop is:</p>

<pre><code>static float e_prev = 0.0;

e = joint_setpoint - joint_angle;
e_i += e/0.001; // dt = 0.001s
e_d = (e - e_prev)/0.001; // dt = 0.001s

e_prev = e;

motor_setpoint += k_p * e + k_i * e_i + k_d * e_d;
</code></pre>

<p>I'm beginning to think that the system might not be able to be modeled by a first order equation, but would this change the implementation of the control loop at all? Any advice is appreciated!</p>

<p>Ben</p>
","pid"
"7915","Arduino mega shield v2.0 compatibility with arduino due","<p>Like the title says.. Will it work? I know about the due 3.3 volt limitations.</p>

<p>I want to build a hexapod with 18 servo's.</p>

<p>The shield I am looking at:</p>

<p><a href=""http://yourduino.com/sunshop2/index.php?l=product_detail&amp;p=195"" rel=""nofollow"">http://yourduino.com/sunshop2/index.php?l=product_detail&amp;p=195</a></p>

<p>If it isn't compatible. Is there an alternative shield which will work? I can't seem to find much for the due.</p>
","arduino"
"7919","Telemetry with APM 2.6 and XBee","<p>The transmission of telemetry data between the ground base station and APM 2.x (Arducopter), using XBee, is not well documented. The only documentation is <a href=""http://copter.ardupilot.com/wiki/common-optional-hardware/common-telemetry-landingpage/common-telemetry-xbee/"" rel=""nofollow"">Telemetry-XBee</a>, but it does not specify what XBee version is used. I have been checking and I guess is version 1 (this one has P2P link and the others not), but I am not sure.</p>

<p>I would like to know, what XBee modules people use for flying drones? Do they have problems with the APM connection? How can I control the drone remotely using the XBee link with <a href=""http://qgroundcontrol.org/mavlink/start"" rel=""nofollow"">Mavlink</a> protocol?</p>
","quadcopter radio-control mavlink"
"7920","Inverse kinematics solution for 6DOF serial arm","<p>My 6 joint robot arm structure doesn't meet the requirements for a closed form solution (no 3 consecutive axes intersecting at a point or 3 parallel axes...).  </p>

<p>What would be best method to adopt to get solution in 1ms or less? Estimation accuracy of 1mm. I'm assuming the computation is done on an average laptop Intel Core i3, 1.7GHz, 4GB RAM</p>
","inverse-kinematics"
"7921","Robot localization without any sensors","<p>Is it possible to localize a robot without any sensors, odometer and servo motors?</p>

<p>Assume robot has dc motors and no obstacles.</p>
","localization mapping"
"7925","Positioning Sensor","<p>I would like to locate the position of a stationary autonomous robot in x-y-z axis relative to a fixed starting point.</p>

<p>Could someone suggest sensors that would be suitable for this application?</p>

<p>I am hoping to move the robot in 3D space and be able to locate it's position wirelessly. The rate of position update is not important as I would like to stop the robot from moving and relay the information wirelessly.</p>

<p>The range I am looking for is roughly 2 KM + (the more the better) with accuracy of  +/- 1 CM.</p>

<p>Is there any system that could do this? Thanks for your help.</p>
","sensors imu"
"7928","Robot path planning","<p>My goal is to move robot in certain points as shown in the figure. It's initial position is (x0,y0) and move along other coordinates.</p>

<p><a href=""http://i.stack.imgur.com/TbgKo.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/TbgKo.png"" alt=""enter image description here""></a></p>

<p>I am able to track robot position using a camera which is connected to pc and camera is located at the top of the arena. I've mounted a ir beacon on the robot, camera find this beacon and locates it's coordinate(in cm) in the arena. Using this coordinate how can I move my robot to another position, say new position (x1,y1)</p>

<p>My robot has arduino mega 2560 with two DC motors, communication between pc and robot is done using bluetooth</p>

<p>Update:</p>

<p>Thanks @Chuck for the answer, however I still have few doubts regarding turning angle.</p>

<p>My robot position setup is as shown in the image.</p>

<p>(xc, yc) is the current position and (xt, yt) is the target position.
<a href=""http://i.stack.imgur.com/MsCA9.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/MsCA9.png"" alt=""enter image description here""></a></p>

<p>If I want to align robot in the direction of the target coordinates, I've to calculate atan2 between target and current coordinates. But the angle remains same since it's current position is not changing with respect to the target point. so I assume robot simply makes 360' rotation at current position?</p>

<p>Update:</p>

<p>The path points is as show below in the image, is my initial heading angle assumption is correct? </p>

<p>'1' is the starting point.</p>

<p><a href=""http://i.stack.imgur.com/buKFd.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/buKFd.png"" alt=""enter image description here""></a></p>

<p>Update</p>

<p>Thank you for your patience and time, I'm still struck at turning, my code goes like this</p>

<pre><code>//current points
float xc = -300;
float yc = 300;

//target points

float xt = -300;
float yt = -300;

//turning  angle
float turnAngle;

void setup() {
  // pin setup
  Serial.begin(9600);
}

void loop() {

  turnAngle = atan2((yt-yc), (xt-xc)); //calculate turning angle

  turnAngle = turnAngle * 180/3.1415; //convert to degrees

  if (turnAngle &gt; 180) {
    turnAngle = turnAngle-360;
  }

  if (turnAngle &lt; -180) {
    turnAngle = turnAngle+360;
  }

  if (turnAngle &lt; -10) {
    //turn right
  }

  if (turnAngle &gt; 10) {
    //turn left
  }
}
</code></pre>

<p>Since angle is always -90' robot only makes right turn in loop at current point, since angle is not changing. I think I'm missing something here.</p>
","arduino navigation"
"7930","Can i charge a lipo nano tech battery over imax b3 charger","<p>Can i charge a lipo nano tech battery over imax b3 charger. 2650mah 35/70c 3s is the battery</p>
","battery lithium-polymer"
"7937","Multiple EKFs or one big","<p>Let's say I would like to use an EKF to track the position of a moving robot. The EKF would not only estimate the position itself but also variables affecting the position estimate, for example IMU biases, wheel radius, wheel slip and so on.</p>

<p>My question is, i<strong>s it better to use one big EKF (state vector containing all estimated variables) or multiple smaller EKFs (each one responsible for tracking a subset of all variables to be estimated)?</strong> Or is there no difference?</p>

<p>As for the example above, the EKF could be split into one for tracking position, one for estimating wheel radius and slip and one for estimating IMU biases. The position EKF would of course use the estimations output from the other concurrent EKFs and vice versa.</p>

<p>To me it seems it would be easier to tune and test multiple smaller EKFs rather than just one big. Are there any other advantages/disadvantages (execution time, ease of debugging etc.) assuming the resulting estimates are equal in the two approaches (or close enough at least)?</p>

<p>Thanks,
Michael</p>
","kalman-filter ekf"
"7940","recommendation for really high precision attitude measurement sensors","<p>I am new in this field, I am looking for some high precision gyroscopes and accelerometers for attitude measurements.The precision requirement is around 0.2~0.5 deg/s dynamic.
    I have done some digging myself, not a single integrated MEMS sensor can do that without costing too much. So some heavy math is needed but that's fine.I need to make sure the prefect sensors are chosen, the budget is less than 100USD.
    can any one help, thanks in advanced.</p>
","sensors research"
"7942","Wall following using hokuyo lidar and sharp IR sensors","<p>I have a mobile robot and I would like it to follow the walls of a room.</p>

<p>I have:</p>

<ul>
<li>A map of the room. </li>
<li>Wheel encoders for the odometry.</li>
<li>A Kalman filter for fusing data from wheel encoders and IMU. </li>
<li>A Hokuyo lidar for localization and obstacle avoidance</li>
<li>A Kinect to see obstacles which can not be seen by the Hokuyo. </li>
<li><a href=""http://wiki.ros.org/amcl"" rel=""nofollow"">Amcl</a> for localization.</li>
<li>A couple of sharp sensors on the side for wall following. </li>
</ul>

<p>I am not planning to use the global or local <a href=""http://wiki.ros.org/costmap_2d"" rel=""nofollow"">costmap</a> because the localization of the robot is not perfect and the robot might think that it is closer (or further away) to the wall than it actually is and therefore, wall following might fail. So, I am planning to just use the data from Hokuyo lidar and sharp sensors to do wall following and maintain constant distance from the wall (say 10 cm). </p>

<p>Now, I would like to know what is the best technique for doing wall following in this manner? Also, how can one deal with the issue of open gaps in the wall (like open doors, etc..) while doing wall following using the above approach?</p>

<p>I know this is a very general question but any suggestions regarding it will be appreciated. Please let me know if you need more information from me.</p>

<p><strong>Update:</strong><br>
I am just trying to do wall following in a given room (I have the vertices of the room in a global reference frame) For example, Lets say I have a map of a room (shown below). I want to make the robot follow the wall very closely (say 10 cm from the wall). Also, if there is an open space (on bottom left), the robot should not go in the adjacent room but should keep on doing wall following in the given room (For this, I have the boundary limits of the room which I can use to make sure the robot is within the given room).<br>
The approach which I am thinking is to come up with an initial global path (set of points close to the wall) for wall following and then make sure robot goes from one point to the next making sure that it always maintains a certain distance from the wall. If there is no wall, then the robot can just follow the global path (assuming localization is good). I am not sure about its implementation complexity and whether there is a better algorithm/ approach to do something like this.</p>

<p><a href=""http://i.stack.imgur.com/9lFqg.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/9lFqg.png"" alt=""enter image description here""></a></p>
","sensors localization navigation"
"7944","Compatibilty of my setup?","<p>I'm building my first quadcopter, and these are the components I intend to buy:</p>

<ol>
<li>Motor: <a href=""http://store.fut-electronics.com/collections/brushless-motors/products/brushless-motor-1400-kv"" rel=""nofollow"">EMAX BL2212 1400 KV Brushless Outrunner Motor</a> around 0.9 kg thrust: </li>
<li>Flight Controller: <a href=""http://store.fut-electronics.com/collections/sensors-controls/products/multiwii-v2-5-flight-controller"" rel=""nofollow"">Multiwii V2.5 Flight Controller</a> </li>
<li>Propellers: I don't know which one to get: <a href=""http://store.fut-electronics.com/collections/propellers"" rel=""nofollow"">fut-electronics propellers collection</a>  </li>
<li>GPS: <a href=""http://store.fut-electronics.com/products/skylab-uart-gps-module-skm58-small-form-factror"" rel=""nofollow"">Skylab UART GPS Module SKM58</a>   (Small Form Factor)</li>
<li>Radio Communication: <a href=""http://store.fut-electronics.com/collections/telemetry/products/radio-telemetry-915-mhz-3dr"" rel=""nofollow"">Radio Telemetry 915 Mhz (3DR)</a>, is there an affordable alternative to buying a radio telemetry maybe using Wi-Fi?  </li>
<li>ESCs: <a href=""http://store.fut-electronics.com/collections/esc/products/4x1-esc-4x25a-speed-controller-for-quadcopter"" rel=""nofollow"">4x1 ESC (4x25A) - Speed Controller for Quadcopter</a> </li>
<li>Battery: I don't know <a href=""http://store.fut-electronics.com/collections/battery-chargers"" rel=""nofollow"">which one to choose</a></li>
</ol>

<p>My questions are:</p>

<ul>
<li>Are the components compatible?</li>
<li>What battery to choose?</li>
<li>If I'm not planning to do GPS planned missions, would the GPS be important for anything else?</li>
</ul>

<p>By the way I intend to attach a camera or a smart-phone to it for video capturing I think it is about an extra 200 grams.</p>
","quadcopter multi-rotor uav"
"7945","Why the name ""combinatorial""?","<p>Why are 'cell decomposition' methods in motion planning given the name, ""combinatorial"" motion planning?</p>
","motion-planning"
"7950","Axis of rotation via IMU","<p>Using an IMU (gyro, accelerometer and magnetometer), as found in most smartphones, can I detect the differences between tilting the device, say forward, along different (parallel) axis positions? </p>

<p>To clarify, if the axis of rotation is far from the sensor, the the motion contains a translational component.</p>

<p>Can the distance and position of this axis be extracted from the IMU data and if so how?<br>
Is there some data fusion algorithm that can do all this?</p>
","imu accelerometer gyroscope magnetometer"
"7953","Hand Eye Calibration Solver","<p>I have a rig for which I have a pretty good estimate of the static transformation between the camera and a joint based off of the CAD. It has some errors though and I was hoping to fix it by doing a hand eye calibration. So, I started off with generating some data based off of the transformation that I have already. From the papers that I have been reading, they all want to solve the $$AX = XB$$ problem by either converting $A$, $B$ to dual quaternions or simplifying the equation to something like 
$$ n_A = Xn_B $$ where $n_A$, $n_B$ are the eigenvectors corresponding to the eigenvalue of 1 for the $A$ and $B$ rotations.</p>

<p>After generating the data, I tested if my data collection was correct and I validated it by checking if $AX = XB$ for all of the $A$s and $B$s that I generated. I used the <a href=""https://github.com/hengli/camodocal"" rel=""nofollow"">CamOdoCal</a> library to try and solve the problem but I got this -</p>

<pre><code>/hand_eye_calib_node    : 
[ 0.00196822,   -0.457069,    0.889429,    0.143463;
   -0.999965, -0.00813605, -0.00196822,    -1.74257;
  0.00813605,   -0.889394,   -0.457069,   0.0270069;
           0,           0,           0,           1]

----------------------------------------

/hand_eye_calib_node    : Actual transform
    0         0         1   0.08891
   -1         0         0 -0.070465
    0        -1         0   0.07541
    0         0         0         1
</code></pre>

<p>The actual transform is the one that I had based my $A$ and $B$ data on. Then I tried implementing the <a href=""http://ieeexplore.ieee.org/xpl/login.jsp?tp=&amp;arnumber=34770"" rel=""nofollow"">Tsai-Lenz</a> and <a href=""https://hal.inria.fr/inria-00590039/document"" rel=""nofollow"">Horaud and Dornaika's Nonlinear optimization techniques</a> using LM solver but to no avail. I do not get the correct transformation out of any of the solvers.</p>

<p>So, I was wondering if you could point me to a hand eye calibration library or paper that has worked.</p>
","kinematics calibration"
"7954","Book on mechanisms","<p>I wanted to know if there is any sort of archive of mechanisms that contains a brief description of mechanisms like there type of motion and forces involved. Not lengthy derivations and other stuff. </p>
","mobile-robot mechanism"
"7959","Tuning PD for line follower","<p>I am trying to make line following robot. I am using atmega328p mcu, pololu 10:1 motors, pololu qtr6-rc sensor, 2s li-po. Here is my code:</p>

<pre><code>/*
* LineFollower.c
*
* Created: 30.04.2015 16:00:05
*  Author: Mikk
*/
#define F_CPU 20000000         //we're running on 20mHz clock

#define numberOfButtons 1

#define READPORT    PORTC
#define READDDR     DDRC
#define READPIN     PINC        // lines connected to PC0 - PC5

#define MAXTICKS    2500
#define QTRCNT      6

#include &lt;avr/io.h&gt;
#include &lt;util/delay.h&gt;
#include &lt;avr/interrupt.h&gt;
#include &lt;Mikk/Button.h&gt;
#include &lt;Mikk/QTRRCSensors.h&gt;

int baseSpeed = 70;
int maxSpeed = 140;
const float Kp = 8.1;
const float Kd = 400;

uint8_t mode = 0;                //indicates in which mode program is 

uint8_t RmotorSpeed = 0;         //
uint8_t LmotorSpeed = 0;         //motors

void button(void);

void setMotors(int ml, int mr)
{
    if(ml &gt; maxSpeed)             //make sure that speed is not out of range for left motor
        ml = maxSpeed;
    if(ml &lt; -maxSpeed)
        ml = -maxSpeed;

    if(mr &gt; maxSpeed)             //make sure that speed is not out of range for right motor
        mr = maxSpeed;
    if(mr &lt; -maxSpeed)
        mr = maxSpeed;

    if(ml &gt; 0)                    //if left motor speed is positive then drive motor forwards
        LmotorSpeed = ml;
    if(ml == 0)                   //if left motor speed is 0 then stop motor
        LmotorSpeed = 0;

    if(mr &gt; 0)                    //if right motor speed is positive then drive motor forwards
        RmotorSpeed = mr;
    if(mr == 0)                   //if right motor speed is 0 then stop motor
        RmotorSpeed = 0;
}

void emittersOn(void)            //function for turning emitters on
{
    PORTD |= (1 &lt;&lt; PIND0);
}

void emittersOff(void)           //function for turning emitters off
{
    PORTD &amp;= ~(1 &lt;&lt; PIND0);
}

void LedOn(void)                 //function for turning led on
{
    PORTB |= (1 &lt;&lt; PINB5);
}

void LedOff(void)               //function for turning led off
{
    PORTB &amp;= ~(1 &lt;&lt; PINB5);
}

void stop(void)                 //stop everything
{
    LedOff();
    setMotors(0, 0);
    emittersOff();
}

void calibration(void)          //calibration takes about 5 seconds
{
    //turn led on
    LedOn();
    //turn emitters on
    emittersOn();
    // reset minimums and maximums
    for (int i = 0; i &lt; QTRCNT; i++)
    {
        QTRmax[i] = 0;
        QTRmin[i] = MAXTICKS;
    }
    //calibrate sensors
    for(int i=0; i&lt;250; i++)
    {
        calibrateQTRs();
        _delay_ms(5);
    }
    //turn emitters off
    emittersOff();
    //turn led off
    LedOff();
}

void start(void)
{
    //turn led on
    LedOn();

    //create all necessary variables
    int power_difference = 0;
    float error = 0;
    float lastError = 0;
    float derivative = 0;
    int position = 0;

    //turn emitters on
    emittersOn();
    _delay_ms(500);               //wait so you can pull your hand away
    while(mode == 2)
    {
        //check for mode change
        button();

        //read position
        position = readLine();
        //make calculations
        error = position - 2500;
        derivative = error - lastError;

        //remember last error
        lastError = error;

        //calculate power_difference of motors
        power_difference = error/(Kp/100) + derivative*(Kd/100);

        //make sure that power difference is in correct range
        if(power_difference &gt; baseSpeed)
            power_difference = baseSpeed;
        if(power_difference &lt; -baseSpeed)
            power_difference = -baseSpeed;

        //drive motors
        if(power_difference &gt; 0)
            setMotors(baseSpeed+power_difference, baseSpeed-power_difference/2);
        else if(power_difference &lt; 0)
            setMotors(baseSpeed+power_difference/2, baseSpeed-power_difference);
        else if(power_difference == 0)
            setMotors(maxSpeed, maxSpeed);
    }
}

void button(void)
{
    char buttonState = 0;
    //check for current button status
    buttonState = ButtonReleased(0, PINB, 1, 200);
    //check if button is pressed
    if(buttonState) //pin change from low to high
    {
        mode++;
        if(mode == 1) calibration();
    }
}

void pwmInit(void)
{
    //set fast-PWM mode, inverting mode for timer0
    TCCR0A |= (1 &lt;&lt; COM0A1) | (1 &lt;&lt; COM0A0) | (1 &lt;&lt; WGM00) | (1 &lt;&lt; WGM01) | (1 &lt;&lt; COM0B1) | (1 &lt;&lt; COM0B0);
    //set fast-PWM mode, inverting mode for timer2
    TCCR2A |= (1 &lt;&lt; COM2A1) | (1 &lt;&lt; COM2A0) | (1 &lt;&lt; WGM20) | (1 &lt;&lt; WGM21) | (1 &lt;&lt; COM2B1) | (1 &lt;&lt; COM2B0);
    //set timer0 overflow interrupt
    TIMSK0 |= (1 &lt;&lt; TOIE0);
    //set timer2 overflow interrupt
    TIMSK2 |= (1 &lt;&lt; TOIE2);

    //enable global interrupts
    sei();
    //set timer0 prescaling to 8
    TCCR0B |= (1 &lt;&lt; CS01);
    //set timer2 prescaling to 8
    TCCR2B |= (1 &lt;&lt; CS21);
}

int main(void)
{   
    DDRB |= 0x2A;                 //0b00101010
    DDRD |= 0x69;                 //0b01101001
    DDRC |= 0x00;                 //0b00000000

    //clear port d
    PORTD |= 0x00;

    //enable pull-up resistor
    PORTB |= (1 &lt;&lt; PINB1);

    initQTRs();
    pwmInit();

    //blink 2 times indicate that we are ready
    for(int i=0; i&lt;4; i++)
    {
        PORTB ^= (1 &lt;&lt; PINB5);
        _delay_ms(500);
    }

    while(1)
    {
        button();
        if(mode == 0) stop();
        if(mode == 2) start();
        if(mode &gt;= 3) mode = 0;
    }
}

//update OCRnx values
ISR(TIMER0_OVF_vect)
{
    OCR0A = RmotorSpeed;
}
ISR(TIMER2_OVF_vect)
{
    OCR2A = LmotorSpeed;
}
</code></pre>

<p>And here is my qtr library:</p>

<pre><code>    #ifndef QTRRCSensors
#define QTRRCSensors

#define SLOW        1
#define FAST        0

static inline void initQTRs(void) 
{
    TCCR1B = (1 &lt;&lt; CS11);
}

uint16_t QTRtime[QTRCNT], QTRmax[QTRCNT], QTRmin[QTRCNT];

static inline void readQTRs(uint8_t forceSlow) {
    uint8_t lastPin, i, done = 0;

    for (i = 0; i &lt; QTRCNT; i++)                    // clear out previous times
        QTRtime[i] = 0;

    READDDR |= 0b00111111;                          // set pins to output
    READPORT |= 0b00111111;                         // drive them high

    _delay_us(10);                                  // wait 10us to charge capacitors

    READDDR &amp;= 0b11000000;                          // set pins to input
    READPORT &amp;= 0b11000000;                         // turn off pull-up registers

    TCNT1 = 0;                                      // start 16bit timer at 0
    lastPin = READPIN;

    while ((TCNT1 &lt; MAXTICKS) &amp;&amp; ((done &lt; QTRCNT) || forceSlow))     // if forceSlow, always take MAXTICKS time
    {
        if (lastPin != READPIN)                     // if any of the pins changed
        {
            lastPin = READPIN;
            for (i = 0; i &lt; QTRCNT; i++)
            {
                if ((QTRtime[i] == 0) &amp;&amp; (!(lastPin &amp; (1&lt;&lt;i))))    // did pin go low for the first time
                {
                    QTRtime[i] = TCNT1;
                    done++;
                }
            }
        }
    }
    if (done &lt; QTRCNT)                              // if we timed out, set any pins that didn't go low to max
        for (i = 0; i &lt; QTRCNT; i++)
            if (QTRtime[i] == 0)
                QTRtime[i] = MAXTICKS;
}

void calibrateQTRs(void) {
    uint8_t i, j;

    for (j = 0; j &lt; 10; j++) {                      // take 10 readings and find min and max values
        readQTRs(SLOW);
        for (i = 0; i &lt; QTRCNT; i++) {
            if (QTRtime[i] &gt; QTRmax[i])
                QTRmax[i] = QTRtime[i];
            if (QTRtime[i] &lt; QTRmin[i])
                QTRmin[i] = QTRtime[i];
        }
    }
}

void readCalibrated(void) {
    uint8_t i;
    uint16_t range;

    readQTRs(FAST);

    for (i = 0; i &lt; QTRCNT; i++) {                  // normalize readings 0-1000 relative to min &amp; max
        if (QTRtime[i] &lt; QTRmin[i])                 // check if reading is within calibrated reading
            QTRtime[i] = 0;
        else if (QTRtime[i] &gt; QTRmax[i])
            QTRtime[i] = 1000;
        else {
            range = QTRmax[i] - QTRmin[i];
            if (!range)                             // avoid div by zero if min &amp; max are equal (broken sensor)
                QTRtime[i] = 0;
            else
                QTRtime[i] = ((int32_t)(QTRtime[i]) - QTRmin[i]) * 1000 / range;
        }
    }
}

uint16_t readLine(void) {
    uint8_t i, onLine = 0;
    uint32_t avg;                                   // weighted total, long before division
    uint16_t sum;                                   // total values (used for division)
    static uint16_t lastValue = 0;                  // assume line is initially all the way left (arbitrary)

    readCalibrated();

    avg = 0;
    sum = 0;

    for (i = 0; i &lt; QTRCNT; i++) {                  // if following white line, set QTRtime[i] = 1000 - QTRtime[i]
        if (QTRtime[i] &gt; 50) {                      // only average in values that are above a noise threshold
            avg += (uint32_t)(QTRtime[i]) * (i * 1000);
            sum += QTRtime[i];
            if (QTRtime[i] &gt; 200)                   // see if we're above the line
                onLine = 1;
        }
    }

    if (!onLine)
    {
        // If it last read to the left of center, return 0.
        if(lastValue &lt; (QTRCNT-1)*1000/2)
            return 0;

        // If it last read to the right of center, return the max.
        else
            return (QTRCNT-1)*1000;

    }

    lastValue = avg/sum;                            // no chance of div by zero since onLine was true

    return lastValue;
}

#endif
</code></pre>

<p>I am trying to find Kp constant but when it's 7 then my robot just turns off the line always on the same spot. When Kp is 8 then it follows staright line but wobbles a lot and can't take corners. I also tried to increase Kd 10 to 20 times when my Kp was 8 but it didn't change much. How can I get it working?</p>

<p>Here is my robot and the track I want to follow.</p>

<p><a href=""http://i.stack.imgur.com/HfCtL.jpg"" rel=""nofollow""><img src=""http://i.stack.imgur.com/HfCtL.jpg"" alt=""""></a></p>

<p><a href=""http://i.stack.imgur.com/o3UxV.jpg"" rel=""nofollow""><img src=""http://i.stack.imgur.com/o3UxV.jpg"" alt=""""></a></p>
","pid line-following avr tuning"
"7964","Total Hand calculations procedure & formulaes of Mega-Quadcopter","<p>I am a student of BE taking Mega-Quadcopter as my final year project.Can u please help me with the total hand calculations of the mega-copter i.e its procedure and formulaes? . I wanted to know how to calculate the dimensions of frame,specifications of motor and propeller,the rating of ESC's and the power rating of the batteries and its total no.s.I do not want direct answers but its procedure and formulaes.I want to lift aload of around 20-30 kgs .Please feel free to help.</p>
","quadcopter"
"7966","Orthogonal projection of laserscanner data","<p>I recently discovered this ROS-package: <a href=""http://wiki.ros.org/laser_ortho_projector"" rel=""nofollow"">http://wiki.ros.org/laser_ortho_projector</a> .
Which is basically exactly what I need. However I am not using ROS, so I need to do what is been done in this package myself.</p>

<p>Basically the information I have is the range measurement r and the angle theta for every measurement point of a 360 degree laserscan + I have the orientation in roll, pitch, yaw angles of the laserscanner. However yaw is not important for me and could be ignored.</p>

<p>I really can't get my head around how to project those points to the ground plane. I mean it is easy for the measurement point which align with the roll and pitch axes, but I don't know what to do with the points in between :D</p>

<p>One solution I thought of is this:</p>

<ol>
<li>Convert the measurement point (r, theta) in cartesian coordinates (x,y,z) - vector</li>
<li>Use rotations matrices: create rotation matrix for rotation around roll axis with roll angle, and adequately for the pitch axis. Multiplay bot matrices and then multiply it with (x,y,z) - vector.</li>
<li>Now the orthogonal projection of the of the measurement would be the (x,y,z) - vector with z=0.</li>
<li>Convert (x,y) - vector back to polar coordinates (r, theta).</li>
</ol>

<p>However, especially step 2 is very complicated, because the rotation matrices change according to the sign of the roll and pitch angles, right?</p>

<p>I would like to note that the absolute value of role and pitch angles will always be &lt; 90°, so there should not be an unambiguity with rotations..</p>

<p>Is there an easier (or maybe more elegant) way to solve my problem?</p>

<p>My guess is, that this problem must have been solved basically for every robot application which uses a 2D-laserscanner that is not fixed to one axis. 
But I can not find the solution anywhere.</p>

<p>So I would be very glad if anyone of you could point me in the right direction.</p>

<p>Kind regards</p>
","quadcopter slam"
"7968","What PID values should i keep","<p>I have built quadcopter but the problem is of balancing. It doesnt goes up. I am using PID techniqe for balancing. But i am not finding the suitable values for PID tuning. I am using mpu6050 as a sensor. I get the accelerometer values of x and y axis and find the error from them. That is lets say if accel on x is not zero then it error cause it should be zero if balanced. I am using +-2g sensitivity scale of accelerometer. The motors i am using are dji 920 kva. What values for kp, ki and kd should i set. I cant set them while in flight cause it completely out of balance.</p>

<p><a href=""http://i.stack.imgur.com/3d4Jm.jpg"" rel=""nofollow""><img src=""http://i.stack.imgur.com/3d4Jm.jpg"" alt=""Quadcopter""></a></p>

<p>This is the design. Completely home made. I have modified it a little after this photo. Accelerometer is at 2g so at balance z will be 32768/2 .</p>

<pre><code>short PID()
{
short error,v;
error = desired-current; 
//error/=390;
integ += error;
der = error - perror;
x=error; 
x2=integ; 
x3=der; 
x*=kp;
x2*=ki;
x3*=kd; 
v=kpi;x/=100;
v=kii;x2/=1000;
v=kdi;x3/=1000;
x=x+x2+x3; 
//x/=390; 
perror = error;
return x;
}
</code></pre>

<p>There are also few more questions, should i scale error or pid output, because error is from ranging from 0 to 16380 at 2g setting, so i am scaling it from 0 to 42. So should i divide error or pid by some value?</p>
","quadcopter pid balance"
"7969","What erector sets will function with normal servo motors?","<p>I need a basic erector set that the parts will fit with servo motors and dc motors. Preferably below $100. I've looked at <a href=""http://mindsirobotics.com/250-piece-basic-construction-set.html"" rel=""nofollow"">Minds-i basic set</a> and it looks good except I don't know if it will function with my servos without hot glue or extensive modifications. </p>

<p>If it matters, I am making a bipedal robot so I don't require any wheels or anything pre-built. I just need a basic set that I can add on to to build a whole bunch of different robots. </p>
","mobile-robot rcservo"
"7970","Firmware upgrade for iRobot Create 2","<p>Is there a firmware upgrade for available for the Create 2? I had some issues in March when using these for assigning a University of Tennessee  programming project. We are getting ready to use them again (we have 10 now) and I'd like to get them all updated to the latest firmware.</p>
","irobot-create"
"7972","Air hockey with a robot as an opponent","<p>I'm not sure if this is the right place to post this but here goes.</p>

<p>So, as the title states, I'm planning on building a desk that doubles as an air hockey table which has a robot on the other side.</p>

<p>The robot would be mounted on a rail which should be able to go left and right using a linear actuator. It should be able to ""attack"" the puck using two servos.</p>

<p>The real problem is how should I detect the puck's location?</p>

<p><strong>My idea:</strong></p>

<p>Since the table would have tiny holes in the corners of a every square(0.5inx0.5in), I could fit in a laser on the bottom part of the table, a laser for ever 1in so a 1inx1in square, the same location would be reflected on the ""ceiling"" of the table but instead of laser diodes, they would be replaced by an ldr. </p>

<p>So I'm planning on doing a matrix and reading the signals of the ldr's columns and rows then performing some logic to locate the center of the puck.</p>

<p><strong>PROBLEMS:</strong></p>

<p>While I don't see any performance flaws in my plan, I see <strong>tons</strong> of flaws when done imperfectly even to the tiniest bit.</p>

<ol>
<li>I have to be exactly accurate regarding the laser diode's position,
it has to be on the center of the holes, right below the z-axis.
This should be easy if I'm just going to place 4 or 5. But I'm not.</li>
<li>According to my estimations, I'm going to have to use 300-700 laser
diodes, depending on if I'm planning on putting the lasers only on
the opponent's side or on the entire board. It would definitely be
costly. Imagine 300...</li>
<li>This isn't really a huge problem, more like a hassle. Wiring 300 of
these. Forget the pcbs, the project area is just to large.</li>
</ol>

<p>I have thought of numerous way to lessen these, like using a color sensor to get the x-axis location and a laser situated on a negative x-axis pointing to the positive x-axis to locate the puck's y location, but I'm still comparing ideas.</p>

<p><strong>Advantages:</strong></p>

<p>I could get a 3d-like graphical representation with 3d-like controls (3d-like in reality but technically 2d since the lasers are only plotted in the x and y axis though facing the z-axis). </p>

<p>Since this project is going to be my room desk, situated in an automated room, I was thinking of making ""desk modes"" which should toggle between a game that takes advantage of the lasers and their controls, A control desk for my room, ordinary desk mode, and an air hockey mode.</p>

<p><strong>My question: (More like a request)</strong></p>

<p>Does anyone have <em>another</em> idea regarding how I should be able to locate the puck's x and y location accurately in real time?</p>

<p>EDIT: The table is roll-able and stored underneath a loft bed which has an under-area height of 5'4"". Which means I can't go grande on the a vertical solution.</p>

<p>EDIT #2: Thanks to the helpful people here, I have come to the conclusion of using a camera.</p>

<p>The camera will be that of a smartphone's, I'll create an app that tracks an object by color and a has fixed size comparison to identify the distance of the robot from the puck. The phone will then process this and send signals via bluetooth.</p>

<p>The phone is anchored at the end of the robot's moving part so the camera is reminiscent of those games with a first-person view.</p>

<p>Incoming problems: I'm looking forward to some delay, given the delay in processing.</p>
","sensors microcontroller design electronics laser"
"7983","Magnetic, low insertion force connector","<p>I'm building a robotic tea-maker/watchdog robot and have a power problem. I would like to be able to have the robot approach a socket and insert the power cord of a cheap immersion heater (120V, 300W, see links below) to turn the heater on. However, the power and precision required to plug it into the wall is beyond the capabilities of my stepper motors/Arduino. </p>

<p>My solution was a magnetic breakaway power cord like the charger on a Mac but at higher voltage. Deep fat fryers have suitable ones (120V, high power, see links below). However, the problem is I need both sides of the connector, and I can only find the magnetic breakaway power cord, not the opposite side, which would normally be built into the deep fat fryer. I don't fancy buying a whole fryer just to get one little part...</p>

<p>Any ideas? Alternatives to a breakaway cord? Anyone know of any (cheap) 120V induction chargers? I'll resort to a mechanical on/off switch and just leave the robot plugged in if I have to, but I was hoping for something a bit sleeker.</p>

<p>Links:</p>

<ul>
<li><a href=""http://rads.stackoverflow.com/amzn/click/B000VK0DRY"" rel=""nofollow"">Immersion heater</a></li>
<li><a href=""http://rads.stackoverflow.com/amzn/click/B00HS5AZJG"" rel=""nofollow"">Fryer cord</a> </li>
</ul>
","untagged"
"7992","How to calculate vehicle detection distance","<p>I would like to know how to calculate the distance to each car when I run my application for an autonomous vehicle in real time. In addition I want to know how implement the calculation in C++. </p>

<p>You can see in the images we can know the distance for each vehicle but I don't know what code I should use to make all these calculations for every vehicle .</p>

<p><strong>Please check the photo to understand more about what I'm trying to achieve.</strong></p>

<p><a href=""http://i.stack.imgur.com/NJ4Nq.jpg"" rel=""nofollow""><img src=""http://i.stack.imgur.com/NJ4Nq.jpg"" alt=""autonomous vehicle obstacle detection""></a></p>

<p><a href=""http://i.stack.imgur.com/SEpbP.jpg"" rel=""nofollow""><img src=""http://i.stack.imgur.com/SEpbP.jpg"" alt=""autonomous vehicle obstacle detection (2) ""></a></p>
","control sensors localization ros cameras"
"7993","Why can two Series 1 XBees only talk in X-CTU?","<p>I have two Series 1 XBees that won't be in transparent mode because they are in AT command mode when I'm not in X-CTU.  I had asked for help elsewhere and no one had the answer except telling me about flow control.</p>

<p>The XBees had been configured properly with the MY and DL settings.  I'm thinking maybe I should shorten the timeout so they supposedly get out of AT command mode but they both stay in AT command mode.  The only time I can get the two Series 1 XBees to talk is under X-CTU.  I need the two Series 1 XBees to automatically be in transparent mode when powered on.  </p>
","wireless"
"7995","Ceiling depth with a monocular camera","<p>Having a camera mounted on my robot and looking upwards, I want to estimate the distance of the ceiling as the robot moves and also the position of landmarks observed on the ceiling (lamps for example).
I know this is a structure from motion problem but I was very confused on how to implement it. This case is a much simpler case than bundle adjustment as the intrinsic calibration of the camera is existing, the camera pose changes just in x and y directions, and the observed thing is a planar ceiling. Odometry might also be available but I would like to start solving it without. Do you know any libraries that offer a good and simple API to do such a thing? preferably based on levenberg-marquardt or similar optimization algorithms taking in more than just two observations. (Python bindings would be nice to have)</p>
","cameras 3d-reconstruction"
"7997","Why can't you buy continuous servos with absolute positioning?","<p>I've been looking at parts for a beginners robotics kit (I teach at a museum) and have been wondering about servos.</p>

<p>You can buy continuous servos with relative position encoders. But I can't find continuous rotation servos with absolute position encoders. Do these exist? If not, why not?</p>

<p>I understand that some forums don't like shopping questions, but I suspect that this part doesn't exist and I'd like to understand why.</p>

<p>Also, I understand that most servos use a potentiometer as a position encoder and that these don't turn more than 1 rotation, but there are other types of encoders that seem like they would do the job.</p>

<p>Thanks for the help!</p>
","servos quadrature-encoder"
"7998","Weave Weld Lincoln Electric Mig Robot","<p>This is a simple question that I can't seem to find the answer for but when setting up the weave function how exactly does frequency (Hz) determine how fast it moves back and forth? </p>

<p>In other words if I raise frequency will it move quicker or slower and what factors must I consider? </p>
","robotic-arm industrial-robot"
"8001","Add hardware reset button for Create2","<p>Is there any way to add a reset button to the Create2 that would be the equivalent  of temporarily disconnecting the battery? </p>
","irobot-create"
"8006","Quadcopter accelerating or not","<p>I am on the project quadcopter. So i have to use PID for stabalizing it. I think i am going wrong because i am adding the pid output to motors thrust. While the motors thrust means to be its acceleraTion. The reason of my previous statment is that when the quad is static in air(not goin up nor below), that time the thrust is enough to cancel gravity, means thrust is negative gravity, that is acceleration. So if i add pid output to thrust that is acceleration of motors, it will be wrong. I have to add pid to speed of motors, which is not visible. My quad is not stabalizing the reason i see is this, that i am adding pid to acc, while it should be added to speed(virtually). What should i do. Should i derivate the pid output and add to thrust? <a href=""https://mbasic.facebook.com/photo.php?fbid=1545278952394916&amp;id=100007384772233&amp;set=a.1447457675510378.1073741830.100007384772233&amp;refid=17&amp;_ft_=top_level_post_id.1545278952394916%3Athid.100007384772233%3A306061129499414%3A69%3A0%3A1443682799%3A-1394728329505289925&amp;__tn__=E"" rel=""nofollow"">https://mbasic.facebook.com/photo.php?fbid=1545278952394916&amp;id=100007384772233&amp;set=a.1447457675510378.1073741830.100007384772233&amp;refid=17&amp;<em>ft</em>=top_level_post_id.1545278952394916%3Athid.100007384772233%3A306061129499414%3A69%3A0%3A1443682799%3A-1394728329505289925&amp;<strong>tn</strong>=E</a></p>

<p><a href=""https://mbasic.facebook.com/photo.php?fbid=1545281645727980&amp;id=100007384772233&amp;set=a.1447457675510378.1073741830.100007384772233&amp;refid=17&amp;__tn__=E"" rel=""nofollow"">https://mbasic.facebook.com/photo.php?fbid=1545281645727980&amp;id=100007384772233&amp;set=a.1447457675510378.1073741830.100007384772233&amp;refid=17&amp;<strong>tn</strong>=E</a></p>

<p>This is the drawing of my circuit. I am giving the current from one esc to whole of the circuit. Other esc's has only pwm wire connected to circuit.</p>
","quadcopter pid"
"8008","Image Based Visual Servoing algorithm in MATLAB","<p>I was trying to implement the IBVS algorithm (the one explained in the Introduction <a href=""http://www.irisa.fr/lagadic/visp/documentation/visp-2.10.0/tutorial-ibvs.html"" rel=""nofollow"">here</a>) in MATLAB myself, but I am facing the following problem : The algorithm seems to work only for the cases that the camera does not have to change its orientation in respect to the world frame.For example, if I just try to make one vertex of the initial (almost) square go closer to its opposite vertex, the algorithm does not work, as can be seen in the following image</p>

<p><a href=""http://i.stack.imgur.com/LeVwK.jpg"" rel=""nofollow""><img src=""http://i.stack.imgur.com/LeVwK.jpg"" alt=""enter image description here""></a></p>

<p>The red x are the desired projections, the blue circles are the initial ones and the green ones are the ones I get from my algorithm.</p>

<p>Also the errors are not exponentially dereasing as they should.</p>

<p><a href=""http://i.stack.imgur.com/lbdoH.jpg"" rel=""nofollow""><img src=""http://i.stack.imgur.com/lbdoH.jpg"" alt=""enter image description here""></a></p>

<p>What am I doing wrong? I am attaching my MATLAB code which is fully runable. If anyone could take a look, I would be really grateful. I took out the code that was performing the plotting. I hope it is more readable now. Visual servoing has to be performed with at least 4 target points, because else the problem has no unique solution. If you are willing to help, I would suggest you take a look at the <code>calc_Rotation_matrix()</code> function to check that the rotation matrix is properly calculated, then verify that the line <code>ds = vc;</code> in <code>euler_ode</code> is correct. The camera orientation is expressed in Euler angles according to <a href=""http://www.coppeliarobotics.com/helpFiles/en/eulerAngles.htm"" rel=""nofollow"">this</a> convention. Finally, one could check if the interaction matrix <code>L</code> is properly calculated.</p>

<pre><code>function VisualServo()

    global A3D B3D C3D D3D A B C D Ad Bd Cd Dd

    %coordinates of the 4 points wrt camera frame
    A3D = [-0.2633;0.27547;0.8956];
    B3D = [0.2863;-0.2749;0.8937];
    C3D = [-0.2637;-0.2746;0.8977];
    D3D = [0.2866;0.2751;0.8916];

    %initial projections (computed here only to show their relation with the desired ones) 
    A=A3D(1:2)/A3D(3);
    B=B3D(1:2)/B3D(3);
    C=C3D(1:2)/C3D(3);
    D=D3D(1:2)/D3D(3);

    %initial camera position and orientation
    %orientation is expressed in Euler angles (X-Y-Z around the inertial frame
    %of reference)
    cam=[0;0;0;0;0;0];

    %desired projections
    Ad=A+[0.1;0];
    Bd=B;
    Cd=C+[0.1;0];
    Dd=D;

    t0 = 0;
    tf = 50;

    s0 = cam;

    %time step
    dt=0.01;
    t = euler_ode(t0, tf, dt, s0);

end


function ts = euler_ode(t0,tf,dt,s0)

    global A3D B3D C3D D3D Ad Bd Cd Dd 

    s = s0;
    ts=[];
    for t=t0:dt:tf
        ts(end+1)=t;
        cam = s;

        % rotation matrix R_WCS_CCS
        R = calc_Rotation_matrix(cam(4),cam(5),cam(6));
        r = cam(1:3);

        % 3D coordinates of the 4 points wrt the NEW camera frame
        A3D_cam = R'*(A3D-r);
        B3D_cam = R'*(B3D-r);
        C3D_cam = R'*(C3D-r);
        D3D_cam = R'*(D3D-r);

        % NEW projections
        A=A3D_cam(1:2)/A3D_cam(3);
        B=B3D_cam(1:2)/B3D_cam(3);
        C=C3D_cam(1:2)/C3D_cam(3);
        D=D3D_cam(1:2)/D3D_cam(3);


        % computing the L matrices
        L1 = L_matrix(A(1),A(2),A3D_cam(3));
        L2 = L_matrix(B(1),B(2),B3D_cam(3));
        L3 = L_matrix(C(1),C(2),C3D_cam(3));
        L4 = L_matrix(D(1),D(2),D3D_cam(3));
        L = [L1;L2;L3;L4];


        %updating the projection errors
        e = [A-Ad;B-Bd;C-Cd;D-Dd];

        %compute camera velocity
        vc = -0.5*pinv(L)*e;

        %change of the camera position and orientation
        ds = vc;

        %update camera position and orientation
        s = s + ds*dt;


    end  
    ts(end+1)=tf+dt;
end

function R = calc_Rotation_matrix(theta_x, theta_y, theta_z)

    Rx = [1 0 0; 0 cos(theta_x) -sin(theta_x); 0 sin(theta_x) cos(theta_x)];
    Ry = [cos(theta_y) 0 sin(theta_y); 0 1 0; -sin(theta_y) 0 cos(theta_y)];
    Rz = [cos(theta_z) -sin(theta_z) 0; sin(theta_z) cos(theta_z) 0; 0 0 1];

    R = Rx*Ry*Rz;

end

function L = L_matrix(x,y,z)

    L = [-1/z,0,x/z,x*y,-(1+x^2),y;
       0,-1/z,y/z,1+y^2,-x*y,-x];
end
</code></pre>

<p>Cases that work:</p>

<pre><code>A2=2*A;
B2=2*B;
C2=2*C;
D2=2*D;

A2=A+1;
B2=B+1;
C2=C+1;
D2=D+1;

A2=2*A+1;
B2=2*B+1;
C2=2*C+1;
D2=2*D+1;
</code></pre>

<p>Cases that do NOT work:
Rotation by 90 degrees and zoom out (zoom out alone works, but I am doing it here for better visualization)</p>

<pre><code>A2=2*D;
B2=2*C;
C2=2*A;
D2=2*B;
</code></pre>

<p><a href=""http://i.stack.imgur.com/0Xhde.jpg"" rel=""nofollow""><img src=""http://i.stack.imgur.com/0Xhde.jpg"" alt=""enter image description here""></a></p>
","control algorithm matlab visual-servoing"
"8014","Arduino mobile robot","<p>Is there a way I can control my arduino robot from anywhere in the world. The robot goes out of range of my home wifi so my wifi shield can't help. Is there a way to make sure the robot is always on the Internet no matter where it goes? </p>
","arduino mobile-robot raspberry-pi"
"8018","Complete Quadrotor Tutorial (Text)Book?","<p>I'm looking for a complete tutorial textbook for how to build and control a quadrotor (dynamics, control, etc.).</p>

<p>I'm an engineer with a broad background in programming, mechanics, and control but it's been several years and I'm rusty. I was just wondering if anyone knew of a great ""from the ground up"" tutorial for quadrotors? I found <a href=""http://rads.stackoverflow.com/amzn/click/0071822283"" rel=""nofollow"">this</a> book which looks interesting but thought I'd ask here too.</p>

<p>Thanks!</p>

<p><strong>EDIT</strong></p>

<p>So, assume I've taken a formal course on all necessary topics: system modeling, mechanics, control theory, state estimation, programming, etc.</p>

<p>I'm looking for a book that assumes the reader is familiar with the topics but also goes step-by-step. For example, instead of just stating ""here are the system equations"" I'm looking for ""let's derive the system equations"" (but assumes you are familiar with modeling/kinematics). I'd like to start a quadcopter as a side project but have precious spare time so I'd prefer a single good reference instead of jumping from each individual topic textbook; maybe I'm just being greedy :)</p>
","quadcopter"
"8019","indoor positioning system: which is better?","<p>Which method is better, in term of accuracy, for detection of indoor localization of a drone. Camera based system or wireless techniques like WLAN or Bluetooth?</p>
","slam"
"8024","PID quaternion contoller","<p>I want to control the attitude(roll, pitch, yaw) of a vehicle capable of pitching and rolling. To do this I have created a quaternion PID controller. First I take the current attitude of the vehicle converting it to a quaternion Qc and do the same for the desired attitude with the quaternion Qd. I then calculate the input of my PID controller as Qr = Qc' x Qd. The imaginary parts of the quaternions are then fed as force requests on the roll, pitch, yaw axes of the vehicle. I test on a simulator and the control works but becomes unstable in some cases (request for R: 60 P: 60 Y:60). I also want this to work around singularities (i.e. pitch 90)</p>

<p>Does anyone know why I get this behavior and if so explain (thoroughly) what I'm doing wrong?</p>
","control pid stability"
"8027","How to sumarize Kalman filter covariances for display?","<p>I'm implementing an extended Kalman filter and I'm facing a problem with showing the covariances to the user.</p>

<p>The covariance matrix estimate contains all the information we have about the current value estimate, but that is too much to display.
I would like to have a single number that says ""our estimate is really good"" when close to 0 and ""our estimate is not worth much"" when large.</p>

<p>My intuitive simple solution would be to average all the values in the covariance estimate matrix (or maybe just the diagonal), except that in my case the values have different units and different ranges.</p>

<p>Is it possible to do something like this?</p>
","kalman-filter"
"8028","What I need to learn to build robots","<p>What subjects are involved in robotics. If I want to build robots then what necessary things I need to learn consecutively as a beginner.</p>
","artificial-intelligence embedded-systems first-robotics"
"8033","continuous vs discrete simulation in robotics","<p>As far as I know, a robot sends orders as discrete signals. However, isn't computer simulation based on continuous simulation? Do you know if it may happen any important difference when comparing reality to simulation in some cases? I heard that cable-driven robots were quite sensitive.</p>
","control simulation"
"8038","Raspberry pi quadcopter drifts like crazy","<p>I have recently built a raspberry pi based quadcopter that communicates with my tablet over wifi. The problem is that it drifts a lot. At first I thought that the problem was vibration, so I mounted the MPU-6050 more securely to the frame. That seemed to help a bit, but it still drifts. I have tried tuning the PID, tuning the complementary filter, and installing a real time OS. Nothing seems to help very much. Below is my code written completely in java. Any suggestions are appreciated.</p>

<p>QuadServer.java:</p>

<pre><code>package com.zachary.quadserver;

import java.net.*;
import java.io.*;
import java.util.*;

import com.pi4j.io.i2c.I2CBus;
import com.pi4j.io.i2c.I2CDevice;
import com.pi4j.io.i2c.I2CFactory;

import se.hirt.pi.adafruit.pwm.PWMDevice;
import se.hirt.pi.adafruit.pwm.PWMDevice.PWMChannel;

public class QuadServer {
    private final static int FREQUENCY = 490; 

    private static final int MIN = 740;
    private static final int MAX = 2029;

    private static Sensor sensor = new Sensor();

    private static double PX = 0;
    private static double PY = 0;
    private static double PZ = 0;

    private static double IX = 0;
    private static double IY = 0;
    private static double IZ = 0;

    private static double DX = 0;
    private static double DY = 0;
    private static double DZ = 0;

    private static double kP = 1.95; //2.0
    private static double kI = 10.8; //8.5
    private static double kD = 0.15; //0.14

    private static long time = System.currentTimeMillis();

    private static double last_errorX = 0;
    private static double last_errorY = 0;
    private static double last_errorZ = 0;

    private static double outputX;
    private static double outputY;
    private static double outputZ;

    private static int val[] = new int[4];

    private static int throttle;

    static double setpointX = 0;
    static double setpointY = 0;
    static double setpointZ = 0;

    static double errorX;
    static double errorY;
    static double errorZ;

    static long receivedTime = System.currentTimeMillis();

    private static String data;

    static int trimX = -70;
    static int trimY = 70;

    public static void main(String[] args) throws IOException, NullPointerException {
        DatagramSocket serverSocket = new DatagramSocket(40002);

        PWMDevice device = new PWMDevice();

        device.setPWMFreqency(FREQUENCY);

        PWMChannel esc0 = device.getChannel(0);
        PWMChannel esc1 = device.getChannel(1);
        PWMChannel esc2 = device.getChannel(2);
        PWMChannel esc3 = device.getChannel(3);

        /*Runtime.getRuntime().addShutdownHook(new Thread(new Runnable() {
                public void run() {
                    System.out.println(""terminating"");
                    try {
                        esc0.setPWM(0, calculatePulseWidth(MIN/1000.0, FREQUENCY));
                    esc1.setPWM(0, calculatePulseWidth(MIN/1000.0, FREQUENCY));
                    esc2.setPWM(0, calculatePulseWidth(MIN/1000.0, FREQUENCY));
                    esc3.setPWM(0, calculatePulseWidth(MIN/1000.0, FREQUENCY));
                } catch (IOException e) {
                    e.printStackTrace();
                }
                }
            }));
            System.out.println(""running"");*/


        Thread read = new Thread(){
                public void run(){
                    while(true) {
                    try {
                            byte receiveData[] = new byte[1024];
                            DatagramPacket receivePacket = new DatagramPacket(receiveData, receiveData.length);
                            serverSocket.receive(receivePacket);
                            String message = new String(receivePacket.getData());

                            data = """"+IX;
                        addData(IY);

                        addData(sensor.readAccelAngle(0));
                        addData(sensor.readAccelAngle(1));

                        byte[] sendData = new byte[1024];
                            sendData = data.getBytes();
                                InetAddress IPAddress = InetAddress.getByName(""192.168.1.9"");
                                DatagramPacket sendPacket = new DatagramPacket(sendData, sendData.length, IPAddress, 1025);
                                serverSocket.send(sendPacket);

                            setpointX = Double.parseDouble(message.split(""\\s+"")[0])*0.7;
                        setpointY = Double.parseDouble(message.split(""\\s+"")[1])*0.7;

                            throttle = (int)(Integer.parseInt((message.split(""\\s+"")[3]))*12.67)+MIN;

                            kP = Math.round((Integer.parseInt(message.split(""\\s+"")[4])*0.05)*1000.0)/1000.0;
                            kI = Math.round((Integer.parseInt(message.split(""\\s+"")[5])*0.2)*1000.0)/1000.0;
                            kD = Math.round((Integer.parseInt(message.split(""\\s+"")[6])*0.01)*1000.0)/1000.0;

                            trimX = (Integer.parseInt(message.split(""\\s+"")[7])-50)*2;
                            trimY = (Integer.parseInt(message.split(""\\s+"")[8])-50)*2;

                            double accelSmoothing = 0.02;//(Integer.parseInt(message.split(""\\s+"")[8])*0.05)+1;
                            double gyroSmoothing = 0.04;//(Integer.parseInt(message.split(""\\s+"")[7])*0.01);

                            sensor.setSmoothing(gyroSmoothing, accelSmoothing);

                            //System.out.println(""trimX: ""+trimX+"" trimY: ""+trimY);

                            System.out.println(""kP: ""+kP+"", kI: ""+kI+"", kD: ""+kD+"", trimX: ""+trimX+"", trimY: ""+trimY);

                        receivedTime = System.currentTimeMillis();

                        } catch (IOException e) {
                            e.printStackTrace();
                        }
                    }
                }
        };
        read.start();

        while(true)
        {
            Arrays.fill(val, throttle);

            errorX = sensor.readGyro(0)-setpointX;
            errorY = -sensor.readGyro(1)-setpointY;
            errorZ = sensor.readGyro(2)-setpointZ;

            double dt = (double)(System.currentTimeMillis()-time)/1000;

            double accelAngleX = sensor.readAccelAngle(0);
            double accelAngleY = sensor.readAccelAngle(1);

            if(dt &gt; 0.005)
            {

                PX = errorX;
                PY = errorY;
                PZ = errorZ;

                IX += (errorX)*dt;
                IY += (errorY)*dt;
                //IZ += errorZ*dt;

                IX = 0.98*IX+0.02*accelAngleX;
                IY = 0.98*IY+0.02*accelAngleY;

                DX = (errorX - last_errorX)/dt;
                DY = (errorY - last_errorY)/dt;
                //DZ = (errorZ - last_errorZ)/dt;

                last_errorX = errorX;
                last_errorY = errorY;
                last_errorZ = errorZ;

                outputX = kP*PX+kI*IX+kD*DX;
                outputY = kP*PY+kI*IY+kD*DY;
                outputZ = kP*PZ+kI*IZ+kD*DZ;

                time = System.currentTimeMillis();
            }

            //System.out.println(IX+"", ""+IY+"", ""+throttle);

            add(-outputX-outputY-outputZ-trimX+trimY, 0);       //clockwise
            add(-outputX+outputY+outputZ-trimX-trimY, 1);   //counterClockwise
            add(outputX+outputY-outputZ+trimX-trimY, 2);    //clockwise
            add(outputX-outputY+outputZ+trimX+trimY, 3);        //counterclockwise

            //System.out.println(val[0]+"", ""+val[1]+"", ""+val[2]+"", ""+val[3]);

            try {
                if(System.currentTimeMillis()-receivedTime &lt; 1000)
                {
                    esc0.setPWM(0, calculatePulseWidth(val[0]/1000.0, FREQUENCY));
                    esc1.setPWM(0, calculatePulseWidth(val[1]/1000.0, FREQUENCY));
                    esc2.setPWM(0, calculatePulseWidth(val[2]/1000.0, FREQUENCY));
                    esc3.setPWM(0, calculatePulseWidth(val[3]/1000.0, FREQUENCY));
                } else 
                {
                    esc0.setPWM(0, calculatePulseWidth(800/1000.0, FREQUENCY));
                    esc1.setPWM(0, calculatePulseWidth(800/1000.0, FREQUENCY));
                    esc2.setPWM(0, calculatePulseWidth(800/1000.0, FREQUENCY));
                    esc3.setPWM(0, calculatePulseWidth(800/1000.0, FREQUENCY));
                }
            } catch (IOException e) {
                e.printStackTrace();
            }

        }
    }

    private static void add(double value, int i)
    {
        if(val[i]+value &gt; MIN &amp;&amp; val[i]+value &lt; MAX)
        {
            val[i] += value;
        }else if(val[i]+value &lt; MIN)
        {
            //System.out.println(""low"");
            val[i] = MIN;
        }else if(val[i]+value &gt; MAX)
        {
            //System.out.println(""low"");
            val[i] = MAX;
        }
    }

    static void addData(double value)
    {
        data += "" ""+value;
    }

    private static int calculatePulseWidth(double millis, int frequency) {
        return (int) (Math.round(4096 * millis * frequency/1000));
    }
}
</code></pre>

<p>Sensor.java:</p>

<pre><code>package com.zachary.quadserver;

import com.pi4j.io.gpio.GpioController;
import com.pi4j.io.gpio.GpioFactory;
import com.pi4j.io.gpio.GpioPinDigitalOutput;
import com.pi4j.io.gpio.PinState;
import com.pi4j.io.gpio.RaspiPin;
import com.pi4j.io.i2c.*;

import java.net.*;
import java.io.*;

public class Sensor {
    static I2CDevice sensor;
    static I2CBus bus;
    static byte[] accelData, gyroData;
    static long accelCalib[] = {0, 0, 0};
    static long gyroCalib[] = {0, 0, 0};

    static double gyroX;
    static double gyroY;
    static double gyroZ;

    static double smoothedGyroX;
    static double smoothedGyroY;
    static double smoothedGyroZ;

    static double accelX;
    static double accelY;
    static double accelZ;

    static double accelAngleX;
    static double accelAngleY;

    static double smoothedAccelAngleX;
    static double smoothedAccelAngleY;

    static double angleX;
    static double angleY;
    static double angleZ;

    static boolean init = true;

    static double accelSmoothing = 1;
    static double gyroSmoothing = 1;

    public Sensor() {
        try {
            bus = I2CFactory.getInstance(I2CBus.BUS_1);

            sensor = bus.getDevice(0x68);

            sensor.write(0x6B, (byte) 0x0);
            sensor.write(0x6C, (byte) 0x0);
            System.out.println(""Calibrating..."");

            calibrate();

            Thread sensors = new Thread(){
                    public void run(){
                        try {
                            readSensors();
                        } catch (IOException e) {
                        e.printStackTrace();
                    }
                    }
            };
            sensors.start();
        } catch (IOException e) {
            System.out.println(e.getMessage());
        }
    }

    private static void readSensors() throws IOException {
        long time = System.currentTimeMillis();
        long sendTime = System.currentTimeMillis();

        while (true) {
            accelData = new byte[6];
            gyroData = new byte[6];

            int r = sensor.read(0x3B, accelData, 0, 6);

            accelX = (((accelData[0] &lt;&lt; 8)+accelData[1]-accelCalib[0])/16384.0)*9.8;
            accelY = (((accelData[2] &lt;&lt; 8)+accelData[3]-accelCalib[1])/16384.0)*9.8;
            accelZ = ((((accelData[4] &lt;&lt; 8)+accelData[5]-accelCalib[2])/16384.0)*9.8)+9.8;
            accelZ = 9.8-Math.abs(accelZ-9.8);


            double hypotX = Math.sqrt(Math.pow(accelX, 2)+Math.pow(accelZ, 2));
            double hypotY = Math.sqrt(Math.pow(accelY, 2)+Math.pow(accelZ, 2));

            accelAngleX = Math.toDegrees(Math.asin(accelY/hypotY));
            accelAngleY = Math.toDegrees(Math.asin(accelX/hypotX));

            //System.out.println(accelAngleX[0]+"" ""+accelAngleX[1]+"" ""+accelAngleX[2]+"" ""+accelAngleX[3]);

            //System.out.println(""accelX: "" + accelX+"" accelY: "" + accelY+"" accelZ: "" + accelZ);

            r = sensor.read(0x43, gyroData, 0, 6);

            gyroX = (((gyroData[0] &lt;&lt; 8)+gyroData[1]-gyroCalib[0])/131.0);
            gyroY = (((gyroData[2] &lt;&lt; 8)+gyroData[3]-gyroCalib[1])/131.0);
            gyroZ = (((gyroData[4] &lt;&lt; 8)+gyroData[5]-gyroCalib[2])/131.0);

            if(init)
            {
                smoothedAccelAngleX = accelAngleX;
                smoothedAccelAngleY = accelAngleY;

                smoothedGyroX = gyroX;
                smoothedGyroY = gyroY;
                smoothedGyroZ = gyroZ;

                init = false;
            } else {
                smoothedAccelAngleX = smoothedAccelAngleX+(accelSmoothing*(accelAngleX-smoothedAccelAngleX));
                smoothedAccelAngleY = smoothedAccelAngleY+(accelSmoothing*(accelAngleY-smoothedAccelAngleY));

                smoothedGyroX = smoothedGyroX+(gyroSmoothing*(gyroX-smoothedGyroX));
                smoothedGyroY = smoothedGyroY+(gyroSmoothing*(gyroY-smoothedGyroY));
                smoothedGyroZ = smoothedGyroZ+(gyroSmoothing*(gyroZ-smoothedGyroZ));

                /*smoothedAccelAngleX = accelAngleX;
                smoothedAccelAngleY = accelAngleY;

                smoothedGyroX = gyroX;
                smoothedGyroY = gyroY;
                smoothedGyroY = gyroY;*/


                /*smoothedAccelAngleX += (accelAngleX-smoothedAccelAngleX)/accelSmoothing;
                smoothedAccelAngleY += (accelAngleY-smoothedAccelAngleY)/accelSmoothing;

                smoothedGyroX += (gyroX-smoothedGyroX)/gyroSmoothing;
                smoothedGyroY += (gyroY-smoothedGyroY)/gyroSmoothing;
                smoothedGyroZ += (gyroZ-smoothedGyroZ)/gyroSmoothing;*/

            }

            angleX += smoothedGyroX*(System.currentTimeMillis()-time)/1000;
            angleY += smoothedGyroY*(System.currentTimeMillis()-time)/1000;
            angleZ += smoothedGyroZ;

            angleX = 0.95*angleX + 0.05*smoothedAccelAngleX;
            angleY = 0.95*angleY + 0.05*smoothedAccelAngleY;

            time = System.currentTimeMillis();

            //System.out.println((int)angleX+""  ""+(int)angleY);
            //System.out.println((int)accelAngleX+"", ""+(int)accelAngleY);
        }
    }

    public static void calibrate() throws IOException {
        int i;
        for(i = 0; i &lt; 100; i++)
        {
            accelData = new byte[6];
            gyroData = new byte[6];
            int r = sensor.read(0x3B, accelData, 0, 6);
            accelCalib[0] += (accelData[0] &lt;&lt; 8)+accelData[1];
            accelCalib[1] += (accelData[2] &lt;&lt; 8)+accelData[3];
            accelCalib[2] += (accelData[4] &lt;&lt; 8)+accelData[5];

            r = sensor.read(0x43, gyroData, 0, 6);
            gyroCalib[0] += (gyroData[0] &lt;&lt; 8)+gyroData[1];
            gyroCalib[1] += (gyroData[2] &lt;&lt; 8)+gyroData[3];
            gyroCalib[2] += (gyroData[4] &lt;&lt; 8)+gyroData[5];
            try {
                Thread.sleep(1);
            } catch (Exception e){
                e.printStackTrace();
            }
        }
        gyroCalib[0] /= i;
        gyroCalib[1] /= i;
        gyroCalib[2] /= i;

        accelCalib[0] /= i;
        accelCalib[1] /= i;
        accelCalib[2] /= i;

        System.out.println(gyroCalib[0]+"", ""+gyroCalib[1]+"", ""+gyroCalib[2]);
        System.out.println(accelCalib[0]+"", ""+accelCalib[1]+"", ""+accelCalib[2]);
    }

    public double readAngle(int axis)
    {
        switch (axis)
        {
            case 0:
                return angleX;
            case 1:
                return angleY;
            case 2:
                return angleZ;
        }

        return 0;
    }

    public double readGyro(int axis)
    {
        switch (axis)
        {
            case 0:
                return smoothedGyroX;
            case 1:
                return smoothedGyroY;
            case 2:
                return smoothedGyroZ;
        }

        return 0;
    }

    public double readAccel(int axis)
    {
        switch (axis)
        {
            case 0:
                return accelX;
            case 1:
                return accelY;
            case 2:
                return accelZ;
        }

        return 0;
    }

    public double readAccelAngle(int axis)
    {
        switch (axis)
        {
            case 0:
                return smoothedAccelAngleX;
            case 1:
                return smoothedAccelAngleY;

        }

        return 0;
    }

    public void setSmoothing(double gyro, double accel)
    {
        gyroSmoothing = gyro;
        accelSmoothing = accel;
    }
}
</code></pre>
","pid raspberry-pi quadcopter"
"8043","How to convert between classic and modified DH parameters?","<p>I currently have a description of my 22 joint robot in ""classic"" DH parameters.  However, I would like the ""modified"" parameters.  Is this conversion as simple as shifting the $a$ and $alpha$ columns of the parameter table by one row?</p>

<p>As you can imagine, 22 joints is a lot, so I'd rather not re-derive all the parameters if I don't have to.  (Actually, the classic parameters are pulled out of OpenRave with the command: <code>planningutils.GetDHParameters(robot)</code>.  </p>
","kinematics dh-parameters"
"8044","What are the best ways to transmit force through air efficiently?","<p>I am taking part in a robotics competition, where the challenge is to create a pair of robots which successfully navigate a series of obstacles. However, the rules state that of the two robots, only one must have a driving actuator. The other must somehow be moved by the other robot, WITHOUT PHYSICAL CONTACT. </p>

<p>I could think of either having sails on the non-driving robot, and moving it with fans on the driving one OR electromangnets on the driving one and permanent magnets with the opposite polarity on the non-driving one. However the problem with both is that efficiency falls off drastically with distance. Thus, I am looking for possible ways to overcome this problem. Thanks :)</p>

<p>Also, the driving robot <strong>has a cable power supply</strong>, while the non-driving one may <strong>only have batteries</strong>.</p>

<p>Rulebook: <a href=""http://ultimatist.com/video/Rulebook2016_Final_website_1_Sep_15.zip"" rel=""nofollow"">http://ultimatist.com/video/Rulebook2016_Final_website_1_Sep_15.zip</a></p>
","force"
"8045","composition of rotation matrices","<p>I am the moment learning about rotation matrices.  It seems confusing how it could be that  $R_A^C=R_A^BR_B^C$ is the rotation from coordinate frame <s>A to C</s> C to A, and A,B,C are different coordinate frames.</p>

<p>$R_A^C$ must for a 2x2 matrix be defined as 
$$
R_A^C=
\left(
\begin{matrix}
xa⋅xb  &amp;  xa⋅xb \\
ya⋅yb &amp;  ya⋅yb
\end{matrix}
\right)
$$</p>

<p>$x_a, y_a and x_b,y_b$ are coordinates for points given in different coordinate frame.
I don't see how, using this standard, the multiplication stated above will give the same matrix as for $R_A^C$. Some form for clarification would be helpful here.</p>
","frame"
"8047","what kp,ki,kd should i keep","<pre><code>  // MPU-6050 Short Example Sketch
// By Arduino User JohnChi
// August 17, 2014
// Public Domain
#include&lt;Wire.h&gt;
#include &lt;Servo.h&gt;
Servo firstESC, secondESC,thirdESC,fourthESC; //Create as much as Servoobject you want. You 
const int MPU=0x68;  // I2C address of the MPU-6050
int speed1=2000,speed2=0,speed3=0,speed4;
int16_t AcX,AcY,AcZ,Tmp,GyX,GyY,GyZ;
float integ=0,der=0,pidx=0,kp = .5,ki=0.00005 ,kd=.01,prerror,dt=100;
void setup(){
  firstESC.attach(3);    // attached to pin 9 I just do this with 1 Servo 
  secondESC.attach(5);    // attached to pin 9 I just do this with 1 Servo 
  thirdESC.attach(6);    // attached to pin 9 I just do this with 1 Servo 
  fourthESC.attach(9);    // attached to pin 9 I just do this with 1 Servo 
  Wire.begin();
  Wire.beginTransmission(MPU);
  Wire.write(0x6B);  // PWR_MGMT_1 register
  Wire.write(0);     // set to zero (wakes up the MPU-6050)
  Wire.endTransmission(true);
  Wire.beginTransmission(MPU);
  Wire.write(0x1c);  // PWR_MGMT_1 register
  Wire.write(0&lt;&lt;3);     // set to zero (wakes up the MPU-6050)
  Wire.endTransmission(true);
  Serial.begin(9600);
  firstESC.writeMicroseconds(0);
  secondESC.writeMicroseconds(0);
  thirdESC.writeMicroseconds(0);
  fourthESC.writeMicroseconds(0);
  firstESC.writeMicroseconds(2000);
  secondESC.writeMicroseconds(2000);
  thirdESC.writeMicroseconds(2000);
  fourthESC.writeMicroseconds(2000);
  delay(2000);
  firstESC.writeMicroseconds(700);
  secondESC.writeMicroseconds(700);
  thirdESC.writeMicroseconds(700);
  fourthESC.writeMicroseconds(700);
  delay(2000);
}
void loop(){
  Wire.beginTransmission(MPU);
  Wire.write(0x3B);  // starting with register 0x3B (ACCEL_XOUT_H)
  Wire.endTransmission(false);
  Wire.requestFrom(MPU,4,true);  // request a total of 14 registers
  AcX=Wire.read()&lt;&lt;8|Wire.read();  // 0x3B (ACCEL_XOUT_H) &amp; 0x3C (ACCEL_XOUT_L)    
  AcY=Wire.read()&lt;&lt;8|Wire.read();  // 0x3D (ACCEL_YOUT_H) &amp; 0x3E (ACCEL_YOUT_L) 
  firstESC.writeMicroseconds(0);
  secondESC.writeMicroseconds(700-(pidx/10));
  thirdESC.writeMicroseconds(700+(pidx/10));
  fourthESC.writeMicroseconds(0);
  PID();
  //if(Serial.available()) 
    //speed1 = Serial.parseInt(); 
  //Serial.print(""AcX = ""); Serial.print(AcX);
  //Serial.print("" | AcY = ""); Serial.print(AcY); 
  //Serial.println();
  //delay(333);
}

void PIdD()
{
  float error;
  error = (atan2(AcY,AcZ)*180/3.14);
  now = millis(); 
  dt = now-ptime;
  if(error&gt;0)error=180-error;
  else error = -(180+error); 
  error=0-error;
  integ = integ+(error*dt) ;
  der = (error - prerror)/dt ;
  prerror=error;
  pidx = (kp*error);
  pidx+=(ki*integ);
  pidx+=(kd*der);
  if(pidx&gt;1000)pidx=1000; 
  if(pidx&lt;-1000)pidx=-1000; 
  ptime = now; 
  }
</code></pre>

<p>the above is my program for my quadcopter, but now i have to tune the PID values, that is kp, ki and kd. my accelome is at 2g. Please point to me what is wrong with the program, is the error signal not appropriate? Please also give me or help me choose correct pid tuning. my limitation is I always have to connect my arduino to pc and change kp ki or kd values, that is i have no remote control available currently.</p>
","quadcopter pid"
"8050","iRobot Create 2 serial battery power","<p>I don's seem to be able to get any battery power from Create 2. I spliced the original cable it came with, and tried to use the power from red/purple(+) and yellow/orange(-) to power a Raspberry Pi2, with no luck. While the serial-to-USB cable still works, and I am able to command the robot via Python, there seems to be no power coming on the red/purple cables. I tried with a multimeter with no luck, even as I moved the device from passive/safe/full modes. There is no power even when Create 2 is charging/docked.</p>
","raspberry-pi irobot-create serial roomba"
"8051","Maximum ball screw speeds","<p>What is the maximum rotational velocity of miniature ball-screw (diameters up to 12mm) for approximately 1000 thrust cycles, and which type/brand would that be, if the speed is limited by the ball return mechanism? The fastest I could find was 4000 rpm at 3000 N thrust, but this was from a datasheet with a big safety margin (millions of cycles).</p>

<p>I'm looking for either experience and data, or a general method/formula that can be used to find the maximum velocity (and load) as function of cycles or the other way round (similar to those of ball bearings). Suggestions and knowledge about faster types and brands of ballscrews than the ones I have been able to find is welcome as well.</p>

<p><em>Some more background information:</em>
Ball screws are very interesting transmissions for electrically actuated legged robotics, since they provide a high-geared rotary-to-linear transmission that is accurate, precise, energy efficient and possibly backlash-free. However, the big downside is their limited rotational speed. The maximum rotational velocity is limited by resonance and the ball return mechanism. The former limit is easy to calculate (eigenfrequency calculation), and mostly not problematic for small spindles. However, the latter is a bigger problem. The balls in a ball screw roll through the threaded spindle and have to be recirculated to the other end of the nut. The recirculation limits the rotational velocity of the ballscrews. The corresponding maximum rotational velocities are not calculate-able (for as far as I know) and are provided by manufacturers in catalogues, either directly in rpm or via a so-called $D_n$-value, where the rotational velocity in rpm is $n=D_n/d$ where d is the diameter of the ball screw. But even then, the maximum rotational velocity of ball screws is capped at 4000 rpm or lower according to datasheets (depending on brand and ball return mechanism). The highest permissible rotational velocities I found were those of Steinmeyer ballscrews, at 4000 rpm, using an end-cap-return mechanism. Note that for electrical motors (up to 200W) ideal (maximum power) velocities are higher than 4000 rpm, and even more than twice as high for many brushless motors. It appears however that ball screws can run at higher speeds than what they are specified for in reality, because the specifications hold for many millions of cycles. I can only find a single <a href=""http://ballscrew-tech.com/2012/08/25/ball-screw-speed-ratings/"" rel=""nofollow"">unofficial source</a> where someone claims to have run their ball-screws up to 6000 rpm, and in missiles (one-time-use) up to 7500 rpm. I'm interested in a theory or more experimental data that backs this up.</p>
","driver"
"8053","How to use the Homogeneous transformation matrix?","<p>I am trying to understand how to use, what it requires compute the homogenous transformation matrix. </p>

<p>I know 2 points from 2 different frames, and 2 origins from their corresponding frames. </p>

<p>I how transformation matrix looks like, but whats confusing me is how i should compute the (3x1) position vector which the matrix needs.  As i understand is, this vector a origin of the old frame compared to the new frame.  But how to calculate it, the obvious answer (I think) would be to subtract both ($O_{new} - O_{old}$ ), but it does not feel right. </p>

<p>I know its a simple question but my head cannot get around this issue, and how can i prove it the right way, with the information i know?</p>
","kinematics frame"
"8054","Motor for DIY Remote controlled shades","<p>I'm currently undertaking a project to build remote controlled shades from scratch. I currently have every piece figured out except I don't know know much about the motors involved in something like this. I am looking for suggestions on what type of motor to search for. I imagine I need a type that can go forward and back as well as stop when the shade is fully retracted. I don't know what to search for though.</p>

<p>Any help is much appreciated. </p>
","motor"
"8056","What factors should i consider when selecting a motor for a free wheeled cart-pole balancing robot?","<p>I'm developing a small scale cart-pole balancing robot consisting of two wheels driven by a single motor at the base (essentially like a unicycle, but with two wheels to constrain balance to a one dimensional problem).</p>

<p>I'm not sure what qualities to look for in that motor.  I think the motor should be able to accelerate quickly in directions opposite of motion as dictated by the control system.  However, i'm not sure if this rapid acceleration should correlate with higher torque motors or faster speed motors.  I think higher torque motors would be too slow to react to control commands.  In contrast, fast speed motors may not be able to overcome the momentum of the cart.</p>

<p>Are there any design equations or other calculations i can make based on my robot's dimensions and weight to determine the right specs needed for my robot's motor?  How can i determine the right motor specs for this application without resorting to brute-force trial &amp; error experiments?</p>
","motor design balance"
"8058","Optimal-time acceleration sequence of a line-following robot following a moving obstacle","<p>Say we have a line-following robot that has a moving obstacle in front, that is a one-dimensional problem. The moving obstacle is defined by its initial state and a sequence of (longitudinal) acceleration changes (the acceleration function is piecewise constant). Let's say the robot can be controlled by specifying again a sequence of acceleration changes and its initial state. However, the robot has a maximum and minimum acceleration and a maximum and minimum velocity. How can I calculate the sequence of accelerations minimizing the time the robot needs to reach a goal. Note that the final velocity must not necessarily be zero.</p>

<p>Can you briefly explain how this problem can be addressed or point me to some references where an algorithm is described? Or point out closely related problems?</p>

<p>Furthermore, does the solution depend on the goal position or could the robot just brake as late as possible all the time (avoiding collisions) and still reach any goal in optimal time?</p>

<p>A more formal problem description:
Given the position of the obstacle $x_B(t) = x_{B,0} + \int_{t_0}^t v_B(t) dt$, and the velocity of the obstacle $v_B(t) = v_{B,0} + \int_{t_0}^t a_B(t) dt$, where $a_B$ is a known piecewise constant function:</p>

<p>$$a_B(t) = \begin{cases} a_{B,1} &amp; \text{for } t_0 \leq t &lt; t_1 \\
a_{B,2} &amp; \text{for } t_1 \leq t &lt; t_2 \\
\dots &amp; \\
\end{cases}$$</p>

<p>and given the initial state of the line-follower $x_{A,0}, v_{A,0} \in \mathbb{R}$ we search for piecewise constant functions $a_A$, where $a_{min} \leq a_A(t) \leq a_{max}$, $v_{min} \leq v_A(t) \leq v_{max}$ and $x_A(t) \leq x_B(t)$ (collision freeness) holds at all times. Reasonable assumptions are e.g. $v_B(t) \geq 0$ and $x_{B,0} \geq x_{A,0}$. Among the feasible solutions I would like to pick those minimizing $\int_{t_0}^{\infty} x_B(t) - x_A(t) dt$ or a similar objective. Approximation algorithms are also ok.</p>

<p>Some numbers for those who would like a test input:
<a href=""http://pastebin.com/iZsm2UhB"" rel=""nofollow"">http://pastebin.com/iZsm2UhB</a></p>
","mobile-robot control motion-planning line-following"
"8062","IR 40kHz receiver","<p>These days I'm trying to build IR 40kHz long range receiver. I use ir phototransistor. I don't want to use components like TSOP... I need to make
daylight filter and intensify filtred signal because out of this sensor I wanna use with some microcontroller. Can someone help me? Any idea? Thanks.</p>
","sensors"
"8065","Solution for INS and GPS integration","<p>I have a GPS module and an IMU (gyro, accelerometer and magnetometer) and I need to build an autonomous navigation system for a quadcopter. It must know its position at any time so that it can track a predefined path. I know that, in order to improve precision, I need to merge both sensors data through a Kalman Filter (or any other technique for that matter, the thing is that the Kalman Filter is way more common according to my research).
The problem is that I am seriously stuck and I know this might be something very simple but I don't seem to find a solution or at least the answer for some of the most basic questions.
As a start, I know how to get the position from the accelerometer readings. I have some filters that help eliminate noise and minimize the integration errors. I also have the GPS readings in latitude and longitude. The first question is, during sensor fusion, how can I make both measurements compatible? The latitude and longitude from the GPS won't simply mix with the displacement given by the accelerometer, so what is the starting point for all of this? Should I calculate the displacement from the GPS readings or should I assume a starting latitude and longitude and then update it with the accelerometer prior to applying the filter?</p>

<p>I have once developed a simple Kalman Filter in which I could plug the new reading values to obtain the next estimate position of a two wheeled car. Now I have two sources of inputs. How should I merge those two together? Will the filter have two inputs or should I find a function that will somehow get the best estimate (average, maybe?) from the accelerometer and GPS? I am really lost here.</p>

<p>Do you guys have any examples of code that I could use to learn? It is really easy to find articles full of boxes with arrows pointing the direction in which data must flow and some really long equations that start to get confusing very soon such as those presented on this article: <a href=""http://isas.uka.de/Material/Samba-Papierkorb/vorl2014_15/SI/Terejanu_tutorialUKF.pdf"" rel=""nofollow"">http://isas.uka.de/Material/Samba-Papierkorb/vorl2014_15/SI/Terejanu_tutorialUKF.pdf</a> (I have no problems with equations, seriously) but I have never seen a real life example of such implementation.</p>

<p>Any help on this topic would be deeply appreciated.</p>

<p>Thank you very much.</p>
","kalman-filter sensor-fusion gps"
"8077","Mobile Robot path reconstruction by using IMU acceleration and Yaw angle","<p>I hope you can help me with my project.</p>

<p>I'm using a skid-steering wheeled mobile robot for autonomous navigation and I'd like to find a way to be able to perform path reconstruction in Matlab.</p>

<p>By using only the robot encoders (installed on the robot) and the yaw rate information (which come from a very accurate IMU sensor mounted on the robot frame), I can successfully do the path reconstruction.
(I'm using XBOW-300CC sensor)</p>

<p>The problem is that I would like to try to reconstruct the path by using only the IMU yaw rate and the IMU acceleration values for X and Y axis.</p>

<p>I'm able to obtain velocity and distance by integrating two times the IMU acceleration values but my problem is that I don't know how to use this data.</p>

<p>Do I have to use a rotation matrix to pass from the IMU frame to the robot frame coordinates? 
I'm asking this because I use a rotation matrix for the encoder values which come from the robot encoder.</p>

<p>At the moment, I use these equations for robot encoders and IMU yaw rate:</p>

<pre><code>tetha(i)=(yaw(i)+yaw(i-1))/2*(encoder(i)-encoder(i-1))+tetha(i-1); %trapezoidal integral

Rx=[1 0 0;0 cos(-roll_angle(i)) -sin(-roll_angle(i)); 0 sin(-roll_angle(i)) cos(-roll_angle(i))];
Ry=[cos(-pitch_angle(i)) 0 sin(-pitch_angle(i)); 0 1 0; -sin(-pitch_angle(i)) 0 cos(-pitch_angle(i))];
Rz=[cos(-tetha(i)) -sin(-tetha(i)) 0; sin(-tetha(i)) cos(-tetha(i)) 0; 0 0 1];
R2=Rz*Ry*Rx;

disp=R2 *[encoder_displacement(i) 0 0]';
X_r(i)=disp(1);
Y_r(i)=disp(2);
Z_r(i)=disp(3);

X(i)=x0+sum(X_r(1:i));
Y(i)=y0+sum(Y_r(1:i));
Z(i)=z0+sum(Z_r(1:i));
</code></pre>

<p>Do I still have to use R2 matrix?</p>

<p>Thank you a lot</p>
","mobile-robot kinematics imu navigation matlab"
"8080","Could anyone tell me what are these things in a Roomba robot and how to clean them, please?","<p>I'm really in doubt whether it is proper to ask this question here, so I'm apologizing if it is not, I'll delete it.</p>

<p>I have a Roomba robot which has worked for me for more than three years, and now while it is working it is producing some strange sounds, so I've decided to clean it thoroughly.</p>

<p>But when I disassembled it down to this point:</p>

<p><a href=""http://i.stack.imgur.com/W2ZTR.jpg"" rel=""nofollow""><img src=""http://i.stack.imgur.com/W2ZTR.jpg"" alt=""Roomba disassembled""></a> </p>

<p>I got stuck with these sort of glass things (marked with the red rectangles at the picture). They are really filthy from the inside and I cannot figure out how to clean them.</p>

<p>Does anyone know how one can remove dust from the inside on these things? May be there are some Roomba creators here.</p>

<p>Thanks in advance.</p>
","roomba"
"8082","Wifi module for Zumo robot","<p>I'm a CS student trying to implement a clustering algorithm that would work for a set of robots in an indoor controlled environment. I'm still starting on Robotics and don't have much experiencing in figuring out what will work together.</p>

<p>My plan is to get 6 of these <a href=""https://www.pololu.com/product/3126"" rel=""nofollow"">Zumo robots</a> and plug in a wifi module like the <a href=""http://www.adafruit.com/products/1491"" rel=""nofollow"">Wifi shield</a>. Then, I would use this to do inter communication and execute my algorithm.</p>

<p>My question: Can the wifi module just be plugged in and would it work? If not, how can I go about achieving this task. I see lots of Arduino boards with different names and I'm not sure which works with which, and whether they can be plugged in. Any help would be appreciated.</p>
","arduino wifi"
"8087","Is it safe to give 5v through 5v pin of arduino uno r3 while usb cable inserted","<p>Is it safe to give 5v through 5v pin of arduino uno r3 while the USB cable is inserted? I have ESCs connected to it which aren't likely to start in other cases. The 5v and gnd is coming from the BEC circuit of a connected ESC. Please help me. Thanks<a href=""http://i.stack.imgur.com/cBaib.jpg"" rel=""nofollow""><img src=""http://i.stack.imgur.com/cBaib.jpg"" alt=""enter image description here""></a></p>
","arduino esc"
"8094","What is the learning (control) algorithm inside Cubli?","<p>As in this video: <a href=""https://www.youtube.com/watch?v=qce5Vguj5Jg"" rel=""nofollow"">https://www.youtube.com/watch?v=qce5Vguj5Jg</a></p>

<p>In this new version (did not see the learning part in the past versions), with three to four trials, Cubli can learn to balance on a new surface.</p>
","control"
"8096","How can I control fast real time sensor (250Hz) with slow system display(60Hz)","<p>We do some experiments of real time representation of sensor position on TV. In this experiments, we used sensors for collect real time position in 3D at 250Hz and TV for Display the sensor position at 60Hz. Also, we used MATLAB and C++ for programming with OpenGL platform.</p>

<p>In programming, Every iteration dat display on the TV, erase and draw the circle (Object, which is represent real time position on the display). In this program I collect to only 60 points and loose other 190 points in every second, becuase, I think that refresh rate of TV is 60Hz.</p>

<p>I have gone through the thread ""How can I control a fast (200Hz) realtime system with a slow (30Hz) system?""(<a href=""http://robotics.stackexchange.com/questions/807/how-can-i-control-a-fast-200hz-realtime-system-with-a-slow-30hz-system"">How can I control a fast (200Hz) realtime system with a slow (30Hz) system?</a>), but i don't understand, How to implement two loop on 200Hz and 30Hz.</p>

<p>My Question is, How can we implement in MATLAB/C++? So I can store 250 data of sensors as well as 60 points for real time display on the TV.</p>

<p>If you help me through pseudo code, I appreciate your help.</p>

<p>Thank You in advance.</p>

<p>Please help me.</p>

<p>P.S. Code</p>

<pre><code>%Display main window using Psychtoolbox

win=Screen(2,'OpenWindow',[1 1 1])

while (1)
  % Setup for data collection at 250Hz
  Error   = calllib('ATC3DG64', 'GetSynchronousRecord',  hex2dec('ffff'), pRecord, 4*numBoards*64);
  errorHandler(Error);
  Record = get(pRecord, 'Value');

  %sensor number
  count=2;

  evalc(['tempPos(1, count) ='  'Record.x' num2str(count - 1)]);
  evalc(['tempPos(2, count) ='  'Record.y' num2str(count - 1)]);
  evalc(['tempPos(3, count) ='  'Record.z' num2str(count - 1)]);

  % Record X and Y position of  sensor 2

  if SensorNumAttached(count)
    % Real time position and minus world origin so that real
    % time position display on the TV
    table1(count,1)=(2.54*tempPos(2,count))-X_world_origin;
    table1(count,2)=(2.54*tempPos(3,count))-Y_world_origin;
  end 

  % Some conversion for the Pixel to centimeter ratio
  x_center_new = x_center - (x_ratio * table1(2,1));
  y_center_new = y_center - (y_ratio * table1(2,2));

  % conversiorn for display circle on the TV, is represent the real time poistion of the sensor
  x1  =round(x_center_new - R_num_data);
  y1 = round(y_center_new - R_num_data);

  x2 = round(x1 + 2*R_num_data);
  y2 = round(y1 + 2*R_num_data);


  % Display command for TV.      
  Screen('FrameOval',win,[255 0 0], [x1 y1 x2 y2]);
  Screen('Flip',win);
end
</code></pre>
","design communication matlab c++"
"8100","Robotic legs technologies","<p>what robotic leg technologies are available.</p>

<p>i'm sorry if this is a basic question i am a software developer looking to get into the field of robotics. i am particularly interested in robotic legs that are similar to those used on Boston Dynamics ATLAS robot.</p>

<p>what is the mechanism required that allows it to move its joints so quickly. if you see any videos of many of Boston Dynamics robots they make an engine sound (presumably because it uses an engine), but i cant find any details in the configuration that is being used.</p>
","legged"
"8103","Overheating/Jamming MG996R servo","<p>I have recently purchased my first ever servo, a cheap unbranded Chinese MG996R servo, for £3.20 on eBay.</p>

<p><a href=""http://i.stack.imgur.com/p57Ha.jpg"" rel=""nofollow""><img src=""http://i.stack.imgur.com/p57Ha.jpg"" alt=""MG996R clone servo""></a></p>

<p>I am using it in conjunction with a Arduino Servo shield (see below):</p>

<p><a href=""http://i.stack.imgur.com/RKe8Z.jpg"" rel=""nofollow""><img src=""http://i.stack.imgur.com/RKe8Z.jpg"" alt=""Arduino Servo Shield""></a></p>

<p>As soon as it arrived, before even plugging it in, I unscrewed the back and ensured that it had the shorter PCB, rather than the full length PCB found in MG995 servos. So, it seems to be a reasonable facsimile of a bona-fide MG996R.</p>

<p>I read somewhere (shame I lost the link) that they have a limited life, due to the resistive arc in the potentiometer wearing out. So, as a test of its durability, I uploaded the following code to the Arduino, which just constantly sweeps from 0° to 180° and back to 0°, and left it running for about 10 to 15 minutes, in order to perform a very simple <em>soak test</em>.</p>

<pre><code>#include &lt;Servo.h&gt; 

const byte servo1Pin = 12;

Servo servo1;                // create servo object to control a servo 
                             // twelve servo objects can be created on most boards

int pos = 0;                 // variable to store the servo position 

void setup() 
{ 
  servo1.attach(servo1Pin);  // attaches the servo on pin 9 to the servo object 
  Serial.begin(9600);
} 

void loop() 
{ 
  pos = 0;
  servo1.write(pos);         // tell servo to go to position in variable 'pos' 
  Serial.println(pos);
  delay(1000);               // waits 15ms for the servo to reach the position 

  pos = 180;
  servo1.write(pos);         // tell servo to go to position in variable 'pos' 
  Serial.println(pos);
  delay(1000);               // waits 15ms for the servo to reach the position 
} 
</code></pre>

<p>When I returned, the servo was just making a grinding noise and no longer sweeping, but rather it seemed to be stuck in the 0° position (or the 180°). I picked the servo up and whilst not hot, it was certainly quite warm. A quick sniff also revealed that hot, burning motor windings smell.  After switching of the external power supply and allowing it to cool, the servo began to work again. However, the same issue occurred a little while later. Again, after allowing it to rest, upon re-powering, the servo continues to work.  However, I am reluctant to continue with the soak test, as I don’t really want to burn the motor out, just yet.</p>

<p>Is there a common “no-no” of not making servos sweep from extreme to extreme, and one should “play nice” and just perform 60° sweeps, or is the cheapness of the servo the issue here?</p>

<p>I am powering the servo from an external bench supply, capable of 3A, so a lack of current is not the issue.</p>

<hr>

<p>Please note that I also have a follow up question, <a href=""http://robotics.stackexchange.com/questions/8104/should-a-mg996r-servos-extreme-position-change-over-time"">Should a MG996R Servo's extreme position change over time?</a></p>
","arduino rcservo"
"8104","Should a MG996R Servo's extreme position change over time?","<p>This question is a follow on from my previous question, <a href=""http://robotics.stackexchange.com/questions/8103/overheating-jamming-servo"">Overheating/Jamming MG996 servo</a>.</p>

<p>I have recently purchased my first ever servo, a cheap unbranded Chinese MG996R servo, for £3.20 on eBay.</p>

<p><a href=""http://i.stack.imgur.com/p57Ha.jpg"" rel=""nofollow""><img src=""http://i.stack.imgur.com/p57Ha.jpg"" alt=""MG996R clone servo""></a></p>

<p>After mounting the servo horn and the bracket, I realised that I had not mounted the horn in a <em>tout a fait</em> 0° orientation, rather the angle between the bracket and the servo side was approximately 20°. However, after switching the servo on and off a couple of times, with each time allowing the servo to perform, say, about 10 sweeps each time, I quickly noted that the servo’s extreme positions were changing over time, so that the initial extremes and then the extremes after about 5 on and off cycles, had changed by about 15°, so that now, 0° and 180° the bracket is now parallel with the body of the servo. </p>

<p>I was quite surprised at this, as I had assumed that the 0° and 180° positions would be fixed, and not change over time, or vary each time that it was switched on and off.</p>

<p>Seeing as there should be a <em>stop peg</em> on the gear connected to the potentiometer inside, how is this even possible?</p>
","rcservo"
"8106","2D Positioning of mobile robot","<p>I am just starting to explore an idea and I am somewhat of a novice in robotics. I am looking to position a mobile robot as accurately as possible on a concrete slab. This would be during new construction of a building and probably not have many walls or other vertical points for reference. the basic premise behind the robot is to print floor plans straight on to the slab. I will have access to the BIM (building information models, CAD, Revit) files of the building. I want the robot to position itself as accurately as possible on the blank slab using the BIM files as a map. What would be the best avenue to track and adjust positioning of the robot in the open space of a slab? Low frequency, Lidar, wifi? Lastly what sensors would be best?</p>
","mobile-robot sensors localization"
"8107","Are there systematic ways to tune the Kalman filter in engineering practice?","<p>Including Q, R, and initial states of x and P.</p>
","kalman-filter"
"8108","How to use specific ESC,BLDC motor through Arduino Uno R3?","<h2>Attempt to clean up:</h2>

<p>I'm trying to use <a href=""http://rctimer.com/product-1127.html"" rel=""nofollow"">this motor</a> with <a href=""http://www.rctimer.com/product-1279.html"" rel=""nofollow"">this ESC</a> and an Arduino Uno R3.</p>

<p>Typically, I used the PWM pins when I use the Arduino and an ESC, but I can't control the motor even if I use the servo library, and I've also tried sample code from different websites.</p>

<p>The ESC has a beep I can't understand. Sometimes it's high-low-high or high for 4 seconds, but I can't find anything on Google. </p>

<p>Sometimes the motor spins periodically for a short time, but I don't know why. Some sites recommend using flash or bootloader, but I'd prefer to use Arduino PWM or the servo library. </p>

<h2>Original post</h2>

<p>Specific ESC is Rctimer Mini ESC16A OPTO SimonK Firmware SN16A ESC..</p>

<p>I can only using ESC(Discussed above..) and RCTimer 1806-1450KV Multi-Rotor BLDC Motor.</p>

<p>Typically, I used PWM pins(3, 9, 10, 11-because similar Signal frequency) when using Arduino-ESC.. but, I can control BLDC Motor even i used to Servo library.. </p>

<p>I've been used usual websites example code.</p>

<p>Just ESC had unknowable beep .. sometime di-ri-di or di(for 4 seconds).. 
I couldn't find that way.. in google (or my country websites)</p>

<p>Sometimes, The Motor spins(In a certain value, periodically) for a short time but I don't know why The motor spins</p>

<p>In google sites, just using flash or Bootloader, but I'll use Arduino PWM or Servo..</p>

<p>So.. Please! would you please help me?
Thank you for reading my thread.</p>

<p><a href=""http://i.stack.imgur.com/vU9oN.jpg"" rel=""nofollow""><img src=""http://i.stack.imgur.com/vU9oN.jpg"" alt=""enter image description here""></a>
<a href=""http://i.stack.imgur.com/MNUMG.jpg"" rel=""nofollow""><img src=""http://i.stack.imgur.com/MNUMG.jpg"" alt=""enter image description here""></a></p>
","arduino brushless-motor esc pwm"
"8111","Multiple control loops with overlapping effects","<p>I'm familiar with using PID to perform closed loop control when there is a single output and a single error signal for how well the output is achieving the desired set-point.</p>

<p>Suppose, however, there are multiple control loops, each with one output and one error signal, but the loops are not fully independent.  In particular, when one loop increases its actuator signal, this changes the impact of the output from other loops in the system.</p>

<p>For a concrete example, imagine a voltage source in series with a resistor, applying a voltage across a system of six adjustable resistors in parallel.  We can measure the current through each resistor and we want to control the current of each resistor independently by adjusting the resistance.  Of course, the trick here is that when you adjust one resistor's resistance, it changes the overall resistance of the parallel set, which means it changes the voltage drop due to the divider with the voltage source's resistance and hence changes the current through the other resistors.</p>

<p>Now, clearly we have an ideal model for this system, so we can predict what resistance we should use for all resistors simultaneously by solving a set of linear equations.  However, the whole point of closed loop control is that we want to correct for various unknown errors/biases in the system that deviate from our ideal model.  The question then: what's a good way to implement closed loop control when you have a model with this kind of cross-coupling?</p>
","control pid"
"8112","Do you know where to get the original iRobot Create?","<p>Does anyone out there know where I can get the original iRobot Create?
The company no longer sells them. 
It was only 2 years ago that it was sold. It is white and its value is the physical design, that it has a large exposed deck for mounting armatures.
It is preprogrammed to operate in different configurations, eg. spinning, figure 8, following the outline of a wall, etc.
I have an ongoing art project using this model and as they are in operation everyday, I will eventually need to replace them with new ones.
To see a video of one of my projects you can go to <a href=""https://vimeo.com/119486779"" rel=""nofollow"">https://vimeo.com/119486779</a>
I currently have it working in a spinning motion.</p>
","irobot-create"
"8116","Covariance and optimization","<p>I am trying to build a map containing lamps as landmarks. I drive around with a robot and a monocular camera looking to the ceiling.</p>

<p>The first step is detect the edges of each observed rectangular lamp and save the position in pixels and also the current position from odometry of the robot. After the lamp disappears from the field of view, there is enough base-line to do a 3D reconstruction based on structure from motion. Once this reconstruction is done there will be uncertainty in the position of the lamps that can be modelled by covariance. Imagine if the robot was driving for a while, its own position estimated from odometry will also have a relatively high incertitude, how can I integrate all of those incertitudes together in the final covariance matrix of the position of each lamp?</p>

<p>If I understand well there would be the following covariances:</p>

<ul>
<li>noise from camera</li>
<li>Inaccurate camera calibration matrix</li>
<li>inaccurate result from optimization</li>
<li>drift in odometry</li>
</ul>

<p>My goal is to manually do loop closure using for example g2o (graph optimization) and for that I think correct covariances are needed for each point.</p>
","slam cameras 3d-reconstruction"
"8118","What sensors and MCU does VectorNav VN300 use internally?","<p>I'm hesitant to open up a <code>mucho-$$$$</code>VectorNav VN300 to see what's inside.  Does anybody here know what underlying sensors it uses?  The accel, gyro, and mag outputs are all very low-noise compared to other ""high end"" 3-axis MEMs devices (ie Kionix KXR94, Maxim 21000, ST LIS3MDL).  Everything fits in such a small package so I'm guessing they're using devices with integrated axes and ADC's, rather than than ""navigation grade"" devices which tend to be analog, fewer than three axes, and enormous compared to to consumer-level MEMs.  Likewise, automotive MEMs sensors (which they mention they are using in one of the web pages) tend to be single or dual axis, and not necessarily less noisy than consumer-grade sensors.  </p>
","imu navigation"
"8120","Device to generate screen tap response","<p>I have extremely limited knowledge in the general topic of robotics and therefore this question is a shot in the dark. Please let me know if the topic is unsuitable for the site.</p>

<p>I am interested in creating a device that would generate a touchscreen tap. In a nutshell, I would like to replicate on a touchscreen the automated mouse functionality you can obtain with software like AutoHotKey in Windows. Since, without jailbreaking the phone, a software solution is basically impossible, it occurs that one of the first components would be a physical device that simulates a tap. Do any options for such a component exist?</p>

<p>I recognize that there are philosophical implications with creating such a device. I am assuming the entire conversation to be theoretical and solely related to the hardware design.</p>

<p>Thanks,
Alex</p>
","automatic"
"8122","Powering my robot with 12V battery which is charged by a gas/petrol generator while the robot is operating?","<p>A have designed a robot to perform tasks in farms.  But the problem now is I'm not sure on the best way to supply continuous power to my robot.  All the motors are rated at 12V and only Arduino and a few sensors work at 5V or less.  </p>

<p>Can I continuously charge a 12V lead acid battery with an adapter (comes with the battery) plugged into the AC output of the generator while the robot is operating? Do I have to worry about overcharging the battery?</p>

<p>Or should I use the generator's DC output which can supply 12V and up to 8.3Amp. Or is there any other suggestions?</p>

<p>Some information about the adapter which are stated on the package:
1. Built-in over-charge protection device.
2. Built-in thermal protection device
3. Output: 6v/12v 2Amp</p>

<p>This is the generator that I have: 
<a href=""http://global.yamaha-motor.com/business/pp/generator/220v-60hz/0-1/et950/"" rel=""nofollow"">http://global.yamaha-motor.com/business/pp/generator/220v-60hz/0-1/et950/</a></p>

<p>This is my first robot which is quite big that requires a lot of electrical/electronic knowledge to power it. I do not have a lot of experience in this field. So any feedback is greatly appreciated.</p>
","electronics power battery"
"8124","Two exclusive inputs control","<p>I have a system with two inputs (throttle and brake) and one output (speed). How does one design a controller in such a way that the two outputs of the controller (throttle and brake) are never both greater than zero (so that it doesn't accelerate and brake simultaneously)?</p>

<p>Thanks</p>
","control automation"
"8127","Can i connect the arduino usb to laptop after the arduino is started","<p>I have some sensors attached to arduino uno r3 and an esc. I start the Motor attached to esc through ardiuno with no usb connected to laptop. It starts correctly. There is a must that i will have to start the arduino from non usb supply so that esc is correctly started, means that my motor doesnt start with usb connected to pc. Now how can i get the sensor values to laptop. If i connect the usb to pc after starting the motor, will this work.</p>
","arduino esc"
"8128","How to calculate the current consumed by a brushless motor on a quadcopter","<p>I want to create a virtual quadcopter model, but I am struggling to come up with a satisfying model for the brushless motors &amp; props.</p>

<p>Let's take an example, based on the great <a href=""http://www.ecalc.ch/xcoptercalc.php?ecalc&amp;lang=en"" rel=""nofollow"">eCalc tool</a>:</p>

<p><a href=""http://i.stack.imgur.com/LbTmc.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/LbTmc.png"" alt=""eCalc""></a></p>

<p>Let's say I want to know how much current is consumed by the motor in a hovering state. I know the mass of the quad (1500g), so I can easily compute the thrust produced by each motor:</p>

<pre><code>Thrust = 1.5 * 9.81 / 4 = 3.68 N per motor
</code></pre>

<p><a href=""http://www.wired.com/2014/05/modeling-the-thrust-from-a-quadcopter/"" rel=""nofollow"">Thrust is produced by moving a mass of air</a> at an average speed of <code>V</code>:</p>

<pre><code>Thrust = 0.5 * rho * A * V²
</code></pre>

<p>Where <code>rho</code> (air density) is 1.225kg/m3 and <code>A</code> (propeller disk area) is <code>PI * Radius² = 0.073m²</code> (12"" props). So I can compute <code>V</code>:</p>

<pre><code>V = sqrt(Thrust / 0.5 / rho / A) = 9.07 m/s
</code></pre>

<p>All right, now I can calculate the <em>aerodynamic</em> power created by the propeller:</p>

<pre><code>P = Thrust * V = 3.68 * 9.07 = 33.4 W
</code></pre>

<p>All right, now I can calculate the <em>mechanical</em> power actually produced by the motor. I use the <code>PConst</code> efficiency term from eCalc:</p>

<pre><code>Pmec = Paero * PConst = 33.4 * 1.18 = 39.4W
</code></pre>

<p>Here, eCalc predicts <strong>37.2W</strong>. It's not too far from my number, I imagine they use more sophisticated hypotheses... Fair enough.</p>

<p>From <a href=""http://www.rcgroups.com/forums/showthread.php?t=587549"" rel=""nofollow"">this post</a>, I know that this power is also equal to:</p>

<pre><code>Pmec = (Vin - Rm * Iin) * (Iin - Io)
</code></pre>

<p>Where I know <code>Rm</code> (0.08 Ohms) and <code>Io</code> (0.9 A). So, finally, my question: How do you calculate <code>Vin</code> and <code>Iin</code> from here? Of course, if I knew the rotation speed of the engine I could get <code>Vin</code> from:</p>

<pre><code>n = Kv * Vin
</code></pre>

<p>Where <code>Kv = 680 rpm/V</code>. But unfortunately I don't know the rotation speed...</p>

<p>(Note that Vin is assumed to be averaged from the <a href=""https://en.wikipedia.org/wiki/Pulse-width_modulation"" rel=""nofollow"">pulse-width-modulated</a> output produced by the ESC)</p>

<p>Thanks for your help!</p>
","motor quadcopter brushless-motor electronics power"
"8129","Why are bipedal robots difficult?","<p>Not sure if this has been asked, but there are lots of simulations of bipedal locomotion algorithms online, some of the evolutionary algorithms converge to very good solutions. So it seems to me that the algorithm part of bipedal locomotion is well-understood.</p>

<p>If you can do well on simulations, you should be able to do it well in the real world. You can model delay and noise, you can model servo's response curve.</p>

<p>What I don't understand is then why is it still difficult to make a walking robot? Even a robot like the Big Dog is rare.</p>
","walk"
"8130","Line following robot path planning","<p>I have built a mobile robot with several ultrasonic sensors to detect obstacles and an infrared sensor to track a line as a path. I have written a simple algorithm to follow the line which works fine, but avoiding obstacles are a problem because the robot doesn't know the layout of the path, so even if it does move around the obstacle, it is not guaranteed that it will find the path line again(unless the line is perfectly straight). Therefore, I think I may need to use a path/motion planning algorithm or find a way to store the layout of the path so that robot could predict where to move and get back to the path line and keep on following after overcoming an obstacle. I would like to hear suggestions or types of algorithms I should focus on for this specific problem.</p>

<p>Picture might help specifying the problem I'm facing.</p>

<p><a href=""http://i.stack.imgur.com/pSmil.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/pSmil.png"" alt=""path issue""></a>
Thank you.</p>
","motion-planning"
"8133","""Ambiguous up to scale"" , Explanation required","<p>I am reading ""Computer Vision: Models, Learning, and Inference"" in which author writes at several points (like on page 428-429) that although matrix <strong>A</strong> seems to have 'n' degree of freedom but since it is ambiguous up to scale so it only has 'n-1' degree of freedom? Can anyone explain what this thing means? Why one degree of freedom is decreased?</p>
","computer-vision"
"8134","Interferences while using two ToF cameras","<p>Im using two time-of-flight (ToF) cameras, a DS325 from SoftKinetic and a Creative Senz3D, at the same time with the DepthSense-SDK and the Point-Cloud-Library (on Ubuntu 15.04). But I got strong interferences.</p>

<p>Is there a possibility to control the laser (software side) either to send the light in a special frequency or to turn it off and on alternating?</p>

<p>Or is there another way to get rid of the interferences?</p>
","cameras laser 3d-reconstruction 3d-model"
"8140","""Thermal Imaging"" with Arduino and/or Lego Mindstorm NXT 2.0?","<p>I'm trying to build a robot that can be sent into rooms/buildings and detect people using nxt and/or Arduino. In addition to this I would like to be able to view what my robot is ""seeing"" in real-time on my PC as an infrared image. The sensors I've shortlisted for this are:</p>

<ol>
<li><p><a href=""http://www.generationrobots.com/en/401177-thermal-infrared-nxt-sensor-dexter-industries.html"" rel=""nofollow"">Thermal Infrared NXT Sensor from Dexter industries - £44</a></p></li>
<li><p><a href=""http://www.robotshop.com/uk/roboard-rm-g212-thermal-array-sensor.html#reviewBox"" rel=""nofollow"">RoBoard RM-G212 16X4 Thermal Array Sensor - £94</a></p></li>
<li><p><a href=""http://uk.farnell.com/omron-d6t-mems-thermal-ir-sensor"" rel=""nofollow"">Omron D6T MEMS Thermal IR Sensor - £31</a></p></li>
</ol>

<p>I believe the RoBoard and Omron sensors are capable of thermography, so I was wondering if anyone here has experience with these sensors and give me some advice.</p>

<p>I was also thinking about using an idea from this project: www.robotc.net/blog/tag/dexter-industries.<br>
In this case I'd use the data read from the sensor to plot a graph showing different temperatures.   </p>
","mobile-robot sensors nxt"
"8142","High-traction thin tires vs Wide moderate-traction tires? [Sumo-bot]","<p>I am building a sumo-bot and our competitors have thin sticky tires, while we have wider and less sticky tires. The diameter is the same, and the gearbox/motor is the same. Who will win?</p>

<p>PS: Sticky tires: <a href=""https://www.pololu.com/product/694"" rel=""nofollow"">https://www.pololu.com/product/694</a> &amp; wide tires: <a href=""https://www.pololu.com/product/62"" rel=""nofollow"">https://www.pololu.com/product/62</a></p>

<p>Thanks!</p>
","movement wheel two-wheeled"
"8144","Using DC motor as a generator to recharge battery of my robot","<p>I am trying to recharge my 12V lead acid battery with a 12V DC motor.  I am using the battery to power the robot when it climbs. When it descends, I notice that I dont need to apply reverse voltage but the dc motor just backdrives instead. This can act as generator to recharge back the battery, am I right?</p>

<p>I know that i need to step up the low voltage that is generated by the backdriven motor to 12V needed to recharge the battery. This is the board that I think can do the job:
<a href=""https://www.pololu.com/product/799"" rel=""nofollow"">https://www.pololu.com/product/799</a></p>

<p>Is this all I need to make it work? With this method, should I be concerned about the 3 stages of battery charging: bulk, absorption and float?</p>

<p>Please advise. Any feedbacks are greatly appreciated.</p>
","mobile-robot battery"
"8146","Using SLAM to create 2D topography","<p>I have a small mobile robot with a LidarLite laser range finder attached to a servo. As of now I have the range finder side-sweeping in a 30 degree arc, taking continuous distance readings to the side of the robot (perpendicular to the robots forward motion).</p>

<p>My goal is to have the robot drive roughly parallel to a wall, side-scanning the entire time, and create a 2D map of that wall it is moving past. The 2D topography map is created post processing (I use R for much of my data processing, but I don't know is popular for this kind of work).</p>

<p>From what I know of it, SLAM sounds like a great tool for what I want to do. But I have two issues:</p>

<p>1: I know my robot will not have a consistent speed, and I have no way to predict or measure the speed of my robot. So I have no way to estimate the odometry of the robot.  </p>

<p>2: The robot will also move further and closer to the wall as it proceeds down it's path. So I can not depend on a steady plane of travel from my robot.</p>

<p>So given that I don't have any odometry data, and my realtive distance to the wall changes over the course of a run, is it possible to use SLAM to create 2D maps?</p>

<p>I'm looking into stitching algorithms that are used for other applications, and some of these can handle the variances in relative distance, but I was hoping SLAM or some other algorithm could be of use here.</p>
","slam servos laser rangefinder"
"8150","Sensor orientation of an external magnetometer","<p>On many drones are already external magnetometers. Unfortunately, the orientation of such sensors is sometimes unknown. E.g. the sensor can be tilted 180° (pitch/roll) or X° in yaw. I was wondering, whether one could calculate the rotation of the sensor relative to the vehicle by application of the accelerometer and gyrometer?
Theoretically, the accelerometer yields a vector down and can be used for calculation of the coordinate system. The discrepancy between magnetometer and gyrometer then, may be used for calculation of the correct orientation of the compass. Later the compass should be used for yaw calculation. 
Below is the starting orientation of both sensors (just an example, the orientation of the compass can be anything). Does someone know a good way to figure out the rotation of the compass? </p>

<p><a href=""http://i.stack.imgur.com/JbG8H.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/JbG8H.png"" alt=""enter image description here""></a></p>
","magnetometer orientation"
"8152","How Should I tie My quadcopter to some thing, to adjust pid on one axis","<p>I am stuck in adjusting the PID of my quadcopter, I cant adjust them on the fly because it just get out of control. I am adjusting them while attaching my quadcopter to something.<a href=""http://i.stack.imgur.com/n7gWh.jpg"" rel=""nofollow""><img src=""http://i.stack.imgur.com/n7gWh.jpg"" alt=""this is how am I doing""></a></p>

<p>Is this method correct. Will the pid values required will be different on the fly or same. Please suggest me how to attach my quad to some thing.</p>
","quadcopter pid balance"
"8153","Can someone explaine to me this code?","<pre><code>static void set_default_param(DPMTTICParam&amp; param)
{

    param.overlap = 0.4;
    param.threshold = -0.5;
    param.lambda = 10;
    param.num_cells = 8;
}
</code></pre>
","c++"
"8160","Implement of a vocal interface on a arducopter","<p>I'm a student working on a robotics project, and I'm a complete beginner in robotics.</p>

<p>I'm working on the ArduCopter structure, using the ArduPilot Mega associated with the Ardupilot IMU as my autopilot board. I have an EasyVR module with an Arduino UNO for my vocal recognition stuff.</p>

<p>I don't know how to give order to the autopilot board with my vocal module. Do I need to change the ArduCopter source code? Do I have to use Mission Planner software?</p>

<p>The final aim is to do <a href=""http://diydrones.com/profiles/blogs/voice-controlled-drone"" rel=""nofollow"">this</a>.</p>
","arduino quadcopter ardupilot"
"8162","When to use multiple batteries vs a UBEC","<p>When should you use multiple separate batteries vs a single battery with multiple UBECs?</p>

<p>I'm trying to design the power system for a small 2-wheeled robot. Aside from the 2 main drive motors, it also has to power an Arduino, a Raspberry Pi and a couple small servos to actuate sensors.</p>

<ul>
<li>the motors are each rated for 6V with a peak stall current of 2.2A</li>
<li>the Arduino uses about 5V@100mA</li>
<li>the Raspberry Pi uses about 5V@700mA</li>
<li>the servos each use 6V and have a peak stall current of 1.2A.</li>
</ul>

<p>So the theoretical max current draw would be <code>2.2*2+.1+.7+1.2*2</code> = 7.6A.</p>

<p>Originally I was planning to use three separate Lipo batteries:</p>

<ul>
<li>one 12V using a step-down converter to power the main drive motors for 6V@4.4A peak</li>
<li>two 3.7V lipos each with step-up converter (rated for 5v@3A) to handle the servos and logic separately</li>
</ul>

<p>Then I discovered <a href=""https://en.wikipedia.org/wiki/Battery_eliminator_circuit"" rel=""nofollow"">UBECs</a>, which sound too good to be true, and they seem to be both cheap (<code>&lt;$10</code>) and efficient (<code>&gt;90%</code>) and able to handle my exact volt/current requirements.</p>

<p>Should I instead use a single high-current 12V lipo with three UBECs to independently power my drive motors, sensor motors and logic? Or will this still suffer from brown-out and power irregularities if a motor draws too much current?</p>

<p>What am I missing?</p>
","battery bec"
"8165","what's confidence level? and how can we use it for vehicle detection using OpenCV?","<p>I'm working on project for the Autonomous vehicle, and i want to know what's confidence level means and how can we use confidence level for vehicle detection in OpenCV ?</p>
","opencv statistics"
"8168","How much offset speed of motors on an axis is required before adjusting pid","<p>For adjusting the pid for quadcopter, how much speed of motors are required before adjusting the pid. Do we need to give so much offset speed so that it cancels weight? I am sure we cant start adjusting pid with zero speed of motors initially.</p>
","quadcopter pid"
"8171","Robot arm reachability of a pose in Cartesian space","<p>Given a set of robot joint angles (i.e. 7DoF) $\textbf{q} = [q_1, ... , q_n]$ one can calculate the resulting end-effector pose (denoted as $\textbf{x}_\text{EEF}$), using the foward kinematic map. </p>

<p>Let's consider the vice-versa problem now, calculating the possible joint configurations $\textbf{q}_i$ for a desired end-effector pose $\textbf{x}_\text{EEF,des}$. The inverse kinematics could potentially yield infinitely many solutions or (and here comes what I am interested in) no solution (meaning that the pose is not reachable for the robot).</p>

<p>Is there a mathematical method to distinguish whether a pose in Cartesian space is reachable? (Maybe the rank of the Jacobian) Furthermore can we still find a reachability test in case we do have certain joint angle limitations? </p>
","mobile-robot robotic-arm kinematics inverse-kinematics jacobian"
"8181","Can digital servo motors be modified for continuous rotation?","<p>In an autonomous mobile robot, we're planning on using digital servo motors to drive the wheels. Servo motors usually don't rotate continuously. However, they can be modified to do so based on many tutorials online which only mention modifying [analog] servo motors.</p>

<p>My question is, can the same method(s) or any other ones be used to modify <em>digital</em> servo motors?</p>

<p>Thanks</p>
","mobile-robot rcservo"
"8182","image processing","<p>I am making a Robot goalie, the robot is supposed to detect whether a ball has been thrown in its direction , sense the direction of the ball and then stop it from entering the goal post. A webcam will be mounted on top of the goal post. The robot is required to only move horizontally (left or right), it shouldn't move forwards or backwards. The robot will have wheels, the image processing will be performed by raspberry pi which will then send the required information to a micro controller which will be responsible for moving the robot in the required direction(using servo motors). Which image processing algorithm will be the best to implement this scenario?</p>
","mobile-robot computer-vision algorithm beagle-bone first-robotics"
"8184","What is the difference between a CNC router versus a CNC mill?","<p>People at the RepRap <a href=""/questions/tagged/3d-printing"" class=""post-tag"" title=""show questions tagged &#39;3d-printing&#39;"" rel=""tag"">3d-printing</a> project often mention <a href=""http://reprap.org/wiki/CNC_Router"" rel=""nofollow"">CNC routers</a> or <a href=""http://reprap.org/wiki/MillStrap"" rel=""nofollow"">CNC mills</a>.</p>

<p>Both kinds of machines almost always have a motorized spindle with stepper motors to move the spindle in the X, Y, and Z directions.</p>

<p>What is the difference between a CNC router versus a CNC mill?</p>

<p>(Is there a better place for this sort of question -- perhaps the <a href=""http://%20http://woodworking.stackexchange.com/"" rel=""nofollow"">Woodworking Stack Exchange</a>?)</p>
","industrial-robot"
"8186","Robot wire follower + how to position on wire","<p>I'm designing my lawn mower robot, and I am in the perimeter stage. 
The electronic part is done, and works quite good, now comes the software.</p>

<p>I need an advice on how to deal with the problem of line following. I mean, once the robot is on the line, parallel to the line, that's relatively easy. 
But how to manage the situation when the robot is driving around and approaches the line (wire)?</p>

<p>I have two sensors, left and right, turned 45° with respect to the forward direction. </p>

<p>The robot could arrive from any angle, so the signal amplitude read from the sensor could be completely random.. 
So I don't understand what to do in order to move it in the right position on the wire...
What's the usual approach? </p>

<hr>

<p>The idea is the same as here:</p>

<p><a href=""http://wiki.ardumower.de/index.php?title=Datei:Ardumower_perimeter.jpg"" rel=""nofollow""><img src=""http://i.stack.imgur.com/UFH2Q.png"" alt=""Ardumower perimeter wire""></a></p>

<p>The wire is all around the yard. on the mower there are 2 sensors, left and right, that sense the signal emitted from the wire, a square wave signal at 34 KHz. The signal amplitude read from the sensors on the mower is about 2 V when it's above the wire.</p>
","line-following magnetometer"
"8187","Is there a simpler way than ROS for 5 DOF Dynamixel arm control","<p>I will have a 5 or 6 DOF arm build with Dynamixel or HerculeX smart servos. I need  to move the gripper along Cartesian trajectory, which I will calculate in my C++ application. I looked at ROS, but the learning curve is pretty steep and it looks like a major overkill for this use case. I don't need a distributed system with all the complexity it brings. Preferably, I would like to call a standalone C++ library or libraries to get the arm actuated. </p>

<p>What are my options? What will be the limitations of not using a full blown robotics framework like ROS or YARP in this case.</p>

<p><strong>EDIT</strong></p>

<p>Here is how I would like to code it:</p>

<pre><code>vector&lt;Point&gt; way_points;
vector&lt;Pose&gt;  way_poses;

compute_Cartesian_trajectory(way_points, way_poses);   // my code
execute_Cartesian_trajectory(way_points, way_poses);   // library call
</code></pre>

<p>The last line can be spread over several library function calls and intermediate data structures, if needed. The end result should be the gripper physically following Cartesian trajectory given by <code>way_points</code> and <code>way_poses</code>.</p>
","robotic-arm ros motion-planning c++"
"8188","How can I avoid Roomba Error 10 Code?","<p>I am trying to run my 600 series Roomba in a large, open space (1700+sf) and it does not recognize the large, open space and throws the Error 10 code.  It does not recognize an edge of 2 12""-3"" either; it will fall off the edge and become stuck.  Any suggestions?</p>
","mobile-robot irobot-create roomba"
"8191","rotation matrix to euler angles with gimbal lock","<p>How do i determine which angle i can negate when gimbal lock occurs. </p>

<p>As i've understood with gimbal lock that it remove one degree of freedom, but how do i determine which degree can be removed when a  value R[1][3]  of a rotation matrix (size 3x3) has the value 1.  Is it the Roll, Pitch or yaw which can be taken out from the equation?</p>
","motion-planning"
"8194","Choosing motors for quadcopter frame","<p>I bought this drone frame : q450 glass fiber quadcopter frame 450mm from <a href=""http://hobbyking.com/hobbyking/store/__49725__Q450_V3_Glass_Fiber_Quadcopter_Frame_450mm_Integrated_PCB_Version.html"" rel=""nofollow"">http://hobbyking.com/hobbyking/store/__49725__Q450_V3_Glass_Fiber_Quadcopter_Frame_450mm_Integrated_PCB_Version.html</a></p>

<p>I'm considering buying 4 AX-4005-650kv Brushless Quadcopter Motor's from <a href=""http://hobbyking.com/hobbyking/store/__17922__AX_4005_650kv_Brushless_Quadcopter_Motor.html"" rel=""nofollow"">http://hobbyking.com/hobbyking/store/__17922__AX_4005_650kv_Brushless_Quadcopter_Motor.html</a></p>

<p>Will these motor's fit this frame ? How can I determine what motor's will fit the frame ?</p>
","quadcopter"
"8196","Camera Calibration fails to run on ROS","<p>I am running ROS Indigo on Ubuntu 14.04. I am doing a mono-camera calibration and trying to follow the <a href=""http://wiki.ros.org/camera_calibration"" rel=""nofollow"">camera calibration tutorial</a> on the ROS Wiki.</p>

<p>I give the following command:</p>

<blockquote>
  <p>rosrun camera_calibration cameracalibrator.py --size 8x6 --square
  0.108 image:=/my_camera/image camera:=/my_camera</p>
</blockquote>

<p>I get the following error:</p>

<blockquote>
  <p>ImportError: numpy.core.multiarray failed to import Traceback (most
  recent call last): File
  ""/opt/ros/indigo/lib/camera_calibration/cameracalibrator.py"", line 47,
  in  import cv2 ImportError: numpy.core.multiarray failed to
  import</p>
</blockquote>

<p>I thought it was to do with updating <code>numpy</code> and did a <code>rosdep</code> update but no difference.</p>

<p>What is a possible way to solve this problem?</p>

<p><strong>UPDATE:</strong>
I uninstalled and reinstalled ROS completely from scratch. I still get the same error. Should I have to look somewhere outside ROS?</p>
","ros cameras calibration"
"8199","Wires or columns which contract on passing electricity","<p>Background: Introductory robotics competition for college freshmen; Bot has to open 8 jars (with two balls in each of them) in ten minutes and load the balls into a shooting mechanism.</p>

<p>So, we were doing this project and we hit upon a challenge that the jar is not opening like we originally intended to. So we decided to get a rack-pinion mechanism and use it for unscrewing the lid. However, it is too large and we are unable to fit the bot in the required dimensions</p>

<p><strong>The actual question:</strong> Are there any wires or rigid columns/things which can contract ~1 cm when electricity is passed through it? And what would their price range be? Our budget is also in the list of constraints for the bot</p>

<p>Edit: We can include a wire of length &lt;1m or a column of length &lt;30 cm. Also, the wire needs to contract only more than 7mm</p>
","mechanism"
"8202","EKF-SLAM: Shrink covariance matrix on one direction","<p>I have implemented an EKF on a mobile robot (x,y,theta coordinates), but now I've a problem.
When I detect a landmark, I would like to correct my estimate only on a defined direction. As an example, if my robot is travelling on the plane, and meets a landmark with orientation 0 degrees, I want to correct the position estimate only on a direction perpendicular to the landmark itself (i.e. 90 degrees).</p>

<p>This is how I'm doing it for the position estimate:</p>

<ul>
<li>I update the x_posterior as in the normal case, and store it in x_temp.</li>
<li>I calculate the error x_temp - x_prior.</li>
<li>I project this error vector on the direction perpendicular to the landmark.</li>
<li>I add this projected quantity to x_prior.</li>
</ul>

<p>This is working quite well, but how can I do the same for the covariance matrix? Basically, I want to shrink the covariance only on the direction perpendicular to the landmark.</p>

<p>Thank you for your help.</p>
","mobile-robot slam kalman-filter ekf"
"8204","velocity of the end-effector","<p>The joint velocities are constant and equal to $\dot{\theta}_{2}$ = 1 and $\dot{\theta}_{1}$ = 1. How to  Compute the
velocity of the end-effector when $\theta_{2} =\pi/2$ and $\theta_{1} = \pi/6$</p>

<p><a href=""http://i.stack.imgur.com/d9IWO.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/d9IWO.png"" alt=""enter image description here""></a></p>
","robotic-arm"
"8207","Quadcopter stability vs (PID error signal lag and sample time)","<p>The question I am asking is that, what is the effect on stability of increasing or decreasing both the sample time and lagging of error signal to PID. Does it helps in stability or degrade it?</p>
","quadcopter pid stability"
"8209","Two DC motors and single output?","<p>I saw one old industrial robot(Year 1988) end effector is having 2 DC motor for roll drive. After roll drive, yaw and pitch drives are connected and it has dc motors separately.</p>

<p>But roll drive has two DC motors. Why are they used like this? why not single with higher torque.</p>

<p>All the roll, pitch and yaw motors are same spec. Total 4 DC motors.</p>

<p>Two DC motor connected to single shaft using gears in roll.</p>
","industrial-robot"
"8212","UWSim Pressure Sensor Units","<p>I am attempting to use the data Underwater Simulator (UWSim) provides through the ROS interface to simulate a number of sensors that will be running on a physical aquatic robot. One of the sensors detects the current depth of the robot so, I want to simulate this with the data provided by the UWSim simulated pressure sensor. The Problem is that nowhere in the UWSim wiki or source code can I find any reference to what units UWSim uses to measure pressure.</p>

<p>That being said, what units does UWSim use to measure pressure? Additionally, I would appreciate general information about what units UWSim uses for the data provided by it's virtual sensors.</p>
","ros simulation underwater"
"8213","Balancing Robot Control Model","<p>I am trying to find a control model for the system of a balancing robot. The purpose of this project is control $\theta_2$ by the 2 motors in the wheels i.e. through the torque $τ$ I started with the dynamic equations and went to find the transfer function. </p>

<p>Then I will find the PID gains that will control the robot and keep it balanced with the most optimum response. For the time being I am only interested in finding the transfer function for the dynamic model only.</p>

<p>Here is an example: <a href=""https://www.youtube.com/watch?v=FDSh_N2yJZk"" rel=""nofollow"">https://www.youtube.com/watch?v=FDSh_N2yJZk</a></p>

<p>However, I am not sure of my result.Here are the free body diagrams for the wheels and the inverted pendulum (robot body) and calculations below: </p>

<p><a href=""http://i.stack.imgur.com/mszPX.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/mszPX.png"" alt=""Diagram""></a></p>

<p>Dynamic Equations:</p>

<p>$$
\begin{array}{lcr}
m_1 \ddot{x}_1 = F_r - F_{12} &amp; \rightarrow &amp; (1)&amp; \\
m_2 \ddot{x}_2 = F_{12} &amp; \rightarrow &amp;  (2) &amp;\\
J_1 \ddot{\theta}_1 = F_r r - \tau &amp; \rightarrow &amp; (3) &amp;\\
J_2 \ddot{\theta}_2 = \tau - mgl\theta &amp; \rightarrow &amp; (4) &amp; \mbox{(linearized pendulum)}\\
\end{array}
$$</p>

<p>Kinematics:</p>

<p>$$
x_1 = r\theta_1 \\
x_2 = r\theta_1 + l\theta_2 \\
$$</p>

<p>Equating (1) and (3):
$$
m_1 \ddot{x}_1 + F_{12} = F_r \\
\frac{J_1 \ddot{\theta}_1}{r} + \frac{\tau}{r} = F_r
$$</p>

<p>Yields:</p>

<p>$$
\frac{J_1 \ddot{\theta}_1}{r} - m_1 \ddot{x}_1 + \frac{\tau}{r} = F_{12} \rightarrow (5)
$$</p>

<p>Equating (5) with (2):</p>

<p>$$
\frac{J_1 \ddot{\theta}_1}{r} - m_1 \ddot{x}_1 + \frac{\tau}{r} - m_2 \ddot{x}_2  = 0 \rightarrow (6) \\
$$</p>

<p>Using Kinematic equations on (6):</p>

<p>$$
(J_1 - m_1 r^2 - m_2 r^2) \ddot{\theta}_1 + m_2 l r \ddot{\theta}_2 = -\tau \rightarrow (7) \\
$$</p>

<p>Equating (7) with (4):</p>

<p>$$
\begin{array}{ccc}
\underbrace{(J_1 - m_1 r^2 - m_2 r^2) }\ddot{\theta}_1 &amp;+&amp; \underbrace{(m_2 l r + J_2 ) }\ddot{\theta}_2 &amp;+&amp; \underbrace{m_2 gl}\theta &amp;= 0 \rightarrow (8) \\
A &amp; &amp;B &amp; &amp; C &amp; \\
\end{array}
$$</p>

<p>Using Laplace transform and finding the transfer function:</p>

<p>$$
\frac{\theta_1}{\theta_2} = -\frac{Bs^2 + C}{As^2} \\
$$</p>

<p>Substituting transfer function into equation (7):</p>

<p>$$
(J_1 - m_1 r^2 - m_2 r^2) \frac{\theta_1}{\theta_2}\theta_2 s^2 + m_2 lr\theta_2 s^2 = -\tau \\
$$</p>

<p>Yields:
$$
\frac{θ_2}{τ} = \frac{-1}{(mlr-B) s^2+C}
$$</p>

<p>Simplifying:
$$
\frac{θ_2}{τ}=  \frac{1}{J_2 s^2-m_2 gl}
$$</p>

<p>Comments:</p>

<p>-This only expresses the pendulum without the wheel i.e. dependent only on the pendulums properties.</p>

<p>-Poles are real and does verify instability.</p>
","arduino control pid wheeled-robot"
"8220","Balancing a plate with an IMU offset from the center","<p>I recently bought a IMU . I am new at this. </p>

<p>My question: Does the positioning of the IMU matter? Are there any differences between placing it at the center of the plate or if it is offset from the center?</p>

<p>I am still learning about this topic. So any help would be greatly appreciated.</p>

<p>Thanks.</p>
","control sensors imu sensor-fusion"
"8222","3D Angular velocity to 3D velocity to predict next state","<p>I have a sensor that gives R, Theta, Phi (Range, Azimuth and Elevation) As such:
<a href=""http://imgur.com/HpSQc50"" rel=""nofollow"">http://imgur.com/HpSQc50</a>
I need to predict the next state of the object given the roll, pitch yaw angular velocities given the above information. But the math is really confusing me.
So far all I've gotten is this:</p>

<pre><code>Xvel = (R * AngularYVel * cos(Theta))
YVel = (R * AngularXVel * cos(Phi))
ZVel = (R * AngularYVel * -sin(Theta)) + (R * AngularXVel * -sin(Phi))
</code></pre>

<p>i worked this out by trigonometry, so far this seems to predict the pitching about the x axis and yawing about my y axis (sorry i have to use camera axis)
But i dont know how to involve the roll (AngularZVel)</p>
","kalman-filter"
"8223","Determining pose from ar_track_alvar message in ROS","<p>I am using the ar_track_alvar package in Indigo to detect AR Tags and determine their respective poses. I am able to run the tracker successfully as I can visualize the markers in RViz. I give the following command to print the pose values</p>

<blockquote>
  <p>rostopic echo /ar_pose_marker</p>
</blockquote>

<p>and I get the following output indicating that the poses are determined.</p>

<blockquote>
<pre><code>header: 
  seq: 0
  stamp: 
    secs: 1444430928
    nsecs: 28760322
  frame_id: /head_camera
id: 3
confidence: 0
pose: 
  header: 
    seq: 0
    stamp: 
      secs: 0
      nsecs: 0
    frame_id: ''
  pose: 
    position: 
      x: 0.196624979223
      y: -0.238047436646
      z: 1.16247606451
    orientation: 
      x: 0.970435431848
      y: 0.00196992162831
      z: -0.126455066154
      w: -0.205573121457
</code></pre>
</blockquote>

<p>Now I want to use these poses in another ROS node and hence I need to subscribe to the appropriate ROS message('ar_pose_marker""). But I am unable to get enough information on the web on the header files and functions to use in order to extract data from the published message. It would be great if somebody can point to a reference implementation or documentation on handling these messages. It might be useful to note that ar_track_alvar is just a ROS wrapper and hence people who have used ALVAR outside of ROSmay also give their inputs.</p>

<p><strong>UPDATE:</strong></p>

<p>I tried to write code for the above task as suggested by @Ben in the comments but I get an error. The code is as follows</p>

<pre><code>#include &lt;ros/ros.h&gt;
#include &lt;ar_track_alvar_msgs/AlvarMarker.h&gt;
#include &lt;tf/tf.h&gt;  
#include &lt;tf/transform_datatypes.h&gt;

void printPose(const ar_track_alvar_msgs::AlvarMarker::ConstPtr&amp; msg)
{   
    tf::Pose marker_pose_in_camera_;    

    marker_pose_in_camera_.setOrigin(tf::Vector3(msg.pose.pose.position.x,
                             msg.pose.pose.position.y,
                             msg.pose.pose.position.z));

}

int main(int argc, char **argv)
{
    ros::init(argc, argv, ""pose_subscriber"");

    ros::NodeHandle nh;

    ros::Subscriber pose_sub = nh.subscribe(""ar_pose_marker"", 1000, printPose);

    ros::spin();

    return 0;

}
</code></pre>

<p>And I get the following error</p>

<pre><code>/home/karthik/ws_ros/src/auto_land/src/pose_subscriber.cpp: In function ‘void printPose(const ConstPtr&amp;)’:
/home/karthik/ws_ros/src/auto_land/src/pose_subscriber.cpp:17:53: error: ‘const ConstPtr’ has no member named ‘pose’
    marker_pose_in_camera_.setOrigin(tf::Vector3(msg.pose.pose));
                                                     ^
make[2]: *** [auto_land/CMakeFiles/pose_subscriber.dir/src/pose_subscriber.cpp.o] Error 1
make[1]: *** [auto_land/CMakeFiles/pose_subscriber.dir/all] Error 2
make: *** [all] Error 2
</code></pre>

<p>Any suggestions?</p>
","ros pose"
"8225","choice for camera sensor to be used with LiDAR","<p>I am doing research on autonomous car and looking for a sensor to be used along with LiDAR laser scanner. Ladybug could be a very good option but the cost!! too expensive. 
Could you please suggest me options for camera sensors with good FOV and which will cost me around $1000. 
Thank you so much!!</p>

<p>-CHIANG CHEN</p>
","computer-vision"
"8226","How would I replicate a tank/zero-turn steering system in a small robotic vehicle?","<p>I'm working on a project that requires me to build a small vehicle (footprint of ~ 14 x 14 inches, less than 6.5 pounds) that can traverse sand. For the steering system, I was thinking of replicating the way tanks and lawn mowers navigate (ability to do zero-point turns), but I want to do this with four wheels instead of tracks like a tank.</p>

<p>I need help with implementing this idea. My preliminary thoughts are to have two motors where each motor power the wheels on one side of the vehicle (I think this would require a gearing system) or to have a motor to power each individual wheel which I'd rather avoid.</p>
","mobile-robot motor wheeled-robot"
"8228","Quad Copter flight module can replace with smart phone?","<p>I want to replace the flight module with smart phone because it has all sensors that are required, like gyroscope, magnetometer, etc. Is that possible?</p>

<p>I am using an Google Nexus 4 Android (OS model 5.1). I will control using another mobile, I am able write an app, with an Arduino acting as a bridge between smartphone and copter. I am using flight controller <a href=""https://www.openpilot.org/product/coptercontrol/"" rel=""nofollow"">OpenPilot CC3D CopterControl</a>.</p>
","arduino quadcopter"
"8229","4dof or 5dof robot arm with stepper motors tool-chain for an hobbyist","<p>In the past I built some simple robot arms at home, using RC servo motors or stepper motors (till 3dof). I would like to build a new arm with 4dof or 5dof with the steppers. Until now I used Arduino and A4988 stepper drivers and Gcode.
For calculating inverse kinematics in real time for a 4dof or 5dof I think the Arduino is not enough powerful. So I'm searching for a new tool-chain Gcode Interpreter + inverse kinematics calculation + stepper controller.
I see LinuxCNC + beaglebone black + cnc cape. Not too expensive for an hobbyist.
But this is the only possibility I found. There are other possibilities for an hobbyist to implement a 4dof or 5dof robot arm working with the stepper motors?</p>
","stepper-motor arm"
"8231","my quadcopter settling time is very large","<p>my quadcopter's settling time is very large, that is it sets its setpoint in very large amount of time, during which it has covered a large distance. But at settle point, when i gives it a jerk or push its returns to settle in normal duration. doesnt over shoots(little). The problem is with the settling time that is when i move the stick front or back it takes huge amount of time. what could be wrong. i have tried giving more P value and I value to PID but then it overshoots and get unstable. This is my PID program. the PID values are given. I read 6 channels from remote using the command pulsein(). which i guess is taking upto 20ms per command.</p>

<pre><code>kp = 1.32;
ki= 0.025;
kd= 0.307;
void PID() {
  error = atan2(lx,ly);
  error *= 1260/22;   
  error = setpoint1 - error;   
  now = millis(); 
  dt = now - ptime;
  ptime = now;
  dt /= 1000;  
  integ = integ + (error * dt);
  der = (error - prerror) / dt;
  pidy = (kp * error); 
  pidy += (ki * integ);
  pidy += (kd * der); 
  //Serial.println(error);
  prerror = error;
}
</code></pre>

<p><code>pidy</code> is added and subtracted to esc speeds respectively.</p>
","arduino quadcopter pid"
"8234","For a quadcopter: Premade flight controller or custom made?","<p>I'm interested in building a quadcopter. The result I'd like to obtain is an autonomous drone. I'd be interested in a GPS to allow it to remain stationary in the air, and also to fly through checkpoints.</p>

<p>Can this be done with a flight controller, or does it need to be programmed? I'm not too sure about what flight controllers really are.</p>

<p>Could someone offer any materials to help me get towards this goal.</p>

<p>Thanks, Jacob</p>
","arduino quadcopter"
"8239","Embedded frame grabber machine vision fusion","<p>Looking to find a solution to save stills from three cameras. Communication protocol can be camera link or Gige, but we are looking for a lightweight solution to save stills (don't require video) as the system will be tested for a multirotor application. Frames will be saved every 3 seconds so we don't require a lot of bandwidth. We don't require to do anything with the frames other than store them.</p>

<p>Thanks</p>
","sensor-fusion uav embedded-systems"
"8241","Linear actuators in a cartesian robots","<p>I would like to make a Cartesian robot with maximum speed of up to $1ms^{-1}$ in x/y plane, acceleration $2ms^{-2}$ and accuracy at least 0.1mm. Expected loads: 3kg on Y axis, 4kg on X axis. Expected reliability: 5000 work hours. From what I have seen in 3D printers, belt drive seems not precise enough (too much backlash), while screw drive is rather too slow. </p>

<p>What other types of linear actuators are available? What is used in commercial grade robots, i.e. <a href=""http://www.janomeie.com/products/desktop_robot/jr-v2000_series/index.html"" rel=""nofollow"">http://www.janomeie.com/products/desktop_robot/jr-v2000_series/index.html</a></p>
","actuator industrial-robot cnc"
"8244","How to connect ethernet based Hokuyo scanner?","<p>This is a very basic beginner question, I know, but I am having trouble connecting to the Hokuyo UST-10LX sensor and haven't really found much in terms of helpful documentation online.</p>

<p>I tried connecting the Hokuyo UST-10LX directly to the ethernet port of a Lubuntu 15.04 machine. The default settings of the Hokuyo UST-10LX are apparently:
ip addr: 192.168.0.10
netmask: 255.255.255.0
gateway: 192.168.0.1</p>

<p>So, I tried going to the network manager and setting IPv4 settings manually, to have the ip addr be 192.168.0.9, netmask of 255.255.255.0, and gateway to 192.168.0.1. I also have a route set up to the settings of the scanner.</p>

<p>I then go into the terminal and run:</p>

<pre><code>rosrun urg_node urg_node _ip_address:=192.168.0.10
</code></pre>

<p>and get this output:</p>

<pre><code>[ERROR] [1444754011.353035050]: [setParam] Failed to contact master at [localhost:11311].  Retrying...
</code></pre>

<p>How might I fix this? I figure it's just a simple misunderstanding on my end, but through all my searching I couldn't find anything to get me up and running :(</p>

<p>Thank you for the help! :)</p>

<p>EDIT:</p>

<p>HighVoltage pointed out to me that I wasn't running <code>roscore</code> which was indeed the case. I was actually running into problems before that when I still had <code>roscore</code> up, and when I tried it again, this was the output of the <code>rosrun</code> command:</p>

<pre><code>[ERROR] [1444828808.364581810]: Error connecting to Hokuyo: Could not open network Hokuyo:
192.168.0.10:10940
could not open ethernet port.
</code></pre>

<p>Thanks again!</p>
","sensors ros rangefinder linux"
"8245","Aligning datasets with drift","<p>I have a dataset that contains position information from tracking a robot in the environment. The position data comes both from a very accurate optical tracking system (Vicon or similar) and an IMU. I need to compare both position data (either integrating the IMU or differentiating the optical tracking data).</p>

<p>The main problem is that both systems have different reference frames, so in order to compare I first need to align both reference frames. I have found several solutions; the general problem of aligning two datasets seems to be called ""the absolute orientation problem"".</p>

<p>My concern is that if I use any of these methods I will get the rotation and translation that aligns both datasets minimizing the error over the whole dataset, which means that it will also compensate up to some extent for the IMU's drift. But I am especially interested in getting a feeling of how much the IMU drifts, so that solution does not seem to be applicable.</p>

<p>Anyone has any pointer on how to solve the absolute orientation problem when you do not want to correct for the drift?</p>

<p>Thanks</p>
","sensors localization imu calibration"
"8247","Angular velocities and rotation matrices","<p>Let us assume I have an object O with axis $x_{O}$, $y_{O}$, $z_{O}$, with different orientation from the global frame S with $x_{S}$, $y_{S}$, $z_{S}$ (I don't care about the position).
Now I know the 3 instantaneous angular velocities of the object O with respect to the same O frame, that is $\omega_O^O = [\omega_{Ox}^O \omega_{Oy}^O \omega_{Oz}^O]$.
How can I obtain this angular velocity with respect to the global frame (that is $\omega_O^S$)?</p>

<p>Thank you!</p>
","mobile-robot imu gyroscope"
"8250","Gyro measurement to absolute angles","<p>Let us assume we have a gyro that is perfectly aligned to a global frame ($X,Y,Z$). </p>

<p>From what I know the gyro data give me the angular rate with respect to the gyro axis ($x,y,z$). So let's say I got $\omega_x,\omega_y,\omega_z$. Since I know that the 2 frames are perfectly aligned I perform the following operations:</p>

<ul>
<li>$\theta_X = dt * \omega_x$</li>
<li>$\theta_Y = dt * \omega_y$</li>
<li>$\theta_Z = dt * \omega_z$ </li>
</ul>

<p>where $\theta_X$ is the rotation angle around $X$ and so on.</p>

<p>My question is: what is this update like in the following steps? Because this time the measurement that I get are no more directly related to the global frame (rotated with respect to the gyro frame).</p>

<p>Thank you!</p>
","imu gyroscope frame"
"8255","Accurate Wheeled Robot Odometry","<p>I'm looking for a ""good"" algorithm/model for wheeled odometry estimation. We have encoders on the two back wheels of the tricycle robot, and IMU on the controller board. Currently we use MEMS gyro for angular velocity estimation and encoders for linear velocity, then we integrate them to get the pose. But it's hard to calibrate gyro properly and it drifts (due to temperature or just imperfect initial calibration). How can we improve the pose estimation? <strong>Should we consider model that incorporates both encoders and gyro for heading estimation? Model slippage, sensor noise? Is there some nice standard model? Or should we just use more/better gyro?</strong> Not considering the visual odometry.</p>
","kalman-filter wheeled-robot odometry"
"8261","Robotic arm [""FAiL""] error display. - Festo / Mitsubishi Melfa RV-2AJ (Controller CR1-571)","<p><em>To avoid wasting your time on this question, you might only want to react on this if you have knowledge of industrial robotic arms specific.
Common troubleshooting is unlikely to fix this problem or could take too much time.</em></p>

<p>We've started a project with the Mitsubishi Melfa RV-2AJ robotic arm.
Everything went fine until the moment we replaced the batteries.</p>

<p>The controller displays: ""FAiL"" and does not respond to any buttons or commands sent through serial connection.</p>

<p>We did replace the batteries of both the robot and the controller. As it took some time to get the batteries delivered, we've left the robot (and controller) withouth power for the weekend. (Which might have caused this problem)</p>

<p>Is there anyone with knowledge of Mitsubishi Robotic arms around here?
I'm kinda hoping it would be a common problem/mistake and anyone with experience on this subject would know about it?</p>
","robotic-arm"
"8262","Real-time video processing on video feed from a drone's camera","<p>I am working on a project where I want to run some computer vision algorithms (e.g. face recognition) on the live video stream coming from a flying drone. </p>

<p>There are many commercial drones out there that offer video streams, like</p>

<ul>
<li><a href=""http://www.flyzano.com/mens/"" rel=""nofollow"">http://www.flyzano.com/mens/</a></li>
<li><a href=""https://www.lily.camera/"" rel=""nofollow"">https://www.lily.camera/</a></li>
<li>etc..</li>
</ul>

<p>But none of them seem to give access to the video feed for real-time processing. </p>

<p>Another idea is to have the drone carry a smartphone, and do the processing on the phone through the phone's camera. Or just use a digital camera and an arduino that are attached to the drone. </p>

<p>Although these ideas are feasible, I would rather access the video-feed of the drone itself. So my question is that are there any drones out there that offer this feature? or can be hacked somehow to achieve this? </p>
","computer-vision cameras"
"8271","DC Motors for a ROV?","<p>I am planning to build a homemade ROV, and I wanted to know a couple of things about the motors.  First is: Will it be Ok, if I use a brushed DC motor,  instead of a brushless motor, and is there any major disadvantages ? Second : What RPM DC motor should I aim for ? High RPM or low RPM ? Will 600rpm be enough ? The specific motor that I am talking about is    <a href=""http://www.ebay.ca/itm/37mm-12V-DC-600RPM-Replacement-Torque-Gear-Box-Motor-New-/320984491847?hash=item4abc2aa747:m:mEBEQXXpqmNg4-vxmFaZP5w"" rel=""nofollow"">http://www.ebay.ca/itm/37mm-12V-DC-600RPM-Replacement-Torque-Gear-Box-Motor-New-/320984491847?hash=item4abc2aa747:m:mEBEQXXpqmNg4-vxmFaZP5w</a></p>

<p>Will this be a good motor for the propellers of the ROV.  I am planning to have 4 motors / propellers. Two for upward and downward thrusting,  and 2 for forward and side thrusting. The propellers that I plan to use, are basic plastic 3 blade propellers,  with diameter,  between 40mm and 50mm. </p>

<p>My main question is, what RPM and torque should I aim for when choosing the DC motor ?</p>
","motor"
"8272","ROS MoveIt!, virtual joints, planar joints, prismatic joints","<p>I do have a robotic application, where a 7Dof robot arm is mounted on a omnidirectional mobile platform. My overall goal is to get MoveIt! to calculate a sequence of joint movements, such that the robot EEF reaches a desired goal in Cartesian space.</p>

<p>In order to combine a robot platform with a world, the MoveIt! setup assistant lets you assign virtual joints between the ""footprint"" of the platform and the world it is placed in.</p>

<p>I do have two strategies. Either </p>

<ul>
<li>select a planar joint as a virtual joint. (What are the degrees of freedom or respectively the joint information that I can gather from this joint)</li>
</ul>

<p>or </p>

<ul>
<li>select a fixed joint and add a (prismatic-x -> prismatic-y -> revolute-z) chain to the robot model.</li>
</ul>

<p>Are there any significant differences (advantages/ disadvantages) to either of the approaches?</p>
","mobile-robot robotic-arm ros motion-planning"
"8273","Where should I put the angle sensor on my cart-pole robot?","<p>I'm using an accelerometer and gyroscope to detect the angle and tilt rate on my two-wheeled cart-pole robot.  Is there an optimal height to place the sensors?  Should I place them closer to the bottom (near the wheels), the middle (near the center of mass), or the top?  Justification for the optimal choice would be appreciated.</p>
","sensors balance"
"8276","Should the therotical parameters match the physical setup constraints when modeling a robot?","<p>I'm working on modeling and simulation of robotic arm, after I obtained the mathematical model of the robot, I used that to implement some control techniques, to control the motion of the robot. The dimensions and masses of each links are taken from available kit, basically, it's <a href=""http://www.imagesco.com/kits/robotic-arm.html"" rel=""nofollow"">RA02</a> robot with servo at each joint. After the modeling, different parameters, can be plotted: like the joint angles\speeds\torques ... etc. The point now is that, the value obtained for the joint toque is much more higher that the torque limit of the servo, does it mean my design\modeling is not realizable? Is it necessarily to get close (torque)  value for servo's torque? </p>

<p>Any suggestion?</p>
","control robotic-arm servomotor dynamics torque"
"8277","What is torque bandwidth in actuated joints and how does it affect the control systems?","<p>The rest of my student team and I are in the process of redesigning an exoskeleton and building it based on an existing one. From the papers that we have been reading there are some references to low, high and zero impedance torque bandwith.</p>

<p>What is that? Does it have to do with the control system?</p>

<p>It is measured in Hz. Here is a table from one of the papers:</p>

<p><a href=""http://i.stack.imgur.com/yqK0d.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/yqK0d.png"" alt=""Torque bandwith""></a></p>
","control design mechanism joint"
"8279","Ways to estimate the drift rate of the gyrometer","<p>I found not so much literature to the topic, this is why I ask here.
Does someone know some ways to estimate the drift rate of the gyrometer.
I was thinking about basically two approaches. 
One would be to use a low pass filter with a low cut-off frequency to estimate the drift of the angular velocity.
Second would be to use the accelerometer, calculate the attitude dcm and by this also the angular velocity. The difference between the acc angular velocity and gyrometer would be maybe also a drift rate.
Nevertheless, I am not so sure whether this is a good way to get reliable drift rates :D</p>
","sensors gyroscope"
"8286","How to connect an Arduino Uno to an Android phone via USB cable?","<p>Is it possible to set up communication between an Arduino Uno and an Android phone using a wire that directly connects the Android phone and the Arduino?</p>
","arduino usb"
"8288","Sourcing motors by physical dimensions","<p>I have a 1inch square tube that I would like to place a motor into. <img src=""http://i.stack.imgur.com/JvPPH.jpg"" alt=""small gear motor inside steel square tube""></p>

<p>The motor I have takes up approximately 1/2 of the available space (roughly 3/4 inch I.D.) I would like to find the largest motor that will fit in the space without having to cobble too much of a housing. </p>

<p>Where/how can i find motors by physical dimensions?</p>
","motor"
"8289","Crock Pot Knob Turner","<p>I have a Crock Pot with an analog knob and would like to find a way to turn the knob by using and appliance timer. I have no idea where to begin. I need help.
Thanks</p>
","control sensors stepper-motor"
"8292","Are there any aerodynamics modeling/simulation software that is capable to consume a SolidWorks model and to interface with MATLAB/Simulink?","<p>Currently I am developing a control system for an aircraft of a unique design (something in between a helicopter and a dirigible). At this moment I can model only the dynamics of this vehicle without any aerodynamic effects taken into account. For this I use the following work-flow:</p>

<p>Mechanical model in SolidWorks -> MSC ADAMS (Dynamics) &lt;--> MATLAB/Simulink (Control algorithms)</p>

<p>Thus, the dynamics of the vehicle is modeled in ADAMS and all control algorithms are in MATLAB/Simulink. Unfortunately, ADAMS can not simulate any aerodynamic effects. As a result, I can not design a control system that is capable to fight even small wind disturbances.</p>
","control simulator uav matlab simulation"
"8293","Advanced Line Following Robot of Maze Solving","<p>I know how to make a line follower. But in <a href=""https://www.youtube.com/watch?v=5At_u5rnh2U"" rel=""nofollow"">this video</a> what have they done exactly? They are giving the source and destination in the map but how the robot moves based on the instruction given in map?</p>

<p>What is the procedure to do it? They have mapped the path. Please do watch the video.</p>
","wheeled-robot mapping line-following"
"8294","Need a mobile robot simulator that provides easier odometry funtions","<p>I want a mobile robot to go from a starting position to a goal position. But, I don't want to calculate the pose from encoders. Instead I want to know if there exist such a simulator that provides pose function that makes the work easier, like go_to(x_coordinate, y_coordinate). That means, the robot will automatically calculate its current position and leads itself to the goal position. </p>
","mobile-robot odometry simulation"
"8296","Using Gazebo installed on same machine in MATLAB","<p>I am planning to use MATLAB and Gazebo for one of my course projects. However all the tutorials I have seen till now use Gazebo by using a virtual machine which has ROS and Gazebo installed. I have already installed ROS and Gazebo on this machine (OS Ubuntu). I also have MATLAB installed on it. Is it possible to use the Gazebo on this machine itself with the MATLAB toolbox? Thanks.</p>
","ros matlab gazebo"
"8302","What is the difference between path planning and motion planning?","<p>What are the main differences between motion planning and path planning?
Imagine that the objective of the algorithm is to find a path between the humanoid soccer playing robot and the ball which should be as short as possible and yet satisfying the specified safety in the path in terms of the distance from the obstacles. </p>

<p>Which is the better terminology? motion planning or path planning?</p>
","mobile-robot motion-planning humanoid"
"8310","What is the required theory behind building a robotic arm?","<p>I am currently planning on building a robotic arm. The arm's specs are as follows: </p>

<ul>
<li>3 'arms' with two servos each (to move the next arm)</li>
<li>single servo clamp</li>
<li>mounted on revolving turntable</li>
<li>turntable rotated by stepper motor</li>
<li>turntable mounted on baseplate by ball bearings to allow rotation</li>
<li>baseplate mounted on caterpillar track chassis</li>
<li>baseplate is smaller in length and width than caterpillar chassis</li>
</ul>

<p>What are the required formulas in determining how much torque each servo must produce, keeping in mind that the arm <em>must</em> be able to lift weights of up to 1 kilogram? Also, considering that the ball bearings will take the load of the arm, how strong does the stepper have to be (just formulas, no answers)?</p>

<p>As far as overall dimensions are concerned, the entire assembly will be <em>roughly</em> 255mm x 205mm x 205mm (l x w x h). I have not finalized arm length, but the aforementioned dimensions give a general estimate as to the size. </p>
","arduino robotic-arm stepper-motor rcservo torque"
"8313","Is modelling a robot and deriving its Equations of Motions more applicable to a system that is inherently unstable?","<p>As someone who is new and is still learning about robotics, I hope you can help me out.</p>

<p>Let's say I have two systems: </p>

<ul>
<li>(a) Inverted Pendulum (unstable system)</li>
<li>(b) Pole Climbing Robot (stable system)</li>
</ul>

<p>For system (a), I would say that generally, it is a more dynamic system that produces fast motion. So, in order to effectively control it, I would have to derive the Equations of Motions (EOM) and only then I can supply the sufficient input to achieve the desired output. Eventually, the program will implement the EOM which enables the microcontroller to produce the right signal to get the desired output.</p>

<p>However for system (b), I assume that it is a stable system. Instead of deriving the EOM, why cant I just rely on the sensor to determine whether the output produced is exactly what I want to achieve? </p>

<p>For unstable system, controlling it is just difficult and moreover, it does not tolerate erratic behavior well. The system will get damaged, as a consequence. </p>

<p>On the contrary, stable system is more tolerant towards unpredictable behavior since it is in fact stable.</p>

<p>Am I right to think about it from this perspective? What exactly is the need for deriving the EOM of systems (a) and (b) above? What are the advantages?  How does it affect the programming of such systems?</p>

<p><strong>Edited:</strong>
Some examples of the climbing robot that I'm talking about: </p>

<ol>
<li><p><a href=""http://i.ytimg.com/vi/gf7hIBl5M2U/hqdefault.jpg"" rel=""nofollow"">i.ytimg.com/vi/gf7hIBl5M2U/hqdefault.jpg</a></p></li>
<li><p><a href=""http://ece.ubc.ca/~baghani/Academics/Project_Photos/UTPCR.jpg"" rel=""nofollow"">ece.ubc.ca/~baghani/Academics/Project_Photos/UTPCR.jpg</a></p></li>
</ol>
","mobile-robot control"
"8319","Filtering angular velocity spikes of a cheap Gyroscope","<p>I would like to filter angular velocity data from a ""cheap"" gyroscope (60$). These values are used as an input of a nonlinear controller in a quadcopter application. I am not interested in removing the bias from the readings.</p>

<p><strong>Edit</strong>:
I'm using a
 l2g4200d gyroscope connected via i2c with an Arduino uno. The following samples are acquired with the arduino, sent via serial and plotted using matlab.</p>

<p>When the sensor is steady, the plot shows several undesired spikes.</p>

<p><a href=""http://i.stack.imgur.com/v2A6F.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/v2A6F.png"" alt=""enter image description here""></a></p>

<p><strong>How can I filter these spikes?</strong></p>

<p><strong>1st approach:</strong> Spikes are attenuated but still present...</p>

<p>Let's consider the following samples in which a couple of fast rotations are performed. Let's assume that the frequency components of the ""fast movement"" are the ones I will deal with in the final application.</p>

<p><a href=""http://i.stack.imgur.com/8LvBD.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/8LvBD.png"" alt=""enter image description here""></a></p>

<p>Below, the discrete Fourier transform of the signal in a normalized frequency scale and the second order ButterWorth low pass filter.</p>

<p><a href=""http://i.stack.imgur.com/SDYpK.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/SDYpK.png"" alt=""enter image description here""></a></p>

<p>With this filter, the main components of the signal are preserved. </p>

<p><a href=""http://i.stack.imgur.com/Xg5Ft.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/Xg5Ft.png"" alt=""enter image description here""></a></p>

<p>Although the undesired spikes are attenuated by a factor of three the plot shows a slight phase shift...</p>

<p><a href=""http://i.stack.imgur.com/lh0cd.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/lh0cd.png"" alt=""enter image description here""></a></p>

<p>And the spikes are still present. How can I improve this result?
Thanks.</p>

<p><strong>EDIT 2:</strong></p>

<p>1./2. I am using a breakout board from Sparkfun. You can find the circuit with the Arduino and the gyro in this post: <a href=""http://userk.co.uk/gyroscope-arduino/"" rel=""nofollow"">Can you roll with a L3G4200D gyroscope, Arduino and Matlab?</a> 
    I have added pullup resistors to the circuit. I would exclude this option because other sensors are connected via the i2c interface and they are working correctly.
    I haven't any decoupling capacitors installed near the integrated circuit of the gyro. The breakout board I'm using has them (0.1 uF). Please check the left side of the schematic below, <strong>maybe I am wrong</strong>.</p>

<p><a href=""http://i.stack.imgur.com/4E2KK.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/4E2KK.png"" alt=""enter image description here""></a></p>

<p>Motors have a separate circuit and I have soldered all the components on a protoboard.</p>

<ol start=""4"">
<li><p>The gyro is in the quadcopter body but during the test the motors were turned off.</p></li>
<li><p>That is interesting. The sampling frequency used in the test was 200Hz.  Increasing the update freq from 200 to 400 hz doubled the glitching.</p></li>
</ol>

<p>I found other comments on the web about the same breakout board and topic. Open the comments at the bottom of the <a href=""http://forum.bildr.org/viewtopic.php?t=443"" rel=""nofollow"">page</a> and Ctrl-F <code>virtual1</code></p>
","quadcopter gyroscope filter"
"8322","Need an idea: automated sim card switcher","<p>First off, sorry if my question is too naive or not related to the forum (this is the best matching one I've found on StackExchange).</p>

<p>I have some amount of SIM-cards. I can programmatically access a single SIM-card if it is inserted into a USB-modem. I want to be able to access the specified card in the set. The best way to achieve this I can think of is to create a device that would somehow replace the current card in the modem with one in the set. I can not use several modems for this because I don't really know the amount of cards and I would like to automate this process anyway.</p>

<p>I am more of a programmer than an engineer so everything that follows (including the entire concept of switching cards) looks pretty weird to me. There probably is a better solution, but this is the best I've come up with. For now I consider building some sort of conveyor that would move cards and insert the ones I need with some sort of a feed device. This looks like an overkill to me that would be both expensive to build and uneffective to work with.</p>

<p>I want an idea of a device that would replace SIM-cards into the modem (or maybe a better solution to the problem). Any disassembly of a modem needed is possible.</p>

<p>This is required to automate receiving SMS from clients that have different contact phones. Unfortunately, a simple redirection of SMS is not an option.</p>
","automatic automation"
"8327","Transforming angular velocity?","<p>I have the following system here:</p>

<p><a href=""http://imgur.com/UTqswOi"" rel=""nofollow"">http://imgur.com/UTqswOi</a>
<a href=""http://i.stack.imgur.com/mQPXP.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/mQPXP.png"" alt=""enter image description here""></a></p>

<p>Basically, I have a range finder which gives me $R_s$ in this 2D model. I also have the model rotate about the Centre of Mass, where I have angular values and velocities Beta ($\beta$) and BetaDot ($\dot{\beta}$).</p>

<p>I can't see, for the life of me, how to figure the formula for the angular velocity in the Range Finder frame. How am I supposed to do this? I have all the values listed in those variables. The object there doesn't move when the vehicle/system pitches. It's stationary.</p>
","robotic-arm sensor-fusion"
"8329","Filtering IMU angle discontinuities","<p>I try to measure Euler angles from an IMU, but some discontinuities happens during measurement, even in vibrationless environment, as shown in the images below. </p>

<p>Can someone explain which type of filter will be the best choice to filter this type discontinuities?</p>

<p><a href=""http://i.stack.imgur.com/gHMgg.jpg"" rel=""nofollow""><img src=""http://i.stack.imgur.com/gHMgg.jpg"" alt=""Discontinuities in Euler angle measurement""></a></p>

<p><a href=""http://i.stack.imgur.com/FBnSc.jpg"" rel=""nofollow""><img src=""http://i.stack.imgur.com/FBnSc.jpg"" alt=""enter image description here""></a>
<a href=""http://i.stack.imgur.com/bvIZa.jpg"" rel=""nofollow""><img src=""http://i.stack.imgur.com/bvIZa.jpg"" alt=""enter image description here""></a></p>
","sensors imu matlab noise filter"
"8331","Designing a 5 bar linkage robot: Plot Clock","<p>I am a beginner at robotics. 
I recently stumbled across this <a href=""https://www.youtube.com/watch?v=iOLFP90DneY"" rel=""nofollow"">robotic clock on youtube</a>.</p>

<p>I am an electrical engineering student and am interested in submitting it as my minor project.</p>

<p>I have studied the basics on forward and inverse kinematics, Greubler's Equation, four bar linkage but this robot seems to be a 5 bar linkage. I want to know how to implement it in a 5 bar linkage.</p>

<p>How to use the inverse kinematics solutions described in <a href=""http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.456.7665&amp;rep=rep1&amp;type=pdf"" rel=""nofollow"">Combined synthesis of five-bar linkages and non-circular gears for precise path
generation</a>, to make the robot follow desired trajectory?</p>

<p>I have been stuck at this for days... any sort of help would be appreciated.</p>
","robotic-arm beginner first-robotics"
"8333","Relation between pole placement and marginal stability?","<p>I'm given an assignment in which I have to design a full state feedback controller by pole placement. The state space system is fully controllable and I've been using Matlab/Simulink to determine the required feedback gain K using the place() command for several sets of poles, however once I use poles that are ""too negative"", for example p=[-100,-200,-300,-400,-500], my controlled system starts showing bounded oscillatory behaviour. </p>

<p>Is it possible that too negative poles can cause marginal stability? And if so, why? I've read that this is only possible when the real part of one or more poles equals 0, which certainly isn't the case here. </p>
","control stability"
"8337","KUKA FRI program using JAVA","<p>I am trying to establish the FRI connection for KUKA LBR iiwa. I know how to configure the FRI connection as there are example programs available in the Sunrise.Workbench. </p>

<p>A sample code is given below. My question is 'how to pass' the joint torque values (or joint position or wrench) to the controller using 'torqueOverlay' as mentioned in the code below. Since I could not find any documentation on this, it was quite difficult to figure out. Any sample code with explanation or any clues would be more than helpful.   </p>

<p>JAVA code:</p>

<pre><code>package com.kuka.connectivity.fri.example;

import static com.kuka.roboticsAPI.motionModel.BasicMotions.ptp;

import java.util.concurrent.TimeUnit;
import java.util.concurrent.TimeoutException;

import com.kuka.connectivity.fri.ClientCommandMode;
import com.kuka.connectivity.fri.FRIConfiguration;
import com.kuka.connectivity.fri.FRIJointOverlay;
import com.kuka.connectivity.fri.FRISession;
import com.kuka.roboticsAPI.applicationModel.RoboticsAPIApplication;
import com.kuka.roboticsAPI.controllerModel.Controller;
import com.kuka.roboticsAPI.deviceModel.LBR;
import com.kuka.roboticsAPI.motionModel.PositionHold;
import com.kuka.roboticsAPI.motionModel.controlModeModel.JointImpedanceControlMode;

/**
 * Moves the LBR in a start position, creates an FRI-Session and executes a
 * PositionHold motion with FRI overlay. During this motion joint angles and
 * joint torques can be additionally commanded via FRI.
 */
public class LBRTorqueSineOverlay extends RoboticsAPIApplication
{
    private Controller _lbrController;
    private LBR _lbr;
    private String _clientName;

    @Override
    public void initialize()
    {
        _lbrController = (Controller) getContext().getControllers().toArray()[0];
        _lbr = (LBR) _lbrController.getDevices().toArray()[0];
        // **********************************************************************
        // *** change next line to the FRIClient's IP address                 ***
        // **********************************************************************
        _clientName = ""127.0.0.1"";
    }

    @Override
    public void run()
    {
        // configure and start FRI session
        FRIConfiguration friConfiguration = FRIConfiguration.createRemoteConfiguration(_lbr, _clientName);
        // for torque mode, there has to be a command value at least all 5ms
        friConfiguration.setSendPeriodMilliSec(5);
        friConfiguration.setReceiveMultiplier(1);

        getLogger().info(""Creating FRI connection to "" + friConfiguration.getHostName());
        getLogger().info(""SendPeriod: "" + friConfiguration.getSendPeriodMilliSec() + ""ms |""
                + "" ReceiveMultiplier: "" + friConfiguration.getReceiveMultiplier());

        FRISession friSession = new FRISession(friConfiguration);
        FRIJointOverlay torqueOverlay = new FRIJointOverlay(friSession, ClientCommandMode.TORQUE);

        // wait until FRI session is ready to switch to command mode
        try
        {
            friSession.await(10, TimeUnit.SECONDS);
        }
        catch (final TimeoutException e)
        {
            getLogger().error(e.getLocalizedMessage());
            friSession.close();
            return;
        }

        getLogger().info(""FRI connection established."");

        // move to start pose
        _lbr.move(ptp(Math.toRadians(90), Math.toRadians(-60), .0, Math.toRadians(60), .0, Math.toRadians(-60), .0));

        // start PositionHold with overlay
        JointImpedanceControlMode ctrMode = new JointImpedanceControlMode(200, 200, 200, 200, 200, 200, 200);
        PositionHold posHold = new PositionHold(ctrMode, 20, TimeUnit.SECONDS);

        _lbr.move(posHold.addMotionOverlay(torqueOverlay));

        // done
        friSession.close();
    }

    /**
     * main.
     * 
     * @param args
     *            args
     */
    public static void main(final String[] args)
    {
        final LBRTorqueSineOverlay app = new LBRTorqueSineOverlay();
        app.runApplication();
    }

}
</code></pre>
","robotic-arm programming-languages"
"8340","Trying to calculate the Thrust of my quadcopter motors","<p>I am trying to Calculate the thrust my 4 quadcopter motors will have.
I am not sure how to do it. Here are the parts I am Using</p>

<p>4S 6600mAh 14.8V Lipo Pack </p>

<p>15x5.5 Prop</p>

<p>274KV motor max output is 28A</p>

<p>ESC 35 Amp</p>

<p>Thank You</p>
","quadcopter"
"8343","How much of a pause should there be between messages? (IRobot Create-2)","<p>When I send several commands in a row some don't get executed. For example I have a script which starts the roomba driving in a circle and plays the john cena theme song through its speakers but sometimes it will only play the music and not drive. I have noticed that in all the guides there are pauses after every command. Is there any documentation which describes when pauses are needed?</p>
","irobot-create"
"8345","Trying to Figure out what parts to buy for my Quadcopter","<p>So I am building a Quadcopter and I already have the frame which is about 39 inches(3.25 feet) and 680 grams. I want to run these Multistar Elite 5010-274KV Multi-Rotor Motor. </p>

<ul>
<li>KV(RPM/V): 274KV</li>
<li>Lipo cells:  6~8S  </li>
<li>Max current: 630W</li>
<li>Max Amps:  28A</li>
<li>No Load Current: 0.43A/10V</li>
<li>Weight: 211g</li>
</ul>

<p>I want use a Multistar High Capacity 4S 6600mAh Multi-Rotor Lipo Pack</p>

<ul>
<li>Minimum Capacity: 6600mAh</li>
<li>Configuration: 4S1P / 14.8V / 4Cell</li>
<li>Constant Discharge: 10C</li>
<li>Peak Discharge (10sec): 20C</li>
<li>Pack Weight: 537g</li>
</ul>

<p>The props I am thinking of using are 15x5.5props. So if anyone knows what is wrong with this setup, or if it will work, or maybe even have some tips and pointers for me since I am a beginner drone builder and I really don't want to spend the $1,044 it will take to build it and have it not work so any advice will be appreciated.</p>

<p><strong>Total Estimated Weight = 1,919 Grams(4.23 Pounds)</strong></p>

<p>Also here is a chart for the motor that my be helpful. The chart is near the price under the files tab</p>

<p><a href=""http://www.hobbyking.com/hobbyking/store/__74762__Multistar_Elite_5010_274KV_Multi_Rotor_Motor.html"" rel=""nofollow"">Multistar Elite 5010-274KV Multi-Rotor Motor</a></p>

<p><strong>Thanks In Advance</strong></p>
","battery multi-rotor"
"8348","Changing STM32 Nucleo Board's Microcontroller","<p>I have a STM32F072RB Nucleo Board which has a 64Pin Microcontroller.</p>

<p>For my application I chose the sTM32F103RG which has a bigger RAM size and Flash size too.</p>

<p>Can i Remove an F072R from a Nucleo board put a F103R on top of it?</p>

<p>I am testing my code with a F103C, but the flash and ram size is not meeting my requirement. I have a F072R Nucleo Board lying around so for a quick developmental test could I swap it for the 103R ? The R series is Pin Compatible!</p>

<p>Anyone Has done microcontroller swapping before? </p>
","microcontroller"
"8350","differences between SCARA arm design","<p>I am currently interested in SCARA arm designs and I have few, beginner questions for which I didn't find answers yet. </p>

<p>1/ While comparing professional arms (made by epson, staubli...) I noticed that the actuator used for the translation on the Z axis is at the end of the arm. On ""hobby"" arms like the makerarm project on kickstarter they use a leadscrew with the actuator at the beginning of the arm.
I thought it was smarter to put the actuator handling this DoF at the begining of the arm (because of its weight) and not at the end, but I assume that these companies have more experience than the company behind the makerarm. So I'm probably wrong, but I would like to understand why :)</p>

<p>2/ Also I would like to understand what kind of actuators are used in these arms. The flx.arm (also a kickstarter project) seems to be using stepper motors but they also say they are using closed loop control, so they added an encoder on the stepper motors right?</p>

<p>Wouldn't it be better to not use stepper and, for instance, use DC brushless motors or servos instead ?</p>

<p>3/ I also saw some of these arms using belts for the 2nd Z axis rotation, what is the advantage ? it only allows to put the actuator at the begining of the arm ?</p>
","robotic-arm design actuator"
"8354","Need help for a quadcopter PID","<p>I'm trying to make a quadcopter with Arduino.
I already have the angles (roll pitch and yaw) thanks to an IMU. They are in degrees and filtered with a complementary filter.
I want to apply a PID algorithm for each axis but I dont know if the inputs should be angles (degrees) or angular velocities in degrees per second so as to calculate the errors with respect referencies.
Which will be the difference? Which will be the best way? </p>

<p>Finally, another question about a PID code: I have seen that many people don't include time in their codes. For example, their derivative term is kd×(last error-actual error) instead kd×(last error-actual error)/looptime and something similar with the integrative term. Which is the difference?</p>

<p>Thank you in advanced.</p>
","quadcopter pid"
"8356","Very low output voltage at the output of L298n?","<p>I am using arduino and L298n motor driving IC to drive 4 12V dc motors (150rpm).
Also I am using 11.1V LiPo battery (3cell, 3300 mAh, 20C).I have connected two PWM pins of L298n to digital HIGH from arduino.Battery positive terminal is connected to the 12V input of IC.Battey negative terminal and<a href=""http://i.stack.imgur.com/efuZt.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/efuZt.png"" alt=""enter image description here""></a> arduino ground is connected to the ground input of IC.Also a 5V input is given from arduino to IC and ground from arduino is connected to other gnd pin which is adjacent to INT3 pin.Motor1 pins from L298n are connected with two motors (connected parallely on right side of bot) and Motor2 pins are connected with other two motors (connected parallely on left side of bot).Appropriate inputs are given to INT1,INT2,INT3,INT4 to drive the bot in forward direction.<strong>But the bot is moving too slowly</strong>.The voltage measured across motor1 pins is only 5V.I have connected the battery directly to the motors,then it is running very fast.<strong>How to run it fast</strong>.Please help.....</p>

<p><a href=""http://i.stack.imgur.com/1HtwM.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/1HtwM.png"" alt=""""></a></p>
","arduino motor"
"8357","How to drive robot without driving actuator?","<p>I am participating in a robotics competition. I am supposed to design and build two robots. Out of these, one <strong>cannot have a driving actuator</strong> (it can have a steering actuator though, fed by a line following circuit). The other is supposed to drive the non-driving robot through an obstacle course, <em>without touching it</em>. This is kind of driving me crazy since at one point the separation between the two robots is 60 cm (23 inches).</p>

<p>Ways I've considered:</p>

<ol>
<li>Wind Energy (wont work, need huge sails)</li>
<li>Magnetic Repulsion of some sort</li>
</ol>

<p>Now repulsion I've spent a lot of time studying. </p>

<p>My solution was to use strong permanent magnets on the non-driving robot (Neodymium,N52) and electromagnets on the driving robot.</p>

<p>But, after doing a huge load of calculations came to the conclusion that not enough force can be transmitted over the distance as magnetic fields fall off too quick.</p>

<p>Rulebook: <a href=""http://ultimatist.com/video/Rulebook2016_Final_website_1_Sep_15.zip"" rel=""nofollow"">http://ultimatist.com/video/Rulebook2016_Final_website_1_Sep_15.zip</a></p>

<p>I am really looking for even a pointer here. Is there a trick somewhere that I am missing?</p>
","line-following"
"8361","6DOF robot arm: Velocity of end effector vs. joint velocities","<p>I have a 6 DOF arm whose velocities I'm controlling as a function of force applied to the end effector. The software for the robot allows me to input either the desired end effector velocity or the desired joint angular velocities, which I know can be found using the inverse Jacobian.</p>

<p>Are there any benefits of using one scheme over the other? Would, for example, one help avoid singularities better? Does one lead to more accurate control than the other does?  </p>
","robotic-arm jacobian force-sensor"
"8363","Cannot command irobot create 2","<p>This might be a dumb question. I have started to play with this robot with raspberry pi two days ago. I did some simple stuff, like- move around and sensor reading etc. But since yesterday night, It seems like I cannot send any command. The built in clean, dock functions are working perfectly but I cannot do anything using the same python code that I already used before. Its behaving like nothing is going through the Rx.</p>

<p>Can you suggest what might go wrong? Thanks</p>
","irobot-create"
"8365","using a device with os instead of microcontrollers","<p>Im working on a robot that needs image processing to analyze data recieves from cameras. </p>

<p>As i searched for ARM and AVR libraries i found that there is no dip library for these micros and their limited ram is hard for image data process. i want to know is there any hardware that connects to a win or android or... devices and make it possible to that device connect to actuators and sensors?</p>

<p>thank you or helping.</p>
","microcontroller"
"8367","ArDrone navdata reading error","<p>I am trying to read navdata from ardrone using following callback function,</p>

<pre><code>int     state, battery;

void callBack(const ardrone_autonomy::Navdata::ConstPtr&amp; msg) {
state = msg-&gt;state;
battery = msg-&gt;batteryPercent;

std::cout&lt;&lt;""State is: "" &lt;&lt; state &lt;&lt; std::endl;
std::cout&lt;&lt;""battery is: "" &lt;&lt; battery;
}
</code></pre>

<p>But this is always returing 0 value, but when i echo navdata in linux terminal, these topics are publishing non-zero value. where is the error?</p>

<p>Edit: After some debugging i found that this callBack function is not being called at all. I wrote my subscriber node like this ros::Subscriber sub = ardrone.subscribe(""/ardrone/navdata"",1,callBack); What is going wrong here?</p>
","quadcopter"
"8368","Solving for DH Parameters","<p>Given three sets of joint angles in which the end effector is in the same position, is it possible to find the DH parameters?</p>

<p>If the robot has 2 DOF in shoulder, 2 DOF in elbow, and 1 dof in wrist, with DH parameters as upper arm length, elbow offset in 1 axis, lower arm length, can this be solved, if so how?</p>

<p>I tried iterating through DH parameters to minimize position of end effector with forward kinematics, but this doesnt seem to work as DH parameters of 0 for everything makes 0 minimal distance.</p>

<p>Reason for this; given a physical robot, no DH parameters are known, and measuring by hand is not accurate.</p>
","dh-parameters"
"8369","Power on iRobot Create 2 via serial port","<p>I have Arduino talk with Create 2 via serial interface. But before sending commands to the robot, I have to power it on by manually pushing the power button on the robot. How to make the robot turned on via that mini din 7 port, instead of pushing that power button? 
I notice when plugin iRobot serial-2-USB cable into that port on the robot, the robot is immediately turned on, ready for received the first command (command 128), so apparently there is way to turn on the robot via that port.</p>
","irobot-create"
"8371","Switching scheme for vector controlled pmac drive","<p>I have 28  pmac motors (3-ph, 230 volt of 0.5 kw, 1.05kw an 1.21kw) in motor control center. Please suggest a time staggered switching scheme in order to avoid tripping due to voltage sag, swell , flicker etc</p>
","power"
"8372","Denavit Hartenberg parameters","<p>Can anybody help figure out HD parameters for the case where two links with a revolute joint are in the same plane, thus that the variable angle is 0, but the twist is not 0. This is a simple drawing. I think that x-axis that is perpendicular to both z-axis, points away and goes through the intercection of z-axis. The link length is 0, the twist is a and the offset is d. Whould it be correct?
Thanks.
<a href=""http://i.stack.imgur.com/8QJXS.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/8QJXS.png"" alt=""enter image description here""></a></p>

<p><a href=""http://i.stack.imgur.com/igWtY.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/igWtY.png"" alt=""enter image description here""></a></p>
","dh-parameters"
"8375","can we detects animals through PIR(passive infrared sensors)","<p>as i dont know that what kind of radiation animals emit. as humans emit IR radiations so PIR sensors help to identify humans. pls suggest me if someone have knowledge about sensors which detects animals.</p>
","arduino raspberry-pi first-robotics"
"8376","kk2.1.5 gyro bubble is not at centre","<p>I have a quadcopter controlled through KK2.1.5 flight controller. I have been flying with it without problems, but now i am facing a problem. When i start and arm the kk2.1.5, by giving throttle it starts turning towards some direction with acceleration. I double checked the motor pins locations and all other things, they are correct. When i took a look to the gyro bubble of kk2.1.5 it wasnt at mid of the crosshair. I turned the quad off and then on. I check bubble again it was at centre. now again when i gave it throttle it started turning towards some direction. I checked the again, and it wasnt on centre this time too. So at the armed state the gyro bubble moves away from centre by giving throttle. due to which quad overcorrects itself. Now i have understood that the gyro is off centre due to vibration  of the FC. What should i do to antivibrate it. What material should i keep in between so that vibrations are almost zero.</p>
","quadcopter"
"8378","Torque control in eye-in-hand visual servoing","<p>In most papers about IBVS the camera velocity is computed and then used as a pseudo-input for the manipulator. (e.g. <a href=""http://www.irisa.fr/lagadic/pdf/2006_ieee_ram_chaumette.pdf"" rel=""nofollow"">this one</a>) Is there any work in which the dynamic Lagrange model $H(q) \ddot q +C(q,\dot q)\dot q+g(q)=\tau$ of the manipulator is taken into consideration in order to compute the torque required to move the joints accordingly?</p>
","dynamics torque visual-servoing"
"8382","How to make a stepper motor to rotate and come to a position of certain degress (say for 90 degrees) from any initial position?","<p>I tried this coding and its working. </p>

<pre><code>void loop()
{
    int y = 104;
    int x2 = vertical2.currentPosition();
    int z2 = y-x2;
    int x1 = horizontal2.currentPosition();
    int z1 = y-x1;

    horizontal2.moveTo(z1);
    horizontal2.run();

    vertical2.moveTo(z2);
    vertical2.run();
}
</code></pre>

<p>But the problem is that if the above coding is placed inside a loop such as 'if loop', its not working. Can anyone help me in figuring out this problem. I am using accelStepper library for the above coding.</p>

<pre><code>void loop()
{
    int dummy=1;
    if(dummy==1)
    {
        int y = 104;
        int x2 = vertical2.currentPosition();
        int z2 = y-x2;
        int x1 = horizontal2.currentPosition();
        int z1 = y-x1;

        horizontal2.moveTo(z1);
        horizontal2.run();

        vertical2.moveTo(z2);
        vertical2.run();
    }
}
</code></pre>
","arduino stepper-motor"
"8387","Optimal trajectory for manipulators using optimal control","<p>I'm trying to implement direct-multiple shooting method to my problem.</p>

<pre><code>Objective function: tf
constraints       : q&lt;q_max
                    v&lt;v_max (v=dq/dt)
                    a&lt;a_max
                    tau&lt;tau_max (tau=M(q)a+B(q,v)+G(q))
                    C(q)=r_0-|P-P_0|  (obstacle avoidance)

Initial condition q(0)=q_0 (q_0 is given) 
                  q(t_f)=q_f (q_f is given) and 
                  v(0)= 0
                  v(t_f)=0
</code></pre>

<p>As I understand from the theory, I have to divide the variables as state variables and control variables.</p>

<pre><code>State variables are: q and v 
Control variable is: tau
In each time interval I'll generate cubic splines which are q(t)=a_0+a_1*t+a_2*t^2+a_3*t^3
</code></pre>

<p>Could you help me how I will implement it? I don't understand what is the ODE here and how I should construct the algorithm?</p>

<p>Are there any example about it?</p>

<p>edit to make the equations clear I'll rewrite them here again:</p>

<p>based on the link</p>

<p>state variables:
<code>x1(t) = (q1(t) , ··· ,qn(t))^T</code> and <code>x2(t) = (q˙1(t) , ··· ,q˙n(t))^T</code>.  and derivatives of the state variables are equal to <code>x˙(t) = f(x(t) ,u(t))</code> where f is
<code>f(x(t), u(t)) = ((q˙1(t), . . . , q˙5(t))^T;
                  M(x(t))−1· (u(t) − N(x(t)))</code> </p>

<p>I don't know how to insert cubic polynomials in that equation system and how to solve ODE Will it be like <code>[T,X]=ode45('f', [0 t_f], [q_0 q_f])</code> </p>
","manipulator"
"8389","3 DOF Inverse Kinematics Implementation: What's wrong with my code?","<p>I am currently trying to implement an Inverse Kinematics solver for Baxter's arm using only 3 pitch DOF (that is why the yGoal value is redundant, that is the axis of revolution). I for the most part copied the slide pseudocode at page 26 of <a href=""http://graphics.cs.cmu.edu/nsp/course/15-464/Fall09/handouts/IK.pdf"" rel=""nofollow"">http://graphics.cs.cmu.edu/nsp/course/15-464/Fall09/handouts/IK.pdf</a> .</p>

<pre><code>def sendArm(xGoal, yGoal, zGoal):
    invJacob = np.matrix([[3.615, 0, 14.0029], [-2.9082, 0, -16.32], [-3.4001, 0, -17.34]])
    ycurrent = 0
    while xcurrent != xGoal:
        theta1 = left.joint_angle(lj[1])
        theta2 = left.joint_angle(lj[3])
        theta3 = left.joint_angle(lj[5])
        xcurrent, zcurrent = forwardKinematics(theta1, theta2, theta3)
        xIncrement = xGoal - xcurrent
        zIncrement = zGoal - zCurrent
        increMatrix = np.matrix([[xIncrement], [0], [zIncrement]])
        change = np.dot(invJacob, increMatrix)
        left.set_joint_positions({lj[1]: currentPosition + change.index(0)/10}) #First pitch joint
        left.set_joint_positions({lj[3]: currentPosition + change.index(1)/10}) #Second pitch
        left.set_joint_positions({lj[5]: currentPosition + change.index(2)/10}) #Third Pitch joint


def forwardKinematics(theta1, theta2, theta3):
    xcurrent = 370.8 * sine(theta1) + 374 * sine(theta1+theta2) + 229 * sine(theta1+theta2+theta3)
    zcurrent = 370.8 * cos(theta1) + 374 * cos(theta1+theta2) + 229 * cos(theta1+theta2+theta3)         
    return xcurrent, zcurrent
</code></pre>

<p>Here is my logic in writing this:
I first calculated the Jacobian 3x3 matrix by taking the derivative of each equation seen in the forwardKinematics method, arriving at:</p>

<p>[370cos(theta1) + 374cos(theta1+theta2) .....   </p>

<p>0                                               0                      0</p>

<p>-370sin(theta1)-374sin(theta1+theta2)-......                            ]</p>

<p>In order to arrive at numerical values, I inputted a delta theta change for theta1,2 and 3 of 0.1 radians. I arrived at a Jacobian of numbers:</p>

<p>[0.954  0.586   .219</p>

<p>0.0000          0.000         0.0000</p>

<p>-.178   -.142   -0.0678]</p>

<p>I then input this matrix into a pseudoinverse solver, and came up with the values you see in the invJacob matrix in the code I posted. I then multiplied this by the difference between the goal and where the end effector is currently at. I then applied a tenth of this value into each of the joints, to make small steps toward the goal. However, this just goes into an infinite loop and my numbers are way off what they should be. Where did I go wrong? Is a complete rewrite of this implementation necessary? Thank you for all your help.</p>
","inverse-kinematics python joint jacobian"
"8391","What is the easiest and efficient way to detect human in close range distance and make the robot follow it?","<p>I am having a thesis right now regarding a robot. My research requires the robot to be attached to linear guide rail. A robot has to detect human in a very close range (of about 2 meters distance). What easiest and efficient method or components shall I use?</p>
","sensors wheeled-robot industrial-robot"
"8394","connecting MPU-9250 GY-9250 SENSOR MODULE to arduino uno","<p>i am using this sensor to make self balancing robot.At first i have soldered the header(only to vcc,gnd,scl,sda ) on the imu borad at the opposite side where there is no component mounted.then connecting it to arduino uno r3(vcc to vcc 3.3v/5v,gnd to 1 of 3 gnd,scl to scl and sda to sda(first time at those next to AREF, second time A5,A4) ) i uploaded the sketch <a href=""https://github.com/adafruit/Adafruit_ADXL345/blob/master/examples/sensortest/sensortest.pde"" rel=""nofollow"">https://github.com/adafruit/Adafruit_ADXL345/blob/master/examples/sensortest/sensortest.pde</a> then when i opened the serial monitor i got</p>

<blockquote>
  <p>Accelerometer Test</p>
  
  <p>FF Ooops, no ADXL345 detected ... Check your wiring!</p>
</blockquote>

<p>i thought may be i have soldered the header in wrong direction(as in picture and videos at internet,they are so) so i desolder(with solder iron,no other technique) the header,but there were still some solder around the hole which i could not remove.then while checking the continuity between pins with multimiter(in resistance mode) i found the resistance to be 20k(scl-sda),220k(scl-gnd),220k(sda-gnd),between vcc and 3 other pins multimieter shows 1(range 2000k). then i soldered it on opposit side(this time where other components are mounted).the serial monitor still shows same output,and so does the muiltimeter.so where is the problem? is it with soldering ?do i need to disolder the header again and clean left out solder(with Chip Quik type desoldering technique ) on the opposite side(no component mounted)?is there any hope that i won't need to buy it again?</p>

<p><a href=""http://i.stack.imgur.com/skSN2.jpg"" rel=""nofollow"">picture of opposite side where no component is mounted and this is after desoldering and resoldering</a></p>
","imu accelerometer gyroscope"
"8399","Updating firmware of kk2","<p>I am facing problems in updating my kk2 board. I have used usbasp header for connecting it to the pc, and kk2firmware software. But it fails. It says not valid vid and pid values etc. Please help me. If any idea on updating firmware other than this. I have used usbasp header connecting kk2 board to pc.</p>
","arduino"
"8400","Using an RGB + Depth Camera to locate X,Y,Z coordinates of a ball","<p>I've recently been trying to use Gazebo to do some modelling for a couple tasks. I have a robot that's effectively able to locate a ball and get x,y coordinates in terms of pixels using a simple RGB camera from the Kinect. I also have a point cloud generated from the same Kinect, where I hope to find the depth perception of the ball using the X,Y coords sent from the circle recognition from my RGB camera. My plan earlier was to convert the X,Y coordinates from the RGB camera into meters using the DPI of the Kinect, but I can't find any info on it. It's much, much harder to do object recognition using a Point Cloud, so I'm hoping I can stick to using an RGB camera to do the recognition considering it's just a simple Hough Transform. Does anybody have any pointers for me?</p>
","localization computer-vision kinect cameras gazebo"
"8403","How to tilt a camera 180 using mirrors","<p>How would you vertically tilt a camera 180 degrees using mirrors?</p>

<p>I'm trying to add a pan/tilt mechanism to a Raspberry Pi's camera. The camera uses one of those flat cables with unstranded wires, and even with a strain gauge, I don't trust it to handle repeated bending, so I'm trying to design a tilt mechanism that allows the camera to be rigidly mounted so no wires move. The tilting also has to happen very quickly, so I'm trying to minimize the amount of mass I need to move.</p>

<p>Then I saw the <a href=""http://www.xaxxon.com/oculus"" rel=""nofollow"">Oculus</a> kit that actuates a mirror to effectively tilt a laptop's fixed webcam. I'm trying to extend this idea, but I having trouble working out the mechanics that would allow the tilt to extend to 180 degrees. The layout in the Oculus's mechanism only supports a tilt angle of about 90 degrees, and the mirrors are relatively large. Is it possible to modify this to support 180 degrees?</p>

<p>Are there other ways to ""bend"" the view of a camera without having to move the actual camera?</p>
","mechanism cameras"
"8406","Samsung IP camera, wifi direct feature","<p>Can we use wifi direct feature of samsung IP cameras to connect them directly to Laptop wifi adapter without the need of any additional router? I am working on an OpenCV project and i want to read stream from 3 cameras simultaneously, so i was thinking that i could connect 3 usb wifi adapter to my laptop and connect them directly with cameras. Is this scenario possible?</p>
","opencv"
"8407","One propeller Drone?How well it works?Hope","<p>Can a one propeller drone work efficiently for a good flight and stable camera footage in a drone flight</p>
","quadcopter"
"8411","Joint Space Singularities","<p>I would like to clarify my self on singularity configurations. If I am moving the robot in joint space only one joint at a time, can I come to a singular configuration? If so how?
Thanks</p>
","joint jacobian"
"8417","Removing PCB from a Dynamixel RX-24F servo?","<p>For a mod on the Dynamixel RX-24F I need to remove the enclosed PCB. I removed all screws but the PCB doesn't come out easily (without applying more force than I'm comfortable with). It seems to be stuck on the three large solder points in the white area. Has someone experience with this particular servo? </p>

<p>It might be glued/soldered to the case, but I'm not quite certain. Any help is appreciated.</p>

<p><a href=""http://i.stack.imgur.com/Xe6Zu.jpg"" rel=""nofollow""><img src=""http://i.stack.imgur.com/Xe6Zu.jpg"" alt=""Dynamixel RX-24F PCB""></a></p>
","dynamixel"
"8418","How to compute the relative pose between two robots?","<h2>How can i compute the relative pose between two robots?</h2>

<p>I have these robots (matlab code):</p>

<pre><code>poseR1 = [1;2;0.5];
poseR2 = [6;4;2.1];
</code></pre>

<p>And the corresponding covariance matrix of the probability distribution of each one (the movement of each robot is asociate with a gaussian distribution)</p>

<pre><code>co1 = diag([0.08,0.6,0.02]);
co2 = diag([0.20,0.09, 0.03]);
</code></pre>

<p><strong>AUXILIAR FUNCTIONS TO SOLVE</strong></p>

<p>Function to compose two poses ([x ; y ; theta])</p>

<pre><code>function tac=tcomp(tab,tbc)
%Composition of transformations given by poses
ang = tab(3)+tbc(3);

if ang &gt; pi | ang &lt;= -pi
   ang = AngleWrap(ang);
end

s = sin(tab(3));
c = cos(tab(3));
tac = [tab(1:2)+ [c -s; s c]*tbc(1:2);
</code></pre>

<p>Function to get the inverse pose of a pose</p>

<pre><code>function tba=tinv(tab)

tba = zeros(size(tab));
for t=1:3:size(tab,1),
   tba(t:t+2) = tinv1(tab(t:t+2));
end
</code></pre>

<p><strong>SOLUTION:</strong></p>

<p>To get the relative pose, we need to compute two things:</p>

<ol>
<li><p>The pose to be composed with poseR1 to get poseR2.   It means that we need to get the ""pose_inc"" that satisfy with this equality</p>

<pre><code>tcomp(psoeR1,**pose_inc**) = poseR2
</code></pre>

<p>The next code compute that relative pose that allows to poseR1 reach poseR2</p>

<pre><code>% Computing the relative pose between R1 and R2

% pose to be composed with poseR1 to reach poseR2

pose_inc = tcomp(tinv(poseR1),poseR2);
</code></pre></li>
<li><p>The covariance matrix of the relative pose. The uncertainty of R1 after reach the position poseR2</p>

<pre><code>    c = cos(poseR1(3)); s = sin(poseR1(3));

    x_1 = poseR1(1); y_1 = poseR1(2);

    x_2 = poseR2(1); y_2 = poseR2(2);

    % Jacobians to compute the covariance of the relative pose
    JacF_1 = [-c -s -(x_2-x_1)*s+(y_2-y_1)*c ;
               s -c -(x_2-c_1)*c-(y_2-y_1)*s ;
               0 0 -1];
    JacF_2 = [c   s 0;
              -s  c 0;
               0 0 1];
    % Computing the covariance of the relative pose
    C_p12 = JacF_1*co1*JacF_1' + JacF_2*co2*JacF_2';`
</code></pre></li>
</ol>

<p>Roughly,</p>

<p><code>JacF_1 is a jacobian that define how difference poseR1 is against pose_inc</code></p>

<p><code>JacF_2 is a jacobian that define how difference poseR2 is against pose_inc</code></p>

<p>C_p12 is the covariance matrix of R1 after move to poseR2 </p>
","sensors movement pose first-robotics"
"8419","Running my 3 DOF Inverse Kinematics Code: Works in MATLAB, not in Python","<p>I asked a question similar to this earlier, but I believe I have a new problem. I've been working on figuring out the inverse kinematics given an x,y,z coordinate. I've adopted the Jacobian method, taking the derivative of the forward kinematics equations with respect to their angles and input it into the Jacobian. I then take the inverse of it and multiply it by a step towards the goal distance. For more details, look at <a href=""http://www.seas.upenn.edu/~meam520/notes02/IntroRobotKinematics5.pdf"" rel=""nofollow"">http://www.seas.upenn.edu/~meam520/notes02/IntroRobotKinematics5.pdf</a> page 21 onwards. </p>

<p>For a better picture, below is something:
<img src=""http://i.stack.imgur.com/7mVwI.png"" alt=""3 DOF Arm""></p>

<p>Below is the code for my MATLAB script, which runs flawlessly and gives a solution in under 2 seconds:</p>

<pre><code>ycurrent = 0; %Not using this  
xcurrent = 0; %Starting position (x)   
zcurrent = 0; %Starting position (y)    
xGoal = .5; %Goal x/z values of (1, 1)   
zGoal = .5;    
theta1 = 0.1; %Angle of first DOF    
theta2 = 0.1; %Angle of second DOF  
theta3 = 0.1; %Angle of third DOF
xchange = xcurrent - xGoal %Current distance from goal
zchange = zcurrent - zGoal
%Length of segment 1: 0.37, segment 2:0.374, segment 3:0.2295 
while ((xchange &gt; .02 || xchange &lt; -.02) || (zchange &lt; -.02 || zchange &gt; .02))    
        in1 = 0.370*cos(theta1); %These equations are stated in the link provided
        in2 = 0.374*cos(theta1+theta2);
        in3 = 0.2295*cos(theta1+theta2+theta3);
        in4 = -0.370*sin(theta1);
        in5 = -0.374*sin(theta1+theta2); 
        in6 = -0.2295*sin(theta1+theta2+theta3); 
        jacob = [in1+in2+in3, in2+in3, in3; in4+in5+in6, in5+in6, in6; 1,1,1];
        invJacob = inv(jacob); 
        xcurrent = .3708 * sin(theta1) + .374 * sin(theta1+theta2) + .229 * sin(theta1+theta2+theta3) 
        zcurrent = .3708 * cos(theta1) + .374 * cos(theta1+theta2) + .229 * cos(theta1+theta2+theta3)        
        xIncrement = (xGoal - xcurrent)/100; 
        zIncrement = (zGoal - zcurrent)/100; 
        increMatrix = [xcurrent; zcurrent; 1]; %dx/dz/phi 
        change = invJacob * increMatrix; %dtheta1/dtheta2/dtheta3  
        theta1 = theta1 + change(1)  
        theta2 = theta2 + change(2)  
        theta3 = theta3 + change(3)
        xcurrent = .3708 * sin(theta1) + .374 * sin(theta1+theta2) + .229 * sin(theta1+theta2+theta3)  
        zcurrent = .3708 * cos(theta1) + .374 * cos(theta1+theta2) + .229 * cos(theta1+theta2+theta3)          
        xchange = xcurrent - xGoal
        zchange = zcurrent - zGoal
end        
</code></pre>

<p>Below is my Python code, which goes into an infinite loop and gives no results. I've looked over the differences between it and the MATLAB code, and they look the exact same to me. I have no clue what is wrong. I would be forever grateful if somebody could take a look and point it out.</p>

<pre><code>def sendArm(xGoal, yGoal, zGoal, right, lj):
    ycurrent = xcurrent = zcurrent = 0
    theta1 = 0.1
    theta2 = 0.1
    theta3 = 0.1
    xcurrent, zcurrent = forwardKinematics(theta1, theta2, theta3)
    xchange = xcurrent - xGoal
    zchange = zcurrent - zGoal
    while ((xchange &gt; 0.05 or xchange &lt; -0.05) or (zchange &lt; -0.05 or zchange &gt; 0.05)):
        in1 = 0.370*math.cos(theta1) #Equations in1-6 are in the pdf I linked to you (inv kinematics section)
        in2 = 0.374*math.cos(theta1+theta2)
        in3 = 0.2295*math.cos(theta1+theta2+theta3)
        in4 = -0.370*math.sin(theta1)
        in5 = -0.374*math.sin(theta1+theta2)
        in6 = -0.2295*math.sin(theta1+theta2+theta3)
        jacob = matrix([[in1+in2+in3,in2+in3,in3],[in4+in5+in6,in5+in6,in6], [1,1,1]]) #Jacobian
        invJacob = inv(jacob) #inverse of jacobian
        xcurrent, zcurrent = forwardKinematics(theta1, theta2, theta3)
        xIncrement = (xGoal - xcurrent)/100 #dx increment
        zIncrement = (zGoal - zcurrent)/100 #dz increment
        increMatrix = matrix([[xIncrement], [zIncrement], [1]])
        change = invJacob*increMatrix #multiplying both matrixes
        theta1 = theta1 + change.item(0)
        theta2 = theta2 + change.item(1)
        theta3 = theta3 + change.item(2)
        xcurrent, zcurrent = forwardKinematics(theta1, theta2, theta3)
        xchange = xcurrent - xGoal
        zchange = zcurrent - zGoal
        print ""Xchange: %f ZChange: %f"" % (xchange, zchange)
    print ""Goals %f %f %f"" % (theta1, theta2, theta3)
    right.set_joint_positions(theta1) #First pitch joint
    right.set_joint_positions(theta2) #Second pitch
    right.set_joint_positions(theta3) #Third Pitch joint


def forwardKinematics(theta1, theta2, theta3):
    xcurrent = .3708 * math.sin(theta1) + .374 * math.sin(theta1+theta2) + .229 * math.sin(theta1+theta2+theta3)
    zcurrent = .3708 * math.cos(theta1) + .374 * math.cos(theta1+theta2) + .229 * math.cos(theta1+theta2+theta3)            
    return xcurrent, zcurrent
</code></pre>
","kinematics inverse-kinematics matlab python jacobian"
"8420","Why does it require more force to turn a servo if it is electronically connected to another servo?","<p>I have two servo motors that I rigged up to use as a telescope remote focuser. The idea is to turn one servo by hand and use the power generated to turn the other, which is geared to a telescope focuser knob. I noticed that when the two servos are electrically connected, it is noticeably harder to turn a servo compared to turning it by itself. I tried changing the polarity of the connection hoping it would help, but it is still harder to turn the servo when they are connected. Does anyone know why this is?</p>
","servos servomotor"
"8421","I can't get motors to turn with Raspberry pi","<p>I want to turn some motors using my raspberry pi. I am able to turn an LED on and off using the 3.3V GPIO pin. For the motors, I tried using a L293D chip as per the instructions on <a href=""http://computers.tutsplus.com/tutorials/controlling-dc-motors-using-python-with-a-raspberry-pi--cms-20051"" rel=""nofollow"">this link</a>.</p>

<p>What happened is that the very first time I set the circuit up for one motor, it worked perfectly. But then, I moved the pi a little and the motor has since refused to work. I even bought a new pi and still no luck with the circuit. I then bought a L298N board that fits smugly on top of the GPIO pins of the pi and followed the instructions on the <a href=""https://www.youtube.com/watch?v=JJXzlCK4vnY"" rel=""nofollow"">this video</a></p>

<p>Still no luck, the motor just won't run with either pi. I am using four AA batteries to power the motor and a connecting the pi to a power supply from the wall. What could possibly be the problem here?</p>
","motor raspberry-pi"
"8425","Relationship between earth frame attitude and acceleration for a quadcopter","<p>For a quadcopter, what is the relationship between roll, pitch, and yaw in the <strong>earth frame</strong> and acceleration in the x, y, and z dimensions in the earth frame? To be more concrete, suppose roll ($\theta$) is a rotation about the earth frame x-axis, pitch ($\phi$) is a rotation about the earth frame y-axis, and yaw ($\psi$) is a rotation about the z-axis. Furthermore, suppose $a$ gives the acceleration produced by all four rotors, i.e. acceleration normal to the plane of the quadcopter. Then what are $f, g, h$ in</p>

<p>$$a_x = f(a,\theta,\phi,\psi)$$
$$a_y = g(a,\theta,\phi,\psi)$$
$$a_z = h(a,\theta,\phi,\psi)$$</p>

<p>where $a_x$, $a_y$, and $a_z$ are accelerations in the $x$, $y$, and $z$ dimensions.</p>

<p>I've seen a number of papers/articles giving the relationship between x,y,z accelerations and attitude, but it's never clear to me whether these attitude angles are rotations in the earth frame or the body frame.</p>
","quadcopter dynamics"
"8426","Denavit Hartenberg parameters - 3DOF articulated manipulator","<p>I am trying to solve a forward kynematics problem for a 3DOF manipulator.</p>

<p>I am working with the Robotics Toolbox for MatLab created by Peter Corke and after calculte the DH parameters and introduce them into MatLab to compute the fordward kynematics the plotted robot is not what it should be.</p>

<p>I guess I made some mistakes calculating the DH parameters.</p>

<p>Attached is the file where you can see what are the DH frames calculated for each joint and the DH parameters for each frame.</p>

<p>Anyone could give me a clue whether this is the correct answer?</p>

<p>Here is the image with the frames calculated by me.
<a href=""http://i.stack.imgur.com/I3wow.jpg"" rel=""nofollow""><img src=""http://i.stack.imgur.com/I3wow.jpg"" alt=""enter image description here""></a></p>

<p>And here the robot I get from Matlab (using the Robotics Toolbox by P.Corke)
<a href=""http://i.stack.imgur.com/lS4Td.jpg"" rel=""nofollow""><img src=""http://i.stack.imgur.com/lS4Td.jpg"" alt=""enter image description here""></a></p>
","robotic-arm inverse-kinematics forward-kinematics"
"8427","What charger to use with my ZIPPY Compact 6200mAh 4s 40c Lipo MultiRotor Battery","<p>Hello I am trying to build a Quadcopter for a school project and I need to finish quick before our mission trip because I was asked to finish it before then so that I could take it. But I am having a problem figuring out which charger would work with my <a href=""http://www.hobbyking.com/hobbyking/store/__63431__ZIPPY_Compact_6200mAh_4s_40c_Lipo_Pack.html?gclid=CMqXo9W_hskCFY-VfgodACwH9w"" rel=""nofollow"">ZIPPY Compact 6200mAh 4s 40c Lipo Pack</a> </p>

<hr>

<p>Here are the specs on the battery</p>

<ul>
<li>Capacity: 6200mAh</li>
<li>Voltage: 4S1P / 4 Cell / 14.8V</li>
<li>Discharge: 40C Constant / 50C Burst</li>
<li>Weight: 589g (including wire, plug &amp; case)</li>
<li>Dimensions: 158x46x41mm</li>
<li>Balance Plug: JST-XH
Discharge Plug: HXT4mm</li>
</ul>

<hr>

<p>Also I will be running the <a href=""http://www.helipal.com/tarot-t4-3d-brushless-gimbal-for-gopro-3-axis.html?gclid=CJuvxaGj5MgCFYeEfgodtWkOaA"" rel=""nofollow"">Tarot T4-3D Brushless Gimbal for GoPro (3-Axis)</a>
and if anyone can tell me a good battery to run it off of or maybe it would be better to run it off my main battery.</p>

<p><strong>Thanks in Advance</strong></p>
","quadcopter battery lithium-polymer"
"8428","Moatech stepper motor salvage - unsure of pinout","<p>I have salvaged a Moatech BL55K-M01 Stepper motor from an old printer and I'd like to play around with it, but I'm unsure of how to use the pinout listed on the board. Pinout reads:</p>

<ul>
<li>24V</li>
<li>24V</li>
<li>GND</li>
<li>GND</li>
<li>SGND</li>
<li>5V</li>
<li>ST/SP</li>
<li>RD</li>
<li>CLK</li>
<li>GAIN</li>
</ul>

<p>Which is all well and good, but I am not sure what SGND, ST/SP, RD, and GAIN should be used for. I understand CLK, but also don't know what frequency this expects. Moatech e-mail bounced back, so I was hoping someone might have a coherent datasheet or know what these designations mean. </p>
","stepper-motor"
"8432","How to use SLAM with simple sensors","<p>What 2D SLAM implementations (preferably included in ROS) can be used with simple distance sensors like IR or ultrasonic rangefinders?</p>

<p>I have a small mobile platform equipped with three forward facing ultrasonic sensors (positioned at 45 degrees, straight ahead, and -45 degrees), as well as a 6-DOF accel/gryo and wheel encoders, and I'd like to use this to play around with a ""toy"" SLAM implementation. I don't want to waste money on a Kinect, much less a commercial laser rangefinder, so methods that require high-density laser measurements aren't applicable.</p>
","slam ros"
"8438","Solar panel set rotation: How to achieve both vertical and horizontal rotation?","<p><a href=""http://i.stack.imgur.com/Gu5Sk.jpg"" rel=""nofollow""><img src=""http://i.stack.imgur.com/Gu5Sk.jpg"" alt=""Solar panel array""></a></p>

<p>How can we achieve this kind of rotation to enable maximum trapping of solar rays during the day?</p>
","mobile-robot"
"8443","iRobot Create 2: Can I load the code in instead of connecting cable? (new learner)","<p>I am a new learner of iRobot. I am trying to program it to control the movement of the Create 2. After glancing through the existing project, I find most of them are based on sending commands to Roomba through a cable. </p>

<p>Is there anyway to embed the code in and let the Roomba behave accordingly? If there is not such method, which kind of API tool do you think is easiest for beginner? </p>
","irobot-create"
"8444","Is there a way to turn the sound off of a Roomba?","<p>I am working with an iRobot Create 2 and I work with others around me. Whenever I turn the robot on, send it an OI reset command, etc., it makes its various beeps and noises. I would like to not have this happen since I find it a little annoying and I'm sure those who have to work around me would like to have things quiet so they can concentrate on their work. Is there a way to accomplish turning off the beeps (while still being able to easily re-enable them), or am I out of luck?</p>
","irobot-create roomba digital-audio"
"8448","Quadrotor - Control system, where to begin?","<p>I am starting to assemble a quadrotor from scratch.</p>

<p>Currently, I have this:</p>

<ul>
<li>Structure;</li>
<li>an IMU (accelerometer, gyro, compass);</li>
<li>4 ESCs and DC motors;</li>
<li>4 propellers;</li>
<li>Raspberry Pi to control the system, and;</li>
<li>LiPo battery.</li>
</ul>

<p>I have calibrated the ESCs and the four motors are already working and ready.
But now I am stuck.</p>

<p>I guess the next step is to dive deeply in the control system, but I am not sure where to begin. I read some articles about the control using PIDs, but I don't know how many should I use, or whether I need to model the quadrotor first to compute kinematic and dynamic of the quadrotor inside the RPi.
Sorry if the question is too basic!</p>

<p><strong>More details</strong></p>

<p>The structure is from a kit. Well, all I have now is the ESCs calibrated, although I do not have documentation of them to adjust the cut off voltage for the LiPo battery. I have been made tests with some Python code I found to have PWM outputs for the motors and to control I2C bus to communicate with IMU. </p>

<p>One of my problems now is that I need RPIO library for PWM and the <code>quick2wire-python-api</code> to work with the I2C libraries from the MIT to control my IMU but as far as I know RPIO works with Python2 and <code>quick2wire</code> works with Python3 so I don't know how to manage this.</p>

<p>So actually, I have no code yet to control the four motors in parallel, only have testing code to test them separately and also with the IMU.</p>

<p>About the IMU, I am still learning how to work with it and how to use the MIT library. The unit includes those sensors:</p>

<ul>
<li>ADXL345</li>
<li>HMC5883L-FDS</li>
<li>ITG3205</li>
</ul>

<p>You can see a picture of the quadrotor below,</p>

<p><a href=""http://i.stack.imgur.com/fV5Oy.jpg"" rel=""nofollow""><img src=""http://i.stack.imgur.com/fV5Oy.jpg"" alt=""Quadrotor""></a></p>

<p>So as I said before, I would like to know how to handle the control system and how it is implemented inside the Raspberry Pi, and then start to work with the Python code to assemble the motors, the IMU and the control.</p>
","control pid raspberry-pi quadcopter beginner"
"8450","Torque / Current control for BLDC motors","<p>I am working on a robotic application, and I want to control the torque (or current) of brushless DC motors. There are many BLDC speed controllers but I could not find anything related to torque or current. </p>

<p>Instead of continuously spinning, the motor is actuating a robotic joint, which means I need to control the torque at steady-state, or low-speed, finite rotation. </p>

<p>I am looking for a low-cost, low weight solution, similar to what <a href=""http://www.mouser.com/new/Texas-Instruments/ti-drv8833c-driver/"" rel=""nofollow"">Texas Instruments DRV8833C Dual 
H-Bridge Motor Drivers</a> does for brushed DC motors.</p>
","brushless-motor"
"8451","What are hardware components to build a modular robot which consists of several 5x5x5cm modules?","<p>I am computer science student and I have no knowledge on robotics. </p>

<p>In my project, I am trying to find controllers for modular robots to make them do specific tasks using evolutionary techniques. For the moment I am doing this in a simulator, but if I want to make physical robots I have to know a priori the components to add to the robot, where do I place them, especially if modules of robot are small (cubes of 5*5*5cm)...</p>

<p>So my questions are:</p>

<ol>
<li>What are must have components to make physical robot ? (arduino, batteries, sensors, ...)</li>
<li>For a small robot how many batteries do I need ?</li>
<li>If modules have to communicate with wifi, do I have to put a wifi card on each module?</li>
<li>I want to add an IMU. Is its position important, I mean do I have to put it in the middle of the robot ?</li>
</ol>

<p>Thank you very much.</p>
","arduino mechanism"
"8452","What is the easiest yet precise method one can make a track and a train","<p>I would like to put a train on a track and control its movement with high precision left and right using a wireless controller.</p>

<p>What is the best way to do it?</p>
","arduino"
"8455","Small IR distance sensor that works on black surfaces","<p>Can anyone recommend an IR distance sensor that works on black surfaces? I'm looking for something to use as a ""cliff"" sensor, to help a small mobile robot avoid falling down stairs or off a table, and I thought the <a href=""https://www.pololu.com/product/1132"" rel=""nofollow"">Sharp GP2Y0D805Z0F</a> would work. However, after testing it, I found any matte black surface does not register with the sensor, meaning the sensor would falsely report a dark carpet as a dropoff.</p>

<p>Sharp has some other models that might better handle this, but they're all much larger and more expensive. What type of sensor is good at detecting ledges and other dropoffs, but is small and inexpensive and works with a wide range of surfaces?</p>
","sensors"
"8457","Create 2 CAD files","<p>I found CAD files for the Create on the <a href=""http://download.ros.org/downloads/turtlebot/"" rel=""nofollow"">ROS TurleBot download page</a> (<a href=""http://download.ros.org/downloads/turtlebot/TurtleBot-Hardware-2011-07-23.zip"" rel=""nofollow"">.zip</a>),
and shells on the <a href=""https://bitbucket.org/osrf/gazebo_models/src/e024fe922c810f468e2a7ab277750354465478cd/create/?at=default"" rel=""nofollow"">gazebo sim page</a>. </p>

<p>Any ideas where the files for the Create 2 could be found?</p>
","design irobot-create"
"8461","Electronic circuit for heating nylon fishing line muscle","<p>I'm trying to make artificial muscles using nylon fishing lines (see <a href=""http://io9.com/scientists-just-created-some-of-the-most-powerful-muscl-1526957560"" rel=""nofollow"">http://io9.com/scientists-just-created-some-of-the-most-powerful-muscl-1526957560</a> and <a href=""http://writerofminds.blogspot.com.ar/2014/03/homemade-artificial-muscles-from.html"" rel=""nofollow"">http://writerofminds.blogspot.com.ar/2014/03/homemade-artificial-muscles-from.html</a>)</p>

<p>So far, I've produced a nicely coiled piece of nylon fishing line, but I'm a little confused about how to heat it electrically.</p>

<p>I've seen most people say they wrap the muscle in copper wire and the like, pass current through the wire, and the muscle acuates on the dissipated heat given the wire resistance.</p>

<p>I have two questions regarding the heating:</p>

<p>1) isn't copper wire resistance extremely low, and thus generates very little heat? what metal should I use? </p>

<p>2) what circuit should I build to heat the wire (and to control the heating)? Most examples just ""attach a battery"" to the wire, but afaik that is simply short-circuiting the battery, and heating the wire very inneficiently (and also may damage the battery and it could even be dangerous). So what's a safe and efficient way to produce the heat necessary to make the nylon muscle react? (I've read 150 centigrads, could that be correct?) for example with an arduino? or a simple circuit in a breadboard?</p>

<p>thanks a lot!</p>
","arduino electronics actuator"
"8462","Automatic sliding window shutter","<p>I want to build an automatic sliding window shutter and need help
with part selection and dimensioning.</p>

<p>Some assumptions: </p>

<ul>
<li>window width 1.4 m</li>
<li>sliding shutter weight 25 kg</li>
<li>max speed 0.07 m/s</li>
<li>max acceleration 0.035 m/s^2</li>
<li>pulley diameter 0.04m.</li>
</ul>

<p>Leaving out friction I need a motor with about 0.02 Nm of torque and a rated speed of 33 rpm.</p>

<p>What I would like to use:</p>

<ul>
<li><a href=""http://electromen.com/en/products/item/motor-controllers/dc-motor-below-10A/EM-140A"" rel=""nofollow"">motor controller</a> with soft-start and jam protection,</li>
<li><a href=""https://www.maedler.de/product/1643/1331/1979/1349/stirnrad-kleingetriebemotoren-sf-24-v-bis-2-nm"" rel=""nofollow"">dc motor 24V</a>,</li>
<li><a href=""https://www.maedler.de/product/1643/1616/986/zahnriemenraeder-t25-fuer-riemenbreite-10-mm-aus-aluminium"" rel=""nofollow"">Pulleys and timing belts</a>.</li>
</ul>

<p>Would you suggest other components or a different setup?
How do I connect motor and pulleys (clamping set?)?
Do I need additional <a href=""https://www.maedler.de/product/1643/1629/stehlager"" rel=""nofollow"">bearings</a> because of the radial load?</p>

<pre><code>M=P
M=B=P
M=P=B
M=B=P=B
</code></pre>

<p>(M motor, P pulley, B bearing, = shaft)</p>

<p>If so I have to extend the motor shaft. What would I use for that (clamp collars, couplings?)?
What width do I need for the belts? Which belt profile (T, AT, HDT) should I use?</p>

<p><strong>Update</strong></p>

<p>The construction I am aiming for resembles the one which can be seen on page 6 (pdf numbering) <a href=""http://www.baier-gmbh.de/media/docs/schiebeladen/Schiebeladen-e.pdf"" rel=""nofollow"">here</a>.</p>
","motor design microcontroller automation"
"8463","Angular velocity to translational velocity","<p>I have a 3D point in space with it's XYZ Coordinates about some Frame A. I need to calculate the new XYZ coordinates, given the angular velocities of each axis at that instant of time about Frame A</p>

<p>I was referring to my notes, but I'm a little confused. This is what my notes say:
<a href=""http://imgur.com/yJD8OUi"" rel=""nofollow"">http://imgur.com/yJD8OUi</a></p>

<p>As you can see, i can calculate the angular velocity vector w given my angular velocities. But I'm not sure how this translates to how to calculate my new XYZ position! How can i calculate the RPY values this equation seems to need from my XYZ, and how can i calculate my new position from there</p>
","mobile-robot"
"8466","Sizing high current power supplies for large robots","<p>I'm a researcher in a lab that's starting work on some larger humanoid/quadruped robots as well as a quadcopter. Currently, we have several power supplies that have a max rating of 30V/30A and our modified quadcopter easily maxes out the current limit with only half of its propellers running. It seems like most power supplies are meant for small electronics work and have fairly low current limits. I think that I want to look for power supplies that are able to provide between 24-48V and higher than 30A for an extended period of time. </p>

<p>1.) Is this unreasonable or just expensive? 
2.) Do most labs just connect PSUs in series to get higher voltages?</p>

<p>Thanks for the input.</p>
","quadcopter power humanoid"
"8470","Xaxxon Oculus Prime Platform: using a different SBC than the one sold by manufacturer","<p>I am using the kit version Oculus Prime Mobile Platform sold by Xaxxon for a project and was planning to use and Odroid XU4 running ROS. But, the problem I faced was that the Odroid XU4 has a 32bit processor whereas the server application and ROS packages written for Oculus Prime run only on 64bit. Here are the links for the <a href=""http://www.xaxxon.com/documentation/view/oculus-prime-contents"" rel=""nofollow"">documentation</a> and <a href=""http://www.xaxxon.com/documentation/view/oculus-prime-software-server-install"" rel=""nofollow"">server application</a>.</p>

<p>Could anyone tell me if:</p>

<ul>
<li><p>There is a 32-bit variant or any other way to run the server application and ROS packages on the Odroid.</p></li>
<li><p>Will any single board computer with a 64-bit architecture be able to run the required packages or is there any special configuration in the motherboard that they sell. (is this the case with all such platforms in general)</p></li>
<li><p>Which single board computers would be ideal for this situation so that I can purchase them?</p></li>
</ul>
","ros ugv platform"
"8471","Dynamic simulation of compliant elements in quadruped robot","<p>I have a preliminary design for a legged robot that uses compliant elements in the legs and in parallel with the motors for energy recovery during impact as well as a pair of flywheels on the front and back that will oscillate back and forth to generate angular momentum. I'd like to create a dynamic simulation of this robot in order to be able to test a few control strategies before I build a real model. What simulation package should I be using and why? </p>

<p>I have heard good things about MSC Adams, namely that it is slow to learn, but has a lot of capability, including integration with matlab and simulink. I have also heard about the simmechanics toolbox in matlab, which would be nice to use since I already am decent with CAD and know the matlab language. I am not yet familiar with simulink, but have used Labview before.</p>
","mobile-robot design dynamics matlab simulation"
"8474","Kinematic decoupling","<p>Is kinematic decoupling of a 5DOF revolute serial manipulator also valid?
The three last joints is a spherical joint. Most literatures only talks about decoupling of 6DOF manipulators.</p>

<p>Thanks in advance,
Oswald</p>
","kinematics"
"8475","How can I reduce a motor's maximum current draw?","<p>I have a <a href=""http://store.amequipment.com/218-series-gearhead-motor-64mm-12v-right-hand-p-354.html"" rel=""nofollow"">motor</a> with a stall current of up to 36A. I also have a <a href=""http://pololu.com/product/705"" rel=""nofollow"">motor controller</a> which has a peak current rating of 30A. Is there any way I could reduce the stall current or otherwise protect the motor controller?</p>

<p>I realize the ""right"" solution is to just buy a better motor controller, but we're a bit low on funds right now.</p>

<p>I thought of putting a resistor in series with the motor and came up with a value of 150mΩ, which would reduce the maximum current draw to 25A (given the 12V/36A=330mΩ maximum impedance of the motor). Is there any downside to doing this? Would I be harming the performance of the motor beyond reducing the stall torque?</p>
","motor microcontroller current"
"8480","What's the difference between feedback and feedforward control?","<p>I'm reading from Astrom &amp; Murray (2008)'s <em>Feedback Systems: An introduction for scientists and engineers</em> about the difference between feedback and feedforward.  The book states:</p>

<blockquote>
  <p>Feedback is reactive: there must be an error before corrective actions are taken.  However, in some circumstances, it is possible to measure a disturbance before the disturbance has influenced the system.  The effect of the disturbance is thus reduced by measuring it and generating a control signal that counteracts it.  This way of controlling a system is called <em>feedforward</em>.</p>
</blockquote>

<p>The passage makes it seem that feedback is reactive, while feedforward is not.  I argue that because feedforward control still uses sensor values to produce a control signal, it is still <em>reactive</em> to the conditions that the system finds itself in.  So, how can feedforward control possibly be any different from feedback if both are forms of reactive control?  What really separates the two from each other?</p>

<p>A illustrative example of the difference between the two would be very helpful.</p>
","control"
"8483","Powering 6 servomotors requiring 6V and 2A each","<p>The title pretty much says it all.  I'm on a team that is currently building a robotic arm for the capstone project of my engineering degree, our design is similar to the <a href=""http://i.ebayimg.com/00/s/MTYwMFgxNjAw/z/M80AAOSwFAZTujbJ/$_35.JPG"" rel=""nofollow"">Dobot</a> (5 degrees of freedom). We purchased our 6 servomotors, and each one requires 2A at 6V.  </p>

<p>From my preliminary research, I haven't been able to find a power source that could satisfy this.  We'd rather not purchase six individual AC/DC power source for each servo, and we've heard that these can introduce problems, as they aren't necessarily voltage-regulated.  Another suggestion we've received is to buy a computer power source, and modify it to output our the voltage and amperage we need. This raises some concerns, since our professor running the course might find this dangerous.</p>

<p>We'd like some input into how we can power our servos effectively, without going overboard on costs (we are students, after all).</p>

<p>Thanks!</p>
","power servos servomotor arm"
"8486","How to build a fast quadcopter","<p>im currently in a (risky) proyect that involves me building the fastest quad i can afford.</p>

<p>Im trying to get something close to this <a href=""https://www.youtube.com/watch?v=xQon7OD5CnY"" rel=""nofollow"">extremely fast warpquad</a></p>

<p>After reading a lot about quadcopters, as i know i can buy all this and it should fit together and fly without any problem.</p>

<pre><code>Motors: Multistar Elite 2306-2150KV
ESC: Afro Race Spec Mini 20Amp
Quanum neon 250 carbon racing frame(I love how it looks)
6Inch Props
CC3D flight controller
4S 1400mah 40-80C Battery
Any 6ch radio
</code></pre>

<p>My questions are, first if im wrong or im missing something as i had only read about it (thinking this is a common build for racer quad).</p>

<p>Then:</p>

<p>Will this overheat (bad consecuences) if i let it drain the full battery at 100% throttle?</p>

<p>Will this fly at least 4 minutes under the previous conditions?</p>

<p>Should i get a higher C-rating battery?</p>

<p>As i can't find better motors of that size, is the only way to improve its speed by putting a 6S battery? and what would happen if i do it?</p>

<p>Should i put the 6inch props or 4inch? I know 4inch should get faster rpm changes but will it be noticeable at this sizes?</p>

<p>And in general any tips to make it faster will be welcome.</p>

<p>Thanks. </p>
","quadcopter"
"8487","Is there a way to disconnect and reconnect from a Create 2 that was streaming sensor readings without having to unplug/replug my USB-serial cable?","<p>I am working with a Create 2 and I am executing a simple sequence like (in pseudocode):</p>

<pre><code>create serial connection from Macbook to Create

start the OI with by sending the 128 code

send a pause-stream command (just to be safe)

initiate the data streaming with ids: [29, 13]

every 0.5 seconds for 15 seconds:
    poll the streamed sensor data and print it

send a pause-stream command before shutdown

send a 128 to put the robot in ""passive mode"" (I have also tried 173)

close the serial connection
</code></pre>

<p>The outcome when I run the above program repeatedly is that it works the first time, I see sensor data (that seems to not change or be reactive) printing to the screen, but on future runs no serial can be read and the program crashes (because I am throwing an exception because I want to get this problem ironed out before getting to far along with other things). If I unplug and replug my USB cable from my Macbook, then the program will work for another run, and then fall back into the faulty behavior.</p>

<p>I do not experience this issue with other things like driving the robot, I am able to run programs of similar simplicity repeatedly. If I mix driving and sensor streaming, the driving works from program run to program run, but the data streaming crashes the program on the subsequent runs.</p>

<p>I have noticed that if I want to query a single sensor, I need to pause the stream to get the query response to come through on the serial port, and then resume it. That is why I am so inclined to pause/restart the stream.</p>

<p>Am I doing something wrong, like pausing the stream too often? Are there other things I need to take care of when starting/stopping the stream? Any help would be appreciated!</p>

<p><strong>EDIT:</strong>
I should note that I am using Python and pyserial. I should also note, for future readers, that the iRobot pushes its streamed data to the laptop every 15ms where it sits in a buffer, and the data sits there until a call to serial.read() or to serial.flushInput(). This is why it seemed that my sensor values weren't updating when I read/polled every half second, because I was reading old values while the current ones were still buried at the back of the buffer. I worked around this issue by flushing the buffer and reading the next data to come in.</p>

<p><strong>EDIT 2:</strong>
Sometimes the above workaround fails, so if I detect the failure, I pause the stream, re-initialize the stream, and read the fresh data coming in. This seems to work pretty well. It also seems to have solved the issue that I originally asked the question about. I still don't know <em>exactly</em> why it works, so I will still accept @Jonathan 's answer since I think it is good practice and has not introduced new issues, but has at least added the benefit of the robot letting me know that it has started/exited by sounding tones.</p>
","mobile-robot irobot-create"
"8491","Relative orientation of two robots","<p>Given two robot arms with TCP (Tool Center Point) coordinates in the world frame is:</p>

<p>$X_1 = [1, 1, 1, \pi/2, \pi/2, -\pi/2]$</p>

<p>and</p>

<p>$X_2 = [2, 1, 1, 0, -\pi/2, 0]$</p>

<p>The base of the robots is at:</p>

<p>$Base_{Rob1} = [0, 0, 0, 0, 0, 0]$</p>

<p>$Base_{Rob2} = [1, 0, 0, 0, 0, 0]$</p>

<p>(The coordinates are expressed as successive transformations, X-translation, Y-translation, Z-translation, X-rotation, Y-rotation, Z-rotation. None of the joint axes are capable or continuous rotations.)</p>

<p>How many degrees does the TCP of robot 2 have to rotate to have the same orientation as the TCP of robot one?</p>

<p>Is the calculation </p>

<p>$\sqrt{(\pi/2 - 0)^2 + (\pi/2 - (-\pi/2))^2 + (-\pi/2 - 0)^2}$</p>

<p>wrong? If yes, please specify why.</p>

<p>UPDATED:
is the relative orientation of the two robots [π/2,π/2,−π/2]−[0,−π/2,0]=[π/2,π,−π/2]? but the euclidean distance cannot be applied to calculate angular distance?</p>

<p>In other words:</p>

<ol>
<li><p>While programming the robot, and tool frame is selected for motion, to match the orientation of the other one, i would have to issue a move_rel($0, 0, 0, \pi/2, \pi, -\pi/2$) command, but the executed motion would have magnitude of $\pi$?</p></li>
<li><p>While programming the robot, and world frame is selected for motion, to match the orientation of the other one, i would have to issue a move_rel($0, 0, 0, \pi, 0, 0$) command, and the executed motion would have magnitude of $\pi$?</p></li>
</ol>
","robotic-arm kinematics geometry"
"8499","Mapping between camera pose and image features in visual servoing","<p>I have a robotic arm and a camera in eye-in-hand configuration. I know that there is a relationship between the body velocity $V$ of the camera and the velocities $\dot s$ in the image feature space that is $\dot s=L(z,s) V$ where $L$ is the interaction matrix. I was wondering if one can find a mapping (a so called diffeomorphism) that connects the image features' vector $s$ with the camera pose $X$. All I was able to find is that it is possible to do that in a structured environment which I don't fully understand what it is.</p>
","mapping visual-servoing"
"8500","Forward kinematic and inverse kinematic... When to use what?","<p>I am not quite sure if I quite understand the difference between these two concepts, and why there is a difference between these two concept. </p>

<p>Yesterday I was trying to compute the jacobian needed for an inverse kinematics, but the usual input I provided my transformation in the Forward kinematics being the Points P and xyz could not be applied, The transformation matrix was given a state vector Q, at which the the Tool position could be retrieved... </p>

<p>I am not sure if understand the concept quite well, and can't seem to the google the topics, as they usually  include terminologies which makes the concepts too simple (Angle calc and so on.. )</p>

<p>I know it might be pretty much to ask, but what form of input is needed to compute the jacobian ?, and what and why is there a difference between forward and inverse kinematics?.. </p>
","inverse-kinematics forward-kinematics jacobian"
"8501","Lifting robotic leg with only one servo","<p><strong>Note before I start:</strong> I have not actually put anything together yet, i'm still just planning, so any changes that require a shape change or anything like that are accepted.</p>

<p>I'm working on making a walking robot with my arduino and 3d printing all the pieces I need. It will have four legs, but since it needs to be mobile, I didn't want the power supply to be huge. I've decided it would be best if I can get each leg to only require 1 servo, at 5V each. I know how to get the leg to move back and forth, but i want to be able to lift it in between; before it brings the leg forward, it needs to lift up the foot.
The only thing I can think of is the rotation maybe locking some sort of gear.</p>

<p>When a motor begins rotating clockwise, how can I have it power a short motion to move an object toward itself, and when it begins moving counterclockwise to power the same object a short distance away from itself?</p>

<p>The servos I am using have 180* of rotation, so they don't go all the way around in a loop.</p>

<p>also: don't know if it will be important or not, but because of the peculiar construction of the foot, it would be best if it was lifted straight up, rather than up at an angle, but it isn't 100% necessary. </p>

<p>Are there any robots that already do this? if so, I'm unaware of them. Thanks for your time.</p>
","mechanism motion-planning servomotor legged gearing"
"8510","What are the approaches for indoor robot positioning?","<p>I understand that most of the self-driving cars solutions are based on Lidar and video SLAM.</p>

<p>But what about robots reserved for indoor usage? Like robot vacuums and industrial AGVs? I see that Lidar is used for iRobot and their latest version uses VSLAM. AGVs also seem to use Lidar.</p>
","slam lidar"
"8511","What engineering problems needs to be solved to build a potato-peeling robot?","<p>OK, let's say we have a tech request for a robotic system for peeling potatoes, and a design is as follows:</p>

<ol>
<li>One ""arm"" for picking up a potato and holding it, rotating when needed.</li>
<li>Another ""arm"" for holding a knife-like something which will peel the skin from the potato.</li>
<li>Arm picks up a potato from first container, holds it over trash bin while peeling, then puts peeled potato in second container.</li>
<li>For simplicity a human rinses peeled potatoes, no need to build automatic system for it.</li>
<li>In first iteration even 100% spherical peeled potatoes are OK, but ideally would be good to peel as little as possible, to minimize the wastes.</li>
</ol>

<p><strong>Question:</strong>
I know that we're very, very far away from building such a system. Nevertheless, what are the purely <strong>technical</strong> difficulties which needs to be solved for such a robot to be built? </p>

<p><strong>EDIT</strong></p>

<p>Let's assume we stick to this design and not invent something radically different, like solving the problem with chemistry by dissolving the skin with something. I know that the problem of peeling the potatoes is currently being solved by other means - mainly by applying friction and a lot of water.</p>

<p>This question is not about it. I am asking specifically about the problems to be solved with the two-arms setup using the humanlike approach to peeling.</p>
","robotic-arm"
"8512","How to connect absolute encoder on the rotating shaft. Please see the three options?","<p><a href=""http://i.stack.imgur.com/IpAjs.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/IpAjs.png"" alt=""enter image description here""></a></p>

<p><a href=""http://i.stack.imgur.com/nzqzu.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/nzqzu.png"" alt=""enter image description here""></a></p>

<p>Hi,
Here I have added 2 options for connecting encoder on shaft.
Motor, gearhead and shaft is connected using coupling. But where will be best place for encoder (To avoid backlash from coupling and gearhead).
whether through hollow encoder is available? (see option 1).
I dont know which one will be best for this kind of system. 
Which one is widely using arrangements?</p>

<p><strong>Options 3</strong> is Encoder will be placed before the motor.</p>
","motor"
"8516","Getting pitch, yaw and roll from Rotation Matrix in DH Parameter","<p>I've calculated a DH Parameter matrix, and I know the top 3x3 matrix is the Rotation matrix. The DH Parameter matrix I'm using is as below, from <a href=""https://en.wikipedia.org/wiki/Denavit%E2%80"" rel=""nofollow"">https://en.wikipedia.org/wiki/Denavit%E2%80</a><a href=""http://i.stack.imgur.com/ZJ2LC.jpg"" rel=""nofollow""><img src=""http://i.stack.imgur.com/ZJ2LC.jpg"" alt=""enter image description here""></a></p>

<p>Above is what I'm using. From what I understand I'm just rotating around the Z-axis and then the X-axis, but most explanations from extracting Euler angles from Rotation matrixes only deal with all 3 rotations. Does anyone know the equations? I'd be very thankful for any help.</p>
","kinematics forward-kinematics dh-parameters"
"8519","Fanuc Robot ""Heat"" control","<p>My work has an older Fanuc robot ( Arc Mate 100-iBe RJ3iB, Fanuc AWE2 teach pendant with  Powerwave 355M) and the old operator/programmer has left. I have taken over his job and cant find out how to turn down the voltage and wire feed speed because it occasionally burns through parts. I tried manually putting in voltage and wire feed speed but it seems it will only accept the previous weld schedules 1-8 and if i mess with them that will affect other programs using those. I just need someone to please point me in the right direction. 
P.S. Typed on phone , sorry if sloppy.</p>
","industrial-robot"
"8522","Recommendation for 3D mechanism modeling and simulation software","<p>I'm working on a robotic hand and I would like to simulate different joints and tendon insertion points before starting to actually build it.</p>

<p>I've been googling and found things like Solidworks and Autodesk, that seem very costly for a hobbyst like me but also I don't quite fully understand their capabilities (just CAD? 3D modelling but not simulation? Simulation but not interactive?). I've also found things like FreeCAD which seem to me somehow abandoned or just for CAD and not for simulation.</p>

<p>Another requirement would be interactivity of the simulation, not just rendering.</p>

<p>I don't have a problem with commercial software, but I'm looking for a reasonable cost for a hobbyst, not an engineering company.</p>

<p>Is there a software out there that meets all this requirements? Or should I use several programs each for a specific purpose?</p>

<p>Thanks!</p>
","design mechanism software simulator 3d-model"
"8525","Quadrocopter problem with stability","<p>I'm building quadcopter from scratch, software is implemented on STM32F4 microcontroller. Frequency of main control loop equals 400Hz.</p>

<p>I've though everything is almost finished but when i've mounted everything and started calibration of PIDs i faced a problem.
It was impossible to adjust PID parameters properly.
So i started test with lower power (not enough to fly) and i've managed quite fast adjust PID for roll but when i've increased power problems with control came back.</p>

<p>After that i've done more measurements.
<a href=""http://i.stack.imgur.com/2Bix8.jpg"" rel=""nofollow""><img src=""http://i.stack.imgur.com/2Bix8.jpg"" alt=""Roll angle with engines off""></a>
<a href=""http://i.stack.imgur.com/y2878.jpg"" rel=""nofollow""><img src=""http://i.stack.imgur.com/y2878.jpg"" alt=""Roll angle with engines on but without blades from complementary filter""></a></p>

<p>I didn't make test with blades but probably this is even worse and that is why i cannot calibrate it.</p>

<ul>
<li>If problem is due to vibration how can i fix it?</li>
<li>If something else is cause of that symptom, what is it?</li>
<li>Can i solve this through better controls and data fusion algorithms?
Now i use complementary filter for acc and gyro sensors data fusion in roll and pitch.</li>
</ul>
","control imu accelerometer gyroscope"
"8526","Which USB interface for Android device I can use for motor driver","<p>I am new to robotics,
I will be controlling DC motors from Android device through USB</p>

<p>For this I have selected L298N motor controller(After watching YouTube videos )
And got some DC motors</p>

<p>I have no idea how do I connect this to Android device via USB cable</p>

<p>Help appreciated</p>

<p>Ref:
<a href=""https://www.bananarobotics.com/shop/L298N-Dual-H-Bridge-Motor-Driver"" rel=""nofollow"">https://www.bananarobotics.com/shop/L298N-Dual-H-Bridge-Motor-Driver</a>
<a href=""https://youtu.be/XRehsF_9YQ8"" rel=""nofollow"">https://youtu.be/XRehsF_9YQ8</a></p>

<p>PS: All I know is programming android</p>
","motor usb"
"8527","Which brushless dc and propeller to choose?","<p>I have a small bot(around 4-5kg with wheels) which is to be pushed without contact by another bot. I plan to do this using a bru and a propeller. I am having problems selecting the right combination. Please help me with these questions:-</p>

<ol>
<li>Should the bldc be high kv or low kv(will i need high rpm or low rpm)</li>
<li>What is the  ideal propeller to use with the motor so that i can create enough thrust to get the 'small' bot in motion and keep it in motion?</li>
<li>What are the other criteria i should keep in mind while selecting.</li>
</ol>
","brushless-motor"
"8538","Setting up a gimbal with the Dji Wookong-M but using a separate transmitter and receiver","<p>I just bought the Wookng-M for my multi copter and I was wondering if it is possible to have a two man system where one controls the main craft with one transmitter and the other controls the camera and Gimbal with another transmitter.
I know you can set up a gimbal with the Wookong-M but I wasn't if you could use a separate transmitter to control it or even control it at all. I am also waiting for my 3-axis GoPro gimbal to arrive but can I even control the pan or do I have to control the gimbal completely separate and not use the Wookong-M for the gimbal and camera.</p>

<p>I am a beginner and this is my 1st time working with the Wookong-M so please keep the answers understandable.</p>

<p>Thanks in Advance</p>
","quadcopter"
"8539","How can I increase the resolution of a PWM signal?","<p>Say I have a motor and I want it to spin at exactly 2042.8878 revolutions per minute.  Say I have a very precise sensor to detect the RPM of the motor to a resolution of 1/1000th of a revolution per minute.  </p>

<ul>
<li><p>Can I produce a PWM signal which can match the speed to that degree of 
precision?  </p></li>
<li><p>What variables in the signal parameters would I have to adjust to get the precision if possible?  </p></li>
<li><p>Would I have to use additional circuitry between the motor and the driver?  </p></li>
<li><p>Would I have to design the signal/circuitry around the specific specifications of the motor?</p></li>
<li><p>Should I just use a stepper motor?</p></li>
</ul>

<p>This is assuming I am using a microcontroller to measure the motor's speed and adjust the signal in real-time to maintain a certain speed.</p>
","motor microcontroller stepper-motor pwm stepper-driver"
"8544","Pole-balancing / inverted-pendulum; is there a need for active control?","<p>Not sure if I am posting this question in the correct community, as it relates primarily to reinforcement learning. Apologies early on if this is not so.</p>

<p>In reinforcement learning many algorithms exist for 'solving' the cart-pole problem; that of balancing a mass on the edge of a stick, connected to a cart on a hinge, which has 1 DoF. There is TD learning, Q-learning and many other on and off-policy methods. There is also the more recent, model-based policy search method <a href=""http://mlg.eng.cam.ac.uk/pilco/"" rel=""nofollow"">PILCO</a>.</p>

<p>What I am really wondering, I suppose is more of a physics question: is there a need for active control? Why is it not possible to find the one point for the cart, which prevents the mass to move, even incrementally, left or right as it sits atop the pole? Why does it always 'fall'?</p>
","control"
"8546","Modelling Point Clouds for Collision Detection in Gazebo","<p>I am currently applying path planning to my robotic arm (in Gazebo) and have chosen to use an RRT. In order to detect points of collision, I was thinking of getting a Point Cloud from a Kinect subscriber and feeding it to something like an Octomap to have a collision map I could import into Gazebo. However, there is no Gazebo plugin to import Octomap files and I do not have enough experience to write my own. The next idea would be to instead feed this point cloud to a mesh generator (like Meshlab) and turn that into a URDF, but before starting I'd rather get the input of somebody far more experienced. Is this the right way to go? Keep in mind the environment is static, and the only things moving are the arms. Thank you. Below is just a picture of an octomap.<a href=""http://i.stack.imgur.com/B8OEZ.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/B8OEZ.png"" alt=""enter image description here""></a></p>
","robotic-arm localization slam kinect gazebo"
"8549","DH parameters and Kinematic Decoupling","<p>Is it possible to decouple a 5DOF manipulator? 
This question I asked earlier and I believe I got the right answers but I never show the drawings of the manipulator and now I'm hesitating during setup of the DH parameters for Forward Kinematics. See drawing depicted here.<a href=""http://i.stack.imgur.com/fUCIF.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/fUCIF.png"" alt=""enter image description here""></a> </p>

<p><a href=""http://i.stack.imgur.com/QiEgK.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/QiEgK.png"" alt=""enter image description here""></a></p>
","kinematics forward-kinematics dh-parameters"
"8555","Which trajectory planning algorithm for minimizing jerk","<p>In order to perform a cyclic task, I need a trajectory planning algorithm. This trajectory should minimize jerk and jounce.</p>

<p>When I search for trajectory planning algorithms, I get many different options, but I haven't found one which satisfies my requirements in terms of which values I can specify. An extra complicating factor is that the algorithm should be used online in a system without too much computing power, so mpc algorithms are not possible...</p>

<p>The trajectory I am planning is 2D, but this can be stripped down to 2 trajectories of 1 dimention each. There are no obstacles in the field, just bounds on the field itself (minimum and maximum values for x and y)</p>

<p>Values that I should be able to specify:</p>

<ul>
<li>Total time needed (it should reach its destination at this specific
time) </li>
<li>Starting and end position</li>
<li>Starting and end velocity</li>
<li>Starting and end acceleration</li>
<li>Maximum values for the position.</li>
</ul>

<p>Ideally, I would also be able to specify the bounds for the velocity, acceleration, jerk and jounce, but I am comfortable with just generating the trajectory, and then checking if those values are exceeded.</p>

<p>Which algorithm can do that?</p>

<p>So far I have used fifth order polynomials, and checking for limits on velocity, acceleration, jerk and jounce afterwards, but I cannot set the maximum values for the position, and that is a problem...</p>

<p>Thank you in advance!</p>
","control algorithm"
"8556","Length and Width of a Line Following Robot","<p>I'm building a line following robot. I have made different chassis designs. The main prototype I'm using is a rectangle base. At one side motors are placed. On the other side of the rectangle caster wheel is placed in the middle. Look at the following image.</p>

<p><a href=""http://i.stack.imgur.com/e89HK.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/e89HK.png"" alt=""Robot Design Prototype""></a></p>

<p>By varying the values of <code>distance</code>, I have seen that the stability of the robot is varying rapidly. </p>

<p>I'm driving the robot using <code>PID</code>. I have seen that for some chassis designs it is very hard(sometimes impossible) to calculate correct constant values. And for some chassis it is very easy. By the word <strong>stability</strong> I meant this. I have a feeling that the robot dimensions, <code>distance</code> values and that stability has a relationship..</p>

<p>Is there an equation or something that can be used to estimate the value of the <code>distance</code> when the width of the robot is known..?</p>

<p>Other than that is there a relationship between robot weight and diameter of the wheel or robot dimensions and the diameter..?</p>

<p>Thanks for the attention!!</p>
","arduino motor pid line-following"
"8559","Is it possible to design robot software or AI to function in different devices?","<p>so i'm really interested in robotics.. I'm not really a robot expert as i have no experience on creating one. I just like them.  Anyway, I am always wondering if its possible to build a robot that can transfer itself to different devices and still function. I mean, if you want that robot to transfer itself(THE DATA that making it function or whatever you call it) to your laptop so you can still use it while you are away or anything.. Does creating one require advanced computing and knowledge? Is it kind of creating a artificial intelligence?. When it think of this i would always thought of J.A.R.V.I.S since he can go to Stark Suit and communicate with him.</p>

<blockquote>
  <p>Translated into robotics terminology by a roboticist:</p>
  
  <p>Is it possible to create software for controlling robot hardware that can transfer itself to different devices and still function. Could it transfer itself to your laptop and collaborate with you using information it gathered while it was in it's robot body? 
  Does creating software like this require advanced knowledge and computing? Is software like this considered to be artificial intelligence?</p>
</blockquote>

<p>I am serious about this question sorry to bother or if anyone will be annoyed./ </p>
","mobile-robot"
"8561","Is Venetian mirror possible in Autodesk Inventor?","<p>I see there are things like glass and mirror in Autodesk Inventor Professional 2016 but is there a possibility to have Venetian mirror? So that from one side it would look like a mirror and from the other side it would look like a transparent glass?</p>
","design mechanism 3d-printing 3d-model visualization"
"8562","Humanoid balancing","<p>I'm currently working on Humanoid robot. I've solved the Forward &amp; Inverse Kinematic relations of the robot, and they turn out to be fine. Now I want to move onto Walking. I've seen tons of algorithms &amp; research papers but none of them make the idea clear. I understand the concept of ZMP &amp; what the method tries to do, but I simply can't get my head around all the details that are required to implement it in a real robot. Do I have to plan my gait &amp; generate the trajectories beforehand, solve the joint angles, store them somewhere &amp; feed it to the motors in real-time? or Do I generate everything at run-time(a bad Idea IMO)? Is there a step-by-step procedure that I can follow to get the job done? or Do I have to crawl all my way through those Research papers, which never make sense(at least for me).</p>
","mobile-robot stability humanoid"
"8565","velocity of link (i+1) with respect to frame (i+1)","<p>I read several textbooks but could not find a good explanation.
Can anybody tell me why velocity of link (i+1) with respect to frame (i+1) is not zero?
My argument is: Since the velocity of the link (i+1) is the velocity of origin of the frame (i+1) it should be zero with respect to itself.</p>

<p>!<a href=""http://i.stack.imgur.com/IsUcT.png"" rel=""nofollow"">from ""Introduction to Robotics
Mechanics and Control""</a></p>
","kinematics forward-kinematics"
"8570","Calculate robot heading to follow wall and avoid obstacles","<p>I have a task that involves implementing robot behaviour that will follow wall and avoid obstacles along it's path. The robot must stay at desired distance from the wall but also stick to it so it should not loose sight of it. Robot is sensing it's surrounding with ultrasonic sensor that is oscillating from left to right and filling an array of small length (10 values) with detected distances (every 10 degrees). From this reading I would like to calculate heading vector that will result in robot path similar to one shown in bottom picture:</p>

<p><strong>Black(walls), red(obstacles), blue(robot), green(desired path)</strong>
<a href=""http://i.stack.imgur.com/YtVGG.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/YtVGG.png"" alt=""Path example""></a></p>
","wheeled-robot navigation"
"8575","PID tuning for 6 dof robotic arm","<p>I'm currently developing a 6 dof robotic arm. The arm is vibrating when it stop moving and I want to reduce it. Another thing is that arm is so heavy (because there is a projector inside it, lol) and I have to use spring between joints. So, can anyone tell me 1. how to select springs because my supervisor told me that proper selection of springs can reduce vibration? 2. how do I tune the PID parameters? All the joints are dynamixel servos and their PID parameters are tunable. I read article about tuning for a single servo. How do I tune these parameters for the whole arm?</p>
","control pid robotic-arm"
"8579","Change PWM values according to encoder output","<p>I have a motor with an encoder. When I set the speed of the motor it should change its speed so that encoder readings per second should fit an equation $y = ax^2 + bx + c$ where <code>x</code> is <code>speed</code> value that is given to the motor and <code>y</code> is the encoder readings per second that should get with motor.</p>

<p>Encoder reading is counted in every <code>1ms</code> and if it is not equal to the value of the encoder output should get from motor (it is calculated using the equation), the PWM input to the motor should vary in-order to get desired encoder output.</p>

<p>I want to control this value using a <code>PID</code> controller but I'm confused in writing equations. Any help would be appreciated..</p>
","motor pid pwm"
"8580","Arduino Power Adapters","<p>I'm shopping for my first Arduino with a specific goal in mind. I need to attach 3 standard servo motors, an ArduCam Mini 2MP camera, and several LEDs. I'm trying to figure out power requirements. I assume that USB power won't be sufficient. I'm looking at 12V AC-to-DC outlet adapters and I noticed that Amps vary from ~500MA to 5A. I don't want to use batteries.</p>

<p>What would you recommend as minimum amperage for this setup? Is there a maximum amperage for Arduino boards? I don't want to plug it in and burn it out. If I plug in both the USB cable and a power adapter at the same time, is power drawn from both cables?</p>

<p>Thanks!</p>
","arduino power"
"8585","ROS and Kinect data without callbacks","<p>I'd like to get rgb and depth data from a kinect, and I found a little tutorial here: <a href=""http://wiki.ros.org/cv_bridge/Tutorials/ConvertingBetweenROSImagesAndOpenCVImagesPython"" rel=""nofollow"">http://wiki.ros.org/cv_bridge/Tutorials/ConvertingBetweenROSImagesAndOpenCVImagesPython</a>. It's fine, but what I'd like is to be able to get the data on demand, and not as whenever the callback is triggered, assuming I won't try to get the data faster than it can be available. I'd appreciate any help - do go easy on the ROS jargon, I'm still learning...Thanks.</p>
","ros kinect"
"8586","How can I measure the actual speed and distance traveled of the robot with an external setup?","<p>Good day to all.</p>

<p><strong><em>First of all, I'd like to clarify that the intention of this question is not to solve the localization problem that is so popular in robotics.</strong> However, the purpose is to gather feedbacks on how we can actually measure the speed of the robot with external setup. The purpose is to be able to compare the speed of the robot detected by the encoder and the actual speed, detected by the external setup.</em></p>

<p>I am trying to measure the distance traveled and the speed of the robot, but the problem is it occasionally experiences slippage. Therefore encoder is not accurate for this kind of application. </p>

<p>I could mark the distance and measure the time for the robot to reach the specified point, but then I would have to work with a stopwatch and then transfer all these data to Excel to be analyzed. </p>

<p>Are there other ways to do it? It would be great if the external setup will allow data to be automatically sent directly to a software like Matlab. My concern is more on the hardware side. Any external setup or sensors or devices that can help to achieve this?</p>

<p>Thanks.</p>
","mobile-robot control wheeled-robot"
"8590","How can I measure the height of an object with a single sharp sensor (GP2Y0A21YK0F)?","<p>I have one sharp sensor and I have to use it to measure the height of a block (6cm - 12 cm). How can I accomplish this ?
Actually it is to be connected to a robot which will move near the box and determine its height.</p>

<p>About GP2Y0A21YK0F:
<a href=""http://www.sharpsma.com/webfm_send/1489"" rel=""nofollow"">http://www.sharpsma.com/webfm_send/1489</a></p>

<p>The robot is like this: <a href=""http://i.imgur.com/8qT8zeQ.jpg"" rel=""nofollow"">http://i.imgur.com/8qT8zeQ.jpg</a></p>

<p><a href=""http://i.stack.imgur.com/2pyYc.jpg"" rel=""nofollow""><img src=""http://i.stack.imgur.com/2pyYc.jpg"" alt=""Robot picture ""></a></p>

<p>If possible please suggest a solution that doesn't require moving the sensor.
But any method will do fine.</p>
","mobile-robot first-robotics"
"8592","Manipulator end-effector orientation with quaternions","<p>I have the following problem:</p>

<p>Given 3 points on a surface, I have to adjust a manipulator end-effector (i.e. pen) on a Baxter Robot, normal to that surface.</p>

<p>From the three points I easily get the coordinate frame, as well as the normal vector. My question is now, how can I use those to tell the manipulator its supposed orientation.</p>

<p>The Baxter Inverse Kinematics solver takes a $(x,y,z)$-tuple of Cartesian coordinates for the desired position, as well as a $(x,y,z,w)$-quaternion for the desired orientation. What do I set the orientation to? My feeling would be to just use the normal vector $(n_1,n_2,n_3)$ and a $0$, or do I have to do some calculation?</p>
","inverse-kinematics orientation"
"8595","In order to integrate MCL and Occupancy Grid to implement Grid-based FastSLAM, do you have to record all data?","<p>It's unclear as to how one goes about integrating Occupancy Grid mapping and Monte Carlo localization to implement SLAM.</p>

<p>Assuming <strong>Mapping</strong> is one process, <strong>Localization</strong> is another process, and some motion generating process called <strong>Exploration</strong> exist. Is it necessary to record all data as sequenced or with time stamps for coherence? </p>

<p>There's Motion: $U_t$, Map: $M_t$, Estimated State: $X_t$, Measurement: $Z_t$</p>

<p>so..</p>

<ul>
<li><p>each <strong>Estimated state</strong>, $X_t$, is a function of the <strong>current motion</strong>, $U_t$, <strong>current measurement</strong>, $Z_t$, and <strong>previous map</strong>, $M_{t-1}$;</p></li>
<li><p>each <strong>confidence weight</strong>, $w_t$, of estimated state is a function of <strong>current measurement</strong>, $Z_t$,  <strong>current estimate state</strong>, $X_t$, and <strong>previous map</strong>, $M_{t-1}$;</p></li>
<li><p>then each <strong>current map</strong>, $M_t$ is a function of <strong>current measurement</strong>, $Z_t$, <strong>current estimated state</strong>, $X_t$,  and <strong>previous map</strong>, $M_{t-1}$.</p></li>
</ul>

<p>So the question is, is there a proper way of integrating mapping and localization processes? Is it something you record with timestamp or sequences? Are you suppose to record all data, like FullSLAM, and maintain full history. 
How can we verify they are sequenced at the same time to be referred to as <strong>current</strong> (i.e. measurement) and <strong>previous</strong> (measurement).</p>
","slam occupancygrid"
"8596","What can this picture/data tell?","<p>I've implemented a model of a ball-on-plate plant and am controlling it over a network. Below is the open loop output when excited by successive sinusoidal inputs with increasing frequencies. I know that the plant is open loop unstable, and it is cool that this figure so nicely captures the instability. </p>

<p>What I'd like to know is if there is other information that I can glean about the plant from the relationship between the input and the output state.</p>

<p>(The state is clipped at 3.1 units.)</p>

<p><a href=""http://i.stack.imgur.com/qWBlU.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/qWBlU.png"" alt=""Green reference signal and blue observed system state""></a></p>
","control balance distributed-systems"
"8598","Comparison of lifting systems","<p>What kind of systems can be used to make a torso lifting system like the one used by this robot (the black part) :</p>

<p><a href=""http://i.stack.imgur.com/r6okd.jpg"" rel=""nofollow""><img src=""http://i.stack.imgur.com/r6okd.jpg"" alt=""enter image description here""></a></p>

<ul>
<li>Rack and pinion</li>
<li>lead screw</li>
<li>scissor lift</li>
<li>can a triple tree help ? </li>
</ul>

<p>What are the pro and cons of each system ?
How do they ensure stability ?
And finally, is there a way to draw current when lowering instead of drawing current when lifting ?</p>
","mechanism"
"8600","ROS + kinect depth data duplication","<p>I am trying to get depth data from a Kinect in a ROS project. It currently looks like this:</p>

<p><a href=""http://i.stack.imgur.com/I02fY.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/I02fY.png"" alt=""enter image description here""></a></p>

<p>To arrive at this, I've done:</p>

<pre><code>depth_sub = rospy.Subscriber(""/camera/depth/image"", Image, depth_cb)
...
def depth_cb(data):
    img = bridge.imgmsg_to_cv2(data, ""32FC1"")
    img = np.array(img, dtype=np.float32)
    img = cv2.normalize(img, img, 0, 1, cv2.NORM_MINMAX)
    cv2.imshow(""Depth"", img)
    cv2.waitKey(5)
</code></pre>

<p>I also launch openni.launch from the openni_launch package, which publishes the depth data.</p>

<p>I also get this weird warning from the node (can be seen in the image):
    ComplexWarning: Casting complex values to real discards the imaginary part.</p>

<p>But as I understand it the data type is an array of 32-bit floats. Yet some of the values appear as <code>nan</code>.</p>

<p>I would like a depth image that directly corresponds to a RGB image array of the same size. I will be doing some tracking in the RGB space, and using the tracked coordinates (X,Y) from that to index into the depth array. Thanks. </p>

<p><strong>edit:</strong>
Turns out, <code>/camera/depth/image</code> is published as an array of uint8s, but the actual data is 32bit floats (which is not listed anywhere, had to hunt it down in other people's code). Thus an array of 480x640 uint8s, interpreted as 32bit floats, in effectively ""quartered"" in the number of data points. Which could explain how the image is 4 times smaller (and hence accessing datapoints out of bounds = nan?), but not why there are two of them.</p>
","ros kinect"
"8601","Choosing a proper sampling time for a PID controller","<p>I have a robotic system I'm controlling with Arduino, is there an heuristic way to determine a proper sampling time for my PID controller? Considering I have some other things to compute on my sketch that require time, but of course a good sampling time is crucial.</p>

<p>Basically I have a distance sensor that needs to detect at a constant rate an object that is moving, sometimes slow, sometimes fast. I don't have a good model of my system so I can't actually tell the physical frequency of the system.</p>
","pid"
"8605","Mechanical robustness/shock resistance LiPo batteries","<p>How mechanically robust are LiPo batteries? How much force or acceleration can they maximally withstand before failure? What is their (mechanical) shock resistance?</p>

<p>For some electrical components used in robots, such as IMU's, it can be found in datasheets that they can suffer mechanical failure if accelerated or loaded beyond given values. For IMU's, this is typically somewhere between $2000g$ and $10000g$ (where $1g = 9.81 m/s^2$).</p>

<p>I'm wondering if similar values are known for LiPo batteries, since they are known to be vulnerable components. But, is there any quantification known for their claimed vulnerability?</p>
","battery"
"8607","Open source implementations for GPS+IMU sensor fusion?","<p>Are there any Open source implementations of GPS+IMU sensor fusion (loosely coupled; i.e. using GPS module output and 9 degree of freedom IMU sensors)? -- kalman filtering based or otherwise.</p>

<p>I did find some open source implementations of IMU sensor fusion that merge accel/gyro/magneto to provide the raw-pitch-yaw, but haven't found anything that includes GPS data to provide filtered location and speed info.</p>
","kalman-filter imu sensor-fusion gps"
"8611","IPhone controlled RC car","<p>I have an R.C car and there is a program in my computer in which I can code the car to perform movements.I would like to have an application with a visual design.Where it shows the cars path.</p>

<p>Is there available software code for this? Saves me lots of time.</p>
","software research"
"8612","Combustion engine controlled with a remote","<p>How can one control a combustion engine using a remote control.
Or how would you make a car controlled using a remote.</p>
","robotic-arm first-robotics"
"8617","What is $\alpha \sin(\theta) + \beta \frac{d \theta}{d t}$ in the inverted pendulum problem?","<p>I am preparing for an exam in neural networks. As an example for self-organizing maps they showed the inverted pendulum problem where you want to keep the pole vertical:</p>

<p><a href=""http://i.stack.imgur.com/CK9VU.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/CK9VU.png"" alt=""enter image description here""></a></p>

<p>Now the part which I don't understand:</p>

<blockquote>
  <p>$$f(\theta) = \alpha \sin(\theta) + \beta \frac{\mathrm{d} \theta}{\mathrm{d} t}$$
  Let $x= \theta$, $y=\frac{\mathrm{d} \theta}{\mathrm{d} t}$, $z=f$.</p>
  
  <p>Solution with SOM:</p>
  
  <ul>
  <li>three-dimensional surface in $(x,y,z)$</li>
  <li>adapt two-dimensional SOM to surface</li>
  <li>Method of control
  
  <ul>
  <li>For a given $(x,y)$ find neuron $k$ for wich $w_k = [w_{k1}, w_{k2}, w_{k2}, w_{k3}]$</li>
  <li>$f(\theta)$ is then $w_{k3}$</li>
  </ul></li>
  </ul>
</blockquote>

<p>I guess we use the SOM to learn the function $f$. However, I would like to understand where $f$ comes from / what it means in this model.</p>
","control stability machine-learning"
"8618","Does C have advantages over C++ in robotics?","<p>I want to build robots, and right now I aim to work with Arduino boards
I know that they are compatible with c and c++, so was wondering which language is better for robotics in general?</p>

<p>I know how to write in java, and the fact that c++ is object oriented makes it look like a better choice for me</p>

<p>does c have any advantages over c++?</p>
","arduino c++ c"
"8621","Forward kinematic computing the transformation matrix","<p>I am the moment trying to compute the transformation matrix for robot arm, that is made of 2 joints (serial robot arm), with which I am having some issues. L = 3, L1 = L2 = 2, and q = ($q_1$,$q_2$,$q_3$) = $(0 , \frac{-\Pi}{6},\frac{\Pi}{6})$</p>

<p>Based on this information I have to compute the forward kinematic, and calculate the position of each joint. </p>

<p>Problem here is though, how do I compute the angle around x,y,z.. for the transformation matrix.  Using sin,cos,tan is of course possible, but what do their angle corresponds? which axis do they correspond to?</p>

<p><a href=""http://i.stack.imgur.com/EIBNn.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/EIBNn.png"" alt=""Sketch of the robot""></a></p>

<p>I tried using @SteveO answer to compute the $P_0^{tool}$ using the method he provided in his answer, but I somehow mess up something, as the value doesn't resemble the answer given in the example.. </p>

<p><a href=""http://i.stack.imgur.com/zcDfO.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/zcDfO.png"" alt=""Mathmatica calculation""></a> </p>
","kinematics forward-kinematics orientation"
"8631","IMU sensor and compensation","<p>Hi I'm using ""minImu 9"" 9 DOF IMU (gyro, accelerometer and compass) sensor and it gives pitch roll and yaw values with a slope on desktop (no touch, no vibration, steady). Y axis is angle in degree and X axis is time in second. X axis length is 60 seconds. How can fix this?</p>

<p>Pitch</p>

<p><a href=""http://i.stack.imgur.com/D0fH9.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/D0fH9.png"" alt=""enter image description here""></a></p>

<p>Roll</p>

<p><a href=""http://i.stack.imgur.com/7hI8v.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/7hI8v.png"" alt=""enter image description here""></a></p>

<p>Yaw</p>

<p><a href=""http://i.stack.imgur.com/gr6XY.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/gr6XY.png"" alt=""enter image description here""></a></p>

<p>Note1: <a href=""https://github.com/pololu/minimu-9-ahrs-arduino"" rel=""nofollow"">minIMU code</a></p>
","sensors imu sensor-fusion"
"8636","sum_error in PID controller","<p>I'm trying to implement a PID controller by myself and I've a question about the sum_error in I control. Here is a short code based on the PID theory.</p>

<pre><code>void pid()
{
  error = target - current;

  pTerm = Kp * error;

  sum_error = sum_error + error * deltaT ;
  iTerm = Ki * sum_error;

  dTerm = Kd * (error - last_error) / deltaT;
  last_error = error;

  Term = K*(pTerm + iTerm + dTerm);
}
</code></pre>

<p>Now, I start my commands:</p>

<p>Phase 1, If at t=0, I set target=1.0, and the controller begins to drive motor to go to the target=1.0,
Phase 2, and then, at t=N, I set target=2.0, and the controller begins to drive motor to go to the target=2.0</p>

<p>My question is, in the beginning of phase 1, the error=1.0, the sum_error=0, and after the phase 1, the sum_error is not zero anymore, it's positive. And in the beginning of phase 2, the error=1.0 (it is also the same with above), but the sum_error is positive. So, the iTerm at t=N is much greater than iTerm at t=0.</p>

<p>It means, the curves between phase 2 and phase 1 are different!!!</p>

<p>But to end-user, the command 1, and the command 2 is almost the same, and it should drive the same effort.</p>

<p>Should I set the sum_error to zero or bound it? Can anyone tell me how to handle the sum_error in typical?</p>

<p>Any comment will be much appreciated!!</p>

<p>Kevin Kuei</p>
","pid"
"8642","Discontinuity in device orientation","<p>Why is there a discontinuity in the quaternion representation of my device orientation?</p>

<p>I'm using a <a href=""http://www.pnicorp.com/products/sentral-mm/"" rel=""nofollow"">SENtral+PNI RM3100+ST LSM330</a> to track orientation. I performed the following test:</p>

<ol>
<li>Place the device in the center of a horizontal rotating plate (""lazy susan"").</li>
<li>Pause for a few seconds.</li>
<li>Rotate the plate 360° clockwise.</li>
<li>Pause for a few seconds.</li>
<li>Rotate the plate 360° clockwise again.</li>
</ol>

<p>I got this output, which appears discontinuous at sample #1288-1289.
<a href=""http://i.stack.imgur.com/B4BQA.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/B4BQA.png"" alt=""enter image description here""></a></p>

<p>Sample #1288 has  <code>(Qx,Qy,Qz,Qw) = (0.5837, 0.8038, 0.0931, 0.0675)</code>, but sample #1289 has <code>(Qx,Qy,Qz,Qw) = (0.7079, -0.6969, -0.0807, 0.0818)</code>.</p>

<p>Plugging in the formulas on page 32 of <a href=""http://www.pnicorp.com/wp-content/uploads/Sentral-MandM-Technical-Datasheet_rG.pdf"" rel=""nofollow"">this document</a>, this corresponds to a change in orientation from <code>(Heading, Pitch, Roll) = (108°, 0°, 142°)</code> to <code>(Heading, Pitch, Roll) = (-89°, 0°, 83°)</code>.</p>

<p>The <a href=""http://i.imgur.com/ZQLhDVg.png"" rel=""nofollow"">graph of (Heading, Pitch, Roll)</a> is also not continuous mod 90°.</p>

<p>Does this output make sense? I did not expect a discontinuity in the first plot, since the unit quaternions are a covering space of SO(3). Is there a hardware problem, or am I interpreting the data incorrectly?</p>

<p><strong>Edit:</strong> The sensor code is in <a href=""http://pastebin.com/raw/sT2F7f5g"" rel=""nofollow"">central.c</a> and <a href=""http://pastebin.com/raw/qu8RFx4z"" rel=""nofollow"">main.c</a>. It is read with this <a href=""http://pastebin.com/raw/7QXJ00cq"" rel=""nofollow"">Python script</a>.</p>
","sensors calibration orientation"
"8644","Is RobotBASIC outdated?","<p>I found this website <a href=""http://robotbasic.org/"" rel=""nofollow"">http://robotbasic.org/</a> and it talks about a language used for programming things related to robotics, and I want to make sure whether or not it's worth investing any time or energy into compared to other languages before I just wipe it from my browser bookmarks for good. Nowadays, are there better languages and methods for going about the same things that it talks about? </p>

<p>I mean, the site looks pretty old, like something from the late 90s or pre-2010, plus I never heard of it anywhere except for this site, so I wonder if it's just not relevant any more if it ever was.</p>
","control software"
"8654","Finding cubic polynomial equation for 3 joints","<p>My professor gave us an assignment in which we have to find the cubic equation for a 3-DOF manipulator. The end effector is resting at A(1.5,1.5,1) and moves and stops at B(1,1,2) in 10 seconds. How would I go about this? Would I use the Jacobian matrix or would I use path planning and the coefficient matrix to solve my problem. I'm assuming coefficient matrix but I am not given the original position in angle form. I was only taught how to use path planing when the original angles are given.</p>
","motion-planning inverse-kinematics motion jacobian"
"8657","P gain tuning for quadcopter (Is my perception for a P-gain too high correct?)","<p>Good day,</p>

<p>I am currently working on a project using Complementary filter for Sensor fusion and PID algorithm for motor control. I viewed a lot of videos in youtube as well as consulted various blogs and papers with what to expect with setting the P gain to high or too low.</p>

<p>P Gain too low</p>

<blockquote>
  <p>easy over correction and easy to turn by hand</p>
</blockquote>

<p>P Gain too high</p>

<blockquote>
  <p>oscillates rapidly</p>
</blockquote>

<p>I have a sample video of what I think a high P gain (3 in my case) looks like. Do this look like the P gain is too high?  <a href=""https://youtu.be/8rBqkcmVS1k"" rel=""nofollow"">https://youtu.be/8rBqkcmVS1k</a> </p>

<p>From the video:</p>

<p>I noticed that the quad sometimes corrects its orientation immediately after turning few degrees (4-5 deg). However, it does not do so in a consitent manner.</p>

<p>It also overcorrects.</p>

<p>The reason behind my doubt is because the quadcopter doesn't react immediately to changes. I checked the complementary filter. It updates (fast) the filtered angle reading from sudden angular acceleration from the gyro as well as updates the long term filtered angle changes from the accelerometer (albeit slowly). If I am right, is the the P gain is responsible for compensating the ""delay""?</p>

<p>The formula I used in the complementary filter is the following:</p>

<pre><code>float alpha = 0.98;
float pitchAngleCF=(alpha)*pitchAngleCF+gyroAngleVelocityArray.Pitch*deltaTime)+(1-alpha)*(accelAngleArray.Pitch);
</code></pre>

<p>Here is a video for a P gain of 1: <a href=""https://youtu.be/rSBrwULKun4"" rel=""nofollow"">https://youtu.be/rSBrwULKun4</a></p>

<p>Your help would be very appreciated :)</p>
","quadcopter pid sensor-fusion tuning filter"
"8661","Calculating required torques for a given trajectory using Lagrange-Euler","<p>I have a 2DOF robot with 2 revolute joints, as shown in the diagram below. I'm trying to calculate (using MATLAB) the torque required to move it but my answers don't match up with what I'm expecting.
<a href=""http://i.stack.imgur.com/K7K0t.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/K7K0t.png"" alt=""Robot configuration""></a></p>

<p>Denavit-Hartenberg parameters:
$$
\begin{array}{c|cccc}
joint &amp; a &amp; \alpha &amp; d &amp; \theta \\
\hline
1 &amp; 0 &amp; \pi/2 &amp; 0 &amp; \theta_1 \\
2 &amp; 1 &amp; 0 &amp; 0 &amp; \theta_2 \\
\end{array}
$$</p>

<p>I'm trying to calculate the torques required to produce a given acceleration, using the Euler-Lagrange techniques as described on pages 5/6 in <a href=""http://www.worldacademicunion.com/journal/1746-7233WJMS/wjmsvol05no01paper02.pdf"" rel=""nofollow"">this paper</a>.
Particularly,
$$ T_i(inertial) = \sum_{j=0}^nD_{ij}\ddot q_i$$
where
$$ D_{ij} = \sum_{p=max(i,j)}^n Trace(U_{pj}J_pU_{pi}^T) $$
and
$$ 
J_i = \begin{bmatrix}
{(-I_{xx}+I_{yy}+I_{zz}) \over 2} &amp; I_{xy} &amp; I_{xz} &amp; m_i\bar x_i \\
I_{xy} &amp; {(I_{xx}-I_{yy}+I_{zz}) \over 2} &amp; I_{yz} &amp; m_i\bar y_i \\
I_{xz} &amp; I_{yz} &amp; {(I_{xx}+I_{yy}-I_{zz}) \over 2} &amp; m_i\bar z_i \\
 m_i\bar x_i &amp; m_i\bar y_i &amp; m_i\bar z_i &amp; m_i \end{bmatrix}
$$</p>

<p>As I was having trouble I've tried to create the simplest example that I'm still getting wrong. For this I'm attempting to calculate the inertial torque required to accelerate $\theta_1$ at a constant 1 ${rad\over s^2}$. As $\theta_2$ is constant at 0, I believe this should remove any gyroscopic/Coriolis forces. I've made link 1 weightless so its pseudo-inertia matrix is 0. I've calculated my pseudo-inertia matrix for link 2:
$$
I_{xx} = {mr^2 \over 2} = 0.0025\\ I_{yy} = I_{zz} = {ml^2 \over 3} = 2/3
$$
$$
J_2 =\begin{bmatrix}
1.3308 &amp; 0 &amp; 0 &amp; -1 \\
0 &amp; 0.0025 &amp; 0 &amp; 0 \\
0 &amp; 0 &amp; 0.0025 &amp; 0 \\
-1 &amp; 0 &amp; 0 &amp; 2 \\
\end{bmatrix}
$$</p>

<p>My expected torque for joint 1:
$$ 
T_1 = I\ddot \omega \\
T_1 = {ml^2 \over 3} \times \ddot \omega \\
T_1 = {2\times1\over3}\times1 \\
T_1= {2\over3}Nm
$$</p>

<p>The torque calculated by my code for joint 1:</p>

<pre><code>q = [0 0];
qdd = [1 0];
T = calcT(q);
calc_inertial_torque(1, T, J, qdd) 
</code></pre>

<p>$$
T_1={4\over3}Nm
$$</p>

<p><strong>So this is my problem</strong>, my code $T_1$ doesn't match up with my simple mechanics $T_1$.</p>

<p>The key functions called are shown below.</p>

<pre><code>function inertial_torque_n = calc_inertial_torque(n, T, J, qdd)
    inertial_torque_n = 0;
    for j = 1:2
        Mnj = 0;
        joint_accel = qdd(j);
        for i = 1:2
            Uij = calcUij(T, i, j);
            Ji = J(:,:,i);
            Uin = calcUij(T, i, n);
            Mnj = Mnj + trace(Uin*Ji*transpose(Uij));
        end
        inertial_torque_n = inertial_torque_n + Mnj * joint_accel;
    end
end

function U=calcUij(T,i,j)  
    T(:,:,j) = derivative(T(:,:,j));

    U = eye(4,4); 
    for x = 1:i
        U = U*T(:,:,x);
    end
end

function T = derivative(T)
    dt_by_dtheta = [0 -1  0  0
                    1  0  0  0
                    0  0  0  0
                    0  0  0  0];

    T = dt_by_dtheta*T;
end
</code></pre>

<p>I realise this is a fairly simple robot, and a complicated process - but I'm hoping to scale it up to more DOF once I'm happy it works. </p>
","dynamics matlab torque"
"8663","How to program an NXT brick to always hit a ball?","<p>I am using a mindstorm robot with an NXT brick, using the graphical interface to create the program. Part of the course my robot will take includes a black line on a white background. At the end of the line there is a gap, and after the gap there is a semi circular line. There is a ball that the robot has to hit soon after the robot crosses the gap. </p>

<p>The robot has a small code to follow the black line for a certain amount of time, enough time so that it stops just before the gap. The the robot just runs forward for 1 second across the gap, then the robot swings an arm to hit the ball, and after that I have the code for line-following again. However, once it crosses the gap, the robot stops in a different place every time, so the arm usually misses the ball. Is it possible to program the robot to hit the ball every time/almost every time in the GUI?   </p>

<p>Among other things I have tried using the ultrasonic sensor to detect the ball, but the sensor does not pick it up.</p>
","nxt mindstorms"
"8665","Color sensor alternatives","<p>I am making a white line follower. I am using an <a href=""http://www.ebay.com/itm/TCRT5000-IR-reflex-sensors-Obstacle-avoidance-modules-tracing-sensor-module-/171344744992?hash=item27e4f15220:g:flgAAOxyB0VRw8n-"" rel=""nofollow"">IR sensor module</a> based on <a href=""http://www.vishay.com/docs/83760/tcrt5000.pdf"" rel=""nofollow"">TCRT5000</a>. I am directly taking the 8bit ADC reading from Arduino Uno and printing the values on serial monitor. I observe that the values for white are around 25-35, which is ok. The problem arises when I try detecting an Orange (158C) surface. The sensor gives me values very close to that of white which is around 25-40. </p>

<p>I can use a <a href=""http://www.ebay.com/itm/TCS230-TCS3200-Color-Recognition-Sensor-Detector-Module-3V-5V-For-MCU-Arduino-/131189365488?hash=item1e8b7edaf0:g:xjQAAOSwnDxUcwFf"" rel=""nofollow"">color sensor</a> but they are bulky and I am not sure how I can get readings faster with them since they take a finite time for sampling 'R','G' and 'B' pulses. Can someone please tell me an alternate approach to detecting the colours or any other possible solution to my problem.</p>

<p>EDIT: I would like to add that the line I wish to follow is 3cm in width. Hence I plan to use three sensors. Two just outside the line on either sides and one exactly at the centre. The sampling frequency of Arduino UNO is around 125KHz. Sampling IR is not an issue because it is quick but using a color sensor takes a lot of time.</p>
","sensors line-following"
"8667","Visual servoing - tracking a point","<p>I am trying resolve some issues i am having with some inverse kinematics. </p>

<p>the robot arm i am using has a camera at the end of it, at which an object is being tracked. I can from the the camera frame retrieve a position, relative to that that frame but how do i convert that position in that frame, to an robot state, that set all the joint in a manner that  the camera keep the object at the center of the frame?...</p>

<p><strong>-- My approach --</strong> </p>

<p>From my image analysis i retrieve a position of where the object i am tracking is positioned => (x,y) - coordinate.</p>

<p>I know at all the time the position (a) of the end tool by the T_base^tool - matrix, and from the image analysis i know the position (b) of the object relative to the camera frame for which i compute the difference as such c = b - a. </p>

<p>I then compute the image jacobian, given the C, the distance to the object and the focal length of the camera. </p>

<p>So... thats where i am at the moment.. I am not sure whether the position change retrieved from the cam frame will be seen as position of the tool point, at which  the equation will become un undetermined as the length of the state vector would become 7 instead of 6.  </p>

<p>The equation that i have must be 
$$J_{image}(q)dq = dp$$</p>

<ul>
<li>J_image(q)[2x6]: being the image jacobian of the robot at current
state q </li>
<li>dq[6x1]: wanted change in q-state </li>
<li>dp[2x1]: computed positional change...</li>
</ul>

<p>Solution would be found using linear least square.. 
but what i don't get is why the robot itself is not appearing the equation, which let me doubt my approach.. </p>
","inverse-kinematics visual-servoing"
"8670","Robotic Manipulator","<p>I have started working on robotic manipulators and got into a project which deals with control of robotic manipulator using artificial neural networks (solution of inverse kinematics and trajectory generation, to be precise!).
Can someone please suggest me where to start as I have no prior knowledge about robotic manipulator and ANN and how to code them?</p>
","arduino robotic-arm inverse-kinematics machine-learning"
"8671","Is RoboCup still vivant and significant robotic issue?","<p>A couple years ago RoboCup competitions seems to be quite vivant issue. Now when I'm looking for some info about it, it seems to be some kind of insignificant but this may be only my first impression (I was looking for 2D simulator league and it seems that it does not even exist anymore).</p>

<p>So is RoboCup still alive and significant robotic issue?</p>
","soccer"
"8673","Is there a .NET library for conveyor belt automation?","<p>I'm a software developer and I work for a company that I think could use some automation in its warehouse. I thought it would be fun to put together a prototype of a conveyor system that automates a manual sorting process that we do on our warehouses. I'm primarily a .NET developer so I'm wondering if there is an .NET SDK for conveyor automation. </p>

<p>Any other information on where to start would be helpful but is not my main question here. </p>
","automation"
"8680","calculating position based on accelerometer data","<p>Please help me with the following task. I have MPU 9150 from which I get acceleration/gyro and magnetometer data. What I'm currently interested in is to get the orientation and position of the robot. I can get the position using quaternions. Its quite stable. Rarely changes when staying still.
But the problem is in converting accelerometer data to calculate the displacement.</p>

<p>As I know its required to to integrate twice the accel. data to get position.
Using quaternion I can rotate the vector of acceleration and then sum it's axises to get velocity then do the same again to get position. But it doesn't work that way. First of all moving the sensor to some position and then moving it back doesn't give me the same position as before. The problem is that after I put the sensor back and it stays without any movement the velocity doesn't change to zero though the acceleration data coming from sensors are zeros.</p>

<p>Here is an example (initially its like this):
the gravity: -0.10  -0.00   1.00<br>
raw accel: -785 -28 8135<br>
accel after scaling to +-g: -0.10   -0.00   0.99<br>
the result after rotating accel vector using quaternion: 0.00   -0.00   -0.00</p>

<p>After moving the sensor and putting it back it's acceleration becomes as:
 0.00   -0.00   -0.01
 0.00   -0.00   -0.01
 0.00   -0.00   -0.00
 0.00   -0.00   -0.01
and so on.
If I'm integrating it then I get slowly increasing position of Z.</p>

<p>But the worst problem is that the velocity doesn't come back to zero</p>

<p>For example if I move sensor once and put it back the velocity will be at:
-0.089 for vx and
0.15 for vy</p>

<p>After several such movements it becomes:
-1.22 for vx
1.08 for vy 
-8.63 for vz</p>

<p>and after another such movement:</p>

<p>vx -1.43
vy 1.23
vz -9.7</p>

<p>The x and y doesnt change if sensor is not moving but Z is changing slowly.
Though the quaternion is not changing at all.</p>

<p>What should be the correct way to do that task?</p>

<p>Here is the part of code for integrations:</p>

<pre><code>vX += wX * speed;
vY += wY * speed;
vZ += wZ * speed;

posX += vX * speed;
posY += vY * speed;
posZ += vZ * speed;
</code></pre>

<p>Currently set speed to 1 just to test how it works.</p>

<p><strong>EDIT 1:</strong> Here is the code to retrieve quaternion and accel data, rotate and compensate gravity and get final accel data.</p>

<pre><code>        // display initial world-frame acceleration, adjusted to remove gravity
        // and rotated based on known orientation from quaternion
        mpu.dmpGetQuaternion(&amp;q, fifoBuffer);
        mpu.dmpGetAccel(&amp;aaReal, fifoBuffer);
        mpu.dmpGetGravity(&amp;gravity, &amp;q);

        //Serial.print(""gravity\t"");
        Serial.print(gravity.x);
        Serial.print(""\t"");
        Serial.print(gravity.y);
        Serial.print(""\t"");
        Serial.print(gravity.z);
        Serial.print(""\t"");


        //Serial.print(""accell\t"");
        Serial.print(aaReal.x);
        Serial.print(""\t"");
        Serial.print(aaReal.y);
        Serial.print(""\t"");
        Serial.print(aaReal.z);
        Serial.print(""\t""); 

        float val = 4.0f;
        float ax = val * (float)aaReal.x / 32768.0f;
        float ay = val * (float)aaReal.y / 32768.0f;
        float az = val * (float)aaReal.z / 32768.0f; 

        theWorldF.x = ax;            
        theWorldF.y = ay;
        theWorldF.z = az;

        //Serial.print(""scaled_accel\t"");
        Serial.print(ax);
        Serial.print(""\t"");
        Serial.print(ay);
        Serial.print(""\t"");
        Serial.print(az);
        Serial.print(""\t""); 

        theWorldF.x -= gravity.x;
        theWorldF.y -= gravity.y;
        theWorldF.z -= gravity.z;

        theWorldF.rotate(&amp;q);
        //gravity.rotate(&amp;q);
        //Serial.print(""gravity_compensated_accel\t"");
        Serial.print(theWorldF.x);
        Serial.print(""\t"");
        Serial.print(theWorldF.y);
        Serial.print(""\t"");
        Serial.print(theWorldF.z);
        Serial.print(""\t"");
        Serial.print(deltaTime); 
        Serial.println();
</code></pre>

<blockquote>
  <p>EDIT 2:</p>
</blockquote>

<p>dmpGetQuaternion, dmpGetAccel functions are just reading from the FIFO buffer of MPU. </p>

<p>dmpGetGravity is:</p>

<pre><code>uint8_t MPU6050::dmpGetGravity(VectorFloat *v, Quaternion *q) {
    v -&gt; x = 2 * (q -&gt; x*q -&gt; z - q -&gt; w*q -&gt; y);
    v -&gt; y = 2 * (q -&gt; w*q -&gt; x + q -&gt; y*q -&gt; z);
    v -&gt; z = q -&gt; w*q -&gt; w - q -&gt; x*q -&gt; x - q -&gt; y*q -&gt; y + q -&gt; z*q -&gt; z;
    return 0;
}
</code></pre>

<p><strong>EDIT 3:</strong>
the library for using MPU 9150:
<a href=""https://github.com/sparkfun/MPU-9150_Breakout"" rel=""nofollow"">https://github.com/sparkfun/MPU-9150_Breakout</a></p>

<p><strong>EDIT 4: Another example</strong> </p>

<p>gravity vector: -1.00   -0.02   0.02<br>
raw accel data: -8459   -141    125 
accel data scaled (+-2g range): -1.03   -0.02   0.02<br>
gravity compensation and rotation of accel data: -0.01  0.00    0.33    </p>
","accelerometer algorithm"
"8684","Is there a way to use a stress-ball-like device as acceleration control interface","<p>I am thinking of a project proposal for my robotics course and we are required to make one that has a potential application on physical therapy or medical fields. One thing that came across my mind is a motorized wheelchair that moves when a stress ball control is squeezed by the user. As a robotics novice, I wonder if I could integrate a sensor circuit with a rubber ball so that when it is pressed, perhaps by a stroke patient, it triggers some driver circuit. is this possible? if so, how? My experience with robotics is limited to arduino, servo motors and basic sensors.</p>
","wheeled-robot"
"8686","increase current draw from serial port of icreate 2","<p>This is from the icreate 2's document:
""Pins 1 and 2 (Vpwr) are connected to the Roomba battery through a 200 mA PTC resettable fuse. The continuous draw from these two pins together should not exceed 200 mA. Do not draw more than 500 mA peak from these pins, or the fuse will reset.""
My project just need to draw a bit more than that number. So is there anyway to disable - short circuit - that fuse or replace that fuse with a bigger one? where does that fuse reside on the bot's circuit board?
If the above 2 is not possible (because the fuse is embedded inside some chip or too difficult to access for example), is it safe to run a small wire from the battery pole to the pin to by pass that fuse?</p>

<ul>
<li>I know I can run a wire directly from battery pole to my circuit or draw power from the motor wires, but I love running through the serial port with keep things simple.</li>
</ul>
","power irobot-create serial"
"8689","How do I if know my quad is powerful enough to take off vertically smoothly and how do I calculate max Thrust(g) for given current(A) and voltage(V)?","<p>If I build my quadcopter with the following components, will it take off vertically smoothly?</p>

<p>Frame: F450 glass fiber and polyamide nylon [280g]</p>

<p>Landing Gear: High Landing Gear for F450 [90g]</p>

<p>Battery: 3000mAh 20C (30C burst) 3s/1p [260g]</p>

<p><a href=""http://robu.in/shop/a2212-1400kv-brushless-motor-30a-esc-1045-propeller-set/"" rel=""nofollow"">Motor</a>: XXD A2212 1400KV brushless outrunner motor [4x50g]</p>

<p>ESC: 30A (40A burst) Brushless with BEC - 2A/5V [4x25g]</p>

<p>Prop: 1045 Propeller set</p>

<blockquote>
  <p>Assume that the total weight of the quad is ~1.1Kg and I would like to have payload upto 400g, making the quad weigh ~1.5Kg</p>
</blockquote>

<p>From all that I've learnt, the Thrust:Weight ratio should never fall below 1.7:1 (and a 2:1 is recommended to have better control) which if does creates problems in lifting my quad vertically smoothly. I'm neither planning to have very high maneuverability nor cruise the sky pushing my quad's limits. I just want it to fly.</p>

<blockquote>
  <p>Here's the motors pull(g)-amps(A)-voltage(V)-power(W)-specificThrust(g/W) information for A2212 1400kv with 10 x 4.5 Props</p>
</blockquote>

<p><a href=""http://i.stack.imgur.com/d1KqE.jpg"" rel=""nofollow""><img src=""http://i.stack.imgur.com/d1KqE.jpg"" alt=""Motor thrust-amps-power information""></a></p>

<p>Since my battery's discharge rate is limited to 20C, if I'm not wrong, it can give out only 60A (since its 3000mAh) for the entire circuit with four motors, each of which can take 15A max, which from the above table would produce ~600g thrust, which is 2400g from all four motors.</p>

<blockquote>
  <p>Does this mean I can get 1.6:1 Thrust:Weight at 100% throttle? Can my quad still take off vertically smoothly?</p>
</blockquote>

<p>I'm also confused about the efficacy of my motor, meaning if I buy a bigger motor, can get more than 600g thrust at 15A/11.1V with same/bigger props? If not, is this the most efficient combination of motor and prop? </p>

<blockquote>
  <p>If yes, what is the maximum thrust (practical pulling force in g, not in N) I can get outta 15A/11.1V? Which equation tells defines that exact relation, provided I fly my quad at usual conditions (1013hPa/25degC extTemp/80%-90% relHumidity/max15mAlti)?</p>
</blockquote>

<p>PS: I tried ecalc, Prop Power Thrust and Efficiency Calculations, and a few other online stuff.</p>

<p><strong>Update</strong>: Will replacing the 3000mAh-20c/30c battery with the 5000mAh-20c/30c 3s1p keeping everything same solve some problems and increase the flight time, provided I keep everything else the same?</p>
","quadcopter brushless-motor battery lithium-polymer"
"8692","What is the max thrust a 30W brushless motor can produce? Can it produce 300g?","<p>i am a newbie,
i got a 30W motor here,<br>
<a href=""http://www.rcbazaar.com/product.aspx?productid=1915"" rel=""nofollow"">http://www.rcbazaar.com/product.aspx?productid=1915</a></p>

<pre><code>Specification:
Motor: Avionic M1818/17 KV4500 MICRO brushless motor
KV (rpm/v): 4500
Power: 30W
Winds: 17
Resistance: 263 mOhm
Idle current: 0.8 A
Weight: 9gms

Combination of usage:
PROP - 6x4 E
● Get  300 gms Thrust
Lipo - 2 cell 7.4V
ESC - 20 amp
</code></pre>

<p>They claim to have 300g thrust,however compaired to other motors like on hobbyking,
this motor seems to have too good ((power input )/thrust) ratio.
could it be true?<br>
Besides,esc of 6 amp (30/7.4) seems sufficient,why did they recommend 20amp?<br>
Also,what is the max thrust i can get from a motor w.r.t. power consumed in decent price.</p>
","quadcopter brushless-motor"
"8693","How to perform odometry on an arduino for a differential wheeled robot?","<p>I am using a differential wheel robot for my project. I need to know the current coordinates of the robot with respect to it's initial position taken as the origin. I m doing the computation on an Arduino UNO and the only sensory input that I get is from the two encoders. I have the function updateOdomenty() called in the loop() and this is it's corresponding code:</p>

<pre><code>void updateOdometry()
{

  static int encoderRPosPrev = 0;
  static int encoderLPosPrev = 0;


  float SR = distancePerCount * (encoderRPos - encoderRPosPrev);
  float SL = distancePerCount * (encoderLPos - encoderLPosPrev);

  encoderRPosPrev = encoderRPos;
  encoderLPosPrev = encoderLPos;


  x += SR * cos(theta);           
  y += SL * sin(theta);
  theta += (SR - SL) / wheelDistance;

  if(theta &gt; 6.28)
    theta -= 6.28;
  else if(theta &lt; -6.28)
    theta += 6.28;
}
</code></pre>

<p>This is the code that me and my team mates made after reading <a href=""http://rossum.sourceforge.net/papers/DiffSteer/"" rel=""nofollow"">this</a> paper. I am wondering if this is the best possible way to solve this problem with an Arduino. If not, how is odometry done in differential wheeled systems?</p>
","arduino kinematics odometry differential-drive"
"8698","What exactly are PPM controlled ESCs? Are most ESCs available to build quadcopters PPM Controlled?","<p>First, I don't see any manufacturer or online store telling whether an ESC is PPM controlled or not. Second, I have also been Googling and asking comments in <a href=""https://www.youtube.com/watch?v=OZNxbxL7cdc"" rel=""nofollow""><em>all about ESCs</em></a> youtube videos for long, but I couldn't find anything useful.</p>

<blockquote>
  <p>Why do I need PPM controlled ESCs?</p>
</blockquote>

<p>I'm doing a project based on <a href=""https://github.com/romainbaud/andro-copter"" rel=""nofollow"">AndroCopter</a> and its clearly mentioned that it specifically requires the use of PPM controlled ESCs.</p>

<blockquote>
  <p>Can I use any ESCs available in the market for this project?</p>
</blockquote>

<p>It's also mentioned in the Github repo that PPM controlled ESCs are the most common ones. However from some who explained ESCs in Youtube video has commented back for my doubt telling that most common ESCs are PWM controlled which is contradicting the previous statement.</p>

<p><strong>PS</strong>: I need to use Arduino Mega to control the four ESCs. And Arduino Mega is programmed to send out PPM signals which is exactly why I need PPM controlled ESCs. Correct me if I made any mistakes.</p>
","arduino quadcopter esc"
"8700","inverse kinematics osciliations..","<p>I am the moment having some issues with an Jacobian going towards a singularity (i think)as some of its values becomes close to zero, and my robot oscillates, and therefore thought that some form of optimization of the linear least square solution is needed. </p>

<p>I have heard about the interior point method, but I am not sure on how i can apply it here. </p>

<p>My equation is as this.. </p>

<p>J(q)dq = du </p>

<p>How would i have to implement the optimization, and would be needed?</p>
","inverse-kinematics c++"
"8702","Malfuction in motors using L298N","<p>I have purchased the undermentioned robot chassis with DC motors supported with plastic gears from a local store. There is a 3A battery holder and when I connect robot to that and put it on the ground both the motors working fine and smooth. But when I connect the L298N motor controller which was made by myself and tested with proteus, only one motor is working. When I switch the wires of both motors, still the motor which worked before is working and the other motor never runs unless I manually give a little rotation to the wheel.</p>

<p>I use a PIC18F4431 to control the motor controller and tried USB power and a 5V regulator created by myself and the result is the same in both occasions. What could be the issue here? If I tried oiling the malfunctioning motor will it work? I heard one of my friends having the same issue, one motor is not working. But both motors work with just two pen torch batteries and can't think of a valid reason that the motor is faulty. May be my motor controller? But when I swap the wires from faulty to the working one, the working one still works as I've mentioned above.</p>

<p><a href=""http://i.stack.imgur.com/Vog3A.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/Vog3A.png"" alt=""Robot car chassis""></a></p>

<p><a href=""http://i.stack.imgur.com/pSqRz.gif"" rel=""nofollow""><img src=""http://i.stack.imgur.com/pSqRz.gif"" alt=""enter image description here""></a><a href=""http://i.stack.imgur.com/O5gOs.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/O5gOs.png"" alt=""enter image description here""></a></p>
","mobile-robot"
"8703","PixHawk or Naza M V2 for Aerial Imaging of Small Study Area with Hexacopter","<p>I have a project that is going to involve capturing ~80 images of a 35m x 50m agricultural study area for image processing.</p>

<p>I am wondering whether to use a NAZA v2 or PixHawk controller to outfit my ship (on a DJI F550 Flamewheel airframe).</p>

<p>My understanding is that Naza is limited as far as the amount of waypoints I can have during a particular mission. Can I break my imaging mission up into 8 or 10 sub-missions?</p>

<p>I also understand that PixHawk is in many ways superior but getting it up and running can be more finicky - I am on a limited time frame for this project.</p>
","gps uav radio-control"
"8704","Using another device instead of RC transmitter","<p>I want to make pc controlled quadrotor. All the tutorials/projects made with rc receiver. I want to use arduino or xbee instead of rc receiver for pc control of quadrotor. How can I do this. </p>

<p>Note: I have arduino, beaglebone, xbee, hc-05, KK2 and multiwii parts. </p>
","pid quadcopter"
"8705","Problem with HC-SR04 sensor","<p>I have three HC-SR04 sensors connected to a PIC18F4431 with the schematic provided below. Before building the PCB I've tried testing all three sensors with a bread board and they worked fine. But now I have my PCB and when I connect them to that and tried testing, they work only for a few seconds and then stop working.</p>

<p>I set timers and a set of LEDs to lit if an item is in between 40cm from each sensor. As I've tried from the bread board, when I cross my arm withing that range, the appropriate bulbs are lit and when I take my arm off the other bulbs are lit. But using the PCB, when I upload the code through a PICKIT2 they work fine for a few seconds and then they freeze. If I reset the MCLR pin it works again for another few seconds and freeze again. And sometimes randomly if I touch the receiving part of the sensor it works but that happens randomly. Not always working. What could be the issue? </p>

<p>Is my oscillator burnt while I was soldering it? Once I connected two 0.33uF polar capacitors and found out that for one second, it takes one minute or more to blink a bulb.</p>

<p><a href=""http://i.stack.imgur.com/uI7mT.jpg"" rel=""nofollow""><img src=""http://i.stack.imgur.com/uI7mT.jpg"" alt=""enter image description here""></a>
<a href=""http://i.stack.imgur.com/uZ2jv.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/uZ2jv.png"" alt=""enter image description here""></a></p>
","mobile-robot"
"8709","BiRRT: Getting path from an array of 7 DOF angle configurations","<p>I've kind of finished implementing a BiRRT for a 7 DOF arm, using a KD-tree from numpy.spatial in order to get nearest queries. A picture is below:</p>

<p><a href=""http://i.stack.imgur.com/QA2Qj.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/QA2Qj.png"" alt=""BIRRT pseudocode""></a></p>

<p>I'm currently having trouble with the fact that it is impossible to retrieve a path from the start node to a particular node using a KD-tree, and while I do have an array of of all the nodes, and there are edges that can be calculated between subsets of the array, but the edges are not in any useful order. Can anyone give me some tips on how I'd retrieve a path from the starting node in the first array, and the ending node in the second array? Are there any useful data structures that would let me do this? Below is my code</p>

<pre><code>def makeLine(distance, q_near, xrand, nodes):
    num = int((distance)/0.01)

    for i in range(1, num+1):
        qnext = (xrand - q_near)/distance * 0.01 * i + q_near
        #check for collision at qnext, if no collision detected:
            nodes = numpy.append(nodes, qnext)
        #else if there is collision, return 0, nodes, ((xrand - q_near)/distance*0.01*(i-1)+q_near
    return 1, nodes, qnext

def BIRRT(start, goal):

    startNode = numpy.array([start])
    goalNode = numpy.array([goal])
    limits = numpy.array([[-2.461, .890],[-2.147,1.047],[-3.028,3.028],[-.052,2.618],[-3.059,3.059],[-1.571,2.094],[-3.059,3.059]])
    for i in range(1, 10000):
        xrand = numpy.array([])
        for k in range(0, len(limits)):
            xrand = numpy.append(xrand, random.uniform(limits[k,:][0], limits[k,:][1]))

        kdTree = scipy.spatial.KDTree(startNode[:, 0:7])
        distance, index = kdTree.query(xrand)
        q_near = kdTree.data[index]     


        success, startNode, qFinal = makeLine(distance, q_near, xrand, startNode)

        kdTree2 = scipy.spatial.KDTree(goalNode[:, 0:7])
        distance2, index2 = kdTree2.query(qFinal)
        q_near2 = kdTree2.data[index2]

        success, startNode, qFinal2 = makeLine(distance2, qFinal, q_near2, startNode)

        if success:
            return 1, startNode, goalNode, 1, qFinal, qFinal2

        xrand = numpy.array([])
        for k in range(0, len(limits)):
            xrand = numpy.append(xrand, random.uniform(limits[k,:][0], limits[k,:][1]))

        kdTree = scipy.spatial.KDTree(goalNode[:, 0:7])
        distance, index = kdTree.query(xrand)
        q_near = kdTree.data[index]     

        success, goalNode, qFinal = makeLine(distance, q_near, xrand, goalNode)

        kdTree2 = scipy.spatial.KDTree(startNode[:, 0:7])
        distance2, index2 = kdTree2.query(qFinal)
        q_near2 = kdTree2.data[index2]

        success, goalNode, qFinal2 = makeLine(distance2, qFinal, q_near2, goalNode)

        if success:
            return 1, startNode, goalNode, 2, qFinal, qFinal2
    return 0
</code></pre>
","inverse-kinematics motion-planning planning rrt"
"8710","ROS on Raspberry Pi Model 2: UbuntuARM vs ROSBerryPi","<p>Before I ask my question, I'd better confirm that I've read the most prominent post about <a href=""http://robotics.stackexchange.com/questions/230/can-ros-run-on-a-raspberry-pi/7844#7844"">running ROS on Raspberry Pi devices</a>.</p>

<p>That post contains some valuable information, but it's a bit dated, and ROS support for ARM devices is much better these days. In fact, ROS 2.0 is evidently going to have excellent support for <a href=""http://design.ros2.org/articles/why_ros2.html#new-use-cases"">running on embedded devices</a> like the Raspberry Pi.</p>

<p>I just got a Pi model 2 for my birthday, and I'm really eager to get ROS running on it so I can build a robot I've been working on, which is based on the Wild Thumper 6WD platform.</p>

<p>From my perspective, here are a few pros &amp; cons regarding UbuntuARM and ROSBerryPi:</p>

<p>UbuntuARM</p>

<p><strong>Pros:</strong></p>

<ul>
<li><p>Ubuntu is the official ROS distro and is the most well-supported ROS OS </p></li>
<li><p>The best documentation on the ROS wiki for running on ARM devices is written for UbuntuARM</p></li>
</ul>

<p><strong>Cons:</strong></p>

<ul>
<li>Raspbian (on which ROSBerryPi is based) is the official distro for Rasbperry Pi and thus has the best support for the board.</li>
</ul>

<p><hr/>
ROSBerryPi</p>

<p><strong>Pros:</strong></p>

<ul>
<li><p>Raspbian (on which ROSBerryPi is based) is the official distro for Rasbperry Pi and thus has the best support for the board.
<strong>Cons:</strong></p></li>
<li><p>ROS is not well supported on OS's other than Ubuntu</p></li>
<li>To use the ROSBerryPi distro, you must build ROS from source.</li>
</ul>

<p>My question is: can anyone provide any further insight into this dilemma? If you've been running ROS on your Raspberry Pi 2 (model 2 only please; the model B+ has completely different issues, like not being well-supported by Ubuntu), what's your experience been? </p>

<p>Which distro would/did you choose, and why?</p>
","ros raspberry-pi arm"
"8711","How to convert rotation matrix in to equivalent Quaternion using Eigen Library.","<p>Eigen library (<a href=""http://goo.gl/cV5LY"" rel=""nofollow"">http://goo.gl/cV5LY</a>), which is used extensively in ROS and PCL.</p>

<p>Thank you.</p>
","ros"
"8717","3 degrees of freedom analytical solution","<p><a href=""http://i.stack.imgur.com/IxIrc.jpg"" rel=""nofollow""><img src=""http://i.stack.imgur.com/IxIrc.jpg"" alt=""enter image description here""></a></p>

<p>I have got a robot that exactly looks like as shown in the figure above. I have worked out the inverse kinematics analytical solution without the base rotation (considering 2 dof alone) but I am not able to find the analytical solution including the base(3 dof). How do I find the anlytical solution for this robot ?? </p>
","robotic-arm"
"8719","Mean shape for initializing the training of a face shape regression tree","<p>I am studying an article about estimating facial points using forests of regressions (random forests). In order to train the trees of the forest, the training algorithm is initialized by a mean shape. I don't understand what a mean shape is. I looked it over the internet and came across it in articles that talk about face detection and facial shape estimation, but there was no clear explanation of it. Can you please tell me what exactly is a mean shape?</p>
","computer-vision artificial-intelligence"
"8720","Which algorithms are used in autonomous robot","<p>I am working on proposal of autonomous fire fight robots but I'm little bit confused about its sensor and algorithms. My friend suggested there are some path finding <code>AI</code> algorithms like <code>BFS</code>, <code>DFS</code>, <code>A*</code>, and <code>Dijkstra's Algorithm</code> which are used in robots, but I didn't believe it. </p>

<p>I want to ask: Are these algorithms used in real world robots or some other genetic algorithms? How does a robot discover path to detect, and differentiate, a human from fire?  I only want some explanation that gives knowledge.</p>
","algorithm machine-learning"
"8722","Microcontroller flashing itself","<p>Can a micro controller flash itself?
What i mean to say is, I have an STM32F103RG with 1Mb Flash Size. 
I have a UART Communication modem connected to it. Can i send a firmware (.HEX or .BIN) to the microcontroller via the radio verify checksums, on sucess the microcontroller saves the file into a SD Card ( via SPI ) and then restarts itself and start flashing itself reading from the file ( in the sD card )?</p>

<p>Can something like this be done or an external MCU is required to carry out the flashing?</p>

<p>The purpose is the microcontroller and radio will be sitting at a remote location and i need a way to change the microcontroller's firmware by sending it a firmware update file remotely. </p>
","microcontroller"
"8726","How to have a 'Auto Go Home' feature, like the DJI Phantom 3, on a project built quadcopter?","<p>What should a quadcopter have, or have access to, in order to make this 'return home' feature work? Is GPS enough? What is the approach needed to make this happen?</p>

<p>I used a Arduino Mega 2560 with IMU to stable my quadcopter.</p>
","arduino quadcopter sensors localization gps"
"8728","DC motor shaft and gear installation","<p>I'm hoping to use a DC motor to drive a cog bar horizontally along a track. AFAIK, I'd need to install a (plastic) cog on the motor shaft, which itself grips on the (plastic) cog bar. Does anyone know how to prevent the cog from shifting on the shaft? The shaft is 10mm long and I'd like to make sure the cog cog sits at 5mm, where the cog bar is.</p>

<p>Any help will be appreciated.</p>

<p>Thanks</p>
","motor"
"8731","Sending commands from Ubuntu","<p>I have a iRobot Create model 4400 and I need to send commands to the open interface through Ubuntu. I'm using gtkterm at 57600 baud but when I press play button, it only drives around itself. I have tried to send commands as raw data and as hexadecimal data but it doesn't work.</p>

<p>What am I doing wrong?</p>
","irobot-create roomba linux"
"8732","Weird magnetometer values","<p>I bought a 3-axis magnetometer (Similar to <a href=""https://www.adafruit.com/products/1746"" rel=""nofollow"">this one</a> ) And plugged into an Arduino in order to read the heading value.
I mounted it on my robot and I drove with the robot for around 30 meters, turned 180 degrees and drove back to my starting position. I plotted the heading value and it shows inconsistent values. The 180 degrees turn started at sec 55, the rest is driving in one direction using a joystick and following a wall as reference so small deviations are expected but not that big as in the image.</p>

<p><a href=""http://i.stack.imgur.com/wWHaz.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/wWHaz.png"" alt=""enter image description here""></a></p>

<p>When the robot is turning in-place, there is no such problem and the heading variation follows the rotation of the robot. The robot (Turtlebot) is a little bit shaky such that the magnetometer doesn't always have the x and y axes parallel to the floor but I don't think few degrees of offset can cause such a huge difference. I calculate the heading as follows:</p>

<blockquote>
  <p>heading = atan2(y field intensity, x field intensity)</p>
</blockquote>

<p>Why does this happen? Could it be form some metals or electric wires under the floor?
Can you suggest a more robust method/sensor for estimating the heading in indoor environments?</p>

<p>EDIT:</p>

<p>I drove the same path again and the pattern similarity is making it even weirder
<a href=""http://i.stack.imgur.com/G5eeT.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/G5eeT.png"" alt=""enter image description here""></a></p>
","mobile-robot navigation magnetometer"
"8742","Could a motor shaft be swapped for a threaded shaft?","<p>I'm looking at the <a href=""http://g01.a.alicdn.com/kf/HTB1i3odKpXXXXcgXpXXq6xXFXXXq/N20-DC-6V-74rpm-Micro-gear-Motor-with-Metal-Gear-Box-Low-speed-Motor-Flip-type.jpg"" rel=""nofollow"">N20 DC motor</a> which is fairly popular. Does anyone know if the shaft could be swapped out for a threaded shaft?</p>
","motor"
"8748","Phantom Omni type robot Inverse kinematics solution","<p>Guys my robot looks like this,<a href=""http://i.stack.imgur.com/hYAGJ.jpg"" rel=""nofollow""><img src=""http://i.stack.imgur.com/hYAGJ.jpg"" alt=""enter image description here""></a></p>

<p>Thanks to @croco I came to know that it looks much similar to Phantom Omni. Since it looks similar to the phantom Omni I am trying to get the inverse kinematics geometric solution for it. Using this inverse kinematics solution I can build my FPGA design. There is a very good research paper on this</p>

<p><a href=""http://ieeexplore.ieee.org/xpl/login.jsp?tp=&amp;arnumber=6318365&amp;url=http%3A%2F%2Fieeexplore.ieee.org%2Fxpls%2Fabs_all.jsp%3Farnumber%3D6318365"" rel=""nofollow"">http://ieeexplore.ieee.org/xpl/login.jsp?tp=&amp;arnumber=6318365&amp;url=http%3A%2F%2Fieeexplore.ieee.org%2Fxpls%2Fabs_all.jsp%3Farnumber%3D6318365</a></p>

<p>but I do not understand in page 2122 how they find the L3 and L4 (shown in the image below). If I find this for my robot then my project is almost done. How do I find L3 and L4 for my robot ? Should I just bring my end effector to the (0,0,0) position as shown in Fig2  amd then measure the L3 and L4, would this work ?? Would be great if you guys could help. Cheers.
<a href=""http://i.stack.imgur.com/xR8M0.jpg"" rel=""nofollow""><img src=""http://i.stack.imgur.com/xR8M0.jpg"" alt=""enter image description here""></a></p>
","inverse-kinematics geometry"
"8749","Bulding a robot arm for neural networks understanding","<p>I am thinking about building a small robotic arm with 4 small servo motors and an arduino uno to apply basic neural networks concepts.</p>

<p>Is it  a good idea to use a hand made robotic arm to learn more the power of neural networks?</p>

<p>Thank your for your time and Merry Christmas</p>
","control"
"8751","How do I go about implementing a Kalman Filter for a pose estimation algorithm?","<p>I am currently in the process of writing a pose estimation algorithm using image data. I receive images at 30 fps, and for every image, my program computes the x,y,z and roll, pitch, yaw of the camera with respect to a certain origin. This is by no means very accurate, there are obvious problems such as too much exposure in the image, not enough feature points in the image, etc., and the positions go haywire every once in a while; so I want to write a Kalman filter that can take care of this part.  </p>

<p>I have read through the basics of KF, EKF etc. and then I was reading through an OpenCV <a href=""http://docs.opencv.org/master/dc/d2c/tutorial_real_time_pose.html#gsc.tab=0"" rel=""nofollow"">tutorial</a> that has an implementation of a Kalman Filter inside an algorithm for the pose estimation of an object. While this matches my use case very well, I don't understand why they are using a linear Kalman Filter while explicitly specifying parameters like (dt*dt) in the state transition matrix. For reference, the state transition matrix they are considering is </p>

<pre><code>             /* DYNAMIC MODEL */
//  [1 0 0 dt  0  0 dt2   0   0 0 0 0  0  0  0   0   0   0]
//  [0 1 0  0 dt  0   0 dt2   0 0 0 0  0  0  0   0   0   0]
//  [0 0 1  0  0 dt   0   0 dt2 0 0 0  0  0  0   0   0   0]
//  [0 0 0  1  0  0  dt   0   0 0 0 0  0  0  0   0   0   0]
//  [0 0 0  0  1  0   0  dt   0 0 0 0  0  0  0   0   0   0]
//  [0 0 0  0  0  1   0   0  dt 0 0 0  0  0  0   0   0   0]
//  [0 0 0  0  0  0   1   0   0 0 0 0  0  0  0   0   0   0]
//  [0 0 0  0  0  0   0   1   0 0 0 0  0  0  0   0   0   0]
//  [0 0 0  0  0  0   0   0   1 0 0 0  0  0  0   0   0   0]
//  [0 0 0  0  0  0   0   0   0 1 0 0 dt  0  0 dt2   0   0]
//  [0 0 0  0  0  0   0   0   0 0 1 0  0 dt  0   0 dt2   0]
//  [0 0 0  0  0  0   0   0   0 0 0 1  0  0 dt   0   0 dt2]
 /  [0 0 0  0  0  0   0   0   0 0 0 0  1  0  0  dt   0   0]
//  [0 0 0  0  0  0   0   0   0 0 0 0  0  1  0   0  dt   0]
//  [0 0 0  0  0  0   0   0   0 0 0 0  0  0  1   0   0  dt]
//  [0 0 0  0  0  0   0   0   0 0 0 0  0  0  0   1   0   0]
//  [0 0 0  0  0  0   0   0   0 0 0 0  0  0  0   0   1   0]
//  [0 0 0  0  0  0   0   0   0 0 0 0  0  0  0   0   0   1]
</code></pre>

<p>I'm a little confused, so my main question can be broken down into three parts:</p>

<ol>
<li>Would a linear Kalman Filter suffice for a 6DOF pose estimation filtering? Or should I go for an EKF?</li>
<li>How do I come up with the ""model"" of the system? The camera is not really obeying any trajectory, the whole point of the pose estimation is to track the position and rotation even through noisy movements. I don't understand how they came up with that matrix.</li>
<li>Can the Kalman Filter understand that, for instance, if the pose estimation says my camera has moved half a meter between one frame and other, that's plain wrong, because at 1/30th of a second, there's no way that could happen?</li>
</ol>

<p>Thank you!</p>
","kalman-filter pose"
"8752","Automatic activation of a spray can","<p>I would like to wirelessly control the button on a spray can such that on pressing it the spray comes. For example, a deodorant bottle.</p>

<p>What is the best way to do this ?</p>

<p>What thing can I mount on the bottle to do this?</p>
","robotic-arm"
"8757","FastSlam 2.0 Implementation?","<p>I have studied <a href=""http://www.clausbrenner.de/slam.html"" rel=""nofollow"">Claus Brenner's lectures</a> on how to implement the FastSLAM 1.0 algorithm, where each particle maintains the robot pose, and maintains EKF's of the landmarks.</p>

<p>However, I'd like to implement FastSlam 2.0. Which I understand uses particle filters completely. Is this the same as FS 1.0, but instead of each particle maintaining an EKF of the landmark, each particle maintains yet another array of particle filters for landmarks?</p>
","slam"
"8758","2D Robot Motion","<p>Good day,</p>

<p>I have a robot with an IMU that tells Yaw Rate, and Magnetic heading. It also tells Xvelocity and YVelocity at that instance of the vehicle, on the vehicle frame. (So irrespective of heading, if the robot moved forward, yvelocity would change for example)</p>

<p>Assuming my robot starts at position (0,0) and Heading based on the Magnetic heading, I need to calculate the next position of the robot based on some world frame. How can I do this?</p>
","mobile-robot"
"8761","Completely autonomous traversal of a planar graph","<p>I have to program an autonomous robot to traverse through a grid given in the following figure.<br/>
But the main problem is that the nodes to visit is not known beforehand, it will be received by the bot in real time.<br/>
E.g.- after reaching the node 19, the bot now has to go to node 6. The shortest path(19-17-7-6) can be calculated by Dijkstra algo but i don't know how to make the robot traverse that path.<br/>
Any idea ?</p>

<p><a href=""http://i.stack.imgur.com/y7EBi.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/y7EBi.png"" alt=""enter image description here""></a></p>

<p>Edit: Sorry for not making the question clear enough.<br/>
I am facing the problem in determining the current position and the direction the robot is facing so i can't define the set of commands (turn left/right/forward) to traverse to the next desired node.<br/>
I am thinking about an extra array for previously visited nodes and the current node and an extra variable for facing direction.<br/>
But for that i will have to define command sets for each node from each and every node.<br/>
Any better idea ?</p>
","mobile-robot automatic dynamic-programming"
"8763","iRobot Create 2 to Vacuum?","<p>I just got a Create 2 for Christmas, and while I'm planning to create with it (obviously), I'd like to use it around the house as a vacuum if at all possible. I've heard that you can buy parts for the Roomba and throw them on to this chassis, but I wanted to confirm/refute that before I bought anything. Is that possible or am I crazy?</p>
","irobot-create"
"8765","Apps for Pepper robot","<p>I recently googled about Pepper robot and I wonder how one could write apps for it and get money for them. As far as I know they have app store, but does it sell apps or give them for free? (All info I googled myself is rather incomplete and old - probably outdated)</p>

<p>Also I believe that apps for such (or similar) robots is the potential multibillion-dollar market. What do you think about that?</p>
","mobile-robot"
"8768","Moving a small plate back and forth","<p>I'm hoping to move a plate (3MM x 45MM) back and forth using a DC motor. Here's my idea so far:</p>

<p><a href=""http://i.stack.imgur.com/mrN0d.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/mrN0d.png"" alt=""enter image description here""></a></p>

<p>The motor drives a threaded shaft which is attached on one side of the plate. To help alignment, a rod is added to the other side of the plate (red). My guess is that if it's just a rod in a through hole, it could potentially jam.</p>

<p>AFAIK, usually, in bigger setups, a linear bearing would come in handy. However, given that the plate is just 3MM thick, are there better ways to help alignment? Could making the edge around the through hole like the inside of a donut help? Something like</p>

<p><a href=""http://i.stack.imgur.com/oqM9j.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/oqM9j.png"" alt=""enter image description here""></a></p>

<p>Is it easy to make? In fact, is my concern actually valid?</p>

<p>Thanks</p>

<p><strong>EDIT</strong> The centre area of the plate needs to be kept clear. This is intended to be part of a (~10MM thick) pole climber, where several guide rollers are fitted on the left side of the plate and a motor driven roller is on the left of the part (not depicted). So the idea is the press the guide roller against the pole until the two rollers have a good grip on the pole. The whole car is fairly light, so the force expected is around 30N.</p>

<p>Here's a more complete depiction:</p>

<p><a href=""http://i.stack.imgur.com/AkVD0.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/AkVD0.png"" alt=""enter image description here""></a></p>

<p>The rollers are spring loaded, but they need to be released and retracted - and adjusted for different pole widths.</p>
","motor linear-bearing tracks"
"8775","Attaching a M5 screw shaft to a cog wheel","<p>Does anyone know of a good (low profile) way to attach the threaded (chamfered) end of a M5 screw shaft to a 30MM (30T 1M) plastic cog wheel? Would it be a good idea to widen the shaft hole and just let the threads grip the hole?</p>

<p><strong>EDIT</strong> The torque will be around 6kg-cm and the plastic I'm looking at is POM.</p>
","mechanism torque gearing"
"8779","Panasonic MSMO22F2G (stepper motor hook-up)","<p>I have a Panasonic MSMO22F2G Servo Motor that I'm using as a stepper motor. The motor has 4 wires coming out of a port farther to the front, and 11 wires coming out of a port towards the rear of the unit (presumed to be the encoder). I am trying to drive the motor with an arduino motor shield. </p>

<p>My question is this, how do i hook it up?</p>

<p>I have read that a stepper with 4 leads is a bipolar stepper, and that you group same coil wires together. I found that 3 wires on my stepper were on the same coil and the fourth seems to have no effect on the stepper. My checking process was using an ohm meter to see what was connected to what, as well as connecting wires and feeling the resistance.</p>
","arduino stepper-motor"
"8785","Roomba schedule opcode: 167, byte 1","<p>Just a short question: The iRobot Create 2 Open Interface spec says:</p>

<p>Serial sequence: [167] [Days] [Sun Hour] [Sun Minute] [Mon Hour] etc.</p>

<p>Can somebody explain to me, what ""Days"" stands for?</p>
","irobot-create"
"8786","Are there any Lithium Ion battery monitors designed for hobbyists (quadcopters)?","<p>I have a friend who is getting into quadcopters and being the good techie buddy, I'm trying to find the right technology for battery monitoring so his expensive machine does not fall out of the sky unexpectedly. </p>

<p>So far the only technology for hobbyists that I am seeing is voltage monitors, which aren't really useful for this battery chemistry. With the flat voltage curve LiIon has I'd expect a voltage monitor to falsely report a low battery when you draw extra current and indeed I'm seeing exactly this when my buddy does fast maneuvers mid flight.</p>

<p>In my day job we use charge counting battery monitors (BMS) for this battery chemistry. Usually custom designed for the battery pack (just like for laptop batteries, etc). Sometimes built into the <a href=""http://www.bren-tronics.com/batteries/rechargeable/bt-70791ck.html"" rel=""nofollow"">battery pack</a>, or sold by <a href=""http://www.batteryspace.com/Smart-Battery-Systems-SBS-with-SmBus-V1.1-support-for-14.8V-6.4Ah/10Ah.aspx"" rel=""nofollow"">cell suppliers</a>.</p>

<p>Have I missed a product for electric aircraft? Are hobbyists in the battery dark ages?</p>
","battery lithium-polymer"
"8787","Best power solution for my robot","<p>I've built quadruped robot which is using 12 servos (TowerPro SG90 Servo) and Raspberry Pi (model B 1). Right now I'm ""feeding"" it with 5V 2.5A charger. </p>

<p>How can I make it un-tethered? What should I look out for when selecting batteries? Also, I think that I need to separate powering of RPi and servos because power is ""jumping"" when it moves and that isn't good for RPi.  </p>

<p>A little video - <a href=""https://www.youtube.com/watch?v=o63akKmY07w"" rel=""nofollow"">Testing Walking Algorithm</a></p>
","mobile-robot raspberry-pi power battery walking-robot"
"8797","Suitable gear construction for a robotic extender - plastic?","<p>I have a rather simple setup for my robotic extender. The DC motor turns a shaft with a worm on it. The worm connects to a small worm gear (green) which itself has a small gear (red) on the same shaft connecting to a large gear (blue):</p>

<p><a href=""http://i.stack.imgur.com/v2PNY.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/v2PNY.png"" alt=""enter image description here""></a></p>

<p>The DC motor's gearbox gives around 50-100rpm and has a stall torque of around 2kg-cm. The small gear should be around 15MM tall and the large gear should be around 45MM tall.</p>

<p>If the load (on the large gear) has a maximum torque of 5kg-cm and a typical torque of 3kg-cm, could the two gears be made from plastic and be of module (metric form for pitch, as @Chuck pointed out) 0.5? Is a higher module needed? How about the worm, could it be made of plastic (nylon?)?</p>

<p>Any help will be appreciated.</p>

<p><strong>EDIT</strong> Fixed typos and updated diagram.</p>
","motor torque gearing"
"8804","See CC3D actual configuration","<p>just received my first hobby-grade quadcopter.</p>

<p>Its the Eachine racer 250 and comes preassembled with transmitter and receiver also included.</p>

<p>It comes with some kind of CC3D flight board, most people say its not the original one, but can be configured with the same software.</p>

<p>It is actually flying very well right out of the box so im not sure if i want to touch the FC config.</p>

<p>Im mostly interested in learning to fly in manual/acro mode, the transmitter seems to have a switch with 3 flight modes, first 2 looks like low/high rates in self-level mode, im expect the third to be the acro mode, but im not sure right now, i couldnt test it because of the weather, it could be a third higher rate?.</p>

<p>So, is there any way i can look at the actual FC config without changing anything? witch software do i need? and are the flight modes actually set on the FC or transmitter so i could be able to see and edit them?</p>

<p>Thanks in advance.</p>
","quadcopter"
"8807","POM gears and metal fittings","<p>I'm looking at <a href=""https://img.alicdn.com/imgextra/i1/650686955/TB2i_qfbpXXXXXPXpXXXXXXXXXX-650686955.jpg"" rel=""nofollow"">this setup</a>: </p>

<p><a href=""http://i.stack.imgur.com/UYPrg.jpg"" rel=""nofollow""><img src=""http://i.stack.imgur.com/UYPrg.jpg"" alt=""enter image description here""></a></p>

<p>where the POM bevel gears are fitted with some kind of metal (bronze?) tube inside which fits over the shaft. What benefits does this method provide? Is it to allow the shaft to free-spin? The metal fitting wouldn't be able to grip on the shaft - right?</p>

<p>Is it supposed to be a canonical approach to fitting a POM gear for (relatively) high-load applications?</p>
","motor mechanism gearing"
"8810","Gazebo or SimMechanics","<p>I'm now considering to choose Gazebo or SimMechanics for simulating my quadruped robot.</p>

<p>I set some standards for the simulation:</p>

<ol>
<li>Support Real-time application with ROS</li>
<li>Simulate contact impact well(with ground) (deformable if possible)</li>
<li>Good rendering Quality.</li>
</ol>

<p>I have learned Gazebo for months, and see it has some limits to meet my requirements, especially the contact and friction problems. </p>

<p>I didn't use SimMechanics, but when i'm very impressed on it when i see this <a href=""https://www.youtube.com/watch?v=ShCSr7XCego"" rel=""nofollow"">video</a>.</p>

<p>Anyone who has experiences on Quadruped Simulation can share me some advice?</p>

<p>Thank you so much.</p>
","simulator gazebo"
"8811","Will a 5200mAh 30C 22.2V 6S Lipo battery work with","<p>I am building a Quadcopter and I was wondering if a 5200mAh 30C 22.2V 6S Lipo battery will work with a 40Amp Esc's, MT4108 370 KV Motors, and GEMFAN 1470 Carbon Fiber Props. The over all payload will be about 5-6 pounds.</p>
","quadcopter"
"8816","Create 2: Wheel interface board replacement","<p><a href=""http://www.irobotweb.com/~/media/MainSite/PDFs/About/STEM/Create/Create_2_Wheel_Hack.pdf?la=en"" rel=""nofollow"">http://www.irobotweb.com/~/media/MainSite/PDFs/About/STEM/Create/Create_2_Wheel_Hack.pdf?la=en</a></p>

<p>Scroll to page 3.</p>

<p>I'm trying to interface the Roomba's preloaded navigation system with a pair of motors not attached to the roomba itself- however, to do this I need an interface board of the same dimensions as the one pictured in the above document. It has 0.050"" (1.27) contact centers, which don't seem to be commercially available. Can anyone provide any help locating PCBs of this size?</p>
","irobot-create electronics navigation roomba"
"8818","Accurate technique to locate position?","<p>What is the most accurate way to locate the position and orientaion of the body during some motion (rotation and translation)?</p>

<p>I need to track the body very precisely, the required accuracy is 100-200 microns, and with rather high frequency - at least 1kHz. The body has one rotation axis. This axis can translate along the path. Normally the track has ellipse like shape, but translation path can change, that's why I need to track the body. The limit of motion is 50 cm on any direction. Maximum velocity is 5 m/s. </p>

<p>Requirments about sensors: it's possible to place any sensor on the surface, but it's impossible to change the construction. So it's impossible to use encoders at the rotation axis to measure the angle. </p>

<p>I tried to do it with MEMS 9DOF sensors, but because of the noise it's very difficult to understand when there is a motion and when it's a noise.
Another idea is to use magnet and magnetomemter, but how is it possible to measure the resolution in this way? </p>
","kinematics navigation tracks orientation"
"8820","Arduino project: Turning with a fixed front wheel axis","<p>I am working on an Arduino robot project. This project requires a base with 4 wheels where the back wheels are attached to two DC motors that can be controlled independently of each other. I am thinking that the robot will turn by giving power to just one of the motors but I am having trouble with how the front axis should look like. Would it be possible to have a solid front axis with 2 wheels and still possible for the robot to turn or would the friction be too great?</p>
","arduino wheeled-robot brushless-motor"
"8822","Orienting rectangular plastic bricks","<p>As part of a sorting machine, I need to orient a pile of plastic brick-shaped objects (which are all identical in size - about 3cm x 2cm x 1.5cm) so that they always end up with the white side facing up:</p>

<p><a href=""http://i.stack.imgur.com/zGW0d.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/zGW0d.png"" alt=""enter image description here""></a></p>

<p>These will then be fed into a bowl feeder type of machine for further re-orienting. </p>

<p>How can I accomplish this, preferably without optical sensors? I was thinking about cutting into the bricks and putting magnets inside, but is there a more elegant solution?</p>
","mechanism orientation"
"8823","Does the mAh of a battery mean longer power or more power?","<p>If a Lipo battery has more mAh will it be slower to run out of energy or will it have larger power output?</p>

<p>Thanks in Advance</p>
","power battery"
"8831","Are joint state vectors limited?","<p>Are joint-state vectors $q$, which define the position and orientation of a set of joints, limited somehow? </p>

<p>I know they are used for the rotation part of the transformation matrix => therefore I would think they were limited within 0 and 2$\pi$</p>
","joint"
"8833","Erratic motor behavior. Is it due to the faulty remote control or Grounding or something else?","<p>I am using arduino mega to run 4 motors via 4 motor controllers. I am using a ps2 controller as the remote control. When the joystick is at rest, the motor should stop.</p>

<p>The problem is the motor still moves randomly when the joystick is at rest. Sometimes, the joystick can only produce forward motion on the motor but not backward. Is this the grounding issue or the PS2 remote control issue ir others.. Does the GND from the arduino board have to be connected to the GND from the external battery? </p>

<p>How can I troubleshoot this? </p>

<p>Thanks.</p>
","mobile-robot control power"
"8835","q-state vector used to define the transformation matrix? how?","<p>How can it be used to determine the transformation matrix?</p>

<p>an example could be at computing the inverse kinematics for small displacements:
J(q)$\Delta$q = $\Delta$u</p>

<p>$\Delta$U is a vector defining the difference between current and desired position. The desires position can always be computed, but if keep solving this in such manner that every time you solve $$J(q)\Delta q = \Delta u$$</p>

<p>you do this </p>

<ul>
<li>q:= q + $\Delta$q</li>
<li><strong>Compute $T_{base}^{tool}(q)$</strong></li>
<li>Compute the difference between $[T^{tool}_{base}]_{desired~position} $ and $T_{base}^{tool}(q)$.</li>
<li>If change is less than 10^-5 finish and output Q, if not resolve. </li>
</ul>

<p>How would you compute The transformation matrix based on q state vector. </p>
","inverse-kinematics jacobian"
"8840","RC car circuit: what does it do?","<p>I have just disassembled a RC car (a BBR 1:18 Scale Ferrari Enzo from 2005). Attached to the stepper motor that controls the steering of the car was a small circuit board with a cog wheel attached to it. I am trying to figure out what this circuit does. My idea is that it is responsible of keeping track of the wheels position but I am not certain. Here is a picture of it:</p>

<p><a href=""http://i.stack.imgur.com/bN3Ne.jpg"" rel=""nofollow""><img src=""http://i.stack.imgur.com/bN3Ne.jpg"" alt=""Photo of Sensor - 1""></a></p>

<p><a href=""http://i.stack.imgur.com/sIisH.jpg"" rel=""nofollow""><img src=""http://i.stack.imgur.com/sIisH.jpg"" alt=""Photo of Sensor - 2""></a></p>

<p>As you can see there are 5 wires coming out of it and I know that the green one is GND. Does anyone have an idea of what its function might be?</p>
","stepper-motor radio-control circuit"
"8842","Path following with precise positioning system (RTK)","<p>Is there any general problem in using precise positioning (centimeter-accurate GNSS RTK system meant) for autonomous car guidance given I have a predefined path the car should follow? I mean, autonomous cars were the topic #1 at CES 2016 yet no such simple system seems to have been introduced to date... Of course the ""path planning"" is only a part of the ""autonomous package"" and other problems need to be solved (collision prevention etc.) but I really wonder whether something simple like RTK guidance could be used.</p>

<p>An RTK system relies on very little amount of live correction data (about 1 kB/s) and mobile networks are really ubiquitous today so I can not really see a technical problem in such solution given there are enough RTK base stations around.</p>

<p><strong>EDIT:</strong></p>

<p>This question is only about using precise positioning to follow a predefined track in obstacle-free environment. I am not asking about other systems that need to be implemented in an autonomous car like collision prevention etc. (such systems may employ LIDAR or a stereo camera). Surely a collision prevention is a must for an autonomous system but I consider a theoretical case only.</p>

<p><strong>An updated question may be:</strong> Is precise satellite positioning accurate enough to guide/navigate a full-scale passenger car in an obstacle-free outdoor environment in the speed of about 100 km/h given I have a precise-enough path prerecorded that is to follow?</p>

<p>Some answers below already say yes, this is a solved problem. It would be nice to have the answers elaborated in more detail regarding the existing solutions (accuracy, possible problems etc.). One of the solutions may probably be the open source <a href=""http://rover.ardupilot.com/"" rel=""nofollow"">APM autopilot</a> which work for rovers too (<a href=""http://www.emlid.com/maiden-drive-of-apmrover-on-rpi-with-navio/"" rel=""nofollow"">example by Emlid</a>) but that does not seem to use RTK so the accuracy may be rather low.</p>
","navigation gps precise-positioning"
"8844","DC Motor to open a door","<p>I want to open a door using a DC motor. I've estimated that the required power in the worst case would be around 35-40W (considering a ~80% efficiency). The whole is controlled by a <a href=""https://docs.particle.io/datasheets/photon-datasheet/"" rel=""nofollow"">Particle Photon</a>.</p>

<p>I was thinking to use a <a href=""https://www.sparkfun.com/products/9479"" rel=""nofollow"">L298N</a> to control the output of current to the motor. However, when I looked for powerful enough motors, they would all consume too much current when stalling (> 4A part of the L298N datasheet).</p>

<p>Do you have ideas of how to overcome this? Maybe there's another dual-bridge that can handle more current, maybe there exists a DC motor that is ok for a L298N, or maybe I need to have simultaneous DC motors?</p>

<p>Edit: this part should be a question by itself. I'll keep it here so that future visitors know what the sub-question was about, but please ignore it from now on if answering.</p>

<blockquote>
  <p>As a sub-question, would it be better to use a brushed or a brushless DC motor?</p>
</blockquote>
","motor h-bridge"
"8845","Exchange air and maintain thermal insulation","<p>My application is composting with worms outdoors inside an a styrofoam cooler.  I use a heat lamp and a thermo-electric cooler to maintain the temperature in the bin when the temperature outside is out of bounds for healthy worms.  When the temperature outside <em>is</em> in bounds, I'd like to exchange the air in the bin with fresh air from outside, but I don't want to permanently compromise the insulating properties of my bin with lots of air holes.  So I'm looking for actuator solutions that would allow me to open/close a window of sorts.  I'm considering a solenoid air valve but I don't necessarily need/want an air compressor - a simple fan is sufficient to circulate the air. Any suggestions?</p>
","actuator valve"
"8849","Tracking objects from camera; PID controlling; Parrot AR Drone 2","<p>I am working on a project where I should perform object tracking using the camera of Parrot AR Drone 2.0. So the main idea is, a drone should be able to identify a specified colour and then follow it by keeping some distance.</p>

<p>I am using the <a href=""https://github.com/puku0x/cvdrone"" rel=""nofollow"">cvdrone</a> API to establish communication with the drone. This API provides function:</p>

<pre><code>ARDrone::move3D(double vx, double vy, double vz, double vr)
</code></pre>

<p>which moves the AR.Drone in 3D space and where </p>

<ul>
<li>vx: X velocity [m/s]</li>
<li>vy: Y velocity [m/s]</li>
<li>vz: Z velocity [m/s]</li>
<li>vr: Rotational speed [rad/s]</li>
</ul>

<p>I have written an application which does simple image processing on the images obtained from the camera of the drone using OpenCV and finds needed contours of the object to be tracked. See the example below:
<a href=""http://i.stack.imgur.com/NVDgg.jpg"" rel=""nofollow""><img src=""http://i.stack.imgur.com/NVDgg.jpg"" alt=""enter image description here""></a></p>

<p>Now <strong><em>the part I am struggling</em></strong> is finding the technique using which I should find the <strong><em>velocities</em></strong> to be sent to the <strong><em>move3D</em></strong> function. I have read that common way of doing controlling is by using <strong><em>PID</em></strong> controlling. However, I have read about that and could not get how it could be related to this problem.</p>

<p>To summarise, my question is how to move a robot towards an object detected in its camera? How to find coordinates of certain objects from the camera? </p>
","control pid quadcopter tuning opencv"
"8852","Quadcopter Charging","<p>Our thesis is about Multicopter using two batteries wherein the first battery is used and when the power goes out, a switch is placed use to charge the first battery while the second battery is powering the multicopter, the process repeats again and again until all the power of the battery is drained. Is the process possible to achieved?</p>

<p>Edit 1:
Just to clear everything up again. The thesis is about a <strong>Quadcopter</strong> that uses two batteries as its power source. These <strong>two batteries</strong> are attached to a <strong>""switching circuit system""</strong> that will allow one battery to be drained then the quadcopter will be transferred to the other battery. The solar panels serve as the ""charger"" of whatever battery is in its standby mode (the discharged battery). So while one battery is being used by the quadcopter, one will be connected to a solar panel which will be charging that battery. <em>Once the other battery is drained, the quadcopter will switch to the charged battery.</em> This will continue to go on until both batteries are drained. Is this achievable?</p>
","battery multi-rotor"
"8855","How can I detect ground collision, for an hexapod robot?","<p>I'm planning to build a small (probably around 30 centimeters in diameter, at rest) hexapod robot, but, currently, it would only be able to walk on even ground. To improve this, I would have to, somehow, detect when each leg collides with the ground. </p>

<p>Ideally, I would be able to know how much weight each leg is supporting, so that I could both balance the weight and adapt to moving (up or down) terrain (so, if you put a finger below one leg and lifted it, the leg would go up); however, if that's not possible, a simple binary signal would do.</p>

<p>Is there a simple and compact method to do this? </p>
","sensors hexapod"
"8857","imu position without GPS or camera","<p>I have a IMU that has 3-axis accelerator, 3-axis magnetometer, 3-axis gyroscope and row, yaw, pitch value. I want to get the location of the IMU coordinate(the beginning point is (0,0,0)) but I know just using double integration will has dead reckoning problem. And I found a lot of paper just talking about combining IMU with GPS or camera by using Kalman filter. Is it possible that I just use a IMU to get a slightly precise position data? Because in the future work I will use multiple IMUs bounded on human arms to increase the accuracy. </p>
","imu gps"
"8860","Angle Random Walk vs. Rate Noise Density (MPU6050)","<p>I’ve made a datalog from a MPU6050 (IMU: gyroscope and accelerometer) at 500Hz sample rate. Now I want to calculate the characteristics from the gyro to evaluate the sensor. </p>

<p>For the gyro I’ve found following values in the datasheet:</p>

<p>Total RMS Noise = 0.05 °/s</p>

<p>Low-frequency RMS noise = 0.033 °/s </p>

<p>Rate Noise Spectral Density = 0.005 °/s/sqrt(Hz)</p>

<p>Now I want to ask how I can calculate these values from my dataset?</p>

<p>At the moment I’ve the following values from the dataset:</p>

<p>Standard deviation = 0.0331 °/s</p>

<p>Variance = 0.0011</p>

<p>Angular Random Walk (ARW) = 0.003 °/sqrt(s) (From Allan deviation plot)</p>

<p>Bias Instability = 0.0012 °/s</p>

<p>Is the ARW equal to the Rate Noise Spectral Density mentioned in the datasheet? And also is the RMS Noise from the datasheet equal to the standard deviation? </p>

<p>edit:
I found following website: <a href=""http://www.sensorsmag.com/sensors/acceleration-vibration/noise-measurement-8166"" rel=""nofollow"">http://www.sensorsmag.com/sensors/acceleration-vibration/noise-measurement-8166</a>
There is the statement:  ""...Because the noise is approximately Gaussian, the standard deviation of the histogram is the RMS noise"" So I guess the standard deviation is the RMS noise from the datasheet. But how about the ARW?</p>
","sensors imu gyroscope sensor-fusion statistics"
"8863","cracked GPS chip antenna","<p>My quadcopter has an onboard GPS unit; and in a crash today I happen to come down on top of it.</p>

<p>I split apart the broken case and noticed that the chip antenna has cracked; I am unfamiliar with how this type of antenna functions, will this be an issue?</p>

<p><a href=""http://i.stack.imgur.com/UpfRD.jpg"" rel=""nofollow""><img src=""http://i.stack.imgur.com/UpfRD.jpg"" alt=""enter image description here""></a></p>

<p>This is a closed source drone so I have little insight as to the number of GPS locks acquired, and the signal strengths, would a broken antenna cause any type of signal degradation, or will I not even be able to acquire a lock?</p>
","gps"
"8866","Tf frame origin is offset from the actual base_link","<p>I have built my differential drive mobile robot in solidworks and converted that to URDF file using soliworks2urdf converter. I successfully launched and robot and simulated with tele-operation node. Since i am intended to use navigation stack in i viewed the transform of the robot in rviz which resulted as below.</p>

<p><a href=""http://i.stack.imgur.com/2FgIo.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/2FgIo.png"" alt=""enter image description here""></a></p>

<p>As you can see the base plate is the one which supports the wheels and castors but the tf of base plate is shown away from the actual link and even odom is away from the model. Where have i gone wrong and how to fix this. Refer the URDF of model below.</p>

<pre><code>    &lt;?xml   version=""1.0""?&gt;
&lt;robot
  name=""JMbot""&gt;
  &lt;link
    name=""Base_plate""&gt;
    &lt;inertial&gt;
      &lt;origin
        xyz=""-0.3317 0.71959 -0.39019""
        rpy=""0 0 0"" /&gt;
      &lt;mass
        value=""0.55378"" /&gt;
      &lt;inertia
        ixx=""0.0061249""
        ixy=""0.00016086""
        ixz=""-8.6651E-18""
        iyy=""0.0041631""
        iyz=""-1.4656E-17""
        izz=""0.010283"" /&gt;
    &lt;/inertial&gt;
    &lt;visual&gt;
      &lt;origin
        xyz=""0 0 0""
        rpy=""0 0 0"" /&gt;
      &lt;geometry&gt;
        &lt;mesh
          filename=""package://jmbot_description/meshes/Base_plate.STL"" /&gt;
      &lt;/geometry&gt;
      &lt;material
        name=""""&gt;
        &lt;color
          rgba=""0.74902 0.74902 0.74902 1"" /&gt;
      &lt;/material&gt;
    &lt;/visual&gt;
    &lt;collision&gt;
      &lt;origin
        xyz=""0 0 0""
        rpy=""0 0 0"" /&gt;
      &lt;geometry&gt;
        &lt;mesh
          filename=""package://jmbot_description/meshes/Base_plate.STL"" /&gt;
      &lt;/geometry&gt;
    &lt;/collision&gt;
  &lt;/link&gt;
  &lt;link
    name=""Wheel_R""&gt;
    &lt;inertial&gt;
      &lt;origin
        xyz=""0.010951 1.1102E-16 -1.1102E-16""
        rpy=""0 0 0"" /&gt;
      &lt;mass
        value=""0.45064"" /&gt;
      &lt;inertia
        ixx=""0.00091608""
        ixy=""-1.2355E-19""
        ixz=""1.0715E-18""
        iyy=""0.00053395""
        iyz=""-6.7763E-20""
        izz=""0.00053395"" /&gt;
    &lt;/inertial&gt;
    &lt;visual&gt;
      &lt;origin
        xyz=""0 0 0""
        rpy=""0 0 0"" /&gt;
      &lt;geometry&gt;
        &lt;mesh
          filename=""package://jmbot_description/meshes/Wheel_R.STL"" /&gt;
      &lt;/geometry&gt;
      &lt;material
        name=""""&gt;
        &lt;color
          rgba=""0.74902 0.74902 0.74902 1"" /&gt;
      &lt;/material&gt;
    &lt;/visual&gt;
    &lt;collision&gt;
      &lt;origin
        xyz=""0 0 0""
        rpy=""0 0 0"" /&gt;
      &lt;geometry&gt;
        &lt;mesh
          filename=""package://jmbot_description/meshes/Wheel_R.STL"" /&gt;
      &lt;/geometry&gt;
    &lt;/collision&gt;
  &lt;/link&gt;
  &lt;joint
    name=""Wheel_R""
    type=""continuous""&gt;
    &lt;origin
      xyz=""-0.14688 0.40756 -0.73464""
      rpy=""-2.7127 -0.081268 -3.1416"" /&gt;
    &lt;parent
      link=""Base_plate"" /&gt;
    &lt;child
      link=""Wheel_R"" /&gt;
    &lt;axis
      xyz=""1 0 0"" /&gt;
  &lt;/joint&gt;
  &lt;link
    name=""Wheel_L""&gt;
    &lt;inertial&gt;
      &lt;origin
        xyz=""-0.039049 2.2204E-16 2.498E-15""
        rpy=""0 0 0"" /&gt;
      &lt;mass
        value=""0.45064"" /&gt;
      &lt;inertia
        ixx=""0.00091608""
        ixy=""-9.6693E-19""
        ixz=""-1.7816E-18""
        iyy=""0.00053395""
        iyz=""1.3553E-19""
        izz=""0.00053395"" /&gt;
    &lt;/inertial&gt;
    &lt;visual&gt;
      &lt;origin
        xyz=""0 0 0""
        rpy=""0 0 0"" /&gt;
      &lt;geometry&gt;
        &lt;mesh
          filename=""package://jmbot_description/meshes/Wheel_L.STL"" /&gt;
      &lt;/geometry&gt;
      &lt;material
        name=""""&gt;
        &lt;color
          rgba=""0.74902 0.74902 0.74902 1"" /&gt;
      &lt;/material&gt;
    &lt;/visual&gt;
    &lt;collision&gt;
      &lt;origin
        xyz=""0 0 0""
        rpy=""0 0 0"" /&gt;
      &lt;geometry&gt;
        &lt;mesh
          filename=""package://jmbot_description/meshes/Wheel_L.STL"" /&gt;
      &lt;/geometry&gt;
    &lt;/collision&gt;
  &lt;/link&gt;
  &lt;joint
    name=""Wheel_L""
    type=""continuous""&gt;
    &lt;origin
      xyz=""-0.46668 0.40756 -0.70859""
      rpy=""2.512 0.081268 3.4272E-15"" /&gt;
    &lt;parent
      link=""Base_plate"" /&gt;
    &lt;child
      link=""Wheel_L"" /&gt;
    &lt;axis
      xyz=""-1 0 0"" /&gt;
  &lt;/joint&gt;
  &lt;link
    name=""Castor_F""&gt;
    &lt;inertial&gt;
      &lt;origin
        xyz=""2.2204E-16 0 0.031164""
        rpy=""0 0 0"" /&gt;
      &lt;mass
        value=""0.056555"" /&gt;
      &lt;inertia
        ixx=""2.4476E-05""
        ixy=""-2.8588E-35""
        ixz=""1.0281E-20""
        iyy=""2.4476E-05""
        iyz=""-1.2617E-20""
        izz=""7.4341E-06"" /&gt;
    &lt;/inertial&gt;
    &lt;visual&gt;
      &lt;origin
        xyz=""0 0 0""
        rpy=""0 0 0"" /&gt;
      &lt;geometry&gt;
        &lt;mesh
          filename=""package://jmbot_description/meshes/Castor_F.STL"" /&gt;
      &lt;/geometry&gt;
      &lt;material
        name=""""&gt;
        &lt;color
          rgba=""0.75294 0.75294 0.75294 1"" /&gt;
      &lt;/material&gt;
    &lt;/visual&gt;
    &lt;collision&gt;
      &lt;origin
        xyz=""0 0 0""
        rpy=""0 0 0"" /&gt;
      &lt;geometry&gt;
        &lt;mesh
          filename=""package://jmbot_description/meshes/Castor_F.STL"" /&gt;
      &lt;/geometry&gt;
    &lt;/collision&gt;
  &lt;/link&gt;
  &lt;joint
    name=""Castor_F""
    type=""continuous""&gt;
    &lt;origin
      xyz=""-0.31952 0.39256 -0.57008""
      rpy=""-1.5708 1.1481 -1.3614E-16"" /&gt;
    &lt;parent
      link=""Base_plate"" /&gt;
    &lt;child
      link=""Castor_F"" /&gt;
    &lt;axis
      xyz=""0 0 1"" /&gt;
  &lt;/joint&gt;
  &lt;link
    name=""Castor_R""&gt;
    &lt;inertial&gt;
      &lt;origin
        xyz=""-1.1102E-16 0 0.031164""
        rpy=""0 0 0"" /&gt;
      &lt;mass
        value=""0.056555"" /&gt;
      &lt;inertia
        ixx=""2.4476E-05""
        ixy=""0""
        ixz=""-3.9352E-20""
        iyy=""2.4476E-05""
        iyz=""-1.951E-20""
        izz=""7.4341E-06"" /&gt;
    &lt;/inertial&gt;
    &lt;visual&gt;
      &lt;origin
        xyz=""0 0 0""
        rpy=""0 0 0"" /&gt;
      &lt;geometry&gt;
        &lt;mesh
          filename=""package://jmbot_description/meshes/Castor_R.STL"" /&gt;
      &lt;/geometry&gt;
      &lt;material
        name=""""&gt;
        &lt;color
          rgba=""0.75294 0.75294 0.75294 1"" /&gt;
      &lt;/material&gt;
    &lt;/visual&gt;
    &lt;collision&gt;
      &lt;origin
        xyz=""0 0 0""
        rpy=""0 0 0"" /&gt;
      &lt;geometry&gt;
        &lt;mesh
          filename=""package://jmbot_description/meshes/Castor_R.STL"" /&gt;
      &lt;/geometry&gt;
    &lt;/collision&gt;
  &lt;/link&gt;
  &lt;joint
    name=""Castor_R""
    type=""continuous""&gt;
    &lt;origin
      xyz=""-0.34387 0.39256 -0.86909""
      rpy=""1.5708 -0.93144 3.1416"" /&gt;
    &lt;parent
      link=""Base_plate"" /&gt;
    &lt;child
      link=""Castor_R"" /&gt;
    &lt;axis
      xyz=""0 0 1"" /&gt;
  &lt;/joint&gt;

&lt;gazebo&gt;
&lt;plugin name=""differential_drive_controller""
    filename=""libgazebo_ros_diff_drive.so""&gt;
    &lt;leftJoint&gt;Wheel_L&lt;/leftJoint&gt;  
    &lt;rightJoint&gt;Wheel_R&lt;/rightJoint&gt;
    &lt;robotBaseFrame&gt;Base_plate&lt;/robotBaseFrame&gt;
    &lt;wheelSeparation&gt;0.235&lt;/wheelSeparation&gt;
    &lt;wheelDiameter&gt;0.12&lt;/wheelDiameter&gt;
    &lt;publishWheelJointState&gt;true&lt;/publishWheelJointState&gt;
    &lt;/plugin&gt;
&lt;plugin name=""joint_state_publisher""
    filename=""libgazebo_ros_joint_state_publisher.so""&gt;
    &lt;jointName&gt;Castor_F, Castor_R&lt;/jointName&gt;
    &lt;/plugin&gt;

&lt;/gazebo&gt;

&lt;/robot&gt;
</code></pre>
","mobile-robot ros navigation odometry gazebo"
"8874","How can this mobile robot rotate so perfectly?","<p>Please have a look at <a href=""https://www.youtube.com/watch?v=qU4YMDJNzpg"" rel=""nofollow"">this youtube video</a>.</p>

<p>When this mobile robot operates, the rack (which is said to weight up to 450 kg) can have its Center Of Mass (COM) distributed at any location. For example, this COM can be located 5 or 10 cm off from the center of the robot. Because of this, when the robot revolves, the center of rotation will not be at the center of the robot anymore. However as you see in the video, it can still rotate many circles perfectly around its up-right axis.</p>

<p>So what do you think about this? Is this possible by mechanical design only? Or did they use some kind of advanced feedback control system to counter the effect of the off-center COM?</p>
","mobile-robot control mechanism"
"8877","Need the uav to perform a circle turn","<p>I've got a project that will require my drone to perform a circle turn while the drone is always facing the tangent of the turning curve. Similiar to a car that is performing a frictionless banked turn. 
Just wondering which method should i use to achieve it, the throttle control can be ignored since i already have a pid on height control.</p>

<p>Any suggestions would be appreciated. </p>
","quadcopter pid multi-rotor"
"8879","Can i use DC Brushed motors for building a drone?","<p>I want to make a drone. But my budget is very low. Brushless motors are very expensive. I want use the Brushed CHEAP ones. can i us them ?</p>
","quadcopter motor brushless-motor"
"8882","Canny's Roadmap Algorithm","<p>Where can I find a general implementation of Canny's Roadmap Algorithm(or Silhouette Method) for Robot Motion Planning?</p>
","motion-planning"
"8883","How to find center of a disk using robotic arm","<p>Hello I am new to the field of robotics but have some knowledge of raspberry pi, arduino, python.
I want to make Robotic arm which can be used to find the centre of any disk.
There may be disk of different diameter coming one after another on conveyor.
I need to make hole at the center of disk using robotic arm. How can I do this ?
What techniques and sensors I should use to implement the mechanical and electronic part.
(I don't want to use camera and openCV). Thanks in advance.</p>

<p><a href=""http://i.stack.imgur.com/xwqCh.jpg"" rel=""nofollow""><img src=""http://i.stack.imgur.com/xwqCh.jpg"" alt=""enter image description here""></a></p>
","robotic-arm mechanism electronics movement"
"8887","How does the Kirobo robot make up sentences / know how to respond to humans?","<p>Basically, how does the Kirobo, which went on the ISS and spoke some sentences, think? Does it talk to itself?</p>

<p>Does it learn (become more intelligent with time) to have better conversations with ""experience""?</p>
","mobile-robot humanoid"
"8888","CC3D OpenPilot - Communication port","<p>I am building a quadrotor with a CC3D (OpenPilot) and a RPi. </p>

<p>My first idea is trying to communicate the CC3D with the RPi using the main port of the first one but I can't find any information about how to communicate the board with any device (or other information about the commands or serial configuration).</p>

<p>Anybody knows whether this is possible and where to find that info??</p>
","raspberry-pi quadcopter"
"8891","Figure out PID values from drone specs","<p>I have all the specs from a quadcopter, everthing, would it be possible to figure out the pid from those specs?</p>
","quadcopter pid"
"8893","Gazebo: moving joint with model plugin","<p>This is my first week with Gazebo.  The tutorials are clear (except for my dearth of C++ knowledge) but now that I'm working to move out on my own things are getting cloudy.  I made a model comprising two boxes and a revolute joint.  The file one_r_test.world loads this model.  A plugin is ""loaded"" (?) in model.sdf and that plugin, ModelControl, comes from model_push.cc in the ""Model plugins"" tutorial (<a href=""http://gazebosim.org/tutorials?tut=plugins_model&amp;cat=write_plugin"" rel=""nofollow"">http://gazebosim.org/tutorials?tut=plugins_model&amp;cat=write_plugin</a>), which uses SetLinearVel to move a box.  I can get this same behavior out of model_control.cc if I just copy the tutorial code (and change the plugin name as appropriate), but that's not what I want.  I'm seeking to eventually simulate joint control of robotic manipulators and what's not working in this basic simulation is my attempt to move the model joint via the ModelControl plugin.  It moves in the GUI if I set the velocity (or torque) that way.  The model_control.cc code is pasted below in hopes that you can identify a problem.</p>

<p><strong>model_control.cc</strong></p>

<pre><code>
#include ""boost/bind.hpp""
#include ""gazebo/gazebo.hh""
#include ""gazebo/physics/physics.hh""
#include ""gazebo/common/common.hh""
#include ""stdio.h""
// In the real file these quotes are greater-than and less-than but I
// don't know how to get that to show up in my question

namespace gazebo
{
  class ModelControl : public ModelPlugin
  {
  public: void Load(physics::ModelPtr _parent, sdf::ElementPtr /*_sdf*/)
    {
      // Store the pointer to the model
      this->model = _parent;

      // Store the pointers to the joints
      this->jointR1_ = this->model->GetJoint(""r1"");

      // Listen to the update event. This event is broadcast every
      // simulation iteration.
      this->updateConnection = event::Events::ConnectWorldUpdateBegin(boost::bind(&ModelControl::OnUpdate, this, _1));
    }

    // Called by the world update start event
  public: void OnUpdate(const common::UpdateInfo & /*_info*/)
    {
      // Apply a small linear velocity to the model.
      //this->model->SetLinearVel(math::Vector3(.03, 0, 0));

      // Apply angular velocity to joint
      this->jointR1_->SetParam(""vel"", 0, 99);
      this->jointR1_->SetParam(""max_force"", 0, 9999999999);

      double currAngle = this->jointR1_->GetAngle(0).Radian();
      printf(""Current angle is \t %f\n"", currAngle);
    }

    // Maybe I want to keep track of time?
    common::Time last_update_time_;

    // Pointer to the model
  private: physics::ModelPtr model;

    // Pointer to the update event connection
  private: event::ConnectionPtr updateConnection;

    // Pointers to joints
    physics::JointPtr jointR1_;
  };

  // Register this plugin with the simulator
  GZ_REGISTER_MODEL_PLUGIN(ModelControl)
}
</code></pre>

<p><strong>edit:</strong> If I change</p>

<p><code>this-&gt;jointR1_-&gt;SetParam(""vel"", 0, 99);</code></p>

<p>to</p>

<p><code>this-&gt;jointR1_-&gt;SetVelocity(0, 99);</code></p>

<p>then the joint moves (yes, very, very quickly).  What's wrong with SetParam vs SetVelocity?</p>
","simulator joint gazebo"
"8894","How a 3d printer moves the header vertically in a MakerBot printer","<p>I would like to know how the header in this MakerBot printer moves in the vertical up /down direction.  Is there a detailed explanation of it including the parts involved?</p>

<p><a href=""http://i.stack.imgur.com/CQHnL.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/CQHnL.png"" alt=""enter image description here""></a></p>
","mechanism 3d-printing"
"8895","Quadcopter PID Control: Is it possible to stabilize a quadcopter considering only angle measurements?","<p>Good day,</p>

<p>I am a student currently working on an autonomous quadcopter project, specifically the stabilization part as of now. I am using a tuned propeller system and I also already considered the balancing of the quadcopter during component placements. I had been tuning the PID's of my quadcopter for the past 3 1/2 weeks now and the best I've achieved is a constant angle oscillation of the quadcopter by +-10 degrees with 0 degrees as the setpoint/desired angle. I also tried a conservative 7 degrees setpoint with the same results on the pitch axis.</p>

<p>As of now my PID code takes in the difference of the angle measurement from the complementary filter <code>( FilteredAngle=(0.98)*(FilteredAngle + GyroAngleVel*dt) + (0.02)*(AccelAngle) )</code> and the desired angle.</p>

<p><a href=""http://i.stack.imgur.com/QbCmL.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/QbCmL.png"" alt=""enter image description here""></a></p>

<p>I have read somewhere that it is IMPOSSIBLE to stabilize the quadcopter utilizing only angle measurements, adding that the angular rate must be also taken into consideration. But I have read a lot of works using only a single pid loop with angle differences (Pitch Yaw and Roll) as the input. </p>

<p>In contrast to what was stated above, I have read a comment from this article (<a href=""https://www.quora.com/What-is-rate-and-stabilize-PID-in-quadcopter-control"" rel=""nofollow"">https://www.quora.com/What-is-rate-and-stabilize-PID-in-quadcopter-control</a>) by Edouard Leurent that a Single PID control loop only angle errors and a Cascaded PID loop (Angle and Rate) that utilizes both angle errors and angular velocity errors are equivalent Mathematically.</p>

<p>If I were to continue using only the Single PID loop (Angle) method, I would only have to tune 3 parameters (Kp, Ki &amp; Kd).</p>

<p>But if I were to change my code to utilize the Cascaded Loop (Angle and Angular Velocity),</p>

<ol>
<li>Would I have to tune two sets of the 3 parameters (Kp, Ki &amp; Kd for angle and Kp, Ki &amp; Kd for the angular velocity)?</li>
<li>Would the cascaded PID control loop give better performance than the single PID control loop?</li>
<li>In the Cascaded Loop, is the set point for the angular velocity for stabilized flight also 0 in deg/sec? What if the quadcopter is not yet at its desired angle?</li>
</ol>

<p>Thank you :)</p>
","control quadcopter pid raspberry-pi stability"
"8900","SLAM : Why is marginalization the same as schur's complement?","<p>Consider the system 
$$
\tag 1
H\delta x=-g
$$
 where $H$ and $g$ are the Hessian and gradient of some cost function $f$ of the form $f(x)=e(x)^Te(x)$. The function $e(x)=z-\hat{z}(x)$ is an error function, $z$ is an observation (measurement) and  $\hat{z}$ maps the estimated parameters to a measurement prediction. </p>

<p>This minimization is encountered in each iteration of many SLAM algorithms, e.g.one could think of $H$ as a bundle adjustment Hessian. Suppose $x=(x_1,x_2)^T$, and let $x_2$ be some variables that we seek to marginalize. Many authors claim that this marginalization is equivalent to solving a smaller liner system $M\delta x_1=-b$ where $M$ and $g$ are computed by applying Schur's complement to (1), i.e. if
$$H=
\begin{pmatrix}
H_{11} &amp; H_{12}\\
H_{21} &amp; H_{22}
\end{pmatrix}
$$
then
$$
M=H_{11}-H_{12}H_{22}^{-1}H_{21}
$$ 
and 
$$
b=g_1-H_{12}H_{22}^{-1}g_2
$$</p>

<p>I fail to understand why that is equivalent to marginalization... I understand the concept of marginalization for a Gaussian, and I know that schur's complement appears in the marginalization if we use the canonical representation (using an information matrix), but I don't see the link with the linear system. </p>

<p><strong>Edit:</strong> I understand how Schur's complement appears in the process of marginalizing or conditioning $p(a,b)$ with $a,b$ Gaussian variables, as in the link supplied by Josh Vander Hook. I had come to the same conclusions, but using the canonical notation: If we express the Gaussian $p(a,b)$ in canonical form, then $p(a)$ is gaussian and its information matrix is the Schur complement of the information matrix of $p(a,b)$, etc. Now the problem is that I don't understand how Schur's complement appears in marginalization in bundle adjustment (for reference, in these recent papers: <a href=""https://www.google.fr/url?sa=t&amp;rct=j&amp;q=&amp;esrc=s&amp;source=web&amp;cd=1&amp;cad=rja&amp;uact=8&amp;ved=0ahUKEwiDgNiC9K3KAhVL1RoKHcXvAWkQFgggMAA&amp;url=http%3A%2F%2Fwww-users.cs.umn.edu%2F~stergios%2Fpapers%2FRSS_2013_Workshop_CKLAM_Esha_Kejian.pdf&amp;usg=AFQjCNEsKknpqqMNJcYSWawCinb5R3qzAg&amp;sig2=gikqqcwFFZWWA_g_p60J3w"" rel=""nofollow"">c-klam</a> (page 3 if you want to look) and in <a href=""https://www.google.fr/url?sa=t&amp;rct=j&amp;q=&amp;esrc=s&amp;source=web&amp;cd=1&amp;ved=0ahUKEwjStJyd9K3KAhWBCxoKHfLWAK4QFgglMAA&amp;url=http%3A%2F%2Fwww.roboticsproceedings.org%2Frss09%2Fp37.pdf&amp;usg=AFQjCNHpE92Meqxmuja-N5gs0Oh3HbqB_g&amp;sig2=wh5gcv6W3tGrJ9TN1lGJOA"" rel=""nofollow"">this</a> (part titled marginalization). In these papers, a single bundle adjustment (<strong>BA</strong>) iteration is performed in a manner similar to what I initially described in the question. I feel like there is a simple connection between marginalizing a Gaussian and the marginalization in BA that I am missing. For example, one could say that optimizing $f$ (one iteration) is equivalent to drawing a random variable following a denstiy $$e^{-\frac{1}{2}(x-\mu)^T\Sigma^{-1}(x-\mu)}$$  where $\Sigma$ is the inverse of the Hessian $H$ of $f$, and $\mu$ is the true value for $x$ (or an approximation of that value), and that marginalizing this density is equivalent to using Schur's compelement in the bundle? I am really confused...  </p>
","slam computer-vision"
"8902","Real-time GY-85 IMU sensor interfacing with Simulink","<p>How do I read real time values from the GY-85 IMU sensor at Simulink connected via Arduino? Also, I intend to interact with the Virtual Reality environment at Simulink using this GY-85 IMU sensor.
Is this possible?
How do I make MATLAB read real time values from this GY-85 IMU sensor connected to Arduino via I2C communication ?
Please help!</p>
","imu matlab"
"8906","How do I create a portable solar panel lipo charger?","<p>We're working on a quadcopter that will carry a solar panel on top that will continually charging the lipo battery of the quad. What's the smallest and easiest way to recreate a charger that will allow safe charging for the lipo battery?</p>
","quadcopter battery"
"8907","Inverse Kinematics of DLR/HIT II Hand","<p>I am trying to find the inverse kinematics formulation of DLR/HIT II hand. Till now I had success in finding an analytical method described in the thesis of Mavrogiannis, Christoforos I. named <a href=""http://dspace.lib.ntua.gr/handle/123456789/7858"" rel=""nofollow"">Grasp Synthesis Algorithms for Multifingered Robot Hands</a>, given in appendix B. 
My question is regarding the A.28 where the author calculates q3 but has mentioned previously in the text that q3 is equal to q2. </p>

<p>Note: q here denotes joint angles</p>
","inverse-kinematics"
"8909","Typical Problem in Simple Line Follower Using 3 sensors","<p>I am working on building a line follower robot using ATmega2560 and I want its <strong>movement to be more precise</strong>. I am facing a very typical problem.
It consists of three(3) IR sensors. The <strong>thickness of the line</strong> to be followed is <strong>1.2cm</strong> and the <strong>gap between the sensors</strong> is more than that, around <strong>1.8cm</strong>. </p>

<p>So if the <strong>black line comes between the center and any of the side sensors</strong>, <strong>all the three sensors</strong> are on <strong>white</strong> and it stops.
And I need the robot to stop over white, due to my application. 
So please can anyone suggest me a good algorithm to tackle this situation.
I think PID control can be of good use, as i searched on Google. But I don't understand how to implement it with three sensors.
Please Help</p>
","microcontroller line-following avr"
"8910","How to interpret the result of image rectification?","<p>I've been trying to understand image rectification. Most of the time the result of image rectification is illustrated by the original image (i.e the image before rectification) and the rectified image, like this:</p>

<p>The original image:</p>

<p><a href=""http://i.stack.imgur.com/t7R79.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/t7R79.png"" alt=""enter image description here""></a></p>

<p>The rectified image:</p>

<p><a href=""http://i.stack.imgur.com/Qd0gC.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/Qd0gC.png"" alt=""enter image description here""></a></p>

<p>To me the original image makes more sense, and is more 'rectified' than the second one. I mean, why does the result of rectification looks like that? and how are we supposed to interpret it? what information does it contain? 
An idea has just dawned on me : could it be that this bizarre shape of the rectified image is dependent on the method used for rectification, because  here polar rectification was used (around the epipole)?</p>
","mobile-robot stereo-vision 3d-reconstruction"
"8911","How to find kinematics of differential drive caster robot?","<p>I'm working on a little project where I have to do some simulations on a small robot.</p>

<p>I my case I'm using a differential-drive robot as one of the wheels of a bigger robot platform (which has two differential-drive casters), and I really do not understand how to find its kinematics in order to describe it in a model for finding the speed <strong>V_tot</strong> of the platform.</p>

<p><a href=""http://i.stack.imgur.com/uBhDr.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/uBhDr.png"" alt=""enter image description here""></a></p>

<p>This is my robot and I know the following parameters</p>

<ul>
<li><strong>d</strong> is the distance between a joint where my robot is costrained</li>
<li><strong>blue point</strong> is the joint where the robot is linked to the robot platform</li>
<li><strong>L</strong> is distance between the wheels</li>
<li><strong>r</strong> the radius of the wheel</li>
<li>the robot can spin around the <strong>blue point</strong> and with and <strong>THETA</strong> angle</li>
</ul>

<p>As I know all this dimensions, I would like to apply two velocities <strong>V_left</strong> and <strong>V_right</strong> in order to move the robot.</p>

<p>Let's assume that <strong>V_left</strong> = <strong>- V_right</strong> how do I find analitically the ICR (Istantaneous Center of Rotation) in this costrained robot?</p>

<p>I mean that I cannot understand how to introduce <strong>d</strong> in the formula.</p>
","mobile-robot kinematics wheeled-robot differential-drive two-wheeled"
"8916","Quadcopter program execution time optimization using Raspberry Pi by increasing i2c baudrate","<blockquote>
  <p>Is it possible to speed up execution time of a c++ program in raspberry pi solely by increasing the i2c baudrate and increasing the sampling frequency of the sensors? </p>
</blockquote>

<p>I have the issue of sudden jerkiness of my quadcopter and found the culprit which is the frequency at which my loop excecutes which is only about 14Hz. The minimum requirement for a quadcopter is 100-200hz. It is similar to the issue he faces here <a href=""http://robotics.stackexchange.com/questions/6720/raspberry-pi-quadcopter-thrashes-at-high-speeds"">Raspberry Pi quadcopter thrashes at high speeds</a></p>

<p>He said that he was able to increase his sampling rate from 66hz to 200hz by increasing the i2c baudrate. I am confused on how that is done.</p>

<p>In the wiring pi library, it says that we can set the baudrate using this command:</p>

<pre><code>gpio load i2c 1000

will set the baud rate to 1000Kbps – ie. 1,000,000 bps. (K here is times 1000)
</code></pre>

<p>What I am curious about is how to set this baudrate to achieve my desired sampling rate?</p>

<p>I plan on optimizing it further to achieve at least a 100Hz sampling rate</p>

<p>As of now, the execution time of each loop in my quadcopter program is at 0.07ms or 14Hz.</p>

<p>It takes 0.01ms to 0.02ms to obtain data from the complementary filter.</p>

<p>I have already adjusted the registers of my sensors to output readings at 190Hz (Gyroscope L3GD20H) and 200Hz (Accelerometer LSM303) and 220Hz (Magnetometer LSM303).</p>
","quadcopter pid raspberry-pi sensor-fusion c++"
"8917","how to actuate pneumatic muscle by the signals received by emg sensors interfaced with raspberry pi?","<p>My aim is to actuate a pneumatic muscle based on signals received by EMG sensors placed on the biceps. Is there any Matlab code which can process the received EMG signals and convert them into any form which can be useful for muscle actuation?</p>

<p>The linked video gives better insight to my question: <a href=""https://www.youtube.com/watch?v=BdoblvmTixA&amp;ab_channel=JamesiosoHo"" rel=""nofollow"">exoskeleton arm pneumatic muscle</a></p>
","raspberry-pi matlab"
"8918","Canny's Silhouette Method","<p>How to construct silhouette curves (roadmap) in Canny's Silhouette Method, when 3D model of the environment (obstacles and robot) is given? </p>

<hr>

<p><em>I want to specifically implement Cannys Roadmap Method, which works for any n dimensional Configuration Space.</em></p>
","motion-planning"
"8930","Modified DH Parameters?","<p>Is the notation of the geometry of robots from Khalil and Kleinfinger be considered as one of the probably ""many"" Modified DH Parameters?</p>
","dh-parameters"
"8931","How to combine odometry information with time-shifted information from imu","<p>I'm working with a differential-drive robot that has odometry measurements from wheel shaft encoders and heading information from an imu (I'm using BNO055 in imu mode to get Euler angles, primarily the heading angle).</p>

<p>I'd like to use the imu header angle to augment the odometry which is prone to slipping and other errors, but the imu lags the odometry by up to 100ms.</p>

<p>How can I combine these measurements to get the best estimate of the robot pose?</p>

<p>Thanks for any word on this.</p>
","mobile-robot imu odometry"
"8934","Autonomous robots hardware structure planning","<p>I ask this question to clear my concept about hardware structure of humanoid autonomous fire robot, Here is scenario a fire robot detect humans from fire, there are some vision cameras some temperature and smoke sensors which help to perform this task. Now a days in market there are many processors like <code>ARMV7</code> and <code>Snapdragon</code> which process tasks in any device and control the system i don't think autonomous fire robot use some kind of processors.</p>

<p>Does autonomous robots like fire robot use processors or micro controllers. Does it require <code>OS</code> or <code>Rams</code> environment? Like any computer system which use these kind of things.</p>
","embedded-systems humanoid"
"8935","Gazebo laser plug-in fails to publish scan results","<p>I have added hokuyo laser plug-in to mu urdf file. I successfully launched the robot in gazebo and done required changes to visualize it in rviz. I didn't get any error but the laser scan results are not published.</p>

<p>Fine my results below </p>

<p>Terminal while launching gazebo model in world</p>

<p><a href=""http://i.stack.imgur.com/7NAwf.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/7NAwf.png"" alt=""enter image description here""></a></p>

<p>Result for <code>rostopic echo /scan</code></p>

<p><a href=""http://i.stack.imgur.com/hSWa6.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/hSWa6.png"" alt=""enter image description here""></a></p>

<p>I have also verified the tf of the model in rqt and its fine</p>

<p>Aslo find my urdf file below, can someone help me to fix this?</p>

<p>Link &amp; joint:</p>

<pre><code>&lt;link name=""hokuyo_link""&gt;
    &lt;collision&gt;
      &lt;origin xyz=""0 0 0"" rpy=""0 0 0""/&gt;
      &lt;geometry&gt;
        &lt;box size=""0.1 0.1 0.1""/&gt;
      &lt;/geometry&gt;
    &lt;/collision&gt;
    &lt;visual&gt;
      &lt;origin xyz=""0 0 0"" rpy=""0 0 0""/&gt;
      &lt;geometry&gt;
        &lt;box size=""0.1 0.1 0.1""/&gt;
      &lt;/geometry&gt;
    &lt;/visual&gt;
    &lt;inertial&gt;
      &lt;mass value=""1e-5"" /&gt;
      &lt;origin xyz=""0 0 0"" rpy=""0 0 0""/&gt;
      &lt;inertia ixx=""1e-6"" ixy=""0"" ixz=""0"" iyy=""1e-6"" iyz=""0"" izz=""1e-6"" /&gt;
    &lt;/inertial&gt;
  &lt;/link&gt;

  &lt;joint name=""hokuyo_joint"" type=""fixed""&gt;
    &lt;axis xyz=""0 1 0"" /&gt;
    &lt;origin xyz=""0 -0.055 0.2"" rpy=""0 0 0""/&gt;
    &lt;parent link=""Base_plate""/&gt;
    &lt;child link=""hokuyo_link""/&gt;
  &lt;/joint&gt;
</code></pre>

<p>Controller:</p>

<pre><code>&lt;gazebo reference=""hokuyo_link""&gt;
        &lt;sensor type=""gpu_ray"" name=""hokuyo""&gt;
          &lt;pose&gt;0 0 0 0 0 0&lt;/pose&gt;
          &lt;visualize&gt;false&lt;/visualize&gt;
          &lt;update_rate&gt;40&lt;/update_rate&gt;
          &lt;ray&gt;
            &lt;scan&gt;
              &lt;horizontal&gt;
                &lt;samples&gt;100&lt;/samples&gt;
                &lt;resolution&gt;1&lt;/resolution&gt;
                &lt;min_angle&gt;-1.570796&lt;/min_angle&gt;
                &lt;max_angle&gt;1.570796&lt;/max_angle&gt;
              &lt;/horizontal&gt;
            &lt;/scan&gt;
            &lt;range&gt;
              &lt;min&gt;0.10&lt;/min&gt;
              &lt;max&gt;30.0&lt;/max&gt;
              &lt;resolution&gt;0.01&lt;/resolution&gt;
            &lt;/range&gt;
          &lt;/ray&gt;
          &lt;plugin name=""gpu_laser"" filename=""libgazebo_ros_gpu_laser.so""&gt;
            &lt;topicName&gt;/scan&lt;/topicName&gt;
            &lt;frameName&gt;hokuyo_link&lt;/frameName&gt;
          &lt;/plugin&gt;
        &lt;/sensor&gt;
      &lt;/gazebo&gt;
</code></pre>
","mobile-robot ros navigation simulation gazebo"
"8938","iRobot Create 2 IR bump light sensor specifications","<p>Can someone tell me where can I find some specifications about the iRobot Create 2 IR bump light sensors?</p>

<p>We have an SDF model of the Create 2 that uses the Hoyuko laser range finder sensor and we have to simulate the behavior of the IR sensors starting from the laser scan data. </p>

<p>Hence we would like to have some additional information on the IR sensors which we can't find anywhere on the web - such as their exact position in the robot chassis, their maximum range, the shape of the obstacle detection field and so on.</p>
","sensors irobot-create"
"8940","Robotic manipulator Jacobian by product of exponentials","<p>I've taken a class and started a thesis on robotics and my reference for calculating the Jacobian by product of exponentials seems incorrect, see:</p>

<p><a href=""http://www.cds.caltech.edu/~murray/books/MLS/pdf/mls94-complete.pdf"" rel=""nofollow"">http://www.cds.caltech.edu/~murray/books/MLS/pdf/mls94-complete.pdf</a></p>

<p>Specifically the resulting Jacobian matrix for the SCARA manipulator on page 118 would have us believe that the end effector translational velocity depends on joints 2 and 3 rather than 1 and 2.</p>

<p>Could someone please explain me why?</p>
","jacobian manipulator product-of-exponentials"
"8941","Localization with only IMU","<p>What will be the best approach to get the most localization accuracy out of only an accelerometer and gyroscope?</p>
","localization imu accelerometer gyroscope algorithm"
"8946","Modified DH parameters","<p>Is the notation of the geometry of robots from Khalil and
Kleinfinger be considered as one of the probably ""many""  Modified DH Parameters?</p>
","dh-parameters"
"8949","mbed dc motor speed control using optocoupler encoder","<p>i have been playing with mbed since few week ago and would like to create a test program for dcmotor speed control. im using a 24V dc motor with encoder attach.. then test,
at first i test with supplying a 5v and 3.3V from mbed to the motor. the thing work fine giving a 16 and 10rpm respectively.
later i try using 12v it give 40rpm. 
then i try a 24v input, now the speed wont come out. and also the counter that count each on/off tick not counting up.</p>

<p>any possible reason?
my though that maybe when it move so fast that the ISR routine couldn't catch up to the speed..</p>

<p><a href=""http://i.stack.imgur.com/px4jZ.jpg"" rel=""nofollow""><img src=""http://i.stack.imgur.com/px4jZ.jpg"" alt=""enter image description here""></a></p>

<p><a href=""http://i.stack.imgur.com/oVQIB.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/oVQIB.png"" alt=""enter image description here""></a></p>
","control motor"
"8950","Wheel encoder triggers interrupt too many times","<p>I am building a simple robot with two driving wheel.
I want to control the wheel rotation using a wheel encoder like this <a href=""http://www.tinyosshop.com/index.php?route=product/product&amp;product_id=541"" rel=""nofollow"">one</a>.</p>

<p>Here is a code I have on Arduino to try to understand the problem I'm facing:</p>



<pre><code>int count = 0;
void setup() {
  Serial.begin(9600);
  attachInterrupt(digitalPinToInterrupt(2), upL, RISING);
  pinMode(2, INPUT);
}

void loop() {
  Serial.println(String(digitalRead(2)) + ""__"" + String(count));
}

void upL(){
  count++;
}
</code></pre>

<p>What I notice is:<br>
The interrupt is triggered <strong>multiple times</strong> when the sensor beam is cut <strong>once</strong>.<br>
But when I <em>digitalRead</em> the pin, then there is only one change.  </p>

<p>I also noticed that the interrupt is also triggered when going from HIGH to LOW.</p>

<p>Here is an example of the ouput I have:</p>

<pre><code>0__0
0__0
0__0
0__0
...
...
0__0
0__0
0__0   &lt;&lt;&lt; change from LOW to HIGH... 
1__9   &lt;&lt;&lt; the interrupt must have incremented only once... 9 instead !
1__9
1__9
1__9
...
...
1__9
1__9   &lt;&lt;&lt; change from HIGH to LOW. the interrupt shouldn't be triggered
0__24  &lt;&lt;&lt; still... we have 15 increments
0__24
0__24
0__24
0__24
...
...
0__24
0__24   &lt;&lt;&lt; same here...
1__51   &lt;&lt;&lt; 26 increments
1__51
...
...
1__51  &lt;&lt;&lt; same here...
0__67  &lt;&lt;&lt; 16 increments
0__67
0__67
</code></pre>

<p>The only way I can explain that is that during a change of state, the signal received from the sensor is not really square, but somehow noisy.<br>
Like in this image :<br>
<a href=""http://i.stack.imgur.com/DzMIK.gif"" rel=""nofollow""><img src=""http://i.stack.imgur.com/DzMIK.gif"" alt=""explanation?""></a><br>
Therefore we would have, indeed, many RISING on one change....<br>
<em>(However reading the output of the sensor on an analog pin shows a direct variation from 880(HIGH) to 22(LOW))</em></p>

<p>Does anyone have another explanation? Or a solution to this problem ?</p>

<hr>

<p><strong>EDIT</strong></p>

<p>Thanks to @TobiasK I know that this is called a <em>bouncing effect</em>. By doing further research I came across this solution:<br>
playground.arduino.cc/Main/RotaryEncoders (Ctrl+F for rafbuff).<br>
I'm trying it and i'll let you know.</p>
","arduino wheel two-wheeled interrupts"
"8951","Establishing Data Transfer between two Raspberry Pi's using GPIO","<p>Good day</p>

<p>I am currently implementing an autonomous quadcopter with stereo vision using raspberry Pi. One (Let's call this Pi_1) is responsible for stereo vision the other is responsible for motor control and trajectory planning (Pi_2). I was looking for a way to transfer a 480 element float vector via GPIO from Pi_1 to Pi_2. Pi_1 stereovision program runs at 2Hz while Pi_2 motor control runs at 210Hz. </p>

<blockquote>
  <p>Is there any protocol fast enough to deliver this amount of information to the second raspberry pi via GPIO? </p>
</blockquote>

<p>I am currently looking at SPI but I saw that the Raspberry Pi cannot be turned to a Slave making it not an option. I also looked at UART however it is too slow for my needs. All the I2c ports on the Pi are currently being used by the stereo vision cameras and the IMU's. If the gpio option is not feasible, I am also open for other suggestions such as using other hardware (middle man) or wireless options.   </p>
","quadcopter raspberry-pi communication"
"8954","Software to control an Arduino setup with a timing belt and stepper motors","<p>I would like to know what software is available to control a timing belt with stepper motors for arduino board.Much like how its done in 3d printing.
But in this case i wont be making a 3d printer.Just one simple setup.</p>
","arduino robotic-arm stepper-driver"
"8960","What to do when position control with trajectories is interrupted?","<p>What are strategies used when trajectories, which are applied to a robotic joint, are interrupted? Say a robotic arm hits an obstacle, the controller just keeps applying the trajectory. Then at the end, the error gets so large, the torque can get quite strong and damage the robot or snap.</p>
","control robotic-arm joint"
"8962","using the brush assembly and Aerovac bin space?","<p>A lot of the Create 2's interior space is taken up by the brush assembly and the Aerovac bin.  I'd like to take these out and put in my own stuff, but I'm concerned that the Roomba might get confused by the fact that I've unplugged these items.  Is there anything special I need to do, aside from adding an appropriate amount of weight in that area?</p>
","irobot-create"
"8965","CC3D - Replacing RC emitter with an RPi","<p>I am trying to control a quadcopter using te OpenPilot CC3D board and a RaspberryPi. The main idea was first replace the signals from the RC emitter to the CC3D RC receiver for an RPi connected directly to the RC receiver inputs of the CC3D.</p>

<p>As far as I know the RC signals to the CC3D are PWM so the RPi should be able to control the channels using RPIO library to create the PWM by software. </p>

<p>But after make some tests I haven't find any way to move the motors. I am using the Ground Control System (OpenPilot Software) to configure the CC3D.</p>

<p>I am not sure whether I need to send the PWM signals in any order or something like that. I am also not sure how the Flight Mode Switch works, I suposse it works the same way as the other channels, using PWM.</p>

<p>Anyone have made anything similar to this? </p>
","raspberry-pi quadcopter uav"
"8968","Do these motors really have enough torque to lift 130 pounds","<p>I was looking at these dc motors and I converted the torque to force at a meter distance and I got that two of them should be able to lift over 130 pounds, is this right? <a href=""http://www.active-robots.com/high-torque-dc-servo-motor-10rpm-with-step-dir-drive"" rel=""nofollow"">http://www.active-robots.com/high-torque-dc-servo-motor-10rpm-with-step-dir-drive</a> There has to be some catch i am not seeing</p>
","motor torque"
"8970","Is AllJoyn a good ROS alternative regarding message passing cross multiple devices?","<p>I've used ROS for a while, my environment is Raspberry Pi + Ubuntu + OpenCV + ROS + C/C++. I also use several ROS packages (tf2, usb_camera, slam related, and laser scanner capture.) Also, in my projects, nodes are in multiple devices, and I'm using multiple master package for one project.</p>

<p>I did review some tutorials about AllJoyn, but no handon experience so far.</p>

<p>The questions are:</p>

<ol>
<li><p>regarding to message (especially, ROS image) passing cross devices, is AllJoyn a good ROS alternative? (devices are connected by wifi or bluetooth.)</p></li>
<li><p>For AllJoyn, does it still need single master (like roscore in ROS) to coordinate the nodes (or similar)? </p></li>
</ol>

<p>Thanks.</p>
","ros alljoyn"
"8973","How to estimate the position of multiple static ground targets captured from a down facing camera?","<p>An aerial vehicle captures images of the ground using its down facing camera. From the images, multiple targets are converted from their pixel position to the camera reference frame using the pinhole camera model. Since the targets are static and there is information of the vehicle attitude and orientation, each sample is then converted to the world referencial frame. Note that all targets are on a flat, level plane.</p>

<p>The vehicle keeps ""scanning"" for the targets and converting them to the world referencial frame. Due to the quality of the camera and detection algorithm, as well as errors on the altitude information, the position of the ""scanned"" targets is not constant (not accurate). A good representation might be a gaussian distribution around the target true position, however it will also be influenced by the movement of the aerial vehicle.</p>

<p>What's the best approach to estimate the position of the targets from multiple readings?
This basically resumes to a problem of noise removal (as well as outlier removal) and estimation, so I would like to know what algorithms and strategies could solve the problem. In the end I expect to implement and test a collection of different approachs to understand their performance on this specific problem.</p>

<p>Furthermore, this system is implemented using ROS, so if you know of packages that already do what I'm searching for I would be glad to hear. You can also cite papers on the topic that you think might be of my interest.</p>
","localization ros computer-vision algorithm uav"
"8975","How does one implement a third order complementary filter for estimating altitude using data from an accelerometer and a barometer?","<p>I am working with the CJMCU build of cleanflight on a small drone. As of now, the algorithm for altitude hold uses a first order complementary filter to combine data from the barometer and the accelerometer (after integrating the accelerations twice). However, I have noticed a considerable lag in the altitude readings and this seems to be hampering the control algorithm's performance.</p>

<p>The filter in question has been implemented in <a href=""https://github.com/diydrones/ardupilot/blob/db8a2f7e8bb2183e6d281e7a348d455d855cf5e1/libraries/AP_TECS/AP_TECS.cpp"" rel=""nofollow"">https://github.com/diydrones/ardupilot/blob/db8a2f7e8bb2183e6d281e7a348d455d855cf5e1/libraries/AP_TECS/AP_TECS.cpp</a>
However, I'm unable to understand how this works.</p>

<p>Pardon me for any errors I may have committed.</p>
","quadcopter sensors sensor-fusion"
"8976","iRobot Create 2 stuck in Clean mode?","<p>I'm using the Delphi example to command my Create 2, I just adapted the demo code to Unicode (DelphiXE). I use the original iRobot USB to serial cable. </p>

<p>My Create 2 seemed to be responding fine to all the commands send via serial yesterday and correctly received all sensor data back this morning, until I recharged the battery. Now when I send ""7"" ""Soft reset"" the robot attempts every time to start a clean cycle. It also attempts to start the clean cycle when I press the clean button. It tells me to move the Roomba to a new location, which is normal in cleaning mode because my wheels are not touching my desk. Communication via serial seem to be fine because I still get the Soft Reset response texts in the log memo of my app when I use the 2 buttons method to soft reset my Create 2, so there is still communication both ways.</p>

<p>I must say I had the same yesterday after charging but after a while unexpectedly, don't know why, the robot responded again fine to my commands.</p>

<p>It really seems to me the Create 2 is stuck in the Cleaning mode, or am I missing something?</p>

<p>BTW, I also tried to fix the problem by removing the battery.</p>
","irobot-create"
"8981","How do I get the Create 2 to communicate with a laptop via the serial to USB cable?","<p>My computer will not recognize the Serial to USB cable.  I have tried it on a Mac and an HP.  </p>

<p>Is there a driver that I need to install?  If it is supposed to install automatically, it is not.</p>
","irobot-create serial usb"
"8987","Error while building map in ROS slam_gmapping","<p>I have recorded the rosbag data by simulating the robot in gazebo. I played back the logged bag file and tried to build the map using slam_gmapping node and i ended up in error below  </p>

<blockquote>
  <p>[ WARN] [1453398305.145461344]: Laser has to be mounted planar!
  Z-coordinate has to be 1 or -1, but gave: -0.03982</p>
</blockquote>

<p>After few iterations, i was able to build the map by modifying laser scanner joint origin in my URDF file 
from </p>

<pre><code>origin xyz=""0 -0.040 0.2"" rpy=""-1.5708 -1.5708 0""
</code></pre>

<p>to</p>

<pre><code>origin xyz=""0 -1 0.2"" rpy=""1.5708 -1.5708 0""
</code></pre>

<p>but my robot model become weird as below (laser scanner is away from the robot).
<a href=""http://i.stack.imgur.com/bfjPT.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/bfjPT.png"" alt=""enter image description here""></a></p>

<p>How to get around the issue without relocating the laser scanner.
Find my URDF file <a href=""http://%20https://www.dropbox.com/s/af99gstvnv1e3s0/jmbot1.1.urdf?dl=0"" rel=""nofollow"">here</a> </p>
","slam ros navigation mapping simulation"
"8988","Building a quadcopter, what motors, props and what are the calculations?","<p>Before I start, I am a 13 year old, I would like to apologise because I am a beginner to all this and I wouldn't really understand any professional talk, I am only doing this for a hobby.</p>

<p>I am building a quadcopter,</p>

<ul>
<li>Flight controller: KK 1.2.5</li>
<li>ESC: Q Brain 25amp</li>
<li>Frame: KK 260 / FPV 260</li>
<li>Frame Addon: KK/FPV 250 Long Frame Upgrade Kit</li>
<li>Tx &amp; Rx: HobbyKing 6ch tx rx (Mode 2)</li>
<li>Battery: Turnigy Nano-Tech 2200 mAh 3S</li>
</ul>

<p>I am not sure about what motor and propellers I should use.
All I know is: for the frame the motor mounts are: 16mm to 19mm with M3 screws
I am not sure what 1806 and 2208 means.</p>

<p>Here are my questions:</p>

<ol>
<li>What calculations should I do to find out how much thrust the quad needs to produce / any other useful calculations</li>
<li>Using the calculations what would be the best and CHEAPEST motors I could have</li>
<li>And finally, what propeller would be best suited for the motor.</li>
</ol>

<p>p.s: I am looking for a durable and really cheap motors also, I live in London, so shipping might be a problem if there is an immense bill.</p>

<p>Thanks a lot for your time,
Sid</p>
","quadcopter motor"
"8990","Standard equation for steering differential drive robot","<p>I am writing a code in Arduino IDE for NodeMCU Board to control a differential drive 2 wheeled robot.</p>

<p>I am able to steer only one direction for some reason and the steering response time is a little awkward.</p>

<p>Is there perhaps a better strategy for the code that I am using?` </p>

<p>I am using an app called Blynk that has a virtual joystick that controls that feeds the data through Virtual Pins. V1 param 0 and 1 are x and y. x would be left to right on the joystick and y would be forward and back. </p>

<p>Information about the App is available here: <a href=""http://www.blynk.cc/"" rel=""nofollow"">http://www.blynk.cc/</a>. I have it working for the most part, but there is some latency since it is through a cloud service.</p>

<p>The main problem I am stuck on is steering while driving forward and backward. </p>

<pre><code>    //#define BLYNK_DEBUG
//#define BLYNK_PRINT Serial    // Comment this out to disable prints and save space
#define BLYNK_PRINT Serial    // Comment this out to disable prints and save space
#include &lt;ESP8266WiFi.h&gt;
#include &lt;BlynkSimpleEsp8266.h&gt;


int motorA ;
int motorB ;
int X=0;
int Y=0;
int Steer=0;
int maximo=0;

// You should get Auth Token in the Blynk App.
// Go to the Project Settings (nut icon).
char auth[] = ""b41ff7f1659b4badb694be4c59601c2c"";

void setup()
{
  // Set console baud rate
  Serial.begin(9600);


 Blynk.begin(auth,""100Grand"",""Mob4life"");

 pinMode(motorA, OUTPUT); 
 pinMode(motorB, OUTPUT);
 pinMode(0,OUTPUT);
 pinMode(2,OUTPUT);
 pinMode(4,OUTPUT);
 pinMode(5,OUTPUT);

}

 BLYNK_WRITE(V1) 
{
  int X1 = param[0].asInt();
  X=X1;
  int Y1 = param[1].asInt();
 Y=Y1;

}

 BLYNK_WRITE(V0)//      slider  de 100 a 255!!!!
{
 int vel = param.asInt(); 
 maximo=vel;
}

void loop()
{

  if(X == 128  &amp;&amp;  Y == 128)  //  Stop
  {
   motorA = 0;
   motorB = 0;
   analogWrite(5, motorA);  
   analogWrite(4, motorA);
   analogWrite(0, motorB);  
   analogWrite(2, motorB);
   } 

   if(Y &gt; 130 &amp;&amp; X &gt; 127 &amp;&amp; X &lt; 129)   //Forward
  {
    motorA = Y;
    motorB = Y;

    motorA = map(motorA, 450,maximo,130, 255);
    analogWrite(5, motorA);
    digitalWrite(0,LOW);
    motorB = map(motorA, 450,maximo,130, 255);
    analogWrite(4, motorB);
    digitalWrite(2,HIGH);
  }

  else if(Y &lt; 126 &amp;&amp; X &gt; 127 &amp;&amp; X &lt; 129)   //Reverse
  {
    motorA = Y;
    motorB = Y;

    motorA = map(motorA, 450,maximo,126, 0);
    analogWrite(5, motorA);
    digitalWrite(0,HIGH);
    motorB = map(motorA, 450,maximo,126, 0);//something is wrong with HIGH signal
    analogWrite(4, motorB);
    digitalWrite(2,LOW);
  }

   if(Y &gt; 130 &amp;&amp; X &lt; 126)   //Steer Left
  {
    motorA = Y;
    motorB = Y;
    Steer = map(X, 450,maximo, 126,0);
     Steer = X / maximo;

    motorA = map(motorA, 450,maximo,130, 255);
    analogWrite(5, motorA * (1 + Steer));
    digitalWrite(0,LOW);
   motorB = map(motorA, 450,maximo,130, 255);
    analogWrite(4, motorB * (1 - Steer));
    digitalWrite(2,HIGH);
  }

   if(Y &gt; 130 &amp;&amp; X &gt; 130)   //Steer Right
  {
    motorA = Y;
    motorB = Y;
    Steer = map(X, 450,maximo, 126,0);
    Steer = X / maximo;

    motorA = map(motorA, 450,maximo,130, 255);
    analogWrite(5, motorA * (1 - Steer));
    digitalWrite(0,LOW);
    motorB = map(motorA, 450,maximo,130, 255);
    analogWrite(4, motorB * (1 + Steer));
    digitalWrite(2,HIGH);
  }

  Blynk.run();
}
</code></pre>

<p>Any help would be appreciated. Thanks!`</p>
","differential-drive"
"8992","In the SLAM for Dummies, why are there extra variables in the Jacobian Matricies?","<p>I am reading SLAM for Dummies, which you can find on Google, or at this link: <a href=""http://ocw.mit.edu/courses/aeronautics-and-astronautics/16-412j-cognitive-robotics-spring-2005/projects/1aslam_blas_repo.pdf"" rel=""nofollow"">SLAM for Dummies - A Tutorial Approach to Simultaneous Localization and Mapping</a>.</p>

<p>They do some differentiation of matrices on page 33 and I am getting different answers for the resulting Jacobian matrices. </p>

<p>The paper derived </p>

<p>$$
\left[ {\begin{array}{c}
\sqrt{(\lambda_x - x)^2 + (\lambda_y - y)^2} + v_r \\ \tan^{-1}\left(\frac{\lambda_y - y}{\lambda_y - x}\right) - \theta + v_\theta \end{array}} \right]
$$</p>

<p>and got
$$
\left[ {\begin{array}{ccc}
\frac{x - \lambda_y}{r},&amp; \frac{y - \lambda_y}{r},&amp; 0\\ \frac{\lambda_y - y}{r^2},&amp; \frac{\lambda_y - x}{r^2},&amp; -1 \end{array}} \right]
$$</p>

<p>I don't get where the $r$ came from. I got completely different answers. Does anybody know what the $r$ stands for? If not, is there a different way to represent the Jacobian of this matrix?</p>
","slam"
"8994","How to implement transmission in tracked chassis with one motor?","<p>I see that in small robots tracked chassis is implemented with 2 motors, each powering one side of the vehicle, like this: <a href=""http://i.stack.imgur.com/DLfvd.jpg"" rel=""nofollow""><img src=""http://i.stack.imgur.com/DLfvd.jpg"" alt=""enter image description here""></a></p>

<p>(image stolen from here)</p>

<p>But in real scale tanks I assume there is only one motor so there must be some way of applying power to both sides independently.</p>
","tracks gearing chassis"
"8998","Quadcopter PID Controller: Derivative on Measurement / Removing the Derivative Kick","<p>Good day,</p>

<p>I am currently implementing a single loop PID controller using angle setpoints as inputs. I was trying out a different approach for the D part of the PID controller. </p>

<p>What bought this about is that when I was able to reach a 200Hz (0.00419ms) loop rate, when adding a D gain, the quadcopter seems to dampen the movements in a non continous manner. This was not the case when my algorithm was running at around 10Hz. At an angle set point of 0 degrees, I would try to push it to one side by 5 degrees then the quad would try to stay rock solid by resisting the movements but lets go after while enabling me to get it of by 2 degrees (the dampening effect weakens over time) then tries to dampen the motion again.</p>

<p>This is my implementation of the traditional PID:</p>

<blockquote>
  <p>Derivative on Error:</p>
</blockquote>

<pre><code>//Calculate Orientation Error (current - target)
float pitchError = pitchAngleCF - pitchTarget;
pitchErrorSum += (pitchError*deltaTime2);
float pitchErrorDiff = pitchError - pitchPrevError;
pitchPrevError = pitchError;

float rollError = rollAngleCF - rollTarget;
rollErrorSum += (rollError*deltaTime2);
float rollErrorDiff = rollError - rollPrevError;
rollPrevError = rollError;

float yawError = yawAngleCF - yawTarget;
yawErrorSum += (yawError*deltaTime2);
float yawErrorDiff = yawError - yawPrevError;
yawPrevError = yawError;


//PID controller list
float pitchPID = pitchKp*pitchError + pitchKi*pitchErrorSum + pitchKd*pitchErrorDiff/deltaTime2;
float rollPID = rollKp*rollError + rollKi*rollErrorSum + rollKd*rollErrorDiff/deltaTime2;
float yawPID = yawKp*yawError + yawKi*yawErrorSum + yawKd*yawErrorDiff/deltaTime2;

//Motor Control - Mixing    
//Motor Front Left (1)
float motorPwm1 =  -pitchPID + rollPID - yawPID + baseThrottle + baseCompensation;
</code></pre>

<p>What I tried to do now is to implement a derivative on measurement method from this article to remove derivative output spikes. However the Derivative part seems to increase the corrective force than dampen it.</p>

<blockquote>
  <p>Derivative on Measurement:</p>
</blockquote>

<pre><code>//Calculate Orientation Error (current - target)
float pitchError = pitchAngleCF - pitchTarget;
pitchErrorSum += (pitchError*deltaTime2);
float pitchErrorDiff = pitchAngleCF - pitchPrevAngleCF; // &lt;----
pitchPrevAngleCF = pitchAngleCF;

float rollError = rollAngleCF - rollTarget;
rollErrorSum += (rollError*deltaTime2);
float rollErrorDiff = rollAngleCF - rollPrevAngleCF; // &lt;----
rollPrevAngleCF = rollAngleCF;

float yawError = yawAngleCF - yawTarget;
yawErrorSum += (yawError*deltaTime2);
float yawErrorDiff = yawAngleCF - yawPrevAngleCF; // &lt;----
yawPrevAngleCF = yawAngleCF;


//PID controller list // &lt;---- The D terms are now negative
float pitchPID = pitchKp*pitchError + pitchKi*pitchErrorSum - pitchKd*pitchErrorDiff/deltaTime2;
float rollPID = rollKp*rollError + rollKi*rollErrorSum - rollKd*rollErrorDiff/deltaTime2;
float yawPID = yawKp*yawError + yawKi*yawErrorSum - yawKd*yawErrorDiff/deltaTime2;


//Motor Control - Mixing    
//Motor Front Left (1)
float motorPwm1 =  -pitchPID + rollPID - yawPID + baseThrottle + baseCompensation;
</code></pre>

<p>My question now is:</p>

<blockquote>
  <p>Is there something wrong with my implementation of the second method? </p>
</blockquote>

<p>Source: <a href=""http://brettbeauregard.com/blog/2011/04/improving-the-beginner%E2%80%99s-pid-derivative-kick/"" rel=""nofollow"">http://brettbeauregard.com/blog/2011/04/improving-the-beginner%E2%80%99s-pid-derivative-kick/</a></p>

<p>The way I've obtained the change in time or DT is by taking the timestamp from the start of the loop then taking the next time stamp at the end of the loop. Their difference is obtained to obtain the DT. getTickCount() is an OpenCV function.</p>

<pre><code>/* Initialize I2c */  
/* Open Files for data logging */ 
while(1){
    deltaTimeInit=(float)getTickCount();

    /* Get IMU data */
    /* Filter using Complementary Filter */
    /* Compute Errors for PID */
    /* Update PWM's */

    //Terminate Program after 40 seconds
    if((((float)getTickCount()-startTime)/(((float)getTickFrequency())))&gt;20){
            float stopTime=((float)getTickCount()-startTime)/((float)getTickFrequency());
    gpioPWM(24,0);  //1
    gpioPWM(17,0);  //2
    gpioPWM(22,0);  //3
    gpioPWM(18,0);  //4
    gpioTerminate(); 
        int i=0;
    for (i=0 ; i &lt; arrPitchCF.size(); i++){
        file8 &lt;&lt; arrPitchCF.at(i) &lt;&lt; endl;
    }

    for (i=0 ; i &lt; arrYawCF.size(); i++){
        file9 &lt;&lt; arrYawCF.at(i) &lt;&lt; endl;
    }

    for (i=0 ; i &lt; arrRollCF.size(); i++){
        file10 &lt;&lt; arrRollCF.at(i) &lt;&lt; endl;
    }

    for (i=0 ; i &lt; arrPitchAccel.size(); i++){
        file2 &lt;&lt; arrPitchAccel.at(i) &lt;&lt; endl;
    }

    for (i=0 ; i &lt; arrYawAccel.size(); i++){
        file3 &lt;&lt; arrYawAccel.at(i) &lt;&lt; endl;
    }

    for (i=0 ; i &lt; arrRollAccel.size(); i++){
        file4 &lt;&lt; arrRollAccel.at(i) &lt;&lt; endl;
    }

    for (i=0 ; i &lt; arrPitchGyro.size(); i++){
        file5 &lt;&lt; arrPitchGyro.at(i) &lt;&lt; endl;
    }

    for (i=0 ; i &lt; arrYawGyro.size(); i++){
        file6 &lt;&lt; arrYawGyro.at(i) &lt;&lt; endl;
    }

    for (i=0 ; i &lt; arrRollGyro.size(); i++){
        file7 &lt;&lt; arrRollGyro.at(i) &lt;&lt; endl;
    }

    for (i=0 ; i &lt; arrPWM1.size(); i++){
        file11 &lt;&lt; arrPWM1.at(i) &lt;&lt; endl;
    }

    for (i=0 ; i &lt; arrPWM2.size(); i++){
        file12 &lt;&lt; arrPWM2.at(i) &lt;&lt; endl;
    }

    for (i=0 ; i &lt; arrPWM3.size(); i++){
        file13 &lt;&lt; arrPWM3.at(i) &lt;&lt; endl;
    }

    for (i=0 ; i &lt; arrPWM4.size(); i++){
        file14 &lt;&lt; arrPWM4.at(i) &lt;&lt; endl;
    }

    for (i=0 ; i &lt; arrPerr.size(); i++){
        file15 &lt;&lt; arrPerr.at(i) &lt;&lt; endl;
    }

    for (i=0 ; i &lt; arrDerr.size(); i++){
        file16 &lt;&lt; arrDerr.at(i) &lt;&lt; endl;
    }


        file2.close();
        file3.close();
        file4.close();
        file5.close();
        file6.close();
        file7.close();
        file8.close();
        file9.close();
        file10.close();
        file11.close();
        file12.close();
        file13.close();
        file14.close();
        file15.close();
        file16.close();
        cout &lt;&lt; "" Time Elapsed = "" &lt;&lt; stopTime &lt;&lt; endl;
        break;
    }

    while((((float)getTickCount()-deltaTimeInit)/(((float)getTickFrequency())))&lt;=0.00419){ //0.00209715|0.00419
       cout &lt;&lt; "" DT end = "" &lt;&lt; deltaTime2 &lt;&lt; endl;
       deltaTime2=((float)getTickCount()-deltaTimeInit)/(((float)getTickFrequency()));
    }

    cout &lt;&lt; "" DT end = "" &lt;&lt; deltaTime2 &lt;&lt; endl;
}
</code></pre>

<p>Here's my data:</p>

<p><a href=""http://i.stack.imgur.com/H9O7s.jpg"" rel=""nofollow""><img src=""http://i.stack.imgur.com/H9O7s.jpg"" alt=""Derivative on Error without D gain""></a></p>

<p><a href=""http://i.stack.imgur.com/flipn.jpg"" rel=""nofollow""><img src=""http://i.stack.imgur.com/flipn.jpg"" alt=""Derivative on Error with D gain""></a></p>

<p><a href=""http://i.stack.imgur.com/B8x2r.jpg"" rel=""nofollow""><img src=""http://i.stack.imgur.com/B8x2r.jpg"" alt=""Derivative on Measurement with D gain""></a></p>
","control quadcopter pid raspberry-pi stability"
"9001","Robot never goes straight","<p>I am using <strong>2 identical</strong> DC motors and a castor wheel. The motors are connected to <strong>L293D motor driver</strong> and are controlled by <strong>RPi</strong>. </p>

<p>The robot is not going straight. It veers off to the right. I am running both the motors at <strong>100% PWM</strong>.</p>

<p>What I tried to correct the error:</p>

<ol>
<li>I adjusted the PWM of the wheel going faster to <strong>99%</strong>, but the robot just turns to the other side; </li>
<li>I adjusted the weight on the robot and the problem still persists.</li>
</ol>

<p>I once tried to run the motor without any load. Is that the cause of this, as I was later told that, running a DC motor without any load damages them?</p>

<p>If that is not the cause, then please tell me how to solve this problem without using any <strong>sensors</strong> for controlling it. </p>
","control motor wheeled-robot raspberry-pi"
"9007","Is 2 motors needed for a arm that can move anyway not just circular?","<p>If I'm limited to 4 motors, for four legs, how could the robot rotate if they all only go up/down directions? I'd have to use 2 motors on a leg? Correct?</p>
","stepper-motor"
"9009","How to connect Phone to Robot while charging phone from external battery?","<p>I am controlling a robot via usb from an Android phone running the robot's code. This phone has a poor battery and I need to extend its life with a USB charger (can't change phones). How can I charge an android phone via usb, while maintaining a USB connection to the robot? I can solder wires together if needed, or can buy adapters as needed.</p>

<p><img src=""http://i.imgur.com/b7y5cE6.png"" alt=""an image of a robot, phone, and battery all connected by USB. The phone is charged by the battery while still controlling the robot by USB""></p>
","mobile-robot usb"
"9010","Is the distortion introduced by a lens protector significant in practice?","<p>I have a computer-vision application I made to localize a robot in a room; the software has been in use for a while and is working fine.</p>

<p>When I calibrated the camera and got the intrinsics and lens distortion coefficients there was a lens protector on the lens, mounted on the robot's lid.</p>

<p>If I take off the robot's lid (and thus the lens protector) the localization solution becomes erratic and inaccurate, so I think the lens protector might be changing the distortion properties significantly.</p>

<p>Today the lens protector became detached and it was replaced shortly after. So now the calibration may no longer be valid and the localization solution is much more noisy.</p>

<p><em>Can a lens protector can greatly effect the distortion properties of the image, or can someone offer another explanation?</em></p>

<p>I intend to recalibrate and super-glue the lens protector down to the robot's lid, but I am curious if this is my problem, and if anyone else has encountered this with lens protectors.</p>
","computer-vision cameras calibration"
"9013","How many DOFs required to define a 3D pose","<p>I understand that to be able to define point in 3D space, you need three degrees of freedom (DOFs). To additionally define an orientation in 3D space, you need 6 DOFs. This is intuitive to me when each of these DOFs defines the position or orientation along one axis or an orthogonal X-Y-Z system.</p>

<p>However, consider a robot arm such as this: <a href=""http://www.robotnik.eu/robotics-arms/kinova-mico-arm/"" rel=""nofollow"">http://www.robotnik.eu/robotics-arms/kinova-mico-arm/</a>. This too has 6 DOFs, but rather than each DOF defining a position or orientation in an X-Y-Z system, it defines an angular rotation of one joint along the arm. If all the joints were arranged along a single axis, for example, then these 6 DOFs would in fact only define one angular rotation.</p>

<p>So, it is not true that each DOF independently defines a single position or orientation. However, in the case of this robot arm, it can reach most positions and orientations. I'm assuming this is because the geometry of the links between the joints make each DOF define an independent position or orientation, but that is a very vague concept to me and not as intuitive as simply having one DOF per position or orientation.</p>

<p>Can somebody offer some help in understanding these concepts?</p>
","kinematics"
"9020","How does one calculate distance and angle using a target with known measurements?","<p>The target is in the shape of a U where the horizontal segment is 20 inches, and the two vertical segments are 14 inches. We are using a camera to image the target, and then using vision processing to isolate the target from the rest of the image. We know the vertical field of view, and the horizontal field of view of the camera. The resolution of the camera is 640x480 pixels.</p>

<p>The vertical distance between the camera on the robot and the target is constant but as of yet unknown because the robot hasn't been constructed yet. It is known, however, that the target will always have a higher elevation than the camera.</p>

<p>How can we use this data to calculate in real time the robot's distance to the target, and the angle to the target?</p>
","computer-vision real-time"
"9022","The logic of implementing an Auto-level function in a PID flight controller","<p>So I have multi-rotor with a basic PID controller, that keeps its axis stable through the gyroscope. However, the multi rotor, does not keep its height or position. So I would like to use an accelerometer for keeping its rough position (auto level). I want to use both the gyro and accelerometer, but how would the accelerometer values be used, is it implemented through the PID the same ways the gyro values are (degrees per second, which is the rate I used to calculate PID)? And then adjusting the esc through that?? I am confused at that part (the basic logic for using the accelerometer values) </p>
","quadcopter pid imu accelerometer"
"9023","Is there a way to measure 3 axis orientation without a magnetometer?","<p>I have bought an STM iNEMO evaluation board in order to monitor the inclination of a separate magnetic sensor array as it moves in a linear scan outside of a (non-magnetic) stainless steel pipe. I want to measure the inclination of the sensor along the scan and ensure that it does not change. The problem I have found is that the measured magnetic field from the integrated magnetometer varies greatly with position along the pipe, and in turn, causes a large, position dependent error in one axis of the inclination reported by the iNEMO IMU. In fig. 1 below I show the set up of the test, I measured the inclination from the IMU while moving it along the length of the pipe and back again. The board did not change inclination throughout the measurement. In Fig 2 I show the magnetometer and inclination measurements recorded by the ""iNEMO application"" showing the large error in one of the inclinations.</p>

<p>My question is whether you know if there is any way of correcting for the magnetic field variation so that I can still accurately determine the inclination in all three directions? My data suggests to me that the magnetic field variation measured from the magnetometer is much greater than the geomagnetic field, so the inclination measurement will always be inaccurate. A follow up question I then have is: Is there a way to measure 3 axis orientation WITHOUT using a magnetometer?</p>

<p><a href=""http://i.stack.imgur.com/V3Z4I.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/V3Z4I.png"" alt=""Fig. 1: Movement of IMU along length of pipe""></a></p>

<p><a href=""http://i.stack.imgur.com/Ac1rF.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/Ac1rF.png"" alt=""iNemo magnetometer and inclination data when scanning from left to right of the pipe""></a></p>
","imu gyroscope magnetometer orientation"
"9025","Cost of material to 3D print","<p>Can someone please share the typical cost of material to 3D print an object like a raspberry pi case?  Thank you.</p>
","3d-printing"
"9027","Do walking robots use accelerometers?","<p>My understanding of walking robots (e.g. <a href=""https://www.youtube.com/watch?v=xJlkBBdyBYI"" rel=""nofollow"">https://www.youtube.com/watch?v=xJlkBBdyBYI</a>) is that they use a gyroscope to determine the current orientation of the robot, or each joint of the robot. This is because if you just put encoders on each joint, the cumulative error over the entire robot will be too large to maintain stability. Therefore, a gyroscope measures the ""real"" orientation, and this is used for feedback when the robot is walking.</p>

<p>However, I'm also aware that some walking robots use accelerometers to maintain stability. What would be the benefit of using an accelerometer in this case? Would it be used instead of a gyroscope, or together with a gyroscope?</p>

<p>My guess is that gyroscopes do not measure acceleration directly (unless you were to numerically calculate this based on lots of orientation readings), but accelerometers do measure it directly (and more reliably than this numerical method). Know the acceleration as well as the position then enables to robot to more accurately predict its future position, and hence the feedback loop is more robust. Is this correct, or am I missing the point?</p>
","control sensors accelerometer dynamics walking-robot"
"9033","How modular arm joints work","<p>Hello I'm trying to figure out how modular arm joints are designed and what kind of bearings/shafts are used for a modular-type robotic arm. Take ""UR arm"" for example. I believe those 'T-shaped pipes' include both a drive and bearing system. And as you can see from second image, it can be detached easily. So I think it's not just a simple ""motor shaft connecting to the member that we want to rotate"" mechanism. I'm wondering which type of mechanism and bearing system is inside of those T-shaped pipes. How can i transfer rotational motion to a member without using shafts? </p>

<p><a href=""http://i.stack.imgur.com/aJTMs.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/aJTMs.png"" alt=""UR3 Robotic Arm""></a></p>

<p><a href=""http://i.stack.imgur.com/LiRoD.jpg"" rel=""nofollow""><img src=""http://i.stack.imgur.com/LiRoD.jpg"" alt=""UR10 Robotic Arm Joint""></a></p>
","mechanism joint arm"
"9034","What does ""6 degrees of freedom"" mean?","<p>I am looking at <a href=""https://www.sparkfun.com/pages/accel_gyro_guide"">this page</a> that describes various characteristics of gyroscopes and accelerometers. Close to the end (where they speak about IMUs), the names of the items have something like this:</p>

<ul>
<li>9 degrees of freedom</li>
<li>6 degrees of freedom</li>
</ul>

<p>Can anyone explain what does this mean?</p>
","accelerometer gyroscope"
"9038","How to load a PUMA robot in the existing environment in OpenRAVE 0.9","<p>I have a PR2 robot in an environment, which can be seen on the GUI of OpenRAVE. </p>

<p>Now, how can I load a <a href=""https://en.wikipedia.org/wiki/Programmable_Universal_Machine_for_Assembly"" rel=""nofollow"">PUMA</a> robot arm in the same environment?</p>
","robotic-arm motion-planning"
"9039","How can I switch between Autonomous mode and usercontrol?","<p>I want to switch from usercontrol to autonomous. When I have the program running for 120 seconds, how come it wont automatically switch in autonomous mode? Thanks!</p>

<pre><code>#pragma config(Motor,  port1,           driveBR,       tmotorVex393, openLoop)
#pragma config(Motor,  port2,           driveFR,       tmotorVex393, openLoop)
#pragma config(Motor,  port3,           driveFL,       tmotorVex393, openLoop)
#pragma config(Motor,  port4,           flyRight,      tmotorVex393, openLoop)
#pragma config(Motor,  port5,           driveBL,       tmotorVex393, openLoop)
#pragma config(Motor,  port6,           flyLeft,       tmotorVex393, openLoop)
#pragma config(Motor,  port10,          Belt,          tmotorVex393, openLoop)

//*!!Code automatically generated by 'ROBOTC' configuration wizard               !!*//

#pragma platform(VEX)
#pragma competitionControl(Competition)
#pragma autonomousDuration(15)
#pragma userControlDuration(120)

#include ""Vex_Competition_Includes.c""

//Main competition background code...do not modify!

void pre_auton() {
}

task autonomous() {
    while(true == true) {
        motor[flyLeft] = -127;
        motor[flyRight] = 127;
        wait1Msec(500);
        motor[Belt] = -127;
    }
}

task usercontrol() {
    while (true == true) {
        motor[driveFR] = -vexRT[Ch2];
        motor[driveFL] = vexRT[Ch3];
        motor[driveBR] = vexRT[Ch2];
        motor[driveBL] = vexRT[Ch3];

        if(vexRT[Btn6D] == 1) {
            motor[flyRight] = -127;
            motor[flyLeft] = -127;
        }
        if(vexRT[Btn6D] == 0) {
            motor[flyRight] = 0;
            motor[flyLeft] = 0;
        }
        if(vexRT[Btn5D] == 1) {
            motor[Belt] = -127;
        }
        if(vexRT[Btn5D] == 0) {
            motor[Belt] = 0;
        }
    }
}
</code></pre>
","wheeled-robot robotc vex"
"9040","Metal shaft design for a 6mm plastic bevel gear","<p>I have a small POM bevel gear with these dimensions:</p>

<p><a href=""http://i.stack.imgur.com/ARMjQ.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/ARMjQ.png"" alt=""enter image description here""></a></p>

<p>It has a 6mm hole for the shaft and a M4 hole for the set screw.</p>

<p>Suppose this bevel gear is meshed with a 45T bevel gear and give a max. output torque of 0.4kg/cm. How should the design of the 6mm shaft be? Should the diameter be precisely 6mm? Should it be flattened into a 'D' shape (so that the set screw can hold the shaft)? I'm planning to use a metal shaft.</p>

<p>Any help will be appreciated.</p>

<p>Thanks</p>
","mechanism torque gearing"
"9043","Joystick Rate Limit Filter For FRC Java Programming","<p>I am a programmer for my school's FRC robotics team and have received the request from our hardware/driving department to limit the speed at which the robot's motors can accelerate given a joystick input telling it to increase the speed of the motor. For example, when the robot first starts up and the driver decides to move the joystick from the center to the fully up position (0 to full motor power), we don't want it to literally go from 0 to full motor power in an instant - it obviously creates some rather jerky, unstable behavior. How might I receive the target joystick position from the joystick, save it, and build up to it over time (and if any other inputs are sent in this process — like telling it to turn around — stop the current process and enact the new one)?</p>

<p>I am using Java with WPILib's 2016 robotics library: here's the API <a href=""http://first.wpi.edu/FRC/roborio/release/docs/java/"" rel=""nofollow"">http://first.wpi.edu/FRC/roborio/release/docs/java/</a>, and here's the tutorials <a href=""http://wpilib.screenstepslive.com/s/4485/m/13809"" rel=""nofollow"">http://wpilib.screenstepslive.com/s/4485/m/13809</a>.</p>

<p>I am using the ""IterativeRobot"" template class, and teleop is being run in the method teleopPeriodic(), which is continuously called every few milliseconds in the program (it's where i'm receiving joystick input and calling the method RobotDrive.tankDrive() with the inputs).</p>

<p>I realize this is more of a programming question than a robotics question, but I figured it would be better to put it here than in stack overflow, etc. If someone could give me some simple pseudocode or just a conceptual idea of how this might be done (not necessarily as it pertains directly to the library or the language I'm using), that would be great.</p>
","software first-robotics"
"9044","Create 2 Cable with 700 Series Roomba","<p>For the last few months I have been playing with ROS on an nVidia Jetson TK1 development board. Up until this point, it has mostly been playing with the GPIO header, an Arduino Uno, a couple physical contact sensors, and a few custom motor and servo boards that I slapped together. But lately I've been eyeing an old 700 series Roomba that has been gathering dust (was replaced by an 800 series).</p>

<p>Does anyone know if the Communication Cable for Create 2 will work with a 700 series Roomba?</p>

<p>I know there are DIY designs out there, but I have always been a fan of using off-the-shelf components if they exist - you rarely save more money than your time is worth if it is something like a cable or similar component. So if the Create 2 cable will work, I'll use that. If not, I'll see what I can do to make my own.</p>
","ros irobot-create roomba"
"9048","L293D won't turn motor backwards","<p>My small robot has two motors controlled by an L293D and that is controlled via a Raspberry Pi. They will both go forwards but only one will go backwards. </p>

<p>I've tried different motors and tried different sockets in the breadboard, no luck. Either the L293D's chip is broken (but then it wouldn't go forwards) or I've wired it wrong. </p>

<p>I followed the tutorial, <a href=""http://computers.tutsplus.com/tutorials/controlling-dc-motors-using-python-with-a-raspberry-pi--cms-20051"" rel=""nofollow"">Controlling DC Motors Using Python With a Raspberry Pi</a>, exactly.</p>

<p>Here is a run down of what works. Let the 2 motors be A and B:</p>

<p>When I use a python script (see end of post) both motors go ""forwards"". When I change the values in the Python script, so the pin set to HIGH and the pin set to LOW are swapped, motor A will go ""backwards"", this is expected. However, motor B will not move at all. </p>

<p>If I then swap both motors' wiring then the original python script will make both go backwards but swapping the pins in the code will make motor A go forwards but motor B won't move.</p>

<p>So basically, motor A will go forwards or backwards depending on the python code but motor B can only be changed by physically changing the wires.</p>

<p>This is <code>forwards.py</code></p>

<pre><code>import RPi.GPIO as GPIO
from time import sleep

GPIO.setmode(GPIO.BOARD)

Motor2A = 23
Motor2B = 21
Motor2E = 19

Motor1A = 18
Motor1B = 16
Motor1E = 22

GPIO.setup(Motor1A, GPIO.OUT)
GPIO.setup(Motor1B, GPIO.OUT)
GPIO.setup(Motor1E, GPIO.OUT)

GPIO.setup(Motor2A, GPIO.OUT)
GPIO.setup(Motor2B, GPIO.OUT)
GPIO.setup(Motor2E, GPIO.OUT)

print(""ON"")
GPIO.output(Motor1A, GPIO.HIGH)
GPIO.output(Motor1B, GPIO.LOW)
GPIO.output(Motor1E, GPIO.HIGH)

GPIO.output(Motor2A, GPIO.HIGH)
GPIO.output(Motor2B, GPIO.LOW)
GPIO.output(Motor2E, GPIO.HIGH)
</code></pre>

<p>And this is <code>backwards.py</code></p>

<pre><code>import RPi.GPIO as GPIO
from time import sleep

GPIO.setmode(GPIO.BOARD)

Motor2A = 21
Motor2B = 23
Motor2E = 19

Motor1A = 16
Motor1B = 18
Motor1E = 22

GPIO.setup(Motor1A, GPIO.OUT)
GPIO.setup(Motor1B, GPIO.OUT)
GPIO.setup(Motor1E, GPIO.OUT)

GPIO.setup(Motor2A, GPIO.OUT)
GPIO.setup(Motor2B, GPIO.OUT)
GPIO.setup(Motor2E, GPIO.OUT)

print(""ON"")
GPIO.output(Motor1A, GPIO.HIGH)
GPIO.output(Motor1B, GPIO.LOW)
GPIO.output(Motor1E, GPIO.HIGH)

GPIO.output(Motor2A, GPIO.HIGH)
GPIO.output(Motor2B, GPIO.LOW)
GPIO.output(Motor2E, GPIO.HIGH)
</code></pre>

<p>If you see this diff <a href=""https://www.diffchecker.com/skmx6084"" rel=""nofollow"">https://www.diffchecker.com/skmx6084</a>, you can see the difference:</p>

<blockquote>
  <p><a href=""http://i.stack.imgur.com/q2YDS.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/q2YDS.png"" alt=""Screenshot of code diff""></a></p>
</blockquote>

<p>Below are some pictures. You can use the colour of the cables to link them between pictures</p>

<p><a href=""http://i.stack.imgur.com/Ke9VG.jpg"" rel=""nofollow""><img src=""http://i.stack.imgur.com/Ke9VG.jpg"" alt=""enter image description here""></a>
<a href=""http://i.stack.imgur.com/W9gbw.jpg"" rel=""nofollow"">enter image description here</a>
<a href=""http://i.stack.imgur.com/VcWml.jpg"" rel=""nofollow""><img src=""http://i.stack.imgur.com/VcWml.jpg"" alt=""enter image description here""></a>
<a href=""http://i.stack.imgur.com/Oq9ho.jpg"" rel=""nofollow""><img src=""http://i.stack.imgur.com/Oq9ho.jpg"" alt=""enter image description here""></a></p>
","wheeled-robot raspberry-pi"
"9052","Alternative to BeagleBone Black for Node.js based remote control project?","<p>I am working on a remote control project that involves using Node.js and Socket.io to transmit joystick data from a webpage to my BeagleBone Black.</p>

<p>However, I am somewhat disappointed with the BeagleBone - it seems like what should be such simple tasks such as connecting to Wi-Fi can be quite tricky...</p>

<p>My question is: Are there alternative boards I should be looking at? Boards that also have Node.js libraries with PWM support, could stream video from a webcam, but are easier to set up and have a larger developer community?</p>
","control microcontroller pwm beagle-bone"
"9054","Mass Matrix in Lagrange equation","<p>I want to find the equations of motion of RRRR robot.I have studied about it a bit but I am having some confusion.</p>

<p>Here in one of the lectures I found online it describes as Inertia matrix of a link as Ii which is computed by tilde of I also described in picture below???
<a href=""http://i.stack.imgur.com/Qcco6.jpg"" rel=""nofollow""><img src=""http://i.stack.imgur.com/Qcco6.jpg"" alt=""enter image description here""></a></p>

<p>So tilde of I is computed wrt to fixed frame attached to the centre of mass.</p>

<p>However in another example below from another source there is no rotation matrix multiplication with Ic1 and Ic2 as shown above.Am I missing something??
<a href=""http://i.stack.imgur.com/qVbTE.jpg"" rel=""nofollow""><img src=""http://i.stack.imgur.com/qVbTE.jpg"" alt=""enter image description here""></a></p>

<p>What is the significance of multiplying Rotation matrix with Ic1 or tilde of I?</p>

<p>I am using former approach and getting fairly large mass matrix. Is it normal to have such long terms inside Mass matrix?? still need to know though which method is correct?? </p>

<p><a href=""http://i.stack.imgur.com/ECwAB.jpg"" rel=""nofollow""><img src=""http://i.stack.imgur.com/ECwAB.jpg"" alt=""enter image description here""></a>
<a href=""http://i.stack.imgur.com/4j9fK.jpg"" rel=""nofollow""><img src=""http://i.stack.imgur.com/4j9fK.jpg"" alt=""enter image description here""></a>
<a href=""http://i.stack.imgur.com/drhkQ.jpg"" rel=""nofollow""><img src=""http://i.stack.imgur.com/drhkQ.jpg"" alt=""enter image description here""></a></p>

<p>the equation i used for Mass Matrix is<a href=""http://i.stack.imgur.com/U4ox0.jpg"" rel=""nofollow""><img src=""http://i.stack.imgur.com/U4ox0.jpg"" alt=""enter image description here""></a></p>
","robotic-arm dynamics"
"9057","Is this supposed to be a bearing?","<p>I'm looking at the assembly of a tail rotor that should look like this</p>

<p><a href=""http://i.stack.imgur.com/UuQ5b.jpg"" rel=""nofollow""><img src=""http://i.stack.imgur.com/UuQ5b.jpg"" alt=""enter image description here""></a>
<sup><a href=""http://i.stack.imgur.com/8ElkZ.jpg"" rel=""nofollow"">original image</a></sup></p>

<p>I wonder if the ""tail output shaft stopper"" (circled in red) is meant to be a bearing or just a piece of metal stopper:</p>

<p><a href=""http://i.stack.imgur.com/ZH64j.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/ZH64j.png"" alt=""enter image description here""></a></p>

<p>My reading is that, since it's held by 2 set screws, the whole part should rotate with the rod. While rotating, it'd rub against the bevel gear on the tail drive though. Am I missing something?</p>
","mechanism torque gearing"
"9063","iRobot Create 2/Roomba 530 Screw size/thread?","<p>I've looked everywhere I can think of to find this information, but haven't come across anything. Does anyone know what kind of screws I can use to replace the ones on top of my Roomba 530? </p>

<p>I realize that the Create 2 is technically a 600 series, but I would expect they were the same.</p>

<p>I'd like to replace the screws on my Roomba with standoffs so I can stick a mounting plate on top of it. (Additional sensors, CPU, etc.)</p>
","irobot-create roomba"
"9069","Analogue video to digital","<p>I have FPV camera which outputs analog video (RCA, PAL).</p>

<p>I want to capture video and do image processing, therefore I need some way to convert the analog video to digital.</p>

<p>Can some one recommend me how to do it? Is there advice or a shield which can assist?</p>

<p>Please note:</p>

<ol>
<li>I want to convert the frames with minimum latency, because it is a real time flying drone.</li>
<li>I don't need to convert the image to some compressed format (which encoding/ decoding may take time), if I can get the RGB matrix straight, it is preferred.</li>
<li>I thought about digital output camera, but I need one which weighs few grams and I haven't found yet.</li>
</ol>
","quadcopter cameras"
"9073","Determine robot's position in a nearby room","<p><strong>Scenario</strong></p>

<p>I have 2 roaming robots, each in different rooms of a house, and both robots are connected to the house wifi. Each robot only has access to the equipment on itself.</p>

<p><strong>Question</strong></p>

<p>How can the robots be aware of each other's exact position using only their own equipment and the house wifi?</p>

<p><strong>EDIT: Additional Info</strong></p>

<p>Right now the robots only have:</p>

<ul>
<li>RGBDSLAM via Kinect</li>
<li>No initial knowledge of the house or their location (no docks, no mappings/markings, nada) </li>
<li>Can communicate via wifi and that part is open ended</li>
</ul>

<p>I'm hoping to be able to stitch the scanned rooms together before the robots even meet. Compass + altimeter + gps will get me close but the goal is to be within an inch of accuracy which makes this tough. There IS freedom to add whatever parts to the robots themselves / laptop but the home needs to stay dynamic (robots will be in a different home every time).</p>
","mobile-robot localization precise-positioning"
"9074","Software real-time of ROS system","<p>As far as I know, a hardware real-time robot control system requires a specific computing unit to solve the kinematics and dynamics of a robot such as interval zero RTX, which assigns CPU cores exclusively for the calculation, or a DSP board, which does exactly the same calculation. This configuration makes sure that each calculation is strictly within, maybe, 1 ms. </p>

<p>My understanding is that ROS, which runs under Ubuntu, doesn't have a exclusive 
computing unit for that. Kinematics and dynamics run under different threads of the same CPU which operates the Ubuntu system, path plan, and everything else. </p>

<p>My question is that how does ROS achieve software-real time? Does it slow down the sampling time to maybe 100ms and makes sure each calculation can be done in time? Or the sampling time changes at each cycle maybe from 5ms, 18ms, to 39ms each time in order to be as fast as possible and ROS somehow compensates for it at each cycle?</p>
","microcontroller ros real-time"
"9077","PID Tuning for an Unbalanced Quadcopter: When do I know if the I-gain I've set is too high?","<p>Good day,</p>

<p>I am working on an autonomous flight controller for a quadcopter ('X' configuration) using only angles as inputs for the setpoints used in a single loop PID controller running at 200Hz (PID Implementation is Here: <a href=""http://robotics.stackexchange.com/questions/8998/quadcopter-pid-controller-derivative-on-measurement-removing-the-derivative-k"">Quadcopter PID Controller: Derivative on Measurement / Removing the Derivative Kick</a>). For now I am trying to get the quadcopter to stabilize at a setpoint of 0 degrees. The best I was able to come up with currently is +-5 degrees which is bad for position hold. I first tried using only a PD controller but since the quadcopter is inherently front heavy due to the stereo cameras, no amount of D or P gain is enough to stabilize the system. An example is the image below which I added a very small I gain:</p>

<p><a href=""http://i.stack.imgur.com/OvijX.jpg"" rel=""nofollow""><img src=""http://i.stack.imgur.com/OvijX.jpg"" alt=""Small/Almost Negligible I-gain""></a></p>

<p>As you can see from the image above (at the second plot), the oscillations occur at a level below zero degrees due to the quadcopter being front heavy. This means that the quad oscillates from the level postion of 0 degrees to and from a negative angle/towards the front. To compensate for this behaviour, I discovered that I can set the DC level at which this oscillations occur using the I gain to reach the setpoint. An image is shown below with [I think] an adequate I gain applied:</p>

<p><a href=""http://i.stack.imgur.com/o9Qmh.jpg"" rel=""nofollow""><img src=""http://i.stack.imgur.com/o9Qmh.jpg"" alt=""Ok I-gain, I think""></a></p>

<p>I have adjusted the PID gains to reduce the jitters caused by too much P gain and D gain. These are my current settings (Which are two tests with the corresponding footage below):</p>

<p><a href=""http://i.stack.imgur.com/JfZX8.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/JfZX8.png"" alt=""Current Settings""></a></p>

<p>Test 1: <a href=""https://youtu.be/8JsraZe6xgM"" rel=""nofollow"">https://youtu.be/8JsraZe6xgM</a></p>

<p>Test 2: <a href=""https://youtu.be/ZZTE6VqeRq0"" rel=""nofollow"">https://youtu.be/ZZTE6VqeRq0</a></p>

<p>I can't seem to tune the quadcopter to reach the setpoint with at least +-1 degrees of error. I noticed that further increasing the I-gain no longer increases the DC offset. </p>

<blockquote>
  <p>When do I know if the I-gain I've set is too high? How does it reflect on the plot?</p>
</blockquote>

<p>EDIT:
The Perr in the graphs are just the difference of the setpoint and the CF (Complementary Filter) angle.
The Derr plotted is not yet divided by the deltaTime because the execution time is small ~ 0.0047s which will make the other errors P and I hard to see.
The Ierr plotted is the error integrated with time.</p>

<p>All the errors plotted (Perr, Ierr, Derr) are not yet multiplied by the Kp, Ki, and Kd constants</p>

<p>The 3rd plot for each of the images is the response of the quadcopter. The values on the Y axis correspond to the value placed as the input into the gpioPWM() function of the pigpio library. I had mapped using a scope the values such that 113 to 209 pigpio integer input corresponds to 1020 to 2000ms time high of the PWM at 400Hz to the ESC's</p>

<p>EDIT:</p>

<p>Here is my current code implementation with the setpoint of 0 degrees:</p>

<pre><code>cout &lt;&lt; ""Starting Quadcopter"" &lt;&lt; endl;

float baseThrottle = 155; //1510ms
float maxThrottle = 180; //This is the current set max throttle for the PITCH YAW and ROLL PID to give allowance to the altitude PWM. 205 is the maximum which is equivalent to 2000ms time high PWM 
float baseCompensation = 0; //For the Altitude PID to be implemented later


delay(3000);

float startTime=(float)getTickCount();
deltaTimeInit=(float)getTickCount(); //Starting value for first pass

while(1){
//Read Sensor Data
readGyro(&amp;gyroAngleArray);
readAccelMag(&amp;accelmagAngleArray);

//Time Stamp
//The while loop is used to get a consistent dt for the proper integration to obtain the correct gyroscope angles. I found that with a variable dt, it is impossible to obtain correct angles from the gyroscope.

while( ( ((float)getTickCount()-deltaTimeInit) / ( ((float)getTickFrequency()) ) ) &lt; 0.005){ //0.00209715|0.00419
       deltaTime2=((float)getTickCount()-deltaTimeInit)/(((float)getTickFrequency())); //Get Time Elapsed
       cout &lt;&lt; "" DT endx = "" &lt;&lt; deltaTime2 &lt;&lt; endl;
}
//deltaTime2=((float)getTickCount()-deltaTimeInit)/(((float)getTickFrequency())); //Get Time Elapsed
deltaTimeInit=(float)getTickCount(); //Start counting time elapsed
cout &lt;&lt; "" DT end = "" &lt;&lt; deltaTime2 &lt;&lt; endl;


//Complementary Filter
float pitchAngleCF=(alpha)*(pitchAngleCF+gyroAngleArray.Pitch*deltaTime2)+(1-alpha)*(accelmagAngleArray.Pitch);
float rollAngleCF=(alpha)*(rollAngleCF+gyroAngleArray.Roll*deltaTime2)+(1-alpha)*(accelmagAngleArray.Roll);
float yawAngleCF=(alpha)*(yawAngleCF+gyroAngleArray.Yaw*deltaTime2)+(1-alpha)*(accelmagAngleArray.Yaw);


//Calculate Orientation Error (current - target)
float pitchError = pitchAngleCF - pitchTarget;
pitchErrorSum += (pitchError*deltaTime2);
float pitchErrorDiff = pitchError - pitchPrevError;
pitchPrevError = pitchError;

float rollError = rollAngleCF - rollTarget;
rollErrorSum += (rollError*deltaTime2);
float rollErrorDiff = rollError - rollPrevError;
rollPrevError = rollError;

float yawError = yawAngleCF - yawTarget;
yawErrorSum += (yawError*deltaTime2);
float yawErrorDiff = yawError - yawPrevError;
yawPrevError = yawError;


//PID controller list
float pitchPID = pitchKp*pitchError + pitchKi*pitchErrorSum + pitchKd*pitchErrorDiff/deltaTime2;
float rollPID = rollKp*rollError + rollKi*rollErrorSum + rollKd*rollErrorDiff/deltaTime2;
float yawPID = yawKp*yawError + yawKi*yawErrorSum + yawKd*yawErrorDiff/deltaTime2;

//Motor Control - Mixing    
//Motor Front Left (1)
float motorPwm1 =  -pitchPID + rollPID - yawPID + baseThrottle + baseCompensation;

//Motor Front Right (2)
float motorPwm2 =  -pitchPID - rollPID + yawPID + baseThrottle + baseCompensation; 

//Motor Back Left (3)
float motorPwm3 = pitchPID + rollPID + yawPID + baseThrottle + baseCompensation; 

//Motor Back Right (4)
float motorPwm4 = pitchPID - rollPID - yawPID + baseThrottle + baseCompensation;


 //Check if PWM is Saturating - This method is used to fill then trim the outputs of the pwm that gets fed into the gpioPWM() function to avoid exceeding the earlier set maximum throttle while maintaining the ratios of the 4 motor throttles. 
    float motorPWM[4] = {motorPwm1, motorPwm2, motorPwm3, motorPwm4};
    float minPWM = motorPWM[0];
    int i;
    for(i=0; i&lt;4; i++){ // Get minimum PWM for filling
        if(motorPWM[i]&lt;minPWM){
            minPWM=motorPWM[i];
        }
    }

    cout &lt;&lt; "" MinPWM = "" &lt;&lt; minPWM &lt;&lt; endl;

    if(minPWM&lt;baseThrottle){
        float fillPwm=baseThrottle-minPWM; //Get deficiency and use this to fill all 4 motors
        cout &lt;&lt; "" Fill = "" &lt;&lt; fillPwm &lt;&lt; endl;
        motorPwm1=motorPwm1+fillPwm;
        motorPwm2=motorPwm2+fillPwm;
        motorPwm3=motorPwm3+fillPwm;
        motorPwm4=motorPwm4+fillPwm;
    }

    float motorPWM2[4] = {motorPwm1, motorPwm2, motorPwm3, motorPwm4};
    float maxPWM = motorPWM2[0];
    for(i=0; i&lt;4; i++){ // Get max PWM for trimming
        if(motorPWM2[i]&gt;maxPWM){
            maxPWM=motorPWM2[i];
        }
    }

    cout &lt;&lt; "" MaxPWM = "" &lt;&lt; maxPWM &lt;&lt; endl;

    if(maxPWM&gt;maxThrottle){
        float trimPwm=maxPWM-maxThrottle; //Get excess and use this to trim all 4 motors
        cout &lt;&lt; "" Trim = "" &lt;&lt; trimPwm &lt;&lt; endl;
        motorPwm1=motorPwm1-trimPwm;
        motorPwm2=motorPwm2-trimPwm;
        motorPwm3=motorPwm3-trimPwm;
        motorPwm4=motorPwm4-trimPwm;
    }

//PWM Output
    gpioPWM(24,motorPwm1);  //1
    gpioPWM(17,motorPwm2);  //2
    gpioPWM(22,motorPwm3);  //3
    gpioPWM(18,motorPwm4);  //4
</code></pre>
","pid raspberry-pi quadcopter tuning"
"9082","Optimal location of the center of mass for an inverted pendulum","<p>I'm building an inverted pendulum to be controlled by DC motors, but I've run across a conundrum.  Personal life experience tells me that it's better to have a lower center of mass to maintain balance.  On the other hand, the greater the moment of inertia (e.g. the higher the center of mass), the easier it is to maintain balance as well.</p>

<p>These two views both seem plausible, and yet also seem contradictory.   For an inverted pendulum, is there an optimal balance between the two perspectives?  Or is one absolutely right while the other absolutely wrong?  If one is wrong, then where is the error in my thinking?</p>
","dynamics balance"
"9090","XBee xtend recovery/troubleshoot","<p>I have a broke my xbee xtend 900MHz module. While it was doing firmware-writing job I suddenly restarted the windows.</p>

<p>I've tried recovery procedure of XCTU new version and it does not work, therefore I'm still working with old version.</p>

<p><strong>here is the error i'm facing</strong> </p>

<p><a href=""http://i.stack.imgur.com/plADZ.jpg"" rel=""nofollow""><img src=""http://i.stack.imgur.com/plADZ.jpg"" alt=""""></a></p>
","radio-control"
"9096","Is there an alternative to manifolds when using quaternions for orientation representation in Pose Graph SLAM?","<p>I want to implement my own pose graph SLAM following [1]. Since my vehicle is moving in 3D-space i represent my pose using a 3D-translation vector and a quaternion for orientation. [1] tells me that it's necessary to adapt their algorithm 1 by using manifolds to project the poses into euclidean space.
I also studied the approach of [2]. In section ""IV.B. Nonlinear Systems"" they write that their approach remains valid for nonlinear systems. I conclude that for their case it's not obligatory to make use of a manifold. But I don't understand how they avoid it. So my questions are:</p>

<ol>
<li>Is it correct that there is an alternative to manifolds?</li>
<li>If yes, how does this alternative look like?</li>
</ol>

<hr>

<p>[1] Grisetti, G., Kummerle, R., Stachniss, C., &amp; Burgard, W. (2010). A tutorial on graph-based SLAM. Intelligent Transportation Systems Magazine, IEEE, 2(4), 31-43.</p>

<p>[2] Kaess, M., Ranganathan, A., &amp; Dellaert, F. (2008). iSAM: Incremental smoothing and mapping. Robotics, IEEE Transactions on, 24(6), 1365-1378.</p>
","slam pose"
"9097","A simple function plotter project","<p>To plot any curve or a function on a paper we need points of that curve, so to draw a curve, I will store a set of points in the processor and use motors, markers and other mechanism to draw straight lines attaching these points and these points are so close to each other that the resultant will look an actual curve.</p>

<p>So I am going to draw the curve with a marker or a pen.</p>

<ol>
<li>Yes to do this project I need motors which would change the position of a marker but which one?</li>
</ol>

<p>With my knowledge stepper motor and servo motors are appropriate but not sure whether they are appropriate since I have never used them, so will they work?</p>

<p>The dimension of paper on which I will be working on is 30x30 cms.</p>

<p>I have two ideas for this machine </p>

<p>a. A rectangular one as shown <a href=""http://i.stack.imgur.com/CplxN.jpg"" rel=""nofollow""><img src=""http://i.stack.imgur.com/CplxN.jpg"" alt=""enter image description here""></a></p>

<p>I would make my marker holder movable with help of rack and pinion mechanism but I am not sure that this would be precise and I may have to alter to some other mechanism and if you know such then that can really help me.</p>

<p>b. A cylindrical one <a href=""http://i.stack.imgur.com/Q4ytj.jpg"" rel=""nofollow""><img src=""http://i.stack.imgur.com/Q4ytj.jpg"" alt=""enter image description here""></a></p>

<p>Here I would roll a paper on this cylinder and this paper will get unrolled as the cylinder rotates and even the marker holder is movable but only in X direction and the rolling of paper is nothing but change of Y position.</p>

<ol start=""2"">
<li><p>Which one of the above two methods is good?</p></li>
<li><p>I know about microcontrollers and I want to control the motors using them so I decided to go with Atmega 16 microcontroller. But here i might need microstepping of signals how would I be able to do that with microcontrollers?</p></li>
</ol>

<p>If you know the answer to atleast one of the questions then those answers are always welcomed. 
If you need any clarifications about these then please leave a comment.</p>

<p>Thankyou for your time.</p>

<p>Your sincerely,
Jasser</p>

<p>Edit : To draw lines of particular slope I would have to know the slope between two points and the depending on the slope I would rotate motors with particular speed so that marker will move in a straight fashion with that slope.</p>
","microcontroller stepper-motor servomotor"
"9099","Reference request: Path accuracy algorithm in the joint angle space","<p>I am currently reviewing a path accuracy algorithm. The measured data are points in the 7 dimensional joint space (the robot <strike>under test</strike>  is a 7 axes Robot, but this is not of importance for the question). As far as I know path accuracy is measured and assessed in configuration (3 D) space. Therefore I am wondering if a path accuracy definition in joint angle space has any practical value. Sure, if one looks at the joint angle space as a 7 dimensional vector space in the example (with Euclidean distance measure) one can do formally the math. But this seems very odd to me. For instance, an angle discrepancy between measured and expected for the lowest axis is of much more significance than a discrepancy for the axis near the <strike>actuator</strike> end effector.</p>

<p>So here is my Question: Can anyone point me to references where path accuracy in joint space and/or algorithms for its calculation is discussed ?  </p>

<p>(I am not quite sure what tags to use. Sorry if I misused some.)</p>
","algorithm industrial-robot joint"
"9100","Finding Center of Mass for Humanoid Robot","<p>I've been working on Humanoid Robot, and I face the problem of finding the Center of Mass of the Robot which will help in balancing the biped. Although COM has a very simple definition, I'm unable to find a simple solution to my problem.</p>

<p>My view: I have already solved the Forward and Inverse Kinematics of the Robot with Torso as the base frame. So, if I can find the position(and orientation) of each joint in the base frame, I can average all of them to get the COM. Is this approach reasonable? Will it produce the correct COM?</p>

<p>Can anyone offer any series of steps that I can follow to find the COM of the biped? Any help would be appreciated. </p>

<p>Cheers! </p>
","mobile-robot inverse-kinematics humanoid balance"
"9103","Connecting a CC3D board with Raspberry Pi to get telemetry data","<p>I want to get telemetry data in my Raspberry Pi that will be connected to a <a href=""http://www.openpilot.org/product/coptercontrol/"" rel=""nofollow"">CC3D board</a> either via USB cable or Serial communication. How can I get the data? I plan to have wifi communication between the Pi and my Laptop. Also <a href=""http://www.openpilot.org/product/oplink-rf-modems/"" rel=""nofollow"">OPLink modems</a> will be used both in the Pi and the CC3D for the telemetry. Does anyone have a python example that may help to build an interface or output in the Linux shell to get raw telemetry data in RPi? </p>
","serial communication"
"9105","Lag in altitude measurements using a barometer and an acclerometer","<p>I am fusing data from the barometer and accelerometer using a complementary filter. However, there is a considerable lag in the readings which is affecting the Alt-Hold performance. 
Accelerometer : mpu 6050 
Baro : ms 5611. 
Here's the code : <a href=""https://github.com/cleanflight/cleanflight/blob/master/src/main/flight/altitudehold.c"" rel=""nofollow"">https://github.com/cleanflight/cleanflight/blob/master/src/main/flight/altitudehold.c</a></p>

<p>You will find the filter being implemented in the function calculateEstimatedAltitude()
Does anyone have any suggestions to improve the measurements ?</p>
","quadcopter sensors sensor-fusion"
"9106","Sensors for identifying stacked books","<p>I am working on a robotics application that involves moving objects (e.g. books) between several (around 10) stacks. To measure the performance, I'd like to be able to measure which book is located on each of the stacks. The order is not important I just want to know if a book is on one of the stacks. </p>

<p>The stacks are separated by at least one meter and the height of the stacks is less than 30cm (&lt; 8 Books). </p>

<p>If have thought of putting an RFID card in every book and fixing RFID readers above (or below) the stack positions. Several readers could be attached via SPI or I2C to some arduinos or RPis. </p>

<p>What to you think about this approach? Is there a simpler way? Could someone maybe recommend a sensor that could solve this problem? </p>

<p>// Update:
I can modify the books (e.g. add a QR-Marker) to some extent, but can't guarantee that the orientation on the stack is fixed. </p>
","untagged"
"9111","How to make a Raspberry Pi?","<p>I'm currently a robotics hobbyist and am full fledged in Arduino and I have used the Raspberry Pi to make some robots and PCs. Currently, I am thinking of making my own Raspberry Pi, from scratch, on a breadboard or a PCB or something. I surfed the web quite a bit and I did not get the answer I was hoping for. By making a Pi, I mean like instead of buying an Arduino, I can make one myself by buying the Atmega328, Crystal oscillators, etc. I am asking for this because my school requires me to do a project in which I make a computer or a gaming console or something like that and I would hate to look at the disappointed face of the tester all because I just bought a Pi , an connected some devices to it. Thanks in advance!</p>
","arduino raspberry-pi"
"9114","Simple equation to calculate needed motor torque","<p>Suppose I have a DC motor with an arm connected to it (arm length = 10cm, arm weight = 0), motor speed 10rpm.</p>

<p>If I connect a 1Kg weight to the very end of that arm, how much torque is needed for the motor to do a complete 360° spin, provided the motor is placed horizontally and the arm is vertical?</p>

<p>Is there an simple equation where I can input any weight and get the required torque (provided all other factors remain the same)?</p>
","motor torque"
"9115","Sphero's logic, how does it work","<p>I'm willing to make my first robot, and I'd like to make one similar to the <a href=""http://www.sphero.com/sphero"" rel=""nofollow"">Sphero</a>.</p>

<p>I know I have to add <strong>2 motors</strong> in it, and make it work as a hamster ball, but I don't understand how I can make it rotate on the <strong>x axis</strong> aswell and not only on the <strong>y axis</strong>, if we assume that the <strong>y</strong> one is in <strong>front</strong> of the robot and the <strong>x</strong> one on its <strong>sides</strong>.</p>

<p>Any ideas?</p>
","design two-wheeled"
"9119","Weird behaviour with a Create2 connected via XBEE","<p>This is not really a problem but something strange is going on.</p>

<p>When Create2 is connected to a PC via the USB original connector lead, when you start-up the computer the Create2 is activated by the Baud Rate Change (BRC) pulling to ground. If I understand correctly, normal behaviour.</p>

<p>My Create2 is connected to a XBEE via a buck converter, I added a switch so the buck converter and the XBEE should not drain the battery continuously so as mentioned in the specs.</p>

<p>I followed the Bluetooth pdf for the connections, its working well for sending commands but I still just have a few problems with streaming the return data but that will be resolved.</p>

<p>But now, with the XBEE switched off my Create2 still activates when I start-up my PC, how is that possible, how can the BRC be pulled to ground?</p>

<p>There can be no communication between the PC XB and the Create2 XB since the Create2 XB is switched off, only the PC XB is switched on when starting the computer.</p>

<p>Its not a problem, its just that I am puzzled. Can anyone explain why this is happening?</p>
","irobot-create"
"9124","Choosing stepper motor for hand","<p>I'm developing a robotic hand, and decided to place motors inside joints (as in picture) and I'm stuck with finding a stepper motor that can fit there. Approximate size of motor body is radius - 10mm, length - 10 mm. </p>

<p>Any suggestions?</p>

<p><a href=""http://i.stack.imgur.com/HP4Ri.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/HP4Ri.png"" alt=""Diagram of hand joint with stepper motor""></a></p>
","stepper-motor"
"9129","How to compute the error function in graph SLAM for 3D poses?","<p>Given a pose $x_i = (t_i, q_i)$ with translation vector $t_i$ and rotation quaternion $q_i$ and a transform between poses $x_i$ and $x_j$ as $z_{ij} = (t_{ij}, q_{ij})$ I want to compute the error function $e(x_i, x_j) = e_{ij}$, which has to be minimized like this to yield the optimal poses $X^* = \{ x_i \}$:</p>

<p>$$X^* = argmin_X \sum_{ij} e_{ij}^T \Sigma^{-1}_{ij} e_{ij}$$</p>

<p>A naive approach would look like this:</p>

<p>$$ e_{ij} = z_{ij} - f(x_i,x_j) $$</p>

<p>where $z_{ij}$ is the current measurement of the transform between $x_i$ and $x_j$ and $f$ calculates an estimate for the same transform. Thus, $e_{ij}$ simply computes the difference of translations and difference of turning angles:</p>

<p>$$ e_{ij} = \begin{pmatrix} t_{ij} - t_j - t_i \\\ q_{ij} (q_j q_i^{-1})^{-1} \end{pmatrix} $$</p>

<p>Is there anything wrong with this naive approach? Am I missing something?</p>
","slam errors"
"9130","Choosing the state vector for an EKF","<p>Could someone help me understand the logic behind choosing a particular state space vector for an EKF?</p>

<p>Context: Say there is a 4 wheeled robot that operates only in 2D. It is equipped with an inertial unit (a/g/m) and wheel encoders (I understand that these alone might not satisfy accuracy constraints, but consider this as a hypothetical case).</p>

<p>Now, some literature has the state as [q, x, y, vx, vy]' while a few others as [q, q_dot, x, y, vx, vy]'. My question is, what is the advantage with having certain 'rate terms' as opposed to only the normal parameters? Also, what about including bias terms in there?</p>

<p>How do I go about selecting an appropriate state space vector for any use-case (in general)? Is there a set of intuitive/mathematical steps to consider/follow?</p>

<p>Thanks!</p>
","wheeled-robot kalman-filter ekf pose"
"9132","Find orientation through Transformation matrix","<p>I have a robot with 3 rotational joints that I am trying to simulate in a program I am creating. So I have 4 frames, one base frame, and each joint has a frame. I have 3 transformation functions to go from frame 1 or 2 or 3 to frame 0.</p>

<p>By using the transformation matrix, I want to know how much each frame has been rotated (by the X,Y and Z axis) compared with the base frame. Any suggestions?</p>

<p>The reason I want this is because I have made some simple 3D shapes that represent each joint. By using the DH parameters I made my transformation matrices. When ever I change my θ (it does not mater how the θ changes, it just does), I want the whole structure to update. I take the translation from the last column. Now I want to get the rotations.</p>
","robotic-arm dh-parameters"
"9135","Freewheel diode / capacitor with this board?","<p>With DC motors, it is common to put a freewheel diode and/or a capacitor in order to protect the equipment as the motor can induce current into the system.</p>

<p>I plan to use <a href=""http://www.robotshop.com/media/files/pdf2/md10crev2.0usersmanual_2015-07.pdf"" rel=""nofollow"">this board</a> to control a 24V DC motor with a Arduino-like microcontroler. In an example in their documentation, they don't put such protection, so I wanted to know if it's unsafe, or is it that the board already protects the system?</p>

<p>The example in question:
<a href=""http://i.stack.imgur.com/Jv5Hs.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/Jv5Hs.png"" alt=""Example""></a></p>
","arduino motor protection"
"9139","Can I control iRobot Create 2 with NI myRIO and LabVIEW codes?","<p>I need to know if the iRobot Create 2 can be controlled with a NI myRIO that has been programmed through LabVIEW. </p>

<p>The goal is to program an autonomous robot for real-time tracking using a Kinect sensor. </p>
","mobile-robot irobot-create labview"
"9143","How is homotopy used in planning algorithms?","<p>What is an intuitive understanding for homotopy?  At what stage is homotopy (I understand it as stretching or bending of path) in a planning algorithm?  Is homotopy involved, for example, while implementing an algorithm like RRT?</p>
","motion-planning rrt"
"9144","Making a car go straight","<p>I'm trying to work a car that's being controlled by an Arduino. I'm using the following chassis: <a href=""http://www.aliexpress.com/item-img/New-2WD-car-chassis-DC-gear-motor-wheels-easy-assembly-and-expansion-car-toys-robot-toys/32310649967.html"" rel=""nofollow"">New 2WD car chassis DC gear motor, wheels easy assembly and expansion</a> and an L298N motor driver.</p>

<p>The problem is that it's hard to make the car go straight. Giving the same PWM value to the motors still makes them spin in different speeds, trying to calibrate the value is hard and every time I recharge my batteries the value changes.</p>

<p>What are my options on making the car go straight when I want (well, sometimes I'll want to turn it around of course)?</p>

<p>I've thought about using an encoder but I wish to avoid that since it will complicate the whole project, is there any other viable option? and even when using an encoder, Does it means I will need to keep track all the time and always adjust the motors value continuously? is there some built-in library for that?</p>
","arduino wheeled-robot calibration two-wheeled"
"9145","List of books similar to Thrun's Probabilistic Robotics for robot mechanics and manipulation","<h2>What?</h2>

<p>Put together here <strong>a list of books (like the one for C/C++ on StackOverflow) that are spiritually similar to Sebastian Thrun's Probabilistic Robotics for robotic manipulation and mechanics</strong>.</p>

<h2>Why?</h2>

<p>Thrun's book is a wonderful resource for implementable algorithms while also dealing with the mathematics/theory behind them. In somewhat similar vein for robotic mechanics there is ""A Mathematical Introduction to Robotic Manipulation - S.Sastry, Z.Li and R.Murray"" which has a lot of mathematical/theoretical content. What is missing however in this book are the algorithms concerned with how should/would one go about implementing the theoretical stuff.</p>

<h2>Requirements</h2>

<ol>
<li>Ideally list books dealing with diverse areas of robotics.</li>
<li>The books have to present algorithms like what Thrun does in his book.</li>
<li>Algorithms presented have to be language agnostic and as much as possible not be based on packages like MATLAB in which case they should be categorized appropriately.</li>
</ol>
","kinematics inverse-kinematics algorithm dynamics books"
"9152","Inverse kinematics after calibration","<p>I am working on a 6DOF robot arm project and I have one big question. When I first derived the inverse kinematics (IK) algorithm after decoupling (spherical wrist), I could easily get the equations based on nominal DH values, where alpha are either 0 or 90 degrees and there are many zeros in $a_i$ and $d_i$. However, after kinematics calibration, the identified DH parameters are no longer ideal ones with a certain small, but non-zero, bias added to the nominal values. </p>

<p>So my question is, can the IK algorithm still be used with the actual DH parameters? If yes, definitely there will be end-effector errors in actual operation. If not, how should I change the IK algorithm? </p>

<p>P.S. I am working on a modular robot arm which means the DH bias could be bigger than those of traditional robot arms. </p>
","inverse-kinematics calibration"
"9155","How to import catia assembly to Matlab. Simmechanics","<p>In catia .stl format is available only for part file not for assembly file. Please help how to import asembly in simmechanics</p>

<p>.CATProduct to .stl
Or Is there any other way to do?</p>
","matlab"
"9157","regarding finding out sensor","<p>I am looking for a sensor which is able to read the displayed data from a LCD display (4 digits). The output of this sensor must be fetched to a  microcontroller.
Can anyone suggest me such sensor please soon?</p>
","sensors hall-sensor"
"9159","Multi-Rate Sensor Fusion using EKF","<p>Context: I have an IMU(a/g/m) + Wheel Odometry measurement data that I'm trying to fuse in order to localize a 2D (ackermann drive) robot.</p>

<p>The state vector <code>X = [x y yaw]</code>.
I'm using the odometry data to propagate the state through time (no control input).
The update step includes the measurement vector <code>Z = [x_odo y_odo yaw_imu]</code>.</p>

<p>I have two questions:</p>

<p>1.Does it make sense to use the odometry data(v_linear, omega) in both the prediction as well as update steps?</p>

<p>2.How do I account for the frequency difference between the odometry data(10Hz) and the imu data(40Hz)? Do I run the filter at the lower frequency, do I dynamically change the matrix sizes or is there any other way?</p>

<p>Thanks!</p>
","localization wheeled-robot imu ekf odometry"
"9161","Robot autonomous variable terrain with yaw sensor","<p>I am programming a robot to drive over variable terrain obstacles autonomously. The variable terrain could potentially knock the robot off of its initial heading, but I would like to design an autonomous sequence to correct for any change in direction. I am using a very accurate sensor with compass and yaw. What is the best way to have it correct for any changes and maintain its heading? Side to side motion does not have to stay perfect, but the heading needs to stay the same.We are currently correcting it by overpowering one side of the wheels (depending on direction of correction needed) until the heading is correct again, but this seems to be a slightly antiquated method, so I'm looking for a cleaner and more smooth method.</p>
","compass automation"
"9163","Making a VEX central controller","<p>Is there any way to make the central controller for all VEX parts without using the VEX Cortex (or anything by VEX)? </p>

<p>I am wondering whether you can make your own custom controller for VEX parts.</p>
","vex"
"9167","Check collision between robot and environment in OpenRAVE","<p>I have a robot arm in an environment. How can I check for collision between this robot arm and the environment?</p>
","robotic-arm motion-planning python"
"9168","memory as a benchmarking criteria for motion planning algorithm","<p>Are there still applications where memory is still a criteria with respect to motion planning algorithms. Are memory efficient motion planning algorithms still relevant?</p>
","mobile-robot quadcopter motion-planning"
"9169","PID Control: Integral error does not converge to zero","<p>Good day,</p>

<p>I had been recently reading up more on PID controllers and stumbled upon something called integral wind up. I am currently working on an autonomous quadcopter concentrating at the moment on PID tuning. I noticed that even with the setpoint of zero degrees reached in this video, the quadcopter would still occasionally overshoot a bit:  <a href=""https://youtu.be/XD8WgVFfEsM"" rel=""nofollow"">https://youtu.be/XD8WgVFfEsM</a></p>

<p>Here is the corresponding data testing the roll axis:
<a href=""http://i.stack.imgur.com/gAatW.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/gAatW.png"" alt=""enter image description here""></a></p>

<p>I noticed that the I-error does not converge to zero and continues to increase:
<a href=""http://i.stack.imgur.com/jS8oS.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/jS8oS.png"" alt=""enter image description here""></a></p>

<blockquote>
  <p>Is this the integral wind-up?</p>
  
  <p>What is the most effective way to resolve this?</p>
</blockquote>

<p>I have seen many implementations mainly focusing on limiting the output of the system by means of saturation. However I do not see this bringing the integral error eventually back to zero once the system is stable.</p>

<p>Here is my current code implementation with the setpoint of 0 degrees:</p>

<pre><code>cout &lt;&lt; ""Starting Quadcopter"" &lt;&lt; endl;

float baseThrottle = 155; //1510ms
float maxThrottle = 180; //This is the current set max throttle for the PITCH YAW and ROLL PID to give allowance to the altitude PWM. 205 is the maximum which is equivalent to 2000ms time high PWM 
float baseCompensation = 0; //For the Altitude PID to be implemented later


delay(3000);

float startTime=(float)getTickCount();
deltaTimeInit=(float)getTickCount(); //Starting value for first pass

while(1){
//Read Sensor Data
readGyro(&amp;gyroAngleArray);
readAccelMag(&amp;accelmagAngleArray);

//Time Stamp
//The while loop is used to get a consistent dt for the proper integration to obtain the correct gyroscope angles. I found that with a variable dt, it is impossible to obtain correct angles from the gyroscope.

while( ( ((float)getTickCount()-deltaTimeInit) / ( ((float)getTickFrequency()) ) ) &lt; 0.005){ //0.00209715|0.00419
       deltaTime2=((float)getTickCount()-deltaTimeInit)/(((float)getTickFrequency())); //Get Time Elapsed
       cout &lt;&lt; "" DT endx = "" &lt;&lt; deltaTime2 &lt;&lt; endl;
}
//deltaTime2=((float)getTickCount()-deltaTimeInit)/(((float)getTickFrequency())); //Get Time Elapsed
deltaTimeInit=(float)getTickCount(); //Start counting time elapsed
cout &lt;&lt; "" DT end = "" &lt;&lt; deltaTime2 &lt;&lt; endl;


//Complementary Filter
float pitchAngleCF=(alpha)*(pitchAngleCF+gyroAngleArray.Pitch*deltaTime2)+(1-alpha)*(accelmagAngleArray.Pitch);
float rollAngleCF=(alpha)*(rollAngleCF+gyroAngleArray.Roll*deltaTime2)+(1-alpha)*(accelmagAngleArray.Roll);
float yawAngleCF=(alpha)*(yawAngleCF+gyroAngleArray.Yaw*deltaTime2)+(1-alpha)*(accelmagAngleArray.Yaw);


//Calculate Orientation Error (current - target)
float pitchError = pitchAngleCF - pitchTarget;
pitchErrorSum += (pitchError*deltaTime2);
float pitchErrorDiff = pitchError - pitchPrevError;
pitchPrevError = pitchError;

float rollError = rollAngleCF - rollTarget;
rollErrorSum += (rollError*deltaTime2);
float rollErrorDiff = rollError - rollPrevError;
rollPrevError = rollError;

float yawError = yawAngleCF - yawTarget;
yawErrorSum += (yawError*deltaTime2);
float yawErrorDiff = yawError - yawPrevError;
yawPrevError = yawError;


//PID controller list
float pitchPID = pitchKp*pitchError + pitchKi*pitchErrorSum + pitchKd*pitchErrorDiff/deltaTime2;
float rollPID = rollKp*rollError + rollKi*rollErrorSum + rollKd*rollErrorDiff/deltaTime2;
float yawPID = yawKp*yawError + yawKi*yawErrorSum + yawKd*yawErrorDiff/deltaTime2;

//Motor Control - Mixing    
//Motor Front Left (1)
float motorPwm1 =  -pitchPID + rollPID - yawPID + baseThrottle + baseCompensation;

//Motor Front Right (2)
float motorPwm2 =  -pitchPID - rollPID + yawPID + baseThrottle + baseCompensation; 

//Motor Back Left (3)
float motorPwm3 = pitchPID + rollPID + yawPID + baseThrottle + baseCompensation; 

//Motor Back Right (4)
float motorPwm4 = pitchPID - rollPID - yawPID + baseThrottle + baseCompensation;


 //Check if PWM is Saturating - This method is used to fill then trim the outputs of the pwm that gets fed into the gpioPWM() function to avoid exceeding the earlier set maximum throttle while maintaining the ratios of the 4 motor throttles. 
    float motorPWM[4] = {motorPwm1, motorPwm2, motorPwm3, motorPwm4};
    float minPWM = motorPWM[0];
    int i;
    for(i=0; i&lt;4; i++){ // Get minimum PWM for filling
        if(motorPWM[i]&lt;minPWM){
            minPWM=motorPWM[i];
        }
    }

    cout &lt;&lt; "" MinPWM = "" &lt;&lt; minPWM &lt;&lt; endl;

    if(minPWM&lt;baseThrottle){
        float fillPwm=baseThrottle-minPWM; //Get deficiency and use this to fill all 4 motors
        cout &lt;&lt; "" Fill = "" &lt;&lt; fillPwm &lt;&lt; endl;
        motorPwm1=motorPwm1+fillPwm;
        motorPwm2=motorPwm2+fillPwm;
        motorPwm3=motorPwm3+fillPwm;
        motorPwm4=motorPwm4+fillPwm;
    }

    float motorPWM2[4] = {motorPwm1, motorPwm2, motorPwm3, motorPwm4};
    float maxPWM = motorPWM2[0];
    for(i=0; i&lt;4; i++){ // Get max PWM for trimming
        if(motorPWM2[i]&gt;maxPWM){
            maxPWM=motorPWM2[i];
        }
    }

    cout &lt;&lt; "" MaxPWM = "" &lt;&lt; maxPWM &lt;&lt; endl;

    if(maxPWM&gt;maxThrottle){
        float trimPwm=maxPWM-maxThrottle; //Get excess and use this to trim all 4 motors
        cout &lt;&lt; "" Trim = "" &lt;&lt; trimPwm &lt;&lt; endl;
        motorPwm1=motorPwm1-trimPwm;
        motorPwm2=motorPwm2-trimPwm;
        motorPwm3=motorPwm3-trimPwm;
        motorPwm4=motorPwm4-trimPwm;
    }

//PWM Output
    gpioPWM(24,motorPwm1);  //1
    gpioPWM(17,motorPwm2);  //2
    gpioPWM(22,motorPwm3);  //3
    gpioPWM(18,motorPwm4);  //4
</code></pre>
","control quadcopter pid stability tuning"
"9171","Plotting location using wheel encoder data","<p><strong>Context</strong>: I am working with the SFU Mountain Dataset<a href=""http://autonomylab.org/sfu-mountain-dataset/"" rel=""nofollow""> [http://autonomylab.org/sfu-mountain-dataset/]</a></p>

<p>The UGV image - via the SFU Mountain Dataset website:
<a href=""http://i.stack.imgur.com/exx7h.jpg"" rel=""nofollow""><img src=""http://i.stack.imgur.com/exx7h.jpg"" alt=""enter image description here""></a></p>

<p>I have used the following state update equations (Husky A200 - differential drive)</p>

<p>State Update - from Prob. Robotics, Thrun et. al <code>[x' y' theta'] represent the state at the next time step</code></p>

<p><a href=""http://i.stack.imgur.com/UAh17.jpg"" rel=""nofollow""><img src=""http://i.stack.imgur.com/UAh17.jpg"" alt=""enter image description here""></a>
<a href=""http://i.stack.imgur.com/sVtB6.jpg"" rel=""nofollow""><img src=""http://i.stack.imgur.com/sVtB6.jpg"" alt=""State Update - from Prob. Robotics, Thrun et. al.""></a></p>

<p>After plotting the x and y positions based on <strong>just</strong> the wheel encoder data (v_fwd and w -> the dataset provides these directly, instead on the vr and vl), the curve seems to be quite weird and unexpected.
Wheel Odometry Data - <a href=""http://autolab.cmpt.sfu.ca/files/datasets/sfu-mountain-workshop-version/sfu-mountain-torrent/encoder-dry-a.tgz"" rel=""nofollow"">http://autolab.cmpt.sfu.ca/files/datasets/sfu-mountain-workshop-version/sfu-mountain-torrent/encoder-dry-a.tgz</a></p>

<p>Blue - Wheel Odom  |  Red - GPS
<a href=""http://i.stack.imgur.com/zw32B.jpg"" rel=""nofollow""><img src=""http://i.stack.imgur.com/zw32B.jpg"" alt=""Blue - Wheel Odom  |  Red - GPS""></a></p>

<p>Actual path!
<a href=""http://i.stack.imgur.com/v6Fsf.jpg"" rel=""nofollow""><img src=""http://i.stack.imgur.com/v6Fsf.jpg"" alt=""Actual path!""></a></p>

<p><strong>Question</strong>: Is the above curve expected (considering the inaccuracy of wheel odometry) or is there something I'm missing? If the wheel encoder data is that bad, will an EKF (odom + imu) even work?</p>

<p>PS: I'm not worried about the EKF (update step) just as yet. What concerns me more is the horrible wheel odometry data.</p>
","localization wheeled-robot ekf odometry differential-drive"
"9174","What kind of sensor I can use to identify which fruit it is (like mango or apple)?","<p>What kind of sensor I can use to identify which fruit it is (like mango or apple). Moreover, is there any sensor to identify different varieties of apples or mangoes. </p>
","sensors"
"9175","What is the best way to attach a 3D printed part to a servo for robotics use?","<p>I am trying to make custom parts that fit directly onto a servo. Doing this has proved more difficult than I've expected so far.</p>

<p>I was hoping to avoid incorporating the provided servo horns into the 3D printed part, so I've been trying this method out. Below are images of my current test - a 3D printed attachment to the servo, with an indentation for an M3 nut (the servo accepts an M3 bolt) for attachment to the servo. The plastic ring doesn't have the spline (I can't print that level of detail I think) but is tight around it. The top piece attaches to a 3/8"" nut for use with the 3/8"" threaded rod I had laying around.</p>

<p><a href=""http://i.stack.imgur.com/T7Q8s.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/T7Q8s.png"" alt=""The nut casing""></a></p>

<p><a href=""http://i.stack.imgur.com/LGIfh.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/LGIfh.png"" alt=""The servo attachment piece - note the M3 nut indent""></a></p>

<p>So far, I'm having difficulty of this setup working with any level for torque and not just spinning in place.</p>

<p>So... is this the correct approach? Am I going to have to design a piece with the servo horn inside of it to get the servo to connect? Are there better approaches I haven't considered?</p>
","servos 3d-printing"
"9180","How does information gain based exploration differ from frontier based?","<p>I've recently come across the concept of using information gain (or mutual information criteria) as a metric for minimizing entropy on a map to aid in robotic exploration. I have somewhat of a basic question about it. </p>

<p>A lot of papers that talk about minimizing entropy consider an example case of something like a laser scanner and try to compute the 'next best pose' so that the maximum entropy reduction is achieved. Usually this is mentioned like ""information gain based approaches help finding the best spot to move the robot such that the most entropy is minimized using raycasting techniques, as opposed to frontier based exploration which is greedy"" etc. But I don't understand what the underlying reason is for information gain/entropy based exploration being better. </p>

<p>Let's say a robot in a room with three walls and open space in front. Because of range limitations, it can only see two walls: so in frontier based exploration, the robot has two choices; move towards the third wall and realize it's an obstacle, or move towards the open space and keep going. How does an information gain based method magically pick the open space frontier over the wall frontier? When we have no idea what's beyond our frontiers, how can raycasting even help?</p>
","mapping exploration information-gain"
"9183","EKF SLAM (prediction of new landmarks)","<p>Prediction of new landmarks are commonly expressed as:</p>

<pre><code>        Xm = Xr + r*cos(phi + theta_r),
        Ym = Yr + r*sin(phi + theta_r)
</code></pre>

<p>However this is only true for point landmarks. What if I am extracting line feature?</p>
","slam ekf"
"9185","Which ultrasonic sensor can be used for detecting hydraulic flow?","<p>I am looking to make a non contact hydraulic flow meter. I was wondering, which ultrasonic sensor to use? I don't have any specific ideas on how to go forward. Are there any articles or documented builds out there about this subject?</p>
","ultrasonic-sensors"
"9186","Using genetic algorithm for tuning controllers","<p>I've read some <a href=""http://link.springer.com/chapter/10.1007%2F978-0-387-73137-7_14"" rel=""nofollow"">papers</a> for controlling nonlinear systems (e.g. nonlinear pendulum). There are several approaches for targeting nonlinear systems. The most common ones are <strong><a href=""https://en.wikipedia.org/wiki/Feedback_linearization"" rel=""nofollow"">feedback linearizaing</a></strong>, <strong><a href=""https://en.wikipedia.org/wiki/Backstepping"" rel=""nofollow"">backstepping</a></strong>, and <strong><a href=""https://en.wikipedia.org/wiki/Sliding_mode_control"" rel=""nofollow"">sliding mode</a></strong> controllers. </p>

<p>In my case, I've done the theoretical and practical parts of controlling nonlinear model of a simple pendulum plus other manipulators problems in C++. For the pendulum, I've utilized a backstepping controller for solving the tracking task for the angular displacement and velocity. The results are </p>

<p>$$
\ddot{\theta} + (k/m) \dot{\theta} + (g/L) \sin\theta= u 
$$</p>

<p>where $m=0.5, k=0.0001, L=.2$ and $g=9.81$.</p>

<p><a href=""http://i.stack.imgur.com/yo9LV.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/yo9LV.png"" alt=""Graph 1""></a></p>

<p><a href=""http://i.stack.imgur.com/ljCHu.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/ljCHu.png"" alt=""Graph 2""></a></p>

<p>The results are good. However, tuning the controller is time consuming. The majority of papers use <em>genetic algorithms</em> for tuning their controllers such as PD, PID, and backstepping controllers. <em>I'm clueless in this field and I hope someone sheds some light on this concept, preferable if there is a MATLAB sample for at least controlling a simple pendulum.</em></p>

<p>So far I've designed a simple GUI in C++/Qt in order to tune the controller manually. In the below picture, the response of the controller for step function. </p>

<p><a href=""http://i.stack.imgur.com/1MPVB.jpg"" rel=""nofollow""><img src=""http://i.stack.imgur.com/1MPVB.jpg"" alt=""Screenshot of GUI of the controller tuning application""></a> </p>
","control"
"9187","Ultrasonic flow sensor","<p>The goal is to have a non-invasive flow meter that I can clamp over hydraulic lines.</p>

<p>As a student of hydraulics, I ended up looking and poking around for a good way to make an ultrasonic flow sensor with Arduino and possibly the hc-sr04. Not married to either idea. </p>

<p>So, I admit, I know nothing, but is it possible to do this?<br>
Is there an easier way?</p>
","ultrasonic-sensors"
"9194","what determines the speed of quadrotor","<p>How to design a quadrotor which travels at particular maximum speed? and how to determine the power required for a quadrotor to hover?</p>
","mobile-robot quadcopter power"
"9195","How to align solidworks global origin with assembly origin while exporting in solidworks to urdf","<p>I have created a robot model in solidworks and exported in solidworks to urdf plug-in. When exporting the co-ordinates of the model is misaligned which is causing problem while using in ROS.</p>

<p><a href=""http://i.stack.imgur.com/xGkyg.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/xGkyg.png"" alt=""enter image description here""></a></p>

<p>As you could see in picture the Z-axis is horizontal in assembly whereas vertical in solidworks. How to align these co-ordinates. The generated co-ordinate system must be similar to solidworks' co-ordinates</p>

<p>PS: I have mated the assembly origin and base_link origin</p>
","mobile-robot ros navigation odometry gazebo"
"9200","A Vector Field Histogram implementation in Python 2.7","<p>I am trying to implement the Vector Field Histogram as described by <a href=""http://www.cs.cmu.edu/~motionplanning/papers/sbp_papers/integrated1/borenstein_VFHisto.pdf"">Borenstein, Koren, 1991</a> in Python 2.7 using the <a href=""https://scipy.org/"">SciPy stack</a>.</p>

<p>I have already been able to calculate the polar histogram, as described in the paper, as well as the smoothing function to eliminate noise. This variable is stored in a numpy array, named <code>self.Hist</code>.</p>

<p>However, the function <code>computeTheta</code>, pasted below, which computes the steering direction, is only able to compute the proper direction if the valleys (i.e. consecutive sectors in the polar histogram whose obstacle density is below a certain threshold) do not contain the section where a full circle is completed, i.e. the sector corresponding to <code>360º</code>.</p>

<p>To make things clearer, consider these two examples: </p>

<ul>
<li>If the histogram contains a peak in the angles between, say, <code>330º</code> and <code>30º</code>, with the rest of the histogram being a valley, then the steering direction will be computed correctly.</li>
<li><p>If, however, the peak is contained between, say, <code>30º</code> and <code>60º</code>, then the valley will start at <code>60º</code>, go all the way past <code>360º</code> and end in <code>30º</code>, and the steering direction will be computed incorrectly, since this single valley will be considered two valleys, one between <code>0º</code> and <code>30º</code>, and another between <code>60º</code> and <code>360º</code>.</p>

<p>def computeTheta(self, goal):</p>

<pre><code>thrs = 2.
s_max = 18


#We start by calculating the sector corresponding to the direction of the target.
target_sector = int((180./np.pi)*np.arctan2(goal[1] - self.VCP[1], goal[0] - self.VCP[0]))
if target_sector &lt; 0:
    target_sector += 360
target_sector /= 5

#Next, we assume there is no best sector.
best_sector = -1
dist_best_and_target = abs(target_sector - best_sector)

#Then,  we find the sector within a valley that is closest to the target sector.
for k in range(self.Hist.shape[0]):
    if self.Hist[k] &lt; thrs and abs(target_sector - k) &lt; dist_best_and_target:
        best_sector = k
        dist_best_and_target = abs(target_sector - k)

#If the sector is still -1, we return it as an error.
print (target_sector, best_sector)
if best_sector == -1:
    return -1

#If not, we can proceed...
elif best_sector &gt; -1:
    #... by deciding whether the valley to which the best sector belongs is a ""wide"" or a ""narrow"" one.
    #Assume it's wide.
    type_of_valley = ""Wide""

    #If we find a sector that contradicts our assumption, we change our minds.
    for sector in range(best_sector, best_sector + s_max + 1):
        if sector &lt; self.Hist.shape[0]:
            if self.Hist[sector] &gt; thrs:
                type_of_valley = ""Narrow"" 

    #If it is indeed a wide valley, we return the angle corresponding to the sector (k_n + s_max)/2.
    if type_of_valley == ""Wide"":

        theta = 5*(best_sector + s_max)/2
        return theta

    #Otherwise, we find the far border of the valley and return the angle corresponding to the mean value between the best sector and the far border.
    elif type_of_valley == ""Narrow"":
        for sector in range(best_sector, best_sector + s_max):
            if self.Hist[sector] &lt; thrs:
                far_border = sector

        theta = 5*(best_sector + far_border)/2
        return theta
</code></pre></li>
</ul>

<p>How can I address this issue? Is there a way to treat the histogram as circular? Is there maybe a better way to write this function?</p>

<p>Thank you for your time.</p>
","motion-planning python planning"
"9203","Helicopter Stabilization Algorithm","<p>I've hacked a rc helicopter, and I am able to control it by running a program on my computer. I am interested in writing algorithms that will stabilize the helicopter. For instance, the helicopter is hovering, and then if it is shoved off balance it can return to its previous position in a stable state. Any help on an algorithm would be awesome.</p>
","algorithm stability"
"9206","iRobot Create: Making Noise and Flashing Red Light While Charging","<p>My iRobot Create is playing a tune about every 30 seconds and continuously flashing a red light when I attempt to charge it. What is the issue?</p>
","irobot-create"
"9210","What is the difference between planning for kinematic car, dynamic car, blimp and quadrotor","<p>I am working with a sampling based planning library. When I looked into the implementation, I found for kinematic car a SE2 state space(x, y, yaw), for dynamic car a SE2 compound state space (space allowing composition of state spaces), for blimp and quadrotor a SE3 compound state space was used. I could understand the design of SE2 and SE3 state spaces but the compound state spaces of dynamic car, blimp and quadrotor I could not comprehend or differentiate.</p>

<p>what is the difference in terms of state space for motion planning for kinematic car, dynamic car, blimp and quadrotor?</p>
","mobile-robot quadcopter motion-planning"
"9214","Wireless Mini Camera","<p>I am looking for a mini, wireless, chargeable, camera that can stream video in real time to my computer. I will put the mini camera on my helicopter and send the live video feed to my pic and make certain calculations to make the helicopter navigate in a certain way. Any help would be great.</p>
","cameras"
"9218","Programming 4-digit seven segment display using interrupts only","<p>I have to program an autonomous bot (using an ATmega2560). It has a 4-digit seven segment display attached to it. I have to make the bot traverse through arena while continuously displaying the time in seconds on the seven segment display.</p>

<p>I can't use the code to display on seven segment display in my <code>main()</code> function.</p>

<p>Any help? </p>
","interrupts"
"9225","How to communication with old robotic arms from NakkaNippon Electric?","<p>I have a few robotic manipulators from NakkaNippon Electric and I'm trying to communicate with them using RS232 without success. The robot model are microrobots 88-4 or 88-5.</p>

<p>I'm sending commands via COM port, but I can't received anything from the box. I'm using a USB-to-DB9 converter (FTDI) with a DB9-DB25 cable.</p>

<p>On the net, the only reference I have for the robot is from an old post from year 2000 that was from user @peterkneale.</p>

<p>If it can help, <a href=""http://techinfoshawinigan.com/files/"" rel=""nofollow"">here is the link to the scanned PDF manual</a>.</p>

<p>You can see the commands on page 23-24 of the pdf (page 21-22 in document). </p>

<p>Any advice would be grateful</p>
","robotic-arm"
"9226","Wind Flow Diagram Of A Quadcopter","<p>I'm trying to determine the wind flow diagram around a quad-copter when it is in action. I looked up on internet but couldn't find any reliable source.</p>

<p>By wind flow diagram what I mean is when my Quad-copter is in mid-air hovering at some fixed position. How the air is moving around it. All the directions are needed to be kept in mind, from <strong><em>Top to bottom</em></strong> ( Vertical direction) and also the Horizontal Direction.
Thank You.</p>
","quadcopter dynamics"
"9227","Solar Cells Charging a Li-Po Battery","<p>Is there a way to charge a Li-Po battery using solar panels to increase the flight time of a quadcopter during its flight?</p>
","quadcopter power"
"9228","Evaluating the similarity of two 7 Degree of Freedom Arms","<p>I am working on the Baxter robot where I have a first arm configuration and a bunch of other arm configurations, where I want to find the closest arm configuration to the first among the many other arm configurations. The trick here is that the end effector location/orientation is the exact same for all the arm configurations, they are just different ik solutions. Can anyone point me towards the right direction towards this? Thank you.</p>
","robotic-arm inverse-kinematics"
"9232","How to interrupt on a data ready trigger when communications to the sensor are interrupt driven?","<p><strong>Background:</strong> I'm using the L3GD20H MEMS gyroscope with an Arduino through a library <a href=""https://github.com/pololu/l3g-arduino"" rel=""nofollow"">(Pololu L3G)</a> that in turn relies on interrupt-driven I2C (Wire.h); I'd like to be able to handle each new reading from the sensor to update the calculated angle in the background using the data ready line (DRDY). Currently, I poll the STATUS register's ZYXDA bit (which is what the DRDY line outputs) as needed.</p>

<p><strong>General question:</strong> With some digital output sensors (I2C, SPI, etc.), their datasheets and application notes describe using a separate (out-of-band) hardware line to interrupt the microcontroller and have it handle new sets of data. But on many microcontrollers, retrieving data (let alone clearing the flag raising the interrupt line) requires using the normally interrupt-driven I2C subsystem of a microcontroller. <strong>How can new sensor data be retrieved from the ISR for the interrupt line when also using the I2C subsystem in an interrupt-driven manner?</strong></p>

<p>Possible workarounds:</p>

<ol>
<li><p>Use nested interrupts (as @hauptmech mentioned): re-enable I2C interrupt inside of ISR. Isn't this approach discouraged?</p></li>
<li><p>Use non-interrupt-driven I2C (polling)--supposedly a dangerous approach inside of ISRs. The sensor library used depends on the interrupt-driven Wire library.</p></li>
<li><p>[Edit: professors' suggestion] Use a timer to interrupt set to the sample rate of the sensor (which is settable and constant, although we measure it to be e.g. 183.3Hz rather than 189.4Hz per the datasheet). Handling the I2C transaction still requires re-enabling interrupts, i.e. nested interrupts or performing I2C reads from the main program.</p></li>
</ol>

<p>[Edit:] Here's a comment I found elsewhere on a similar issue that led me to believe that the hang encountered was from I2C reads failing inside an interrupt handler: <a href=""https://www.sparkfun.com/tutorials/326#comment-4f4430c9ce395fc40d000000"" rel=""nofollow"">https://www.sparkfun.com/tutorials/326#comment-4f4430c9ce395fc40d000000</a> </p>

<blockquote>
  <p>…during the ISR (Interrupt Service Routine) I was trying to read the
  device to determine which bit changed. Bad idea, this chip uses the
  I2C communications which require interrupts, but interrupts are turned
  off during an ISR and everything goes kinda south.</p>
</blockquote>
","arduino microcontroller gyroscope i2c interrupts"
"9233","Uncented Kalman Filter for Dummies","<p>I need some help here because I can't figure how the Unscented Kalman Filter works.
I've searched for examples but all of them are too hard to understand.</p>

<p>Please someone can explain how it works step by step with a trivial example like position estimation, sensor fusion or something else?</p>
","kalman-filter"
"9234","Syncing camera with other signals","<p>I am not sure if this is the best place to ask this question, but hopefully someone here can give me some advice. I have a device hooked up to a data acquisition system that can provide sync out signal and record sync in signals. I need to synchronize my recordings with this device to a video feed. I am having trouble finding a camera that can provide a sync signal or any other good way to accomplish this. Thanks for your help.</p>
","sensors cameras"
"9236","Controlling Hubsan X4 with Crazyflie USB Dongle","<p>I have the Crazyflie USB dongle and it works with the Python Crazyflie software, and the USB dongle can control the Crazyflie drones as well as other drones that operate on 2.4 ghz. How can I get the dongle and software to work with the Hubsan X4? Any help would be great.</p>

<p>The link for the USB dongle and software:</p>

<p><a href=""http://www.seeedstudio.com/depot/Crazyradio-PA-long-range-24Ghz-USB-radio-dongle-with-antenna-p-2104.html"" rel=""nofollow"">http://www.seeedstudio.com/depot/Crazyradio-PA-long-range-24Ghz-USB-radio-dongle-with-antenna-p-2104.html</a></p>

<p><a href=""https://github.com/bitcraze/crazyflie-clients-python"" rel=""nofollow"">https://github.com/bitcraze/crazyflie-clients-python</a></p>
","python"
"9239","Cartesian space velocity profile to minimize jerk","<p>I am working with a 6 DOF manipulator. Currently I have implemented a simple velocity controler along a fixed direction on xyz space. I control the xyz space velocity (xdot) by using a predefined velocity profile against time. 
Joint values are updated based on the defined velocity profile.</p>

<p>Assume I want to move robot along a direction parallel to z axis, I define a trapezoidal velocity profile (z dot) over time as following, <a href=""http://i.stack.imgur.com/0DgId.jpg"" rel=""nofollow""><img src=""http://i.stack.imgur.com/0DgId.jpg"" alt=""enter image description here""></a></p>

<p>In the robot controller program, I convert this (z dot) to velocity at a time in joint space by multiplying by inverse of jacobian. In this way I can move robot as needed.</p>

<p><strong>My question is how we can define the above velocity profile over time, so that the <em>total jerk in joints</em> over time is minimized ?</strong></p>

<p>Your help is really appreciated.</p>
","control robotic-arm manipulator"
"9240","Simulation software for KUKA LBR iiwa robot?","<p>I have been working with KUKA LBR iiwa 7 R800 robot, with the KUKA's IDE, which is the 'Sunrise.Workbench'. Since it does not have any virtual platform to verify the code (simulate), it's been quite difficult, as I need to test each code by deploying to the robot.</p>

<p>Can anyone suggest if there is any simulation software available where I can test the code written using the Robotics API in Sunrise.Workbench?</p>

<p>I came across V-REP simulation software, but, not sure if I can use my code in the workbench platform.</p>

<p>Appreciate if anyone can shed some light on it.
Thanks in advance.</p>
","robotic-arm simulation"
"9241","Use MATLAB Compiler SDK generated package in KUKA Sunrise.Workbench","<p>The MATLAB Compiler SDK allows to create a wrapper for a MATLAB function which can be accessed by Java software. Based on my understanding, KUKA's Sunrise.Workbench IDE uses most of the standard Java functions. </p>

<p>I was trying to read the package generated using the MATLAB Compiler SDK (the new version of MATLAB Builder JA) into the workbench platform. I could successfully read the package into Eclipse IDE, but not into Workbench.</p>

<p>The reason for using the Compiler SDK is that, I have some functions is MATLAB, and I want to use the same in the workbench programming.</p>

<p>Does anyone have experience with the same? Appreciate any help.</p>
","matlab robotc"
"9243","How is it possible to decouple Mimo transfer function of robot to multi Siso system","<p>Here there is a <strong>Mimo</strong> transfer function with size of <strong>3*7 (3 inputs and 7 outputs)</strong>.</p>

<pre><code>G = [G11 G12 G13 G14 G15 G16 G17;
     G21 G22 G23 G24 G25 G26 G27;
     G31 G32 G33 G34 G35 G36 G37]
</code></pre>

<p>Is it possible to decouple the interaction between the loops and how can we get multi Siso system from G ?</p>
","control"
"9244","ROS tutorials no longer working","<p>Has anyone ever run into a case where a fresh install of ROS cannot run its tutorial packages?</p>

<p>I am running ROS Indigo on an nVidia Jetson TK1, using the nVidia-supplied Ubuntu image. I just did a fresh install, Ubuntu and ROS, just to keep things clean for this project. I am building a kind of 'demo-bot' for some students I will be teaching; it will use both the demo files and some of my own code. Now, after setting things up, I try to run the talker tutorial just to check to make sure that everything is running, and rospack is pretty sure that the tutorials don't exist.</p>

<p>For example, inputting this into the terminal</p>

<pre><code>rosrun rospy_tutorials talker
</code></pre>

<p>Outputs</p>

<pre><code>[rospack] Error: package 'rospy_tutorials' not found
</code></pre>

<p>This is the case for every tutorial file; python and C++. Now, I am sure the tutorials are installed. I am looking right at them in the file system, installed from the latest versions on github. So I think it is something on ROS' side of things.</p>

<p>Has anyone ever bumped into something similar before, where a ROS package that supposedly was installed correctly isn't found by ROS itself? I would rather not have to reinstall again if I can avoid it.</p>

<p><strong>EDIT #1</strong></p>

<p>After playing with it some more, I discovered that multiple packages were not running. All of them - some turtlebot code, and some of my own packages - returned the same error as above. So I suspect something got messed up during the install of ROS.</p>

<p><a href=""http://wiki.ros.org/roswtf"" rel=""nofollow"">roswtf</a> was able to run, but it did not detect any problems. However, going forward.</p>

<p><strong>EDIT #2</strong></p>

<p>I double checked the bashrc file. One export was missing, for ROS directory I was trying to work within. Adding it did not solve the problem. </p>

<p>I am still looking for a solution, that hopefully does not involve reflashing the TK1.</p>

<p><strong>EDIT #3</strong></p>

<p>Alright, so I've been poking at this for a few days now and pretty much gave up trying to get ROS to work correctly, and decided a re-flash was necessary. But I think I found something when I booted up my host machine. In my downloads folder, I have the v2.0 and the v1.2 JetPack. I know I used the v2.0 for this latest install, and it has been the only time I have used it (it provides some useful updates for OpenCV and bug fixes, among other things). I'm going to re-flash using the v1.2 JetPack this time, see if things behave better with ROS under that version. Its a long shot, but it is all I have to work with at the moment, and it shouldn't lose any ROS capabilities (aside from some of the stuff I wanted to do with OpenCV). I'll update everyone if that seems to work.</p>

<p><strong>EDIT #4</strong></p>

<p>Ok, everything seems to be working now. The problem does seem to be an issue with Jetpack v2.0. I suspect that some change, somewhere between v1.2 and v2.0 (made to accommodate the new TX1 board), messes with running ROS indigo on a TK1. I'm going to be a more detailed explanation in an answer to this question.</p>
","ros"
"9246","simulation of robots","<p>I am designing a new mechanism similar to robot arm. It would be a 6 or 7 axis with arms but not the same with traditional articulated arms. As a result, new DH matrix,and inverse kinematics involve. I would like to consult the robot professionals in this forum that do you suggest any simulation tool of this mechanism?</p>

<p>I plan to start with start and end points. Then I will do a trapezoid velocity plan and take sample points with sampling time along the path. After that, I would like to transfer these sampling points to motor joints by DH matrix and inverse kinematics. Finally I would do some basic 3D animation to visualize the movement temporally. I do not plan to simulate controller behavior because in my application motor drivers deal with it. I only need to focus on sending reasonable commands to motor drivers. </p>

<p>In my opinion, Matlab, octave, VC++, and some third-party tools are candidates. Starting from ground zero would be a time-consuming work. I would appreciate if any experts can share a tool or open source code from his or her experience. I did some search on Matlab robotics toolbox but I am not sure if it fits my need because it is expensive and optimized for ROS. In Octave there are also some robotics toolbox but I am not sure about what it can do and what it cannot. </p>
","robotic-arm matlab simulation c"
"9250","Quadcopter PID: Controller is Saturating","<p>Good day,</p>

<p>I am currently creating an autonomous quadcopter using a cascading PID controller specifically a P-PID controller using angle as setpoints for the outer loop and angular velocities for the inner loop. I have just finished tuning the Roll PID last week with only +-5 degrees of error however it is very stable and is able to withstand disturbances by hand. I was able to tune it quickly on two nights however the pitch axis is a different story.</p>

<p><strong>Introduction to the Problem:</strong>
The pitch is asymmetrical in weight (front heavy due to the stereo vision cameras placed in front). I have tried to move the battery backwards to compensate however due to the constraints of the DJI F450 frame it is still front heavy.</p>

<p>In a PID controller for an asymmetrical quadcopter, the I-gain is responsible for compensating as it is the one able to ""remember"" the accumulating error.</p>

<p><strong>Problem at Hand</strong>
I saw that while tuning the pitch gains, I could not tune it further due to irregular oscillations which made it hard for me to pinpoint whether this is due to too high P, I or D gain. The quadcopter pitch PID settings are currently at Prate=0.0475 Irate=0.03 Drate=0.000180 Pstab=3 giving an error from the angle setpoint of 15degrees of +-10degrees. Here is the data with the corresponding video.</p>

<p>RATE Kp = 0.0475, Ki = 0.03, Kd = 0.000180 STAB Kp=3
Video: <a href=""https://youtu.be/NmbldHrzp3E"" rel=""nofollow"">https://youtu.be/NmbldHrzp3E</a></p>

<p>Plot:
<a href=""http://i.stack.imgur.com/NcoAu.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/NcoAu.png"" alt=""enter image description here""></a></p>

<p><strong>Analysis of Results</strong>
It can be seen that the controller is saturating.
The motor controller is currently set to limit the pwm pulse used to control the ESC throttle to only 1800ms or 180 in the code (The maximum is 2000ms or 205) with the minimum set at 155 or 1115ms (enough for the quad to lift itselft up and feel weightless). I did this to make room for tuning the altitude/height PID controller while maintaining the throttle ratio of the 4 motors from their PID controllers.  </p>

<blockquote>
  <p>Is there something wrong on my implementation of limiting the maximum throttle?</p>
</blockquote>

<p>Here is the implementation:</p>

<pre><code> //Check if PWM is Saturating - This method is used to fill then trim the outputs of the pwm that gets fed into the gpioPWM() function to avoid exceeding the earlier set maximum throttle while maintaining the ratios of the 4 motor throttles. 
    float motorPWM[4] = {motorPwm1, motorPwm2, motorPwm3, motorPwm4};
    float minPWM = motorPWM[0];
    int i;
    for(i=0; i&lt;4; i++){ // Get minimum PWM for filling
        if(motorPWM[i]&lt;minPWM){
            minPWM=motorPWM[i];
        }
    }

    cout &lt;&lt; "" MinPWM = "" &lt;&lt; minPWM &lt;&lt; endl;

    if(minPWM&lt;baseThrottle){
        float fillPwm=baseThrottle-minPWM; //Get deficiency and use this to fill all 4 motors
        cout &lt;&lt; "" Fill = "" &lt;&lt; fillPwm &lt;&lt; endl;
        motorPwm1=motorPwm1+fillPwm;
        motorPwm2=motorPwm2+fillPwm;
        motorPwm3=motorPwm3+fillPwm;
        motorPwm4=motorPwm4+fillPwm;
    }

    float motorPWM2[4] = {motorPwm1, motorPwm2, motorPwm3, motorPwm4};
    float maxPWM = motorPWM2[0];
    for(i=0; i&lt;4; i++){ // Get max PWM for trimming
        if(motorPWM2[i]&gt;maxPWM){
            maxPWM=motorPWM2[i];
        }
    }

    cout &lt;&lt; "" MaxPWM = "" &lt;&lt; maxPWM &lt;&lt; endl;

    if(maxPWM&gt;maxThrottle){
        float trimPwm=maxPWM-maxThrottle; //Get excess and use this to trim all 4 motors
        cout &lt;&lt; "" Trim = "" &lt;&lt; trimPwm &lt;&lt; endl;
        motorPwm1=motorPwm1-trimPwm;
        motorPwm2=motorPwm2-trimPwm;
        motorPwm3=motorPwm3-trimPwm;
        motorPwm4=motorPwm4-trimPwm;
    }
</code></pre>

<p><strong>Possible solution</strong>
I have two possible solutions in mind</p>

<ol>
<li><p>I could redesign the camera mount to be lighter by 20-30 grams. to be less front heavy</p></li>
<li><p>I could increase the maximum throttle but possibly leaving less room for the altitude/throttle control.</p></li>
</ol>

<blockquote>
  <p>Does anyone know the optimum solution for this problem?</p>
</blockquote>

<p><strong>Additional information</strong>
The quadcopter weighs about 1.35kg and the motor/esc set from DJI (e310) is rated up to 2.5kgs with the recommended thrust per motor at 350g (1.4kg). Though a real world test <a href=""http://www.rcgroups.com/forums/showthread.php?t=2409123"" rel=""nofollow"" title=""here"">here</a> showed that it is capable at 400g per motor with a setup weighing at 1600g take-off weight </p>

<p>How I tune the roll PID gains</p>

<p>I had set first the Rate PID gains. at a setpoint of zero dps</p>

<ol>
<li>Set all gains to zero.</li>
<li>Increase P gain until response of the system to disturbances is in steady oscillation.</li>
<li>Increase D gain to remove the oscillations.</li>
<li>Increase I gain to correct long term errors or to bring oscillations to a setpoint (DC gain).</li>
<li>Repeat until desired system response is achieved</li>
</ol>

<p>When I was using the single loop pid controller. I checked the data plots during testing and make adjustments such as increasing Kd to minimize oscillations and increasing Ki to bring the oscillations to a setpoint. I do a similar process with the cascaded PID controller.</p>

<p>The reason why the rate PID are small because rate Kp set at 0.1 with the other gains at zero already started to oscillate wildy (a characteristic of a too high P gain). <a href=""https://youtu.be/SCd0HDA0FtY"" rel=""nofollow"">https://youtu.be/SCd0HDA0FtY</a></p>

<p>I had set the Rate pid's such that it would maintain the angle I physically placed it to (setpoint at 0 degrees per second) </p>

<p>I then used only P gain at the outer loop stabilize PID to translate the angle setpoint to velocity setpoint to be used to control the rate PID controller.</p>

<p>Here is the roll axis at 15 degrees set point <a href=""https://youtu.be/VOAA4ctC5RU"" rel=""nofollow"">https://youtu.be/VOAA4ctC5RU</a>
Rate Kp = 0.07, Ki = 0.035, Kd = 0.0002 and Stabilize Kp = 2
<a href=""http://i.stack.imgur.com/p3RyT.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/p3RyT.png"" alt=""enter image description here""></a></p>

<p>It is very stable however the reaction time/rise time is too slow as evident in the video.</p>
","quadcopter pid stability"
"9254","Pure Arduino Quadcopter","<p>I recently bought a set of escs, brushless outrunner motors and propellers. I'm trying to perform a calibration on the esc, but I can't find how I can do that without using components other than the arduino uno itself. The setup I've managed to make is the one shown in the picture. The escs are a mystery, as there is no manual to be found. If it helps, the buy link is this : <a href=""http://www.ebay.co.uk/itm/4x-A2212-1000KV-Outrunner-Motor-4x-HP-30A-ESC-4x-1045-prop-B-Quad-Rotor-/111282436897"" rel=""nofollow"">http://www.ebay.co.uk/itm/4x-A2212-1000KV-Outrunner-Motor-4x-HP-30A-ESC-4x-1045-prop-B-Quad-Rotor-/111282436897</a>
There might also be a problem with the battery (LiPo 3.7V, 2500mAh).
<a href=""http://i.stack.imgur.com/vF9YJ.jpg"" rel=""nofollow""><img src=""http://i.stack.imgur.com/vF9YJ.jpg"" alt=""My Setup""></a></p>

<p>Can andybody figure out what I'm doing wrong?
The sample arduino code I found was this:</p>

<pre><code>#include &lt;Servo.h&gt;

#define MAX_SIGNAL 2000
#define MIN_SIGNAL 700
#define MOTOR_PIN 9

Servo motor;

void setup() {
  Serial.begin(9600);
  Serial.println(""Program begin..."");
  Serial.println(""This program will calibrate the ESC."");

  motor.attach(MOTOR_PIN);

  Serial.println(""Now writing maximum output."");
  Serial.println(""Turn on power source, then wait 2 seconds and press any key."");
  motor.writeMicroseconds(MAX_SIGNAL);

  // Wait for input
  while (!Serial.available());
  Serial.read();

  // Send min output
  Serial.println(""Sending minimum output"");
  motor.writeMicroseconds(MIN_SIGNAL);

}

void loop() {  
}
</code></pre>
","arduino quadcopter esc"
"9256","3 way check valve","<p>I am working on a micro dispensing system, using syringe pump. The design involves a syringe on top to be moved by stepper motor. There would be one liquid reservoir form which the syringe would pull liquid from, and push it to eject liquid from other end. </p>

<p>When we pull the syringe, the liquid is sucked into the syringe, while the other opening is shut. When the syringe is pushed, the liquid is ejected from the other end.</p>

<p>The quantity of liquid to be dispensed would be very small (400mg) so i am using small syringe of 1 or 2 ml .. as per my measurement, after every 100 dispensing operations, 1 ml syringe would be empty and we would need to pull liquid from the reservoir into the syringe, and do the dispensing again. </p>

<p>My question is, I am unsure about the check valve here. Is there a 'Single' check valve available which would allow this kind of flow to happen ?<a href=""http://i.stack.imgur.com/TWKZy.jpg"" rel=""nofollow""><img src=""http://i.stack.imgur.com/TWKZy.jpg"" alt=""enter image description here""></a></p>
","motor design"
"9260","How difficult it is to build simple robots (for example Line follower) using raspberry pi and ROS?","<p>I want to build a low cost robot, running ROS for educational purposes. It can be a simple line follower using raspberry pi and an IR sensor. Is it overambitious as a beginner project? How difficult is it to make ROS run on custom hardware?</p>

<p>P.S. I am newbie in both robotics and programming and I am more interested in building actual robots than running simulations. Also, I cant afford to buy ROS compatible robots.</p>
","ros raspberry-pi electronics"
"9262","Topics of object perception for pr2","<p>I started to use ROS hydro (Robot Operating System) on ubuntu, using the simulator ""Gazebo"" and roscpp library, in order to program some robots. </p>

<p>In case of pick up and place known objects by robots, what are the topics of object perception for pr2 in ROS??</p>
","ros gazebo"
"9263","Are operational space and joint space dependent on each other?","<p>Some questions about this, my friends and I argued with this problem.</p>

<p>Are operational space and joint space dependent on each other?</p>

<p>I know that $x_e$ (end effector's pos.) and $q$ (joint var.) can be expressed by an equation with non-linear function $k$:</p>

<p>$x_e = k(q)$</p>

<p>But I don't think that it tells us operational space and joint space are dependent. </p>
","kinematics"
"9265","Waiting for the r_gripper_sensor_controller/gripper_action action server to come up","<p>I started to use ROS hydro (Robot Operating System) and roscpp.
I tested some examples to move the gripper of pr2 in Gazebo (especially the code in :  <a href=""http://wiki.ros.org/pr2_gripper_sensor_action/Tutorials/Grab%20and%20Release%20an%20Object%20Using%20pr2_gripper_sensor_action"" rel=""nofollow"">http://wiki.ros.org/pr2_gripper_sensor_action/Tutorials/Grab%20and%20Release%20an%20Object%20Using%20pr2_gripper_sensor_action</a> ) with catkin package.</p>

<p>I launch : roslaunch pr2_gazebo pr2_empty_world.launch </p>

<p>and when I run the node of code with : rosrun pack_name node_name, I get :: Waiting for the r_gripper_sensor_controller/gripper_action action server to come up ... Waiting for the r_gripper_sensor_controller/gripper_action action server to come up ...</p>

<p>I want to know the cause of those lines in order to see the results. what should I do??</p>

<p>it is notable that when I launch : roslaunch pr2_gripper_sensor_action pr2_gripper_sensor_actions.launch<br>
      in the previous link, I get : </p>

<p>[pr2_gripper_sensor_actions.launch] is neither a launch file in package [pr2_gripper_sensor_action] nor is [pr2_gripper_sensor_action] a launch file name</p>
","ros gazebo"
"9267","6D localization with 6 lasers","<p>I have to know where a multi-rotor is, in a rectangular room, via 6 lasers, 2 on each axis.</p>

<p>The problem is like this: </p>

<h1>Inputs :</h1>

<ul>
<li>Room : square => 10 meters by 10 meters</li>
<li>6 positions of the lasers : <strong>Fixed</strong> on the frame</li>
<li>6 orientations of the lasers : <strong>Fixed</strong> on the frame</li>
<li>The 6 measurements of the lasers</li>
<li>The quaternion from the IMU of my flight controller (PixHawk).</li>
<li>The origin is centered on the gravity center of the multi-rotor and defined as if the walls are perpendicular to each axes (the normal of the wall in X is (-1,0,0)) </li>
</ul>

<h1>Output :</h1>

<ul>
<li>Position in 3D (X,Y,Z)</li>
<li>Angular position (quaternion)</li>
</ul>

<p>Since I got the angular position of the multi-rotor, I rotated the laser positions and orientations via the quaternion, then extrapolate via the 6 measurements and I got the 3 walls. (orientations of the walls are trivial, then only one point is enough to determine its position.</p>

<p>Badly, I noticed that the <strong>yaw</strong> (rotation about z) measurement from the PixHawk is <strong>unreliable</strong>. Then I should measure the yaw from the lasers, but I do not success to do it. Event if the 2D problem is easy, I am lost in 3D.</p>

<p>Does someone know if it [<em>Algorithm to know XYZ position and quaternion from 6 measurments</em>] exists somewhere ? Or what is the right way to go on this problem ? </p>

<p><strong>The question :</strong> How could I get the yaw from 2 measurements from 2 lasers which I know the original position, orientation and the pitch and roll. </p>

<p>NOTE : Green pointers are the origin position, Red pointers are the ""final"" position, but could be rotated around the red circle (due to yaw).</p>

<p><a href=""http://i.stack.imgur.com/ARr3D.png""><img src=""http://i.stack.imgur.com/ARr3D.png"" alt=""Representation""></a></p>
","algorithm geometry"
"9269","Suggestions on object types (features) to track from ARDrone 2 camera","<blockquote>
  <p><strong><em>UPDATE</em></strong>
  I have aded 50 bounty for <a href=""http://stackoverflow.com/questions/35672762/finding-features-to-track-from-ar-drone-2-camera-illumination"">this</a> question on the StackOverflow</p>
</blockquote>

<p>I am trying to implement object tracking from the camera(just one camera, no Z info). Camera has 720*1280 resolution, but I usually rescale it to 360*640 for faster processing.</p>

<p>This tracking is done from the robots camera and I want a system which would be as robust as possible. </p>

<p>I will list what I did so far and what were the results.</p>

<ol>
<li>I tried to do <strong><em>colour tracking</em></strong>, I would convert image to hsv colour space, do thresholding, some morphological transformations and then find the object with the biggest area. This approach made a fair tracking of the object, unless there are no other object with the same colour. As I was looking for the max and if there are any other objects bigger than the one I need, robot would go towards the bigger one</li>
<li>Then, I decided to track <strong><em>circled objects of the specific colour</em></strong>. However, it was difficult to find under different angles</li>
<li>Then, I decided to track <strong><em>square objects of specific colour</em></strong>. I used this</li>
</ol>

<blockquote>
<pre><code>       // Approximate contour with accuracy proportional
        // to the contour perimeter
        cv::approxPolyDP(
                cv::Mat(contours[i]),
                approx,
                cv::arcLength(cv::Mat(contours[i]), true) * 0.02,
                true
        );
</code></pre>
</blockquote>

<p>and then I checked this condition</p>

<blockquote>
  <p>if (approx.size() >= 4 &amp;&amp; approx.size() &lt;= 6)</p>
</blockquote>

<p>and afterwards I checked for</p>

<blockquote>
  <p>solidity > 0.85 and aspect ratio between 0.85 and 1.15</p>
</blockquote>

<p>But still result is not as robust as I would expect, especially the size. If there are several squares it would not find the needed one.</p>

<hr>

<p>So, now I need some suggestions on what other <strong><em>features</em></strong> of the object could I use to improve tracking and how? As I mentioned above several times, one of the main problems is <strong><em>size</em></strong>. And I know the size of the object. However, I am not sure how I can make use of it, because I do not know the distance of the object from the camera and that is why I am not sure how to represent its size in pixel representation so that I can eliminate any other blobs that do not fall into that range.</p>

<p><strong><em>UPDATE</em></strong></p>

<p>In the third step, I described how I am going to detect squares with specific colour. Below are the examples of what I am getting. </p>

<p>I used this HSV range for the red colour:</p>

<blockquote>
  <p>Scalar(250, 129, 0), Scalar(255, 255, 255), params to OpenCV's inRange function</p>
  
  <p>HMIN = 250, HMAX = 255; SMIN = 129, SMAX = 255; VMIN = 0, VMAX = 255;
  (Would like to see your suggestions on tweaking this values as well)</p>
</blockquote>

<p>So, in this picture you can see the processing; gaussian blurring (5*5),
morphological closing two times (5*5). And the image with the label ""result"" shows the tracked object (please look at the green square).</p>

<p><a href=""http://i.stack.imgur.com/XSffp.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/XSffp.png"" alt=""Thresholding and tracking based on HSV colour space 1""></a></p>

<p>On the second frame, you can see that it cannot detect the ""red square"". The only main difference between these two pics is that I bended down the lid of the laptop (please look closer if you cannot notice). I suppose this happens because of the <strong><em>illumination</em></strong>, and this causes the thresholding to give not desired results. </p>

<p><a href=""http://i.stack.imgur.com/iJ5Vk.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/iJ5Vk.png"" alt=""Bended the lid of the laptop""></a></p>

<p>The only way, I can think of is doing two separate processing on the image. First, to do thresholding based on the colour as I was doing above. Then if I find the object to move to the next frame. If not to use <a href=""https://github.com/Itseez/opencv/blob/master/samples/cpp/squares.cpp"" rel=""nofollow"">this opencv's find squares</a> method.</p>

<p>However, this method will involve doing too much of processing of the image. </p>
","quadcopter cameras opencv"
"9274","Action cost to get smooth path","<p>What action cost should be used to get a smooth path? Like we use distance traversed to get the shortest path. Will the cost to get a smooth path will be something related to rate of change of slope of the path?</p>
","motion-planning"
"9276","Papers on Algorithms in Robotics","<p>I'm a CS student and I need to give a 30-minute lecture about 1-2 papers describing 1-2 algorithms for any of the main problems in Robotics (navigation, coverage, patrolling, etc.).</p>

<p>I have no background in Robotics specifically, but I did take classes such as Algorithms and AI (including some basic AI algorithms such as <a href=""https://en.wikipedia.org/wiki/A*_search_algorithm"" rel=""nofollow"">A*</a>, <a href=""https://en.wikipedia.org/wiki/Iterative_deepening_depth-first_search"" rel=""nofollow"">IDS</a>, <a href=""https://en.wikipedia.org/wiki/Dijkstra%27s_algorithm#Practical_optimizations_and_infinite_graphs"" rel=""nofollow"">UCS</a>, and subjects such as <a href=""https://en.wikipedia.org/wiki/Decision_tree_learning"" rel=""nofollow"">decision trees</a>, <a href=""https://en.wikipedia.org/wiki/Game_theory"" rel=""nofollow"">game theory</a>, etc.).
The difference between simply describing one of the above is that I need the paper to refer to actual physical robots and their algorithms, with real problems in the physical world, as opposed to AI ""agents"" with more theoretical algorithms.</p>

<p>I am required to lecture on 1-2 academic papers, published from 2012 onward, with a ""respectable"" amount of citations. Any suggestions of such papers would be greatly appreciated!</p>
","navigation algorithm theory coverage"
"9277","How to guide a camera through a circular tube?","<p>Let's say I have a 6-DOF flying camera and I want to make it move through a circular tube autonomously and let's suppose that the camera and the system that makes it fly are considered to be just a point in space. Which feature of the image I get from the camera can I use to move the camera appropriately, that is to get in one end of the tube and get out from the other? </p>

<p>For example, I thought I could use edge detection. As the camera moves forward through the tube, due to the fact that its far plane is not infinitely away, there is a dark circle forming where the camera sees nothing surrounded by the walls of the tube. I think that ""preserving"" this circle might be the way to go (for example if it becomes an ellipse I have to move the camera accordingly for it to become a circle again), but what are the features that will help me ""preserve"" the circle?</p>

<p>I would like to use image-based visual servoing to do that. However, what troubles me is the following. In most visual-servoing applications I have seen, the control objective is to make some features ""look"" in a certain way from the camera point of view. For example, we have the projections of 4 points and we want the camera to move accordingly so that the projections' coordinates have some specific values. But the features are actually the same. </p>

<p>In my case I thought that for example I could say that I want the projections of the 4 ""edge points"" of the circle/ellipse to take specific values so that they define a circle centered at the fov of the camera. But if the camera moves to achive this setup of features, then the 4 new ""edge points"" will correspond to the projections of 4 different real points of the pipe and the theory collapses. Am I right to think that? Any way to get past it?</p>

<p>Any oher ideas or relevant literature?</p>
","localization cameras motion-planning visual-servoing exploration"
"9280","Inverse kinematics with singularity in MATLAB","<p>I want to find the general coordinates q=[alpha,beta,gamma] (3 revolute joints) that minimizes the norm ||rGoal - r||_2 with rGoal not included in the manipulator workspace. </p>

<p>The problem is already solved for coordinates rGoal inside the manipulator workspace, but I really dont know how to do that for Singularities.</p>

<pre><code>% given are the functions 
%   r_BF_inB(alpha,beta,gamma) and
%   J_BF_inB(alpha,beta,gamma) 
% for the foot positon respectively Jacobian

r_BF_inB = @(alpha,beta,gamma)[...
    -sin(beta + gamma) - sin(beta);...
  sin(alpha)*(cos(beta + gamma) + cos(beta) + 1) + 1;...
  -cos(alpha)*(cos(beta + gamma) + cos(beta) + 1)];

J_BF_inB = @(alpha,beta,gamma)[...
                                              0,             - cos(beta + gamma) - cos(beta),            -cos(beta + gamma);...
 cos(alpha)*(cos(beta + gamma) + cos(beta) + 1), -sin(alpha)*(sin(beta + gamma) + sin(beta)), -sin(beta + gamma)*sin(alpha);...
 sin(alpha)*(cos(beta + gamma) + cos(beta) + 1),  cos(alpha)*(sin(beta + gamma) + sin(beta)),  sin(beta + gamma)*cos(alpha)];

% write an algorithm for the inverse kinematics problem to
% find the generalized coordinates q that gives the endeffector position rGoal =
% [0.2,0.5,-2]' and store it in qGoal
q0 = pi/180*([0,-30,60])';
rGoal = [0.2,0.5,-2]';

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%% enter here your algorithm
q=q0;
b=true;
while b 
    q_old=q;
    dr=rGoal-r_BF_inB(q_old(1),q_old(2),q_old(3));
    dq=J_BF_inB(q_old(1),q_old(2),q_old(3))\dr;
    q=q_old + dq;
    if norm(rGoal-r_BF_inB(q_old(1),q_old(2),q_old(3)))&lt; 1E-3
        b=false
    end
end
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

qGoal = q; 
</code></pre>

<p>A stopping criteria I can use is:</p>

<pre><code>norm(rGoal-r_BF_inB(q_old(1),q_old(2),q_old(3)))&lt; norm(rGoal-r_BF_inB(q(1),q(2),q(3)))
</code></pre>

<p>But that does not really give me the correct answer.
How can this be solved in general for a rGoal outside of the workspace?</p>
","inverse-kinematics"
"9282","Mobile phone power packs","<p>For my robotics project I would like to utilise readily available mobile phone 'power banks' to simplify the power system for my robot. However, such power banks output 5V, great for the logic systems but not for the motors. </p>

<p>I was wondering if I could wire the outputs of two power banks in series and get 10V or is this a very bad idea? Should I wire them in parallel and use a boost converter? Is a custom solution using 'ordinary' Li-Po batteries and associated charging circuit the best answer?</p>

<p>Additional Information:</p>

<ul>
<li>This will be a two wheeled robot.</li>
<li>5V Logic</li>
<li>7+V Motor driver</li>
<li>Power Banks: 5V 2.1Amp 2100mAh</li>
</ul>
","mobile-robot power battery"
"9290","Micro Powder Dosing","<p>I am in process of designing a micro powder doser for metallic powder into plastic capsules (the capsule volume would be bigger than what we require, so traditional capsule filling wont work). The quantity I need to dose is 400 mg .. What in your opinion would be the best approach for this? </p>

<p>As per my research, I have found out that there can be 3 approaches</p>

<ol>
<li>Auger based solution, where an auger is used to control the powder drop gravitationally. </li>
</ol>

<p><a href=""http://i.stack.imgur.com/26vbD.jpg"" rel=""nofollow""><img src=""http://i.stack.imgur.com/26vbD.jpg"" alt=""Auger Based Powder doser""></a></p>

<ol start=""2"">
<li>Volumetric
As powder quantities would be small, a metal rods with groove of proper volume can be designed to dispense powder. For keep powder flowing we can use a vibration motor to pour powder in the groove. A stepper motor can rotate the rod for dispensing the powder. I have made the rod bigger to show the concept, and then how it would be mounted in a cap so when we rotate it that much volume of powder is dropped from the cap.</li>
</ol>

<p><a href=""http://i.stack.imgur.com/sKLt6.jpg"" rel=""nofollow""><img src=""http://i.stack.imgur.com/sKLt6.jpg"" alt=""enter image description here""></a></p>

<ol start=""3"">
<li>Weight based measurement.
Dropping powder on to a weighing balance, and control it via feedback .. I think it would be a difficult thing to do, plus time consuming, provided we'd have to fill thousands of capsules.</li>
</ol>

<p>Prio # 1 is accuracy, with an error margin of 1 -2 % .. secondly cost .. I'd like it not to be too costly .. 300 - 500 $. 
The metal powder is not magnetic .. is very fine .. and doesn't clump .. I have read articles and it appears by continues tapping the powder flow can be improved a lot.</p>
","arduino motor"
"9294","Choosing a battery: is a harbor freight solar battery OK for a R/C Lawnmower?","<p>I have built an R/C Lawnmower.  I call it the Honey Badger, because it tears stuff up (that's a good thing).  Well, I used used batteries to get the project going and now it's long past time to get the Honey Badger going again.</p>

<p>The Honey Badger is built on an electric wheelchair frame, and originally used wheelchair deepcycle batteries.  U1 if I recall.  There are 4 of them wired in 2 banks in series and parallel to give 24V for the 24V motors.</p>

<p>Going down to the used wheelchair parts place is about an hour drive and requires a weekend visit and will get me used batteries of unknown condition.  </p>

<p>Contrast that with Harbor Freight, which is 20 minutes away and has <a href=""http://www.harborfreight.com/12-volt-35-amp-hour-universal-battery-68680.html"" rel=""nofollow"">solar batteries</a> the same physical dimensions and comparable (?) electrical characteristics.  I think with coupons, tax, and after playing the game, I can get a battery for ~$50, about the same price as a used U1.</p>

<p>I found that Amazon also has U1 batteries, and they can be had for ~\$120 for 2 with shipping.</p>

<p>Batteries plus will sell me some deepcycle auto batteries of greater Ah capacity for ~$100 each.</p>

<p>Gross for each solution winds up being around the same: ~\$240 - ~\$300.</p>

<p>Is there a difference in technology between a ""solar battery"" and a ""wheelchair battery""?  Is that difference substantial?  Given that I'm pretty rough with this thing, is any particular technology any better suited to these tasks?  Is there a benefit or drawback to using an automotive battery?</p>

<p>I have the charger from the original wheelchair and if I recall, it's good for the capacity and has room to spare.  I think it can put out 5 amps.</p>

<p><a href=""http://i.stack.imgur.com/vKY0O.jpg"" rel=""nofollow""><img src=""http://i.stack.imgur.com/vKY0O.jpg"" alt=""R/C Lawnmower""></a></p>
","battery"
"9296","Why does my RoboClaw seem to be ignoring the PID gain settings?","<p>I'm seeing a behavior in my RoboClaw 2x7 that I can't explain.  I've been trying to manually tune the velocity PID settings (I don't have a windows box so I can't use Ionmc's tuning tool) by using command 28 to set the velocity PID gains, then command 55 to verify that they're set correctly, then 35 to spin the wheel at half of its maximum speed.  The problem is that no combination of PID gains seems to make any difference at all.  I've set it to 0,0,0 and the motor still spins at roughly the set point.</p>

<p>I must be doing something wrong, but I'm pouring over <a href=""http://downloads.ionmc.com/docs/roboclaw_datasheet_2x7a.pdf"" rel=""nofollow"" title=""datasheet"">the datasheet</a> and I just don't see what it is.  By all rights the motor shouldn't spin when I use 0,0,0!  Any ideas?</p>
","control pid"
"9297","Finding rotation quaternion","<p>I am trying to use a quaternions for robotics and there is one thing I don't understand about it. Most likely because I don't understand how to define position with quaternions and how to define rotation with quaternions if there is any difference..<br>
Please watch my ""understanding steps"" and correct if I am wrong somewhere.</p>

<p>Lets assume I we have 2 vehicle positions described by 2 rotation quaternions:
$$
q_1 = w_1 + x_1i + y_1j +z_1k = \cos(\pi/4) + \sin(\pi/4)i
$$
This quaternion is normalized and represents rotation over the $x$ axis for $\pi/2$ angle as I understand it.
$$
q_2 = w_2 + x_2i + y_2j + z_2k = \cos(\pi/4) + \sin(\pi/4)k
$$
And this one represents rotation for the same angle $\pi/2$ over the $y$ axis.</p>

<p>$q_1*q_2 = q_3$ which would be the same rotation as if we made $q_1$ first and $q_2$ second. 
$$q_3 = \frac{1}{2} + \frac{i}{2} +\frac{j}{2} +\frac{k}{2}$$</p>

<p><strong>QUESTION 1:</strong> Given $q_2$ and $q_3$ how can I find $q_1$?</p>

<p><strong>QUESTION 2:</strong> How to find a rotation angle over some other vector, given rotation quaternion? For example I want to find on what angle does $q_3$ turned over $2i+j-k$ quaternion.</p>
","kinematics"
"9298","Finding high torque servo for robotic arm","<p>I am new working with robotic arms but I am having trouble finding the correct servo for the base of the arm. </p>

<p>It is a 2 link robot - each link weighs 1.2 kg and is 40 cm long. I have a gripper of 10 centimeters. The servo in the gripper can hold a max of 4kg. The whole robotic arm, including the maximum load it will carry and the servos and other accessories, is 8.3 kg. The maximum load it needs to carry is 4 kg at the end of the arm at 90 cm. </p>

<p>What servo could I use to move the rotary base and what servo could I use to lift the arm in the base? The last one is to move the link so it would be preferable to have a 2 axis servo.</p>

<p>The only specification I need right now is what servo to use my energy supply are two 12 volts DC batteries connected in series with 18Ah. I need the servo to be DC. The other things can be worked around the servo that can best do the work.</p>
","robotic-arm servomotor"
"9301","Direct vs semi-direct methods for visual inertial odometry","<p>I was reading these papers on visual inertial odometry from IROS 15:</p>

<p><a href=""https://www.google.com/url?sa=t&amp;rct=j&amp;q=&amp;esrc=s&amp;source=web&amp;cd=1&amp;cad=rja&amp;uact=8&amp;ved=0ahUKEwi1mN2quKHLAhUB6GMKHVwiBcsQFggiMAA&amp;url=https%3A%2F%2Fait.inf.ethz.ch%2Fprojects%2F2015%2FPhotometricEKF%2Fdownloads%2Firos2015photometric.pdf&amp;usg=AFQjCNFLHMya8Hvo8FOrRRSspxVXut6Tlw&amp;sig2=X-EzwVGRHqP9CXm0Xk-B-Q"" rel=""nofollow"">Semi-Direct EKF-based Monocular Visual-Inertial Odometry</a></p>

<p><a href=""http://e-collection.library.ethz.ch/eserv/eth:48374/eth-48374-01.pdf"" rel=""nofollow"">Robust Visual Inertial Odometry Using a Direct EKF-Based Approach
</a></p>

<p>I would appreciate if someone could explain how semi-direct and direct methods vary exactly? As far as I understand, direct methods use pixel intensities in their framework. However, both these papers listed above use photometric intensities/pixel intensity values and yet one is semi direct and the other's direct. </p>
","ekf"
"9302","Output angle of DC motor - with current input","<p>What is your preferred <em>time</em>-domain method to simulate <strong>ideal</strong> position of a motor given an electrical-current input?</p>

<p>Assume that the goal is to plot the position output of a motor, based only on a constant electrical-current input.</p>

<p>For example, X amps are given to the motor. Plot the output Angle Y as a function of time, using only time-based tools (not Laplace or MATLAB outputs).</p>

<p>========= Two methods ===========</p>

<p>Here are two methods, which yield different results for some reason. Any ideas why they are not identical?</p>

<p>Input is a constant Current <code>I</code>.</p>

<p><strong>Method 1)</strong> Find updated velocity given an acceleration</p>

<p><code>acc[k] = (I[k]*Kt - b*vel[k-1]) / J</code></p>

<p><code>vel[k] = vel[k-1] + acc[k]*dt</code></p>

<p><code>pos[k+1] = pos[k] + vel[k]*dt +  0.5*acc[k]*dt^2</code></p>

<p>So, as desired, this is just a function of 'I'. This version has a less stable, oscillating output, regardless of which order <code>vel</code> and <code>acc</code> are computed in.</p>

<p><strong>Method 2)</strong> Find velocity using difference between positions. This could obviously be a noisy result, in practice would need to be filtered.</p>

<p><code>acc[k] = (I[k]*Kt - b*vel[k]) / J</code></p>

<p><code>vel[k] = (pos[k] - pos[k-1]) / dt</code></p>

<p><code>pos[k+1] = pos[k] + vel[k]*dt +  0.5*acc[k]*dt^2</code></p>

<p>As before, this position equation is then ultimately just a function of <code>I</code>. However, this version has a smoother output.</p>

<p>=====================</p>

<p><strong>Appendix 1</strong>: For method (1), assume an ideal current-driven DC motor Laplace model:
<code>angVelocity/current = Kt/(Js+b)</code>; <code>Kt</code> is motor constant, <code>J</code> is rotor inertia, and <code>b</code> is damping.</p>

<p>Then <code>acc*J + b*vel = current*Kt</code>, so <code>acc[k+1] = (current[k]*Kt - b*vel[k]) / J</code>. As a reminder, <code>vel[k]</code> was found using the previous acceleration via <code>v[k] = v[k-1] + acc[k-1]*dt</code> as noted above in method #1.</p>
","motor kinematics servomotor simulation"
"9303","Ultrasonic Sensor's Lag (20Hz) effect on PID contol loop rate (150Hz)","<p>Good day, I would like to ask how is it possible to use an ultrasonic sensor for altitude hold in a quadcopter if the sampling rate of the Ultrasonic sensor (HC-SR04) is only 20Hz before incurring any errors through polling when I had tested it. I have seen this sensor being implemented on other projects however I could not find any papers that explain the use of this sensor in better detail. I have seen possible solutions on the raspberry pi one using interrupts and the other using Linux's multithreading.</p>

<p>If my understanding is right, to use interrupts, I need a some sort of data ready signal from the ultrasonic sensor. However this is not available in this particular sensor. Is it possible to use the echo pin as the positive edge trigger for the interrupt service routine (read sonar/calculate distance function). But would this not introduce inconsistent loop execution times which is bad for a consistent PID loop.</p>

<p>Another approach is to use multithreading using the wiring-pi library which enables me to run a function, let's say a function that triggers the sonar and calculates the distance along side the pid control loop. How would this affect the PID control loop rate?</p>

<blockquote>
  <p>Which is the best way to implement sonar sensor based altitude hold?</p>
</blockquote>
","quadcopter pid stability real-time sonar"
"9305","Create 2 light red/green","<p>I am working on a project with the Create 2. Just recently I have run into a problem with the battery state. The Create 2 has been charging all night so its clean light shows green. However, when I unplug it and press the clean button, it shows red and will not consistently run commands from my Arduino that I have hooked up to it. </p>

<p>What could be the problem?</p>
","arduino irobot-create battery"
"9312","KUKA delimiter .NET","<p>I have a chance to develop a user interface program that lets the user control a KUKA robot from a computer. I know how to program stuff with the KUKA utilities, like OrangeEdit, but I don't know how to do what I want to do. I don't even know what's the ""best"" language to talk to the robot.</p>

<p>My idea is to control the robot with the arrow buttons, like up/down controls the Z axis and left/right controls the X/Y axes.</p>

<p>Can someone help me here? I know there's a lot of libraries to control the robot even with an Xbox controller, but if I limit the robot to 3 axes I might be able to control with simple buttons. </p>

<p>Edit: Now imagine that i have a routine that consists on going from P1 to P2 then to P3. I know i can ""touch up"" the points to refresh its coordinates using the console, but can i do it in a .net application? like modifying the src/srcdat files?</p>
","robotic-arm kuka"
"9313","How to detect wall-corners, fans, lights in an indoor using CV?","<p>Im currently working on an autonomous indoor quad-rotor. For this purpose I'm using OpenCV to enable computer vision in my drone. I need to be able to detect wall corners, fans (both stationary and rotating), lights and lamps, wall paintings and any other object associated with the walls and ceiling of an indoor environment. Until now I have come up with two ideas to achieve this. </p>

<p>1) Establish ML (machine learning). Use feature descriptors like SIFT, SURF to collect a set of feature descriptors from a training set and try detect the objects of interest. The main issue with this is the access to SIFT and SURF algorithms as they are not available in OpenCV 3. </p>

<p>2) Implement SLAM algorithm and map the environment and then use the information returned to identify the wall-corners. Of course this way I will be not able to detect fans and lights. </p>

<p>So the question is, is there are any other methods I could use other than ones listed above in order to achieve my goal. Am I missing something on image segmentation, clustering or image transforms (hough line/circle) which could be utilised in my situation? </p>

<p>Thanks</p>
","computer-vision machine-learning opencv"
"9318","Understanding Drift in Simultaneous Localization and Mapping (SLAM)","<p>I am trying to understand the effect of drift in Simultaneous Localization and Mapping (SLAM). My understanding is that drift occurs because the robot tracks its position relative to a set of landmarks it is storing, but each landmark has a small error in its location. Therefore, an accumulation of these small errors over a long trajectory causes a large error by the end of the trajectory.</p>

<p>However, what I am confused about is what would happen when the robot tracks its way back to its starting positions. Suppose the robot starts in position A, and then starts to move along a path, mapping the environment as it does so, until it reaches position B. Now, the robot will have some error in its stored position of B, due to the drift during tracking. But then suppose the robot makes its way back to A, by tracking relative to all the landmarks it created during the first path. When it reaches A, will it be back at the true position of A, i.e. where it started the first path? Or will it have drifted away from A?</p>

<p>My intuition is that it will end up at the true position of A, because even though the landmarks have errors in them, as long as the error is not too large then the robot will eventually get back to the position where it stored the landmarks for A. And once it is there, those landmarks are definitely correct, without error, because they were initialized before any drift errors had started to accumulate.</p>

<p>Any help? Thanks!</p>
","mobile-robot localization slam navigation mapping"
"9320","why is quadrotor motion planning hard?","<p>With introduction of incremental sampling algorithms, like PRM and RRT planning in higher dimensional spaces in reasonable computation time has become possible though it is PSPACE-hard. But why is a quadrotor motion planning problem still difficult even with simplified quadrotor model? </p>

<p>I was solving a dynamic car problem with OMPL, which produced solution within 10s but I set a planning time of 100s for quadrotor, but it still does not find a solution.</p>
","mobile-robot quadcopter motion-planning planning rrt"
"9321","Having a hard time understanding this equation in monocular EKF SLAM","<p>Reading this paper on <a href=""http://e-collection.library.ethz.ch/eserv/eth:48374/eth-48374-01.pdf"" rel=""nofollow"">visual odometry</a>, where they have used a bearing vector to parameterize the features. I am having a hard time understanding what the state propagation equation for the bearing vector term means :</p>

<p><a href=""http://i.stack.imgur.com/dK7d8.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/dK7d8.png"" alt=""Bearing vector and inverse depth parametrization""></a></p>

<p>The vector N is not mentioned in the equations, so its not very clear what it does. Would really appreciate if someone would help me understand it :)</p>
","localization slam navigation ekf mapping"
"9323","Lateral load on a servo motor","<p>Looking at pictures of existing designs for quadropod robots, the servos in the legs seem to usually be mounted inside the chassis, with a second attachment at back of the servo as well, such as this:</p>

<p><a href=""http://i.stack.imgur.com/x6UxD.jpg"" rel=""nofollow""><img src=""http://i.stack.imgur.com/x6UxD.jpg"" alt=""Servo attached front and back""></a></p>

<p>rather than putting what looks like an asymmetrical load, like the knees here:</p>

<p><a href=""http://i.stack.imgur.com/MgzPq.jpg"" rel=""nofollow""><img src=""http://i.stack.imgur.com/MgzPq.jpg"" alt=""Servo attached one side only""></a></p>

<p>Is this for aesthetics or are there real structural reasons to minimize the lateral load on the axle on a robot of this size?</p>
","joint"
"9324","Quadcopter PID Tuning for Altitude Hold/Position Hold along Z axis","<p>Good day, I have just finished tuning the Pitch and Roll PID's. I did this by setting the throttle such that the quad is weightless. I did the tuning of each axes separately.</p>

<p>I would just like to ask what is the best way to tune the PID for maintaining an altitude setpoint.</p>

<blockquote>
  <p>Is it best to turn off the Pitch and Roll PID controllers while tuning the altitude PID or is it best to have them already active while tuning the latter controller?</p>
</blockquote>

<p>I am going to use a cascaded PID controller using the Velocity along the z-axis calculated from the accelerometer output for the inner PID loop (150Hz) and the altitude measurement of the HC-SRO4 ultrasonic sensor (20Hz) for the outer PID loop.</p>
","quadcopter pid stability real-time sonar"
"9327","Vision-based Position Estimation for a quadrotor","<p>As a subtask inside a main project I need to compute the position (x,y,z) of a quadrotor using an homography. </p>

<p>To do this I will use a camera (attached to the quadrotor) pointing down to an artificial landmark on the floor. Basically I need to compute the extrinsic parameters of the camera to know the pose with respect to the landmark. I know the projective points of the landmark in the camera, and the intrinsic matrix of the camera but I also need the real landmark position [X, Y, Z].</p>

<p>I suppose that Z coordinate is equal to 0 because the landmark is plane, but I am not sure how to compute the real [X,Y] coordinates.</p>

<p>Any idea how to do that?</p>

<p>I am also interested in put the (x,y,z) position of the quadrotor into a control path, anybody knows where I can find info about the most common controllers for do this kind of task?</p>
","control computer-vision quadcopter uav visual-servoing"
"9334","Controlling Syma X5C from a laptop","<p>Out of curiosity, it is possible to control my X5C from a computer? I think that I can buy a transmitter to attach to my laptop. Do you think that the communications between my transmitter and the drone be over some proprietary protocol? Or would they adhere to some standard?</p>

<p>If you have any links/advice that could point me in the right direction, that would be appreciated.</p>
","quadcopter"
"9336","Jacobian for Inverse Kinematics with quaternion of end effector","<p>Quaternion has four parameters. Calculating Jacobian for inverse-kinematics, 3 positions and four quaternion parameters make Jacobian $7\times7$ instead of $6\times6$. How to reduce Jacobian to $6\times6$ when using quaternion?</p>
","robotic-arm inverse-kinematics jacobian"
"9337","Suitable D star variant is for non-holonomic motion planning of mobile robots","<p>I am working on a non-holonomic motion planning problem of a mobile robot in a completely unknown environment. After going through some research papers, I found that D-star algorithm is widely used in such conditions. But there are many D-star variants like Focused D*, D*-Lite, Field D* etc... So which of these variants is suitable in this case? Also please suggest any other better approach for this problem?</p>
","mobile-robot motion-planning algorithm"
"9341","Computer stereo vision simulator","<p>In my research project I deal with a mobile robot that perceives through stereo vision. As the stereo input data I currently use several datasets taken from a passenger vehicle that contain real world photos. The datasets are good to get started but have a limited content so I would need to model my own traffic situations to further work on the stereo vision system.</p>

<p>I am thinking about using some kind of synthetic graphics simulation as the input for the stereo system. What are my options? I can imagine a 3D graphics rendering engine whose output would be fed as the input for the stereo vision could probably be used.</p>

<p>I found there are general robotic simulators available like <a href=""http://gazebosim.org/"" rel=""nofollow"">Gazebo</a> but since I am all new to robotic simulation I do not really know where to begin.</p>

<p><strong>EDIT:</strong></p>

<p>I forgot to write that all my code is a pure C++. I use OpenCV and LIBELAS for stereo vision and Point Cloud Library (PCL) for visualization. All glued together into a single C++ project and compiles into single binary.</p>
","mobile-robot simulator stereo-vision"
"9342","ZigBee like network for FPV","<p>ZigBee is not designed for video transmission. I need a mesh network which contains multiple nodes like ZigBee for networking robots FPV. Is there any solution? </p>
","communication"
"9344","Which geo-projection to use for odometry","<p>I would like to make a little survey regarding the (geo)spatial projection that you use when elaborating your GPS and movement data for the spatial awareness of your robots. </p>

<p>Moving all GPS coordinates to a <strong>planar projection</strong> seems to be the more reasonable choice since not only distances (for which several <a href=""http://www.movable-type.co.uk/scripts/latlong.html"" rel=""nofollow"">formulas and approximations</a> exist), but bearings must be computed.</p>

<p>Generally, although scales are pretty small here, avoiding <em>equirectangular approximation</em> seems a good idea in order to keep a more consistent system.</p>

<p>Avoiding working in the 3D world (Haversine and other great-circle stuff) is probably a good idea too to keep computations low-cost.</p>

<p>Moving the world to a 2D projection is hence what seems to be the best solution, despite reprojection of all input GPS coordinates is needed. </p>

<p>I would like to get opinions and ideas on the subject 
(...if ever anyone is interested in doing it U_U).</p>
","odometry geometry"
"9346","Pick and place robot","<p>I have to simulate a pick and place robot (3 DOF). I tried with MATLAB. It should pick and place different objects according to their geometry. </p>

<p>Where can I find similar m-codes and algorithms?</p>
","robotic-arm matlab"
"9349","Applying MoCap data to real life robot","<p>I have a Kinect Sensor, and <a href=""http://ipisoft.com/"" rel=""nofollow"">iPi software</a> I use to create motion capture data to use in film editing. I am looking at creating a small, Raspberry Pi driven bipedal robot just for fun, and I was wondering if it was possible to use the MoCap to control the robot? It will only be 20-30 cm tall, with six servos (hips, knees, ankles). Is it possible to apply the movement from these six joints on the human body to my robot, like having a string directly from my left knee joint to its left knee servo? It could either be in real-time, like following my actions, or using pre-recorded data.</p>

<p>(NOTE: If needed, I can plug it directly to my Apple/Windows computer, if the Pi could not support this. Also, it will have no upper torso <em>at the moment</em>.)</p>
","mobile-robot motion-planning servos"
"9351","Which kind of valve is used for dispensing food grains?","<p>I am doing a project on an automated grain dispenser system using a PLC control. I need a valve for dispensing grain from hopper to packet. I should be able to control the flow of the grain. </p>

<p>So what kind of valve should I use for flow control of the grain? There are different types of grains like rice, wheat, etc., and the valve should be controlled by the PLC (opening and closing of valve).</p>
","automation"
"9353","Why does the ESC stop?","<p>I've built a quadcopter with four brushless motors and ESCs (30A). I'm using an Arduino to control them. I haven't written any complex code; just enough to get them running. Everything is fine until I send a number over 920 to the serial. Then, for some reason, all the motors stop spinning. I'm using three freshly bought and charged LiPo cells (V = 11.1V). Here is the link for the site that I bought them from (I cannot seem to find any other resource about them) : <a href=""http://www.ebay.co.uk/itm/4x-A2212-1000KV-Outrunner-Motor-4x-HP-30A-ESC-4x-1045-prop-B-Quad-Rotor-/111282436897"" rel=""nofollow"">4x A2212 1000KV Outrunner Motor + 4x HP 30A ESC + 4x 1045 prop (B) Quad-Rotor</a>. </p>

<p>When I tried turning on only one motor, I could write up to about 1800 microseconds, while both with 4 and with 1 motor, the minimum that it works is 800. </p>

<p>Can somebody explain why this happens and I how I can fix it? </p>

<p>Here is my code: </p>

<pre><code>#include &lt;Servo.h&gt;

int value = 0;

Servo first,second,third,fourth;

void setup() {

  Serial.begin(9600);    // start serial at 9600 baud
  first.attach(6);
  second.attach(9);
  third.attach(10);
  fourth.attach(11);

}

void loop() {

    first.writeMicroseconds(value);
    second.writeMicroseconds(value);
    third.writeMicroseconds(value);
    fourth.writeMicroseconds(value);

    if(Serial.available() &gt; 0){
      value = Serial.parseInt();
    }

}
</code></pre>
","arduino quadcopter brushless-motor esc"
"9355","Dynamic model of a robot lifted by a balloon (Multibody system)","<p>I'm having a hard time trying to understand how to obtain the dynamic model for a system similar to the image.  </p>

<p>The balloon is a simple helium balloon, however the box is actually an aerial differential drive platform (using rotors). Now there's basically a model for the balloon and another for the actuated box. However I'm lost to how to combine both. </p>

<p>The connection between both is not rigid since it is a string.
How should I do it? Is there any documentation you could point me to, in order to help me develop the dynamics model for this system? </p>

<p>Since I'm so lost, any help will be useful. Thanks in advance!  </p>

<p><a href=""http://i.stack.imgur.com/A3ANA.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/A3ANA.png"" alt=""System: Balloon suporting a box""></a></p>
","mobile-robot kinematics dynamics"
"9365","Difference between an underactuated system, and a nonholonomic system","<p>What's the difference between an underactuated system, and a nonholonomic system? I have read that ""the car is a good example of a nonholonomic vehicle: it has only two controls, but its configuration space has dimension 3."". But I thought that an underactuated system was one where the number of actuators is less than the number of degrees of freedom. So are they the same?</p>
","kinematics"
"9367","Relationship between motor torque and acceleration","<p>I am working on designing and building a small (1 1/2 lbs), 2-wheeled, differential drive Arduino-controlled autonomous robot. I have most of the electronics figured out, but I am having trouble understanding how much torque the motors will actually need to move the robot. I am trying to use the calculations shown <a href=""http://www.robotshop.com/blog/en/drive-motor-sizing-tutorial-3661"" rel=""nofollow"">here</a> and the <a href=""http://www.robotshop.com/blog/en/drive-motor-sizing-tool-9698"" rel=""nofollow"">related calculator tool</a> to determine what speed and torque I will need. I will be using wheels 32mm in diameter and one of Pololu's <a href=""https://www.pololu.com/category/60/micro-metal-gearmotors"" rel=""nofollow"">High-Power Micro Metal Gearmotors</a>. I performed the calculations for a robot weight of 2 lbs to be safe and found that the 50:1 HP Micro Metal Gearmotors (625 RPM, 15 oz-in) should theoretically work fine, moving the robot at 3.43 ft/s with an acceleration of around 29 ft/s^2 up a 5-degree incline. </p>

<p>However, I have not found an explanation for several things that I think would be very important to know when choosing drive motors. When the robot is not moving and the motors are turned on at full power, they should need to deliver their full stall torque. Based on the calculations, it seems that any amount of torque can get the robot moving, but the more torque, the faster the robot's acceleration. Is this true? Also, if the power source cannot supply the full stall current of the motors, will the robot not be able to start moving? In my case, I am powering the robot through a 7.2V (6S) 2200mAh NiMH battery pack that can provide around 2.6A continuously, and when it does that the voltage drops to less than 1V. Will this be able to power my motors? Once the robot reaches full speed and is no longer accelerating, theoretically the motors will not be providing any torque, but I do not think this is the case. Is it, and if so, how will I know how much torque they will be providing? Will the motors I chose have enough torque to move my robot?</p>
","mobile-robot motor"
"9370","Thermal Imaging camera activation upon detection","<p>So I am planning on building a robot that turns on when it detects some kind of heat source, I am currently looking at thermal imaging cameras, but am not sure as to how to go about writing code to send a ping or some sort of message when the camera detects a heat source.</p>

<p>Does anyone know of any way to do this?</p>

<p>Thanks </p>
","wheeled-robot"
"9371","Reliably establishing communication and OI mode with Create 2","<p>I've started tinkering with a Create 2, but I'm having issues reliably getting it to accept my commands.  I can occasionally get it right, but sometimes, it just seems to ignore me.  I'm guessing my cleanup code isn't getting the state fully reset or something.  Is there a good pattern to follow for fail-safe initialization code?</p>

<p>Here's what I'm doing right now:</p>

<ol>
<li>Pulse BRC low for 1 second</li>
<li>Wait 1 second</li>
<li>Send 16x 0 bytes (to make sure if it's waiting for the rest of a command, this completes it - seemed to help a bit when I added this)</li>
<li>Send 7 (reset)</li>
<li>Wait 10 seconds</li>
<li>Send 128 (start)</li>
<li>Wait 2 seconds</li>
<li>Send 149 35 (ask for the current OI state)</li>
<li>Wait 1 second</li>
<li>Send 131 (safe mode)</li>
</ol>

<p>Sometimes I'm then able to issue 137 (drive) commands and have it work.  Most times it doesn't.  The times when it doesn't, I'm seeing a lot of data coming from the Create 2 that I'm not expecting, that looks something like this (hex bytes):</p>

<pre><code>00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 3f 2a ff 73 21 09 cc 0a 88 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 3f 2a ff 73 21 09 cc 0a
</code></pre>

<p>There's more, but my logging cut it off.  I get the same pattern a couple of times, and it seems to be at least partially repeating.  I thought maybe it's the 16 0-bytes I sent followed by <code>003f 2aff 7321 09cc 0a88</code>, but I still don't know how to interpret that.</p>

<p>Sometimes it will make some noise for the reset command, but usually ignores the start/safe mode commands completely (I can tell because the green light stays on).</p>
","irobot-create"
"9372","Real-time object classification for an indoor autonomous quad-rotor","<p>I am designing an indoor autonomous drone. I am currently writing an object classification program in OpenCV for this purpose. My objects of interests for classification are: ceiling fans; AC units; wall and ceiling lamps, and; wall corners. I am using BoW clustering algorithm along with SVM classifier to achieve this (I'm still in the process of developing the code, and I might try other algorithms when testing).</p>

<p>The primary task of the drone is to successfully scan (what I mean by scanning is moving or hovering over the entire ceiling space) a ceiling space of a given closed region while successfully avoiding any obstacles (like ceiling fans, AC units, ceiling and wall lamps). The drone's navigation, or the scanning process over the ceiling space, should be in an organised pattern, preferably moving in tight zig-zag paths over the entire ceiling space.</p>

<p>Having said that, in order to achieve this goal, I'm trying to implement the following to achieve this:</p>

<ol>
<li><p>On take off, fly around the given closed ceiling space and <strong>use SLAM to localise and map its environment</strong>.</p></li>
<li><p>While running SLAM, <strong><em>run the object classier algorithm to classify the objects of interests and track them in real time</em></strong>.</p></li>
<li><p>Once obtained a detail map of the environment and classified all objects of interest in the local environment, <strong><em>integrate both data together to form an unified map. Meaning on the SLAM output, label the classified objects obtained from the classifier algorithm. Now we a have fun comprehensive map of the environment with labeled objects of interest and real-time tracking of them (localization).</em></strong></p></li>
<li><p>Now pick random corner on the map and plan a navigation pattern in order to scan the entire ceiling space. </p></li>
</ol>

<p>So the question now here is, <strong><em>is using object classification in real-time will yield successful results in multiple environments (the quad should be able to achieve the above mentioned tasks in any given environment)?</em></strong>. I'm using a lot of train image sets to train my classifier and BoW dictionary but I still feel this won't be a robust method <strong><em>since in real-time it will be harder to isolate an object of interest</em></strong>. Or, in order to overcome this, should I use real-situation like train images (currently my train images only contain isolated objects of interests)?</p>

<p><strong><em>Or in my is using computer vision is redundant? Is my goal completely achievable only using SLAM?</em></strong> If, yes, how can I classify the objects of interest (I don't want my drone to fly into a running ceiling fan mistaking it for a wall corner or edge). <strong><em>Furthermore, is there any kind of other methods or sensors, of any type, to detect objects in motion?</em></strong> (using optical flow computer vision method here is useless because it's not robust enough in real-time).</p>

<p>Any help and advice is much appreciated.</p>
","mobile-robot quadcopter slam opencv"
"9373","Drawbacks of goal bias in RRT","<p>I am working on RRT planner to remove the goal_bias in it and introduce a NEW_METHOD to direct the planner towards the goal. I find this method good for lower dimension. When the dimension increases, especially for a dynamic car, though the NEW_METHOD works better, I observe that the NEW_METHOD along with goal_bias works even better. </p>

<p>So, what are the possible advantages of removing goal_bias will I be loosing by adding it along with the NEW_METHOD? </p>
","mobile-robot motion-planning algorithm rrt"
"9374","Measure weight of an object using a servo","<p>Assuming a quality industrial servo, would it possible to calculate the weight/resistance of a load? Maybe by comparing current draw in a holding position, or by the time it takes to lift/lower an object. Could it accurately measure grams, kilograms? What kind of tolerance could be achieved?</p>

<p>I'm trying to eliminate the need for a dedicated weight measurement sensor.</p>
","sensors robotic-arm servomotor"
"9377","Real Time Simulation model of Sensors with MATLAB","<p>I am trying to make a real time simulink model of Leddar Sensor (Ledddar sensor evaluation kit) in MATLAB using USB port. But i can't use 'data acquisition toolbox' and 'Instrument Control Toolbox' because the leddar vendor isn't listed in these toolboxes. I would really appreciate if someone could help me with this because i am new to Real time simulink.  </p>
","sensors matlab simulator real-time"
"9380","SLAM Map Quardrant","<p>Transformation of frame from global to local is crucial for measurement update but how do we keep track of the direction ? eg: robot location [10,2] and [-10,2] requires different sign. Is there a way to not have to set a if else case and have a general expression ?</p>
","slam"
"9381","PID tuning for Quadcopter","<p>I have been stabilizing my quadcopter. I tuned my angle PIDies and my quadcopter tries to stabilize itself, but there is some overshooting. Which is, I think, due to gyro rates. I have read that we have to use two PIDies on an axis. I'm having problems to attach these two PIDies. </p>

<p>Can anyone help me in cascading angle PID and rate PID? Will I have to tune rate PID after tuning angle PID?</p>
","quadcopter pid"
"9382","Any books or web resources for robotics mechanical design?","<p>I plan to build a mechanism with multiple axis, which is similar to a robot. To start, I need to define some specifications such as repeatable precision, speed, acceleration, and payload. Then the motor and structure is selected and designed based on these parameters. After that, I need to choose methods to manufacture these components. I would like to consult experienced experts in this forum that is there any suggested books, textbooks, or website resources I can learn these knowledge? </p>
","mechanism manufacturing books"
"9383","How to make a robot arm follow a shape/path","<p>First of all hope this is not a stupid question but I couldn't find any ware a solution.</p>

<p>I have constructed a 3 DOF robot arm. I want it to follow a trajectory on a 2D plane (XY). Tha shapes I want to follow are lines, cycles and splines. I now the math behind these 3 shaped (how they are defined). I have the kinematics, the inverse kinematics the jacobian and the whole control system (with the PID controller). The system receives as inputs, Xd(position), Xd'(velocity) and Xd''(acceleration) over time.</p>

<p>I found the following image that shows (more or less) my system.
<a href=""http://i.stack.imgur.com/VE8CI.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/VE8CI.png"" alt=""enter image description here""></a></p>

<p>So here is were I am stuck. How do I translate the shape to the position, velocity and acceleration that each joint needs to make so the end effector moves in the Cartesian space according to that shape?</p>
","pid robotic-arm line-following"
"9384","PID tuning with methods like GA and PSO","<p>I have recently started reading about PID tuning methods and algorithms, and I encountered the particle swarm optimization algorithm and genetic algorithm.</p>

<p>The problem is, that I don't understand how each particle/chromosome determines his fitness. On real physical system, each particle/chromosome checks his fitness on the system? Wouldn't it take a really long time? I think that I am missing something here... Can those algorithms be implemented on an actual physical system? If so, then how?</p>
","pid algorithm"
"9386","Angle to a circle tangent line","<p>I want to simulate the detection of a moving object by a unicycle type robot. The robot is modelled with position (x,y) and direction theta as the three states. The obstacle is represented as a circle of radius r1 (<code>r_1</code> in my code). I want to find the angles <code>alpha_1</code> and <code>alpha_2</code>from the robot's local coordinate frame to the circle, as shown here:</p>

<p><a href=""http://i.stack.imgur.com/HdqrO.jpg"" rel=""nofollow""><img src=""http://i.stack.imgur.com/HdqrO.jpg"" alt=""Detection of a moving object""></a></p>

<p>So what I am doing is trying to find the angle from the robot to the line joining the robot and the circle's centre (this angle is called <code>aux_t</code> in my code), then find the angle between the tangent and the same line (called <code>phi_c</code>). Finally I would find the angles I want by adding and subtracting <code>phi_c</code> from <code>aux_t</code>. The diagram I am thinking of is shown:</p>

<p><a href=""http://i.stack.imgur.com/xfqZj.jpg"" rel=""nofollow""><img src=""http://i.stack.imgur.com/xfqZj.jpg"" alt=""Diagram #2""></a></p>

<p>The problem is that I am getting trouble with my code when I try to find the alpha angles: It starts calculating the angles correctly (though in negative values, not sure if this is causing my trouble) but as both the car and the circle get closer, <code>phi_c</code> becomes larger than <code>aux_t</code> and one of the alphas suddenly change its sign. For example I am getting this:</p>

<p>$$\begin{array}{c c c c} 
\text{aux_t} &amp; \text{phi_c} &amp; \text{alpha_1} &amp; \text{alpha_2} \\ \hline
\text{-0.81} &amp; \text{+0.52} &amp; \text{-1.33} &amp; \text{-0.29} \\
\text{-0.74} &amp; \text{+0.61} &amp; \text{-1.35} &amp; \text{-0.12} \\
\text{-0.69} &amp; \text{+0.67} &amp; \text{-1.37} &amp; \text{-0.02} \\
\text{-0.64} &amp; \text{+0.74} &amp; \text{-1.38} &amp; \text{+0.1} \\
\end{array}$$</p>

<p>So basically, the <code>alpha_2</code> gets wrong form here. I know I am doing something wrong but I'm not sure what, I don't know how to limit the angles from 0 to pi. Is there a better way to find the alpha angles?</p>
","mobile-robot kinematics matlab geometry"
"9388","Building a robotic clamp","<p>If I had a single stepper motor how could I use it to create a robotic clamp that could simply grab hold of something like a plank of wood and release it?</p>

<p>Are there any standard parts that I could use for this? I'm having trouble finding out what the names of the parts would be.</p>
","robotic-arm"
"9391","Slam Odometer Requirement","<p>How accurate must my odometer reading be for SLAM ?</p>

<p>I am writing this extra section because it says my question body does not meet the quality standard. </p>
","slam"
"9395","KUKA robot - update coordinates","<p>I need to develop something in order to update some coordinates in a KUKA KR C4 robot predefined program.</p>

<p>After some research I've found some ways to do it, but all of them non free.</p>

<p>I had several options, like developing a HMI in the console with 3 buttons, to touch up the 3 coordinates that I have to update for example.</p>

<p>Sending a XML file would work too but I need a RSI connection, and I can't do it without proper software (I guess).</p>

<p>Do you know about something like this? Or a C++ library that allows me to have access the <code>.src</code>/<code>.dat</code> files or to create a new one with the same ""body"" but with different coordinates?</p>

<p>Summing up, imagine that I have a conveyor that carries boxes and I need to develop a pick and place program. So far so good. But every 100 boxes, the size changes (and I can't predict it). So the operator goes and updates the coordinates, but I want to make sure that he won't change anything else in the program. Any ideas?</p>
","robotic-arm industrial-robot kuka"
"9397","If I must fly my drone in bad weather, how can I maintain control of it in strong winds?","<p>If I need to fly a drone in strong winds, how can I stabilize it? Should I use accelerometers and gyroscopes to keep it steady? Or should I just use some flight technique under such circumstances?</p>
","quadcopter navigation accelerometer"
"9400","Choosing motor type for high reliability for many cycles","<p>I am designing a multi modal stent testing machine which will bend, twist, and compress stents (very thin, light, and fragile cylindrical meshes for in arteries) in a tube. The machine will operate at maximum 3.6 Hz for months at a time (> 40 million cycles). As the machine will be in a lab with people, the noise should be minimal. I am choosing actuators for my design but was overwhelmed by the range of products available.</p>

<p>For rotating the stents around their axis, I will need a rotary actuator with the following specs:</p>

<ul>
<li>torque: negligible max angle: 20 deg</li>
<li>angular velocity needed: max 70 deg/s</li>
<li>hollow shafts are a plus</li>
</ul>

<p>For compressing the stents, I will need a linear actuator with the following specs:</p>

<ul>
<li>force: low (&lt;1N)</li>
<li>max stroke: 20mm but if possible 70mm for allowing different stent lengths </li>
<li>stroke velocity needed: max 120mm/s</li>
</ul>

<p>Price of these motors is not the driving factor.</p>

<p>I looked into stepper motors, servo motors, and piezoelectric motors. There seems to be s huge selection that fits my requirements. If all motor types have a reliability that suits my needs, which characteristics/advantages/disadvantages should I consider that determine the selection of suitable actuators? I do know what the difference is between the motor types, but there is a lot of overlap. Concrete suggestions are welcome.</p>
","actuator reliability"
"9403","SLAM starting point / quardrant issue","<p>I have a question that is puzzling me. I have a simple rectangle enclosure and i have extracted the Lidar and odometer data from a test run. If i put my starting position at [0,0,90] it gives bad map. However if i shift it away from [0,0] to something like [50,50,90] the map seems fine. How can this be ? </p>
","slam"
"9405","VEX Cortex Motor Speeds up under load","<p>I am trying to get my robot to drive straight and am having trouble.  I find that when running the motors with no load they run fine.  If I put a load on one motor it accelerates.  The other performs as expected, it tries to maintain speed.  I am running 393 motors with encoders and PID selected.  I am running robot C.</p>

<p>See the following video: <a href=""https://youtu.be/u3P0Wectwco"" rel=""nofollow"">https://youtu.be/u3P0Wectwco</a></p>

<p>program is as follows;</p>

<pre><code>#pragma config(I2C_Usage, I2C1, i2cSensors)
#pragma config(Sensor, dgtl12, killB,          sensorTouch)
#pragma config(Sensor, I2C_1,  ,               sensorQuadEncoderOnI2CPort,    , AutoAssign )
#pragma config(Sensor, I2C_2,  ,               sensorQuadEncoderOnI2CPort,    , AutoAssign )
#pragma config(Sensor, I2C_3,  ,               sensorQuadEncoderOnI2CPort,    , AutoAssign )
#pragma config(Motor,  port2,           rmotor,        tmotorVex393_MC29, PIDControl, reversed, driveRight, encoderPort, I2C_1)
#pragma config(Motor,  port3,           lmotor,        tmotorVex393_MC29, PIDControl, driveLeft, encoderPort, I2C_2)
#pragma config(Motor,  port4,           topmotor,      tmotorVex393_MC29, openLoop, encoderPort, I2C_3)
#pragma config(Motor,  port5,           pmotor,        tmotorVex393_MC29, openLoop)
//*!!Code automatically generated by 'ROBOTC' configuration wizard               !!*//

void StopAll(){
    motor[rmotor] = 0;
    motor[lmotor] = 0;
    motor[topmotor] = 0;
    motor[pmotor] = 0;
}

//Stops the program at the push of a button
task eStop(){
    while (SensorValue(killB) == 0){
        wait1Msec(10);
    }
    StopAll();
    stopAllTasks();
}


task main()
{
    startTask(eStop);

    nMotorEncoder[rmotor] = 0;
    nMotorEncoder[lmotor] = 0;

    motor[rmotor] = 15;
    motor[lmotor] = 15;
    wait1Msec(20000);
    motor[rmotor] = 0;
    motor[lmotor] = 0;
    StopAll();


}
</code></pre>

<p>Thank you,</p>

<p>Mark</p>
","pid robotc vex"
"9409","Why would a drone need a magnetometer? Are an accelerometer and a gyroscope not sufficient?","<p>Why would a drone need a magnetometer? What would the drone do with this information? I think it would be to tell direction, but why would it need this if it has an accelerometer and a gyroscope?</p>
","mobile-robot magnetometer"
"9413","RRT algorithm in C++","<p>I want to implement <a href=""https://en.wikipedia.org/wiki/Rapidly_exploring_random_tree"" rel=""nofollow"">RRT</a> for motion planning of a robotic arm. I searched a lot on the internet to get some sample code of RRT for motion planning, but I didn't get any. Can someone please suggest a good source where I can find RRT implemented in C++ for any type of motion planning.</p>
","robotic-arm motion-planning algorithm"
"9417","How to decide the battery power for my robot","<p>I need my motor to be powered with 12V, 5A for 1 hour continuously. How can i decide the Ah rate of the battery. Please suggest some lithium ion battery for the specification</p>
","mobile-robot motor power battery lithium-polymer"
"9420","Why do I need I-gain in my outer-loop?","<p>I'm implementing a set of loops to control pitch-and-roll angular positions.</p>

<p>In an inner-loop, motor speeds are adjusted to achieve desired angular rates of rotation (the ""inner-loop setpoints"").</p>

<p>An outer-loop decides these desired angular rates (the ""inner-loop setpoints"") based on the aircraft's angular positions.</p>

<hr>

<h3>Outer-loop</h3>

<ul>
<li>Frequency = ~400Hz</li>
<li>Outer PV = input angular position (in degrees)</li>
<li>Outer SP = desired angular position - input angular position (in degrees)</li>
</ul>

<hr>

<h3>Inner-loop</h3>

<ul>
<li>Frequency = ~760Hz</li>
<li>Inner PV = input angular rotation (in degrees-per-second)</li>
<li>Inner SP = constant1 * Outer MV (in degrees-per-second)</li>
<li>PWM = Inner MV / constant2 (as percentile)</li>
</ul>

<hr>

<p>I understand what I-gain does and why this is important, but I'm not able to see any practical reason for also having I-gain specified in the outer-loop. Surely the inner-loop would compensate for any accumulated error, leaving no error to compensate for in the outer-loop, or is my thinking flawed?</p>

<p>Any example gain values to elaborate would be greatly appreciated.</p>
","quadcopter pid"
"9429","Drone Flight Formation","<p>How does one today program a fleet of drones autonomously fly together in a formation with visual feedback from onboard camera?</p>
","quadcopter uav"
"9434","Pose-Graph-SLAM: How to create edges if only IMU-odometry is given?","<p>I want to estimate the poses of a vehicle at certain key frames. The only sensor information I can use is from an IMU which yields translational acceleration and orientation measurments. I obtain a 7D pose, i.e. 3D position vector + unit quaternion orientation, if I integrate the translational acceleration twice and propagate the orientation measurements.</p>

<p>If I want to add a new edge to the graph I need a constraint for that edge. In general, for pose graphs this constraint represents a relational transformation $z_{ij}$ between the vertex positions $x_i$ and $x_j$ that are connected by the edge. </p>

<p>Comparing my case to the literature the following questions arised:</p>

<ol>
<li><p>How do I calculate a prediction $\hat{z}_{ij}$ which I can compare to a measurement $z_{ij}$ when computing the edge error? Initially, I understood that graph slam models the vertex poses as gaussian distributed variables and thus a prediction is simply calculated by $\hat{z}_{ij}=x_i^{-1} x_j$. </p></li>
<li><p>How do I calculate the information (preferred) or covariance matrix? </p></li>
<li><p>How and when do I update the information matrices? During optimization? Or only at edge creation? At loop closure?</p></li>
<li><p>I read about the chi-square distribution and how it relates to the Mahalanobis distance. But how is it involved in the above steps? </p></li>
<li><p>Studying current implementations (e.g. mrpt-graph-slam or g2o) I didn't really discover how predictions (or any probability density function) is involved. In contrast, I was even more confused when reading the mrpt-graph-slam example where one can choose between raw poses and poses which are treated as means of a probability distribution.</p></li>
</ol>
","slam imu data-association"
"9435","Object following robot","<p>Can you please help me out? I need my car prototype to follow the object ahead of it. I'm using two ultrasonic sensors HC-SR04 to sense the distance of the obstacle. I want these sensors to determine the direction of the servo motor MG995 using Python. How do I calibrate this servo to get an appropriate output? </p>

<p>If anybody could help me out with a code in Python on it, I'd be grateful.</p>
","software"
"9436","What is the common process to place a robotic arm gripper","<p>I implemented a simulation for a robotic arm that has to grab things. This arm has a 6DOF structure and a simple gripper on the top. I made a simple CCD IK algorithm to control the arm. I can use it in two ways:</p>

<ol>
<li>Compute the position of the last joint of the arm before the hand
part (which means 1 end-effector). Then use an analytical method to
place the hand in a good orientation.</li>
<li>Compute directly the arm, and the hand position by giving the CCD IK algorithm 2 end-effectors that are the 2 finger of the hand.</li>
</ol>

<p><strong>What is the most used method for a grabbing arm robot ? I'm not willing to find a solution, just to know what people usually do</strong>. </p>
","robotic-arm inverse-kinematics"
"9437","Can we simulate a actuator with a very strong torque with a PID controller","<p>I use gazebo to simulate a robot arm. To control its joints, I use PID controllers. As you might know, PID are sometimes pretty hard to tune and this is the case for a robotic arm. To avoid any tuning, and because I don't need the PID values to be realistic, I set to zero the derivative and integral parameters, increase a lot the proportional gain and add a lot of damping in my joints. By doing this, I can get a well working arm but only if I disable the gravity. </p>

<p>My question is the following. Do you have an idea how I could simulate a very strong actuator with not necessarily realistic parameters?</p>

<p>EDIT 1: Setting the integral and derivative gain is stupid. The integral gain helps in correcting the effect of the gravity. The derivative gain counters the loss of stability and speed due to the integral gain. </p>

<p>This question somehow leads to another. Do you know what tuning do the robotic arm manufacturer (big arms for car industry for example). I guess that this arm use actuators with a very strong torque and a low maximum speed which reduces the need of tuning.  </p>

<p>EDIT 2: More info on my setup. I use gazebo 6, with ODE. The robot description is in SDF. I control the robot with a model plugin. As a PID controler I use the PID class from the common library of gazebo and get directly the JointControler associated to the model. </p>

<p>Let say that I would like actuators very robust without any tuning needed. This way I could have a simulation WITH dynamics (by opposition to the SetPosition method). Do you think it is possible ?    </p>
","pid power servomotor joint gazebo"
"9441","create2 commands with an app called SERIAL","<p>there is an app called <a href=""https://www.decisivetactics.com/products/serial/"" rel=""nofollow"">SERIAL,</a> available in the app store. 
I've downloaded it on my mac and am experimenting with it, any ideas on how to send create2 OI commands using ""Serial""?</p>

<p>so far it seems a handy app, I've bypassed all the need for other drivers. anyone else use SERIAL/something of the like? </p>

<p>*when the SERIAL terminal is open and the number 9 is pressed on my mac it seems to activate cleaning mode. thats all the communication I'm getting after hours of playing around in python and mac terminal.  <a href=""http://i.stack.imgur.com/JpjBA.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/JpjBA.png"" alt=""enter image description here""></a></p>
","mobile-robot irobot-create serial"
"9443","OpenRAVE ChechCollison command in C++","<p>What is the equivalent code of ""env.CheckCollision(robot)"" in C++? Even though it is said that conversion of commands from python to c++ is easy and intuitive, where can I find a proper documentation for this conversion?</p>
","motion-planning algorithm"
"9445","Name of large robotic arms (two finger) with wrist, arm, hands and spinning shoulder axis","<p>I've been looking for large robotic arms (with two fingers) and the arm so they are able to pick up and drop things in a space around the arm (and even spin around the 'wrist').</p>

<p>I'm not sure what the terminology is for such an arm. I've seen this, <a href=""http://www.robotshop.com/en/owi-535-robotic-arm-edge.html"" rel=""nofollow"">OWI-535 Robotic Arm Edge</a>, and it looks close. Is there something larger that can be hooked up to a Raspberry Pi instead of the remote controller?</p>

<p>Is there a particular term for this in a generic context? Or is there a way to build such an arm using off the shelf parts?</p>
","robotic-arm raspberry-pi"
"9447","Programming A Rover","<p>I am part of my College team which is planning to enter a Mars Rover Challenge. In the point of view of a programmer, where should I start? I know C is the main language NASA used for their Rover and I have a basic understanding of it. Plus, how much should I look into the RTOS part for making a rover?</p>

<p>Any books/links to this topic would be greatly appreciated. </p>
","programming-languages c"
"9454","What is the correct name for ""servo brackets""?","<p>I refer to these types of brackets as <em>servo brackets</em>, or <em>robot brackets</em>:</p>

<p><a href=""http://i.stack.imgur.com/mbjUy.jpg"" rel=""nofollow""><img src=""http://i.stack.imgur.com/mbjUy.jpg"" alt=""Short U and Multi Function servo brackets""></a></p>

<p>I know that the two specific brackets, shown above, are known as a short-U (some vendors refer to them as ""C"", en lieu of ""U"") and a multi-function bracket, respectively, and that there are other types available, namely:</p>

<ul>
<li>Long U bracket</li>
<li>Oblique U bracket</li>
<li>i bracket</li>
<li>L bracket</li>
<li>etc.</li>
</ul>

<p>However, I am sure that there is a correct name for these <em>types</em> of bracket (or this range of bracket, if you will), rather than just <em>servo brackets</em> - either a generic name or a brand name. I have seen the term once before, on a random web page, but the name escapes me. They are either named after their creator, or, if I recall correctly, the institution where they were developed.</p>

<p>Does anyone have a definitive answer, preferably with a citation or web reference, or a little historical background?</p>
","mechanism"
"9456","How can we use the accelerometer for altitude estimation?","<p>I am currently implementing an autonomous quadcopter which I recently got flying and which was stable, but is unable to correct itself in the presence of significant external disturbances. I assume this is because of insufficiently tuned PID gains which have to be further tweaked inflight.</p>

<h3>Current progress:</h3>

<ul>
<li>I ruled out a barometer since the scope of my research is only indoor flight and the barometer has a deviation of +-5 meters according to my colleague.</li>
<li>I am currently using an ultrasonic sensor (HC-SR04) for the altitude estimation which has a resolution of 0.3cm.  However I found that the ultrasonic sensor's refresh rate of 20Hz is too slow to get a fast enough response for altitude correction.</li>
<li>I tried to use the accelerations on the Z axis from the accelerometer to get height data by integrating the acceleration to get velocity to be used for the rate PID in a cascaded pid controller scheme. The current implementation for the altitude PID controller is a single loop pid controller using a P controller with the position input from the ultrasonic sensor.</li>
<li>I had taken into account the negative acceleration measurements due to gravity but no matter how much I compute the offset, there is still the existence of a negative acceleration (eg. -0.0034). I computed the gravitational offset by setting the quadcopter to be still on a flat surface then collecting 20,000 samples from the accelerometer z axis to be averaged to get the ""offset"" which is stored as a constant variable. This variable is then subtracted from the accelerometer z-axis output to remove the offset and get it to ""zero"" if it is not accelerating. As said in the question, there is still the existence of a negative acceleration (eg. -0.0034). My quad then proceeds to just constantly climb in altitude. With only the ultrasonic sensor P controller, my quad oscillates by 50 cm.</li>
</ul>

<blockquote>
  <p>How can this consistent negative acceleration reading be effectively dealt with?</p>
</blockquote>

<p><strong>Possible Solution</strong>:
I am planning to do a cascading PID contoller for the altitude hold with the innerloop (PID controller) using the accelerometer and the outer loop (P controller) using the sonar sensor. My adviser said that even a single loop P controller is enough to make the quadcopter hold its altitude even with a slow sensor. Is this enough? I noticed that with only the P gain, the quadcopter would overshoot its altitude.</p>

<p><a href=""http://i.stack.imgur.com/rdARR.jpg"" rel=""nofollow""><img src=""http://i.stack.imgur.com/rdARR.jpg"" alt=""enter image description here""></a></p>

<ul>
<li><p>Leaky Integrator: I found this article explaining how he dealt with the negative accelerations using a leaky integrator however I have a bit of trouble understanding why would it work since I think the negative error would just turn to a positive error not solving the problem. I'm not quite sure. <a href=""http://diydrones.com/forum/topics/multi-rotors-the-altitude-yoyo-effect-and-how-to-deal-with-it"" rel=""nofollow"">http://diydrones.com/forum/topics/multi-rotors-the-altitude-yoyo-effect-and-how-to-deal-with-it</a></p></li>
<li><p>Single loop PD controller with the ultrasonic sensor only:
Is this feasible using feedback from a slow sensor?</p></li>
</ul>

<p><strong>Sources:</strong></p>

<ul>
<li><p>LSM303DLHC Datasheet: <a href=""http://www.st.com/web/en/resource/technical/document/datasheet/DM00027543.pdf"" rel=""nofollow"">http://www.st.com/web/en/resource/technical/document/datasheet/DM00027543.pdf</a></p></li>
<li><p>Leaky integrator: <a href=""http://diydrones.com/forum/topics/multi-rotors-the-altitude-yoyo-effect-and-how-to-deal-with-it"" rel=""nofollow"">http://diydrones.com/forum/topics/multi-rotors-the-altitude-yoyo-effect-and-how-to-deal-with-it</a></p></li>
<li><p>ArduPilot PID Loop: <a href=""http://copter.ardupilot.com/wp-content/uploads/sites/2/2012/12/Alt-Hold-PID-version-3.0.1.jpg"" rel=""nofollow"">http://copter.ardupilot.com/wp-content/uploads/sites/2/2012/12/Alt-Hold-PID-version-3.0.1.jpg</a></p></li>
</ul>
","quadcopter control pid raspberry-pi sensor-fusion"
"9459","Building a stationary robot which can talk","<p>I am a Computer Science major and I only have basic ideas on Robotics. I am planning to build a stationary cubical AI. </p>

<p>The main purpose of this bot will be that, it will have a sensor to check if the door has been opened and immediately asks a question ""who has opened the door?"" I also want it to recognize the correct words to interact the word, I am not talking about voice recognition but word recognition so that who ever speaks the correct words(words in bot's memory) can interact with it. Depending on who opens the door(prolly my family) I want it to speak out different things. I want it to respond to simple questions like, ""what is the date and time?"" , "" a random qoute or a fact or a joke"". </p>

<p>Is this too hard to achieve? Could anyone give me a basic idea on how to approach this project? </p>
","sensors communication first-robotics speech-processing"
"9463","Quadrocopter PID","<p>I am building a quadcopter for my school project. I am trying to program my own flight controller using PID algorithm.</p>

<p>I'll try to make my question simple using as an example below only two motors</p>

<pre><code>                       1-----------2
</code></pre>

<p>Let's say I am trying to stabilize my two motor system using gyro from the diagram below to one above</p>

<pre><code>                       1--
                          -----
                               ----2
</code></pre>

<p>Using the formula Output = (gyro - 0) * Pgain</p>

<p>Do I need to increase the output only on the motor 2 or would I have to do both:
increase the output on the 2nd motor while decreasing the output on the first motor? Thank you</p>
","quadcopter pid ardupilot logic-control"
"9472","Does a controlling system need to be more complex than the system being controlled?","<p>Is there any theoretical principle, or postulate, that states that the controlling system has to be more complex than the system being controlled, in any formal sense of the notion ""complex""?</p>
","control theory"
"9476","Remaking an RC transmitter for controlling aircraft","<p>I am thinking about working on alternative drone controllers. I am looking into making it more easy to use and a natural feel (debating between sensor bracelets, rings, etc.).</p>

<p>The main issue I have is, I've been looking over all the standard RC transmitters that are used to control RC aircraft, but I am not sure what technology is inside of them, what kind of ICs they use for the actual RC signals. </p>

<p>I want more information on how to make an RC transmitter myself, mainly the protocol that's used to send messages, and what circuitry is needed to actually transmit that, what kind of components do I need and how should I implement the software?</p>

<p>I was aiming at doing this as a side project (hobby), but now I have the chance to use it as a uni project as well, so I'd like to give it a shot now, but I lack the proper information before getting started. </p>

<p>I'd rather not take apart my current RC controller and use an oscilloscope to decode the protocol. </p>

<p>Any answers (short or long) and reading material is appreciated.</p>

<p>Other questions, can the protocol be implemented in software on an embedded system (Raspberry Pi, Arduino, Intel Galileo, etc.)?
I am asking this because the frequency for these are 2.4 GHz.</p>

<p>This is part of a bigger project, drone related currently, and I could use alternative methods of sending the information, through other wireless means, as the first prototype, suggestions are welcomed.</p>

<p>Need: aircraft RC transmitter protocol info, RC transmitter components &amp; schematics, anything else that might help with the transmission side</p>
","microcontroller radio-control"
"9482","Fastening sheet steel on nylon","<p>I'm trying to attach a small piece of sheet steel (30mm x 50mm x 1mm) to a small piece of nylon (50mm x 50mm x 4mm). Does anyone know how they could be fastened using small screws (

<p>Any thought appreciated.</p>
","mechanism"
"9488","Fixed Wing UAV: Do inherently unstable systems desire to be stable for all cases when a closed loop control is implemented on them?","<p>As we all know fixed wing vehicles are designed to have inherent instability which is what enables all fixed wing vehicles to fly.</p>

<p>However does this apply to all cases?</p>

<blockquote>
  <p>Do inherently unstable systems desire to be stable for all cases when a closed loop control is implemented on them?</p>
</blockquote>
","control pid microcontroller uav stability"
"9491","Can you interface to a Braava Jet?","<p>Is there any open interface access to the new Braava jet just to drive it around?</p>
","irobot-create"
"9495","What iRobot products support the open interface besides the iRobot Create?","<p>I have read that certain iRobot products support or can be hacked to support something close to the open interace. There is even a book about hacking Roomba. What Robots have this capability?</p>
","irobot-create"
"9500","APM Mission Planner 2.0.18 Install Firmware Failure Mac OS X 10.11","<p>I installed Multiple versions of <code>APM</code> (2.0.7, 2.0.17, 2.0.18) on Windows 7, Ubuntu 14.04, and OSX 10.11. I could connect to my <code>ArduPilot</code> but could not install firmware. Here's the error I would get:</p>

<pre><code>Started downloading http://firmware.diydrones.com/Copter/stable/apm2-hexa/ArduCopter.hex
Finished downloading /var/folders/r4/s_j4c02s3wvcx6wy41__rnwh0000gp/T/APM Planner.uq1800
Opening firmware file...
Unable to open file: /var/folders/r4/s_j4c02s3wvcx6wy41__rnwh0000gp/T/APM Planner.uq1800
</code></pre>
","ardupilot"
"9502","Use data from gyroscope to calculate orientation","<p>From a gyroscope I'm getting angular velocities [dRoll, dPitch and dYaw] as rad/s, sampled at intervals dt = 10ms.</p>

<p>How do I calculate the short term global orientation (drift ignored) of the gyroscope?</p>

<p>Pseudo code would be helpful.</p>
","gyroscope"
"9505","Euler-Lagrange systems, autonomous or nonautonomous?","<p>I was reading an article on Euler-Lagrange systems. It is stated there that since M(q) and C(q,q') depend on q, it is not autonomous. As a result, we cannot use LaSalle's theorem. I have uploaded that page of the article and highlighted the sentence. <a href=""https://www.dropbox.com/s/zdkdvqky1uq09tr/ren.pdf?dl=0"" rel=""nofollow"">(ren.pdf)</a></p>

<p>Then, I read Spong's book on robotics, and he had used LaSalle's theorem. I am confused. <a href=""https://www.dropbox.com/s/4cw1rr4qldglqwu/spong.pdf?dl=0"" rel=""nofollow"">(spong.pdf)</a></p>

<p>I did some research, and found out that non-autonomous means it should not explicitly depend on the independent variable. Isn't independent variable time in these systems? So, shouldn't they be considered autonomous?</p>
","control dynamics robotc"
"9508","Considerations to design actuators, and loop feedback systems, for a robotic arm","<p>Let's say I have an industrial sized 6DOF robotic arm. I want to control each one of the six joints despite the non-linearity produced by the chain structure, the gravity and the weight of the loads it could lift. </p>

<p>I don't focus here on the speed nor the power limitations, I just want the arm to respond well. Moreover, I would like to avoid the use of any prior knowledge such as inertial computation. Then I had these considerations, considering that I can play with both the actuator design, and the loop feedback control system:</p>

<ol>
<li>Limit the maximum speed of each actuator to smooth their error variation.</li>
<li>Increase the damping of the actuators to avoid high frequency instability.     </li>
<li>Find a good control system, such as a PID, to make sure the targets are reached without oscillations. </li>
</ol>

<p>Do you have any other considerations in mind? Do you know what process(es) industrial designers follow?</p>

<p>EDIT: As it is said in the comments, my question concern the design of an adaptive controller for a robot arm, which is, how to design a joint control system (actuator + loop control) that don't need inertia and masses to be computed (the controller could adapt to its own structure, or to the loads it lifts). 
I'll be very much interested if you know some paper about adaptive control in the field of robotic arms.    </p>
","control pid robotic-arm design actuator"
"9512","DC Motor Control","<p>My project requires a DC motor for mobility, very similar to an RC car. If precision isn't critical, can I use a solid state relay instead of a motor driver? If the vehicle moves an extra inch on the ground, I don't really care.</p>
","motor driver"
"9518","Choose and connect a camera to a robot","<p>There are tons of cameras in devices around us these days. There are used photo cameras, smartphones, tablets at my home gathering dust.</p>

<p>I wonder, what the easiest way could be to get a camera module from some device, connect it to my robot project with a soldering iron and make it work from the software point of view. I am planning to use something like Arduino, an STM32 platform, or probably Intel Edison.</p>

<p>May be some camera modules are easier to solder and program for a custom  project? Or shouldn't I look this way and better find a camera module that is specially designed for custom projects?</p>
","cameras"
"9521","Difference between g-value and rhs-value in Lifelong Planning A*","<p>What is the difference between g-value and rhs-value of Lifelong Planning A* algorithm? </p>

<p>According to this link, <a href=""http://idm-lab.org/bib/abstracts/papers/aaai02b.pdf"" rel=""nofollow"">D* Lite</a>, g(s) directly correspond to the
g-values of an A* search, i.e. <strong>g(s) = g(s') + c(s',s)</strong>, and rhs(s) is given as </p>

<p>$$
rhs(s) = \begin{cases}0 &amp; s = s_{start}  \\ \min_{s'\in Pred(s)}(g(s') + c(s', s)) &amp; \text{otherwise} \end{cases}
$$</p>

<p>where, Pred(s) denotes the set of predecessors of node 's'. </p>

<p>Thus, unless node 's' has more than one predecessor, its g-value and rhs-value will remain same. </p>

<p>So, my question is, in which case will the rhs-value and g-value of a node be different?</p>
","mobile-robot robotic-arm wheeled-robot motion-planning algorithm"
"9523","Neural Nework code or library for MSP430G2553 microcontroller","<p>I am new here and i am new to neural network also. :P<br>
I have gone through the concepts of Neural Networks but i want to implement it in my project including microcontroller <strong>MSP430G2553</strong> on <em>LaunchPad</em> Series.<br>
I am using some sensors and i want to use some neural network code to manipulate the data from sensors to get some threshold.<br>
I went through this <a href=""http://robotics.stackexchange.com/questions/568/is-it-possible-to-run-a-neural-network-on-a-microcontroller"">post</a> and tried to implement the codes from the link given but it is giving some error on less ram, i guess it is due to my mcu.  </p>

<p>So, i wanted some help regarding the neural network code or library for <em>Energia</em> which i should use.
Thanks in Advance.</p>
","microcontroller electronics machine-learning embedded-systems"
"9532","Quadcopter propeller physics","<p>In propellers as the airspeed increases thrust decreases. Is the air speed component taken as a vector quantity perpendicular to the propeller? If thats true the its quiet easy to visualize in case of airplanes but for quadcopters will it be  <strong>""copter_airspeed * sin(copter tilt)""</strong>? </p>
","quadcopter"
"9535","Anthropomorphic Arm","<p>I developed an anthropomorphic arm (structure in aluminium) with 6 DOF (3 plus spherical wrist) for direct kinematic. </p>

<p>I chose magnetic rotary encoders to measure angles but I am not satisfied, due to them causing noise on angle measurements. </p>

<p>What do you advise me? </p>

<ul>
<li>To add another sensor and perform a sensor fusion? </li>
<li>To replace magnetic encoders with optical ones? </li>
<li>or... what else?</li>
</ul>
","arm manipulator"
"9536","Meaning of 'sign' in Writhe Matrix","<p>Following is the equation of Writhe matrix from the article <a href=""https://www.google.com/url?sa=t&amp;rct=j&amp;q=&amp;esrc=s&amp;source=web&amp;cd=2&amp;cad=rja&amp;uact=8&amp;ved=0ahUKEwiShMzIuOnLAhVIQyYKHS5RD-oQFggpMAE&amp;url=https%3A%2F%2Fipvs.informatik.uni-stuttgart.de%2Fmlr%2Fpapers%2F13-ivan-IJRR.pdf&amp;usg=AFQjCNHftBdmoPCF6xDv59fxlEfwGkLGvg&amp;sig2=m9a-rNtK1v6NAJxqr3rTqA&amp;bvm=bv.117868183,bs.1,d.eWE"" rel=""nofollow"">Topology based Representation</a>(page no. 6).
<a href=""http://i.stack.imgur.com/k8LQw.jpg"" rel=""nofollow""><img src=""http://i.stack.imgur.com/k8LQw.jpg"" alt=""enter image description here""></a></p>

<p>What is the meaning of 'sign' in the second part of this equation? I am not sure if this is some typo in that article as the other article of <a href=""http://www.roboticsproceedings.org/rss08/p59.pdf"" rel=""nofollow"">Hierarchical Motion Planning</a>(page no. 3), compleletely neglects the term 'sign[...]'
<a href=""http://i.stack.imgur.com/L2LJJ.jpg"" rel=""nofollow""><img src=""http://i.stack.imgur.com/L2LJJ.jpg"" alt=""enter image description here""></a></p>
","mobile-robot control robotic-arm wheeled-robot motion-planning"
"9538","Meaning of symbol, 'curly N' in the equation of Linear Gaussian system dynamics","<p>In the article of <a href=""https://ipvs.informatik.uni-stuttgart.de/mlr/papers/13-ivan-IJRR.pdf"" rel=""nofollow"">Topological Based Representation</a>(Page no. 12), the equation of the Linear Gaussian system dynamics is given as </p>

<p><a href=""http://i.stack.imgur.com/8uSyq.jpg"" rel=""nofollow""><img src=""http://i.stack.imgur.com/8uSyq.jpg"" alt=""enter image description here""></a></p>

<p>In above equation what is the meaning of 'curly N'? </p>
","mobile-robot control robotic-arm wheeled-robot motion-planning"
"9542","Meaning of the equation of graphical model","<p>The paper <a href=""https://ipvs.informatik.uni-stuttgart.de/mlr/papers/13-ivan-IJRR.pdf"" rel=""nofollow""><em>Topology-based Representations for Motion Planning
and Generalisation in Dynamic Environments with
Interactions</em></a> by Ivan
et.al., says on page 10 that the Approximate Inference Control (AICO) framework translates the robot dynamics to the graphical model by the following equation:</p>

<p><a href=""http://i.stack.imgur.com/lQsJ4.jpg"" rel=""nofollow""><img src=""http://i.stack.imgur.com/lQsJ4.jpg"" alt=""enter image description here""></a></p>

<p>What does p(x0:T,u0:T) mean? I feel that p means 'prior of' some uncertain quantity, but I'm not sure about this. </p>
","mobile-robot control robotic-arm motion-planning"
"9544","Enable Bluetooth communication with iRobot Create 2","<p>I just got a new iRobot Create 2. I used to use an Element Direct BAM (Bluetooth Adapter Module) for iRobot Create previously.</p>

<p>How can I communicate with a Create 2 using Bluetooth? What accessories do I need?</p>
","irobot-create"
"9545","Autonomous Indoor Positioning System Robot based on CV approach","<p>I have some questions regarding an IPS autonomous robot system,</p>

<p>Configuration: </p>

<ul>
<li>Mounting a camera to the ceiling of a room</li>
<li>Assume the room is a cube of 5mx5mx5m (LxWxH)</li>
<li>Assume the camera is Microsoft LifeCam Studio (CMOS sensor technology, Sensor Resolution: 1920 X 1080, 75° diagonal field of view, Auto focus from 0.1m to ≥ 10m, Up to 30 frames per second, Frequency Response: 100 Hz – 18 kHz)</li>
<li>A rover</li>
</ul>

<p>Objectives:</p>

<ul>
<li>By putting the rover in an unknown location (x,y) in the room, the system should localize the rover's position</li>
<li>After the rover's coordinates will be known, Navigation will be the next step</li>
<li>We want the rover to navigate from the known coordinates (x1,y1) (let's say point A) to another point B on the map (x2,y2)</li>
<li>Control signals will be sent to the rover's servos to complete the navigation task</li>
</ul>

<p>Methodology:</p>

<ul>
<li>Camera will capture the environment in real time</li>
<li>Environment will be represented as cells (occupancy grid mapping)</li>
<li>Assume each cell represents 5 cm in the environment</li>
<li>Rover will be localized by the system point A</li>
<li>Determine the navigation to point B</li>
<li>Determine the path of the rover in the grid map (ex: go x cells horizontal then y cells vertical)</li>
<li>Control Signal will be sent to rover's servos</li>
</ul>

<p>Questions:</p>

<ul>
<li>Can I use this camera for this task or I need another type of cameras ?</li>
<li>What are the factors affecting the system accuracy ?
(ex: Sensor Resolution - FOV - FPS - Frequency Response - Distance of the camera in the ceiling)</li>
<li>What's is the most important factor to consider to increase the accuracy ?</li>
<li>I would appreciate any opinions regarding the project</li>
</ul>

<p>King Regards,
Thank you</p>
","mobile-robot localization slam computer-vision mapping"
"9547","Reverse engineering commercial drone control algorithms","<p>I'm wondering if there is a way to figure out the actual controllers used in the commercial drones such as <a href=""http://ardrone2.parrot.com/"" rel=""nofollow"">AR drone</a> and <a href=""http://www.dji.com/product/phantom-4"" rel=""nofollow"">Phantom</a>. According to AR drone SDK, users are not allowed to access the actual hardware of the platform yet they are only capable of sending and receiving commands from/to the drone. </p>

<hr>

<p><strong>Edit:</strong>
I'm hoping to to check the actual controller utilized in the software. When I fly AR drone, it seems the platform can't stabilize itself when I perform aggressive maneuvers, therefore, I can guess that they use linearized model which is applicable for using simple controllers such as PD or PID</p>
","quadcopter control"
"9552","How is Topology-based representation invariant to certain change in environment","<p>The article of <a href=""https://ipvs.informatik.uni-stuttgart.de/mlr/papers/13-ivan-IJRR.pdf"" rel=""nofollow"">Topology-based representation</a> (page no. 13, line 5) says that, topology-based representation is invariant to certain changes in the environment. That means the trajectory generated in topology-based space will remain valid even if there are certain changes in the environment. But how is this possible? Is there any simple example to understand this concept?</p>
","mobile-robot control robotic-arm motion-planning"
"9553","Counting number of people entering a room","<p>We are making a project in which we want to count the no. of people entering and leaving a room with one single entrance. We are using IR sensors and detectors for this ,along with an Aurdino. We have a problem in this system,  i.e when two or more persons are entering or leaving the room at a time we are getting a wrong count. Thanks in advance for your valuable time and solution.....If there is any other better way,please state that.</p>
","arduino sensors microcontroller"
"9554","Quadcopter PID Algorithm","<p>I'm trying to implement a PID control on my quadcopter using the Tiva C series microcontroller but I have trouble making the PID stabilize the system. </p>

<p>While I was testing the PID, I noticed slow or weak response from PID controller (the quad shows no response at small angles). In other words, it seems that the quad's angle range has to be relatively large (above 15 degrees) for it to show a any response. Even then, the response always over shoots no matter what I, D gains I choose for my system. At low P, I can prevent overshoot but then it becomes too weak.   </p>

<p>I am not sure if the PID algorithm is the problem or if its some kinda bad hardware configuration (low IMU sample rate or maybe bad PWM configurations), but I have strong doubts about my PID code as I noticed changing some of the gains did not improve the system response.  </p>

<p>I will appreciate If someone can point out whether i'm doing anything wrong in the PID snippet for the pitch component I posted. I also have a roll PID but it is similar to the code I posted so I will leave that one out.</p>

<pre><code>void pitchPID(int16_t pitch_conversion)
{
  float  current_pitch = pitch_conversion;
  //d_temp_pitch is global variable
  //i_temp_pitch is global variable
  float  pid_pitch=0; //pitch pid controller
  float  P_term, I_term, D_term;
  float  error_pitch = desired_pitch - current_pitch;

  //if statement checks for error pitch in negative or positive direction
  if ((error_pitch&gt;error_max)||(error_pitch&lt;error_min))
    {
      if (error_pitch &gt; error_max) //negative pitch- rotor3&amp;4 speed up
        {
          P_term = pitch_kp*error_pitch; //proportional
          i_temp_pitch += error_pitch;//accumulate error
          if (i_temp_pitch &gt; iMax)
            {
            i_temp_pitch = iMax;
            }
          I_term = pitch_ki*i_temp_pitch;
          if(I_term &lt; 0)
            {
            I_term=-1*I_term;
            }
          D_term = pitch_kd*(d_temp_pitch-error_pitch);
          if(D_term&gt;0)
            {
            D_term=-1*D_term;
            }
          d_temp_pitch = error_pitch; //store current error for next iteration
          pid_pitch = P_term+I_term+D_term;
          if(pid_pitch&lt;0)
            {
            pid_pitch=(-1)*pid_pitch;
            }
          //change rotor3&amp;4
          pitchPID_adjustment (pid_pitch, 'n'); //n for negative pitch
        }
      else if (error_pitch &lt; error_min) // positive pitch- rotor 1&amp;2 speed up
        {
        P_term = pitch_kp*error_pitch; //proportional
        i_temp_pitch += error_pitch;
        if (i_temp_pitch &lt; iMin)
          {
          i_temp_pitch = iMin;
          }
        I_term = pitch_ki*i_temp_pitch;
        if(I_term &gt; 0)
          {
          I_term=-1*I_term;
          }
        D_term = pitch_kd*(d_temp_pitch - error_pitch);
        if(D_term &lt; 0)
          {
          D_term=-1*D_term;
          }
        d_temp_pitch = error_pitch;
        pid_pitch = P_term+I_term+D_term;
        if(pid_pitch&lt;0)
          {
          pid_pitch=(-1)*pid_pitch;
          }
        print(pid_pitch);//pitch
        printString(""\r\n"");
        //change rotor1&amp;2
        pitchPID_adjustment(pid_pitch,'p'); //p for positive pitch
      }
    }
  }
</code></pre>

<p> </p>

<pre><code>void pitchPID_adjustment(float pitchPIDcontrol, unsigned char pitch_attitude)
  {
  if (pitchPIDcontrol&gt;(maximum_dutyCycle-set_dutyCycle))
    {
    pitchPIDcontrol=maximum_dutyCycle-set_dutyCycle;
    }
  switch (pitch_attitude){
  //change rotor1&amp;2
  case 'p': //positive status
    PWM0_2_CMPA_R += (pitchPIDcontrol);//(RED)//motor1
    PWM0_0_CMPA_R += (pitchPIDcontrol);//(Yellow)//motor2
    break;
  //change rotor 3&amp;4
  case 'n': //negative status
    PWM0_1_CMPA_R += pitchPIDcontrol;//(ORANGE)//motor3
    PWM1_1_CMPA_R += pitchPIDcontrol;//(green)//motor4
    break;
  }
</code></pre>

<p>Also, can someone please tell me how this motor mixing works?: </p>

<pre><code>Front =Throttle + PitchPID 
Back =Throttle - PitchPID 
Left =Throttle + RollPID 
Right =Throttle - RollPID
</code></pre>

<p>vs what I did in the function:</p>

<pre><code>void pitchPID_adjustment(float pitchPIDcontrol, unsigned char pitch_attitude)
</code></pre>
","quadcopter control pid imu pwm"
"9555","What is the millisecond rate that robot-create can respond to two different drive commands?","<p>I want to issue two slightly different drive commands what is the smallest loop rate that the robot-create accepts new commands?</p>

<p>I know from reading the documentation that it appears the sensors are read every 15ms. </p>

<p>Not sure what the command rate is?</p>
","irobot-create"
"9565","Best sensor to determine ""up"" versus ""down""","<p>I want to start designing an Arduino project and have telemetry readings that indicate tilt or angle of placement. </p>

<p>Would an accelerometer be the best for determining tilt? Are there good tutorials? </p>
","mobile-robot arduino"
"9568","What is a CIM motor?","<p>I'm trying to make decisions for motors on a robot build. I keep running across CIM Motors. What is a CIM Motor? Where does the designation CIM come from? What does CIM mean?</p>
","motor"
"9569","Robotic Arm analysis in Matlab/simulink","<p>I am going through a paper, <a href=""http://cdn.intechopen.com/pdfs-wm/41413.pdf"" rel=""nofollow"">Kinematic Modelling and Simulation of a 2-R Robot Using SolidWorks and Verification by MATLAB/Simulink</a>, which is about a 2-link revolute joint robotic arm. According to the paper, the trajectory analysis of the robot was done via simulations in MATLAB/Simulink. </p>

<p>It shows the following picture, <em>Trajectory generation of 2‐R robot with MATLAB/Simulink</em>:</p>

<p><a href=""http://i.stack.imgur.com/8TWTN.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/8TWTN.png"" alt=""Trajectory generation of 2‐R robot with MATLAB/Simulink""></a></p>

<p>and then, <em>Simulink - Simulation block to calculate the trajectory</em>:</p>

<p><a href=""http://i.stack.imgur.com/ZTl5L.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/ZTl5L.png"" alt=""Simulink - Simulation block to calculate the trajectory""></a></p>

<p>I think this is done in SimMechanics, but I am not sure. Experienced users, can you please tell me what am I looking at and how can I reproduce this?</p>
","robotic-arm matlab simulation"
"9571","Simulating Dynamixel motors in Gazebo","<p>I'm trying to simulate a humanoid robot using Gazebo with plugins. Since our actual model uses Dynamixel motors, I'd like to know how exactly they work to make the simulation as realistic as possible.</p>

<p>Gazebo offers two options to control joints. One is a PID controller, provided by the <a href=""http://osrf-distributions.s3.amazonaws.com/gazebo/api/1.6.3/classgazebo_1_1physics_1_1JointController.html"" rel=""nofollow""><code>JointController</code> class</a>. The other way is to directly set a torque to the joint. (The PID method too is ultimately implemented using torques).</p>

<p>Currently, I'm trying the PID-based implementation. I've used a P-only controller with damping on all joints (I've had to guess both values). However, there is a large amount of noise, and the difference between actual and desired position is at times as much as 10-12 degrees (especially when the foot of the robot hits the ground).</p>

<p>Does the actual motor use a PID controller as well? I can't seem to find the details here, <a href=""http://www.hizook.com/files/users/3/EX-106_Robotis_Dynamixel_Servo_UserGuide.pdf"" rel=""nofollow"">Dynamixel EX-106 User's guide</a>, but this link, <a href=""http://www.trossenrobotics.com/dynamixel-ex-106-robot-actuator.aspx"" rel=""nofollow"">Dynamixel EX-106+ Robot Actuator</a> mentions ""Compliance/PID : Yes"".</p>

<p>If the motor <em>does</em> use a PID controller, then what are the parameters? And how does it allow us to set moving speed then?</p>

<p>If the motor doesn't use a PID controller, then what is the pattern of torque provided? In the manual (first link), I found this</p>

<blockquote>
  <p>From the current position 200 to 491 ( 512-16-5=491 ), movement  is 
  made  with appropriate  torque  to  reach  the  set  speed; from  491 
  to  507  ( 512-5=507 ), torque is continuously reduced to the Punch 
  value; from  507  through 517 (  512+5=517  ), no torque is generated.</p>
</blockquote>

<p>This is rather vague though, and no further details are provided.</p>

<p>Also, I'm aware that extremely high damping and extremely high P-values might do the trick. But I want to simulate what actually happens on the motors, and that is probably not the way to go. </p>

<p>I'd appreciate it if anyone has any idea of what Dynamixel servos do, or examples of simulated Dynamixel motors anywhere else.</p>
","pid simulation gazebo humanoid dynamixel"
"9572","Transfer function of a quadrotor position controller","<p>I'm trying to find the transfer function of a quadrotor with two controller loops, following next structure:
<a href=""http://i.stack.imgur.com/VFN0D.jpg"" rel=""nofollow""><img src=""http://i.stack.imgur.com/VFN0D.jpg"" alt=""enter image description here""></a></p>

<p>I know how to calculate the attitude stability controller, which relate rotor speed and desired angles. However, I have no clear at all how to implement the translational controller transfer function, whose output is the desired angle that the rotors must achieve considering the position I want to translate.</p>

<p>Considering that two controllers are PD, how can you calculate the translational controller transfer function and include it in the system? Time domain equations in the outer loop are next, where U terms relate to the thrust axis components. Thanks</p>

<p><a href=""http://i.stack.imgur.com/M4juk.jpg"" rel=""nofollow""><img src=""http://i.stack.imgur.com/M4juk.jpg"" alt=""enter image description here""></a></p>
","quadcopter control"
"9578","Create2 Serial. Canonical versus Number of Bytes interface","<p>There is a message system which does not appear to be document in the OI spec. This appears to be a a canonical terminal type serial interface in which messages come back such as firmware version and stuff. I am not sure how to determine what the end of this type of message is? It is a fixed number of end lines? or Bytes. One message seems to indicate STR730 which would be a 730 byte string.</p>

<p>The open interface spec seems to indicate a non canonical interface spec in which you read a fixed number of bytes with no processing of end lines. Is this correct?</p>
","irobot-create roomba"
"9580","Ultrasonic sensor range and shape","<p>I have been looking for a cheap ultrasonic sensor that is not blind under +/-30 cm but the only sensors I could find use the following shape, which is not suitable for my project (because of the robot design that only has 1 hole, and not 2..) : <a href=""http://i.stack.imgur.com/PDYXh.jpg"" rel=""nofollow""><img src=""http://i.stack.imgur.com/PDYXh.jpg"" alt=""shape 1""></a></p>

<p>Is there any chance to find a sensor with that other shape with a range starting around 5cm ?</p>

<p><a href=""http://i.stack.imgur.com/FebEN.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/FebEN.png"" alt=""shape 2""></a></p>

<p>Actually I am wondering if that 2nd shape makes this constraint mandatory or if I just did not found the appropriate product.</p>
","ultrasonic-sensors"
"9583","Servo motors for large scale RC car","<p>I want to convert an electric ATV (quad) for kids (like the <a href=""http://www.high-per.com/"" rel=""nofollow"">HIGHPER</a> ATV-6E) to radio control for a robotics project. These small ATVs are about a meter long and weigh about 40 kg. I need to choose servo motors for steering and braking. What grade servos do I need and how much torque do they need to have? Can I use the strongest RC servo I may find (like <a href=""https://www.pololu.com/product/1390"" rel=""nofollow"">this 115kg/cm one</a> or maybe even more, with metal gears of course) or do I need an ""industrial grade"" servo?</p>

<p>I plan to use one servo for steering and one for braking. For braking the ATV has mechanical disc brakes - two discs in the front and one common disc in the rear (there are two brake levers - front/rear). I plan to use only one servo and use it either for front or for rear. The plan is to mount the brake wire to the servo which would ""simulate"" the lever movement.</p>

<p>I guess I could also make a ""weak"" servo stronger by adding a proper gear, but I am not really into mechanical engineering much and would prefer an off-the-shelf component.</p>

<p><a href=""http://i.stack.imgur.com/N8SXx.jpg"" rel=""nofollow""><img src=""http://i.stack.imgur.com/N8SXx.jpg"" alt=""Highper ATV""></a></p>
","servomotor rcservo steering brake"
"9590","Downgrade ROS from Jade to Indigo","<p>Is it possible to downgrade from ROS Jade to Indigo?</p>

<p>For those who are not yet familiar with Robot Operating System (ROS), here: <a href=""http://ros.org"" rel=""nofollow"">ROS</a></p>
","ros"
"9592","Meaning of s_last in D star Lite algorithm","<p>In the D*Lite algorithm, described in line 21 of Figure 3, on page 4, in <a href=""http://idm-lab.org/bib/abstracts/papers/aaai02b.pdf"" rel=""nofollow"">D* Lite</a>, the <code>main()</code> starts with defining $s_{last}=s_{start}$. But value of $s_{last}$ is never updated in the entire algorithm. </p>

<p>So what is the purpose of defining this term and what does it mean?</p>

<p><a href=""http://i.stack.imgur.com/8qTBu.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/8qTBu.png"" alt=""Pseudo Code from figure 3""></a></p>
","mobile-robot control robotic-arm motion-planning algorithm"
"9593","Mechanical odometer with digital output","<p>I would like to mechanically measure the distance a kids electric ATV traveled. The ATV will not be used with kids but as a mobile robot instead. It has a common rear axle for both rear wheels which I think could be a good place to put an odometer on (since the chance both wheels will slip should be minimal). Regarding suspension it has a single shock for rear axle.</p>

<p>My plan is to put a bigger gear on the axle itself and then add a smaller gear to it on which some kind of sensor would measure number of its rotations. One rotation of the axle may be something like 20 rotations of the small gear. What kind of sensor can I use for sensing rotation?</p>

<p>Another way of making an odometer may be some kind of optical solution (disc with holes and an optical sensor) but this seems to be rather complicated and also the the direction of travel could not be easily estimated (unless the motor is running in some direction).</p>

<p>I just found a term called <strong><a href=""https://en.wikipedia.org/wiki/Wheel_speed_sensor"" rel=""nofollow"">Wheel Speed Sensor</a></strong> which looks interesting and seems to employ primarily non-contact sensing (which is definitely better than mechanical gears). Rather then optical solution I like the Hall effect sensor solution which may be simple and mechanically robust. But still, my question is open on how to implement this...</p>

<p>I would like to use the odometer for both speed estimation and distance estimation. I need to read the sensor from C/C++ on a Linux box.</p>

<p>EDIT: The thing I am looking for is probably correctly called a rotary encoder or a wheel encoder.</p>

<p>The ATV may look like one of these:</p>

<p><a href=""http://i.stack.imgur.com/K8Snh.jpg"" rel=""nofollow""><img src=""http://i.stack.imgur.com/K8Snh.jpg"" alt=""atv""></a>
<a href=""http://i.stack.imgur.com/pMs7P.jpg"" rel=""nofollow""><img src=""http://i.stack.imgur.com/pMs7P.jpg"" alt=""atv""></a></p>
","mobile-robot sensors odometry encoding rotation"
"9597","Hand-eye calibration?","<p>I am having an issue with some hand-eye calibration. </p>

<p>So i am using a simple robot which at its tool point has an stereo camera mounted on it. </p>

<p>I want to perform some visual serving/tracking based stereo images extracted from the camera in the ""hand"". The camera provides me x,y,z coordinates of the object I want to track. </p>

<p>I can at all time extract an homogenous transformation matrix from base to tool (not cam) as <code>(T_tool_base)</code>. </p>

<p>Firstly... I guess i would need perform some form of robot to (vice versa) camera calibration, My idea was that would consist of something like this </p>

<pre><code>T_base_world = (T_base_tool) (T_tool_cam) (T_cam_world) 
</code></pre>

<p>Where the T_tool_cam would entail the calibration... since the camera is at the tool point, would that entail the T_tool_cam should entail information on how much the camera is displaced from the tool point, and how it is rotated according to the tool point? or is not like that?</p>

<p>secondly... How do i based purely x,y,z coordinate make an homogeneous transformation matrix, which includes an rotation matrix ?</p>

<p>thirdly.. Having a desired Transformation matrix which in theory this     </p>

<pre><code>T_base_world = (T_base_tool) (T_tool_cam) (T_cam_world) 
</code></pre>

<p>would provide me, would an inverse kinematics solution provide me with one or multiple solution?... In theory should this only provide me one, or what?</p>
","robotic-arm inverse-kinematics rotation"
"9600","How can I determine the pose of the origin after some transformations?","<p>Find the origin and coordinate directions of a frame resulting from a rotation of $90^{\circ}$ about the z axis, followed by a displacement of $\begin{pmatrix}1\\7\\3\end{pmatrix}$. Hence find the position, in the original frame, of the vector $\begin{pmatrix}3\\8\\1\end{pmatrix}$, defined in the resulting frame.</p>
","kinematics inverse-kinematics"
"9602","ComputeShortestPath() in Dstar lite algorithm","<p>In optimized D*Lite algorithm as shown in the figure below (page 5, of the paper <a href=""http://robotics.cs.tamu.edu/dshell/cs625/aaai02b.pdf"" rel=""nofollow"">D*Lite</a>), when the procedure ComputeShortestPath() is called for the first time in line 31, U(list of inconsistent vertices) contains only goal vertex ($s_{goal}$). Thus in the procedure ComputeShotestPath()(line 10-28), $u = s_{goal}$. And as, $k_{old}=k_{new}$ (because $k_m=0$), condition $k_{old}\leq k_{new}$ is satisfied and $u = s_{goal}$ is again inserted in U with same value of $k_{old}=k_{new}$. Thus, it seems that line(11-15) will run forever, and the algorithm will not be able to find the shortest path from goal to start.</p>

<p>I know that this algorithm has been widely used and I am failing to understand it. But where am I going wrong? </p>

<p><a href=""http://i.stack.imgur.com/uXuNY.jpg"" rel=""nofollow""><img src=""http://i.stack.imgur.com/uXuNY.jpg"" alt=""enter image description here""></a></p>
","mobile-robot control robotic-arm motion-planning algorithm"
"9604","Quadcopter heading calculation","<p>I'm working on an autonomous quad copter, I have two GPS co-ordinates (source and destination co-ordinates). I need to move my quad from the source to the destination, for this I need to calculate the heading and set the yaw value of my quad. How can I calculate the heading and make sure the quad is headed in the right direction as the target co-ordinates?</p>

<p>If I use magnetometer the declination angle will vary from place to place and so I will have to keep changing the declination angle. If I'm calculating based on just the GPS co-ordinates, it's not accurate. </p>

<p>What is the best way to do this?  How do I calculate the above? </p>
","quadcopter gps magnetometer"
"9608","Quaternion Kalman Filter Algorithm","<p>I have been stuck on this for weeks, I really hope that someone can help me with this,thank you in advance.
I am trying to write an IMU attitude estimation algorithm using quaternion kalman filter. So based on this research paper: <a href=""https://hal.archives-ouvertes.fr/hal-00968663/document"" rel=""nofollow"">https://hal.archives-ouvertes.fr/hal-00968663/document</a>, I have developed the following pseudo code algorithm:</p>

<p>Predict Stage:</p>

<p>Qk+1/k = Ak * Qk;  where Ak contains the gyro measurement. 
<a href=""http://i.stack.imgur.com/CDjLC.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/CDjLC.png"" alt=""enter image description here""></a></p>

<p>Pk+1/k = Ak * Pk *Ak.transpose() + Q; where Q is assumed to be zero.</p>

<p>After prediction, we can use this formula to get the supposed gravity measurement of accelerometer Yg in body frame :</p>

<p>Yg = R * G;  // R is the rotation matrix generated from quaternion Qk+1/k and G = (0,0,0,9.81).</p>

<p>This equation then translates to the following equation which allows me to get measurement model matrix H.</p>

<p>H * Qk+1/k = 0; //where H stores value related to (Yg-G).</p>

<p>Update Stage:</p>

<p>K = P * H * (H * P * H.transpose()+R)^(-1); //R should be adaptively adjusted but right now initialized as identity matrix</p>

<p>Qk+1/k+1 = (I-KH)Qk+1/k;</p>

<p>Qk+1/K+1 = (Qk+1/K+1)/|Qk+1/k+1|; //Normalize quaternion</p>

<p>Pk+1/K+1 = (I - KH)Pk+1/k;</p>

<p>The following is the main part of my code. The complete C++ code is at here <a href=""https://github.com/lyf44/fcu"" rel=""nofollow"">https://github.com/lyf44/fcu</a> if you want to test.</p>

<pre><code>Matrix3f skew_symmetric_matrix(float a, float b, float c, float d){
    Matrix3f matrix;
    matrix &lt;&lt; a,d*(-1),c,
              d,a,b*(-1),
              c*(-1),b,a;
    return (matrix);
}

void Akf::state_transition_matrix(float dt,float gx,float gy, float gz){
    Vector3f tmp;
    tmp(0) = gx*PI/180;
    tmp(1) = gy*PI/180;
    tmp(2) = gz*PI/180;
    float magnitude = sqrt(pow((float)tmp(0),2)+pow((float)tmp(1),2)+pow((float)tmp(2),2));

    /*q(k+1) = |  cos(|w|*dt/2)       | quaternion_multiply q(k)
               |  w/|w|*sin(|w|*dt/2) |
    */
    //w/|w|*sin(|w|*dt/2)
    tmp = tmp/magnitude*sin(magnitude*dt/2);
    //quaternion multiplication
    A(0,0) = cos(magnitude*dt/2);
    A.block&lt;3,1&gt;(1,0) = tmp;
    A.block&lt;1,3&gt;(0,1) = tmp.transpose()*(-1);

    Matrix3f skew_symmetric;
    skew_symmetric = skew_symmetric_matrix((float)A(0,0),(float)tmp(0),(float)tmp(1),(float)tmp(2));
    A.block&lt;3,3&gt;(1,1) = skew_symmetric;
}

void Akf::observation_model_matrix(Vector3f meas){
    Vector3f G;
    Vector3f tmp;
    G &lt;&lt; 0,0,9.81;
    /* H = | 0        -(acc-G).transpose     |
     *     | (acc-G)  -(acc+G).skewsymmetric |
     */
    tmp = meas-G;
    H(0,0) = 0;
    H.block&lt;3,1&gt;(1,0) = tmp;
    H.block&lt;1,3&gt;(0,1) = tmp.transpose()*(-1);
    tmp = tmp+G+G;
    Matrix3f matrix;
    matrix = skew_symmetric_matrix(0,(float)tmp(0),(float)tmp(1),(float)tmp(2));
    H.block&lt;3,3&gt;(1,1) = matrix*(-1);
    //H = H*(0.5);
    cout&lt;&lt;""H""&lt;&lt;endl;
    cout&lt;&lt;H&lt;&lt;endl;
    cout&lt;&lt;""H*X""&lt;&lt;endl;
    std::cout&lt;&lt;H*X&lt;&lt;std::endl;
}

void Akf::setup(){
    X_prev = Vector4f::Zero(4,1);
    X_prev(0) = 1;
    Q = Matrix4f::Zero(4,4);
    Z = Vector4f::Zero(4,1);
    R = Matrix4f::Identity(4,4);
    P_prev = Matrix4f::Identity(4,4);
    P_prev = P_prev*(0.1);
    I = Matrix4f::Identity(4,4);

    sum = Vector4f::Zero(4,1);
    noise_sum = Matrix4f::Zero(4,4);
    counter=1;
}

void Akf::predict_state(){
    cout&lt;&lt;(60*counter%360)&lt;&lt;endl;
    X = A*X_prev;
    A_T = A.transpose();
    P = A*P_prev*A_T+Q;
}

void Akf::update_state(){
  Matrix4f PH_T;
  Matrix4f tmp;

  PH_T = P*H.transpose();
  S = H*PH_T+R;
  if (S.determinant()!= 0 )
  {
      tmp = S.inverse();
      K = P*H*tmp;
      //std::cout&lt;&lt;""K""&lt;&lt;std::endl;
      //std::cout&lt;&lt;K&lt;&lt;std::endl;
      X_updated = (I-K*H)*X;
      X_updated = X_updated /(X_updated.norm());
      P_updated = (I-K*H)*P;
  }
  else{
      X_updated = X;
      std::cout&lt;&lt; ""error-tmp not inversible!""&lt;&lt;std::endl;
  }
  X_prev = X_updated;
  P_prev = P_updated;
}

void rotation_matrix(Vector4f q,Matrix3f &amp;rot_matrix){
    int i;
    for (i=1;i&lt;4;i++){
        q(i) = q(i)*(-1);
    }
    Matrix3f matrix;
    matrix(0,0) = pow((float)q(0),2)+pow((float)q(1),2)-pow((float)q(2),2)-pow((float)q(3),2);
    matrix(0,1) = 2*(q(1)*q(2)-q(0)*q(3));
    matrix(0,2) = 2*(q(0)*q(2)+q(1)*q(3));
    matrix(1,0) = 2*(q(1)*q(2)+q(0)*q(3));
    matrix(1,1) = pow((float)q(0),2)-pow((float)q(1),2)+pow((float)q(2),2)-pow((float)q(3),2);
    matrix(1,2) = 2*(q(2)*q(3)-q(0)*q(1));
    matrix(2,0) = 2*(q(1)*q(3)-q(0)*q(2));
    matrix(2,1) = 2*(q(0)*q(1)+q(2)*q(3));
    matrix(2,2) = pow((float)q(0),2)-pow((float)q(1),2)-pow((float)q(2),2)+pow((float)q(3),2);
    rot_matrix = matrix;
}

Vector3f generate_akf_random_measurement(Vector4f state){
    int i;
    //compute quaternion rotation matrix
    Matrix3f rot_matrix;
    rotation_matrix(state,rot_matrix);
    //rot_matrix*acceleration in NED = acceleration in body-fixed frame
    Vector3f true_value = rot_matrix*G;
    std::cout&lt;&lt;""true value""&lt;&lt;std::endl;
    std::cout&lt;&lt;true_value&lt;&lt;std::endl;
    for (i=0;i&lt;3;i++){
        noisy_value(i) = true_value(i) + (-1) + (float)(rand()/(float)(RAND_MAX/2));
    }
    return (noisy_value);
}

int main(){
      float gx,gy,gz,dt;
      gx =60; gy=0; gz =0; //for testing, let it rotate around x axis by 60 degree  
      myakf.state_transition_matrix(dt,gx,gy,gz); // dt is elapsed time
      myakf.predict_state();
      Vector4f state = myakf.get_predicted_state();
      Vector3f meas = generate_akf_random_measurement(state);
      myakf.observation_model_matrix(meas);
      myakf.measurement_noise();
      myakf.update_state();
      q = myakf.get_updated_state();
</code></pre>

<p>The problem that I face is that my code does not work.The prediction stage works fine but the updated quaternion state is only correct for the first few iterations and it starts to drift away from the correct value. I have checked my code against the research paper multiple times and ensured that it is in accordance with the algorithm proposed by the research paper.</p>

<p>In my test, I am rotating around x axis by 60 degree per iterations. The number below the started is the angle of rotation. state and updated state is the predicted and updated quaternion respectivly while true value, meas, result are acceleration due to gravity in body frame.As the test result indicates, everything is way off after rotating 360 degrees.
The following is my test result:  </p>

<pre><code>1
started
60
state
0.866025
0.5
0
0
true value
0
8.49571
4.905
meas
0.314533
7.97407
4.98588
updated state
0.866076
0.499913
-2.36755e-005
1.56256e-005
result
0.000555564
8.49472
4.90671

1
started
120
state
0.500087
0.865975
-2.83164e-005
1.69446e-006
true value
0.000306622
8.4967
-4.90329
meas
-0.532868
8.79841
-4.80453  
updated state
0.485378
0.862257
-0.129439
-0.064549
result
0.140652
8.37531
-5.10594

1 
started
180
state
-0.0107786
0.989425
-0.0798226
-0.12062
true value
-2.35843
-0.0203349
-9.52226
meas
-1.39627
-0.889284
-8.74243
updated state
-0.0195091
0.981985
-0.151695
-0.110965
result
-2.19598
-0.0456112
-9.56095 

1
started
240
state
-0.507888
0.840669
-0.0758893
-0.171946
true value
-3.59229
-8.12105
-4.16894
meas
-4.52356
-7.73113
-4.98735
updated state
-0.53758
0.811101
-0.212643
-0.0889171
result
-3.65783
-8.18397
-3.98485

1
started
300
state
-0.871108
0.433644
-0.139696
-0.183326
true value
-3.94732
-6.909
5.73763
meas
-4.36385
-6.98853
5.39759
updated state
-0.86404
0.436764
-0.102296
-0.228487
result
-3.69216
-6.94565
5.86192
1
started  
0
state
-0.966663
-0.0537713
 0.0256525
 -0.249024 
true value
0.749243
0.894488
9.74036
meas
-0.194541
0.318586
10.1868
updated state
-0.78986
-0.0594022
0.0311688
-0.609607
result
1.1935
0.547764
9.72171 

1
started
60
state
-0.654338
-0.446374
0.331797
-0.512351
true value
8.74674
2.39526
3.74078
meas
9.36079
2.96653
3.57115
updated state
-0.52697
-0.512048
0.221843
-0.64101
result
8.73351
2.50411
3.70018
</code></pre>

<p>Can someone help me confirm that my understanding about the theory of this quaternion kalman filter and my pseudo code is correct? Also, if anyone has implemented attitude estimation using maybe a different version of quaternion kalman filter, I would greatly appreciate if you can provide a pseudo code and a little explanation.
Thank you guys very much!</p>
","quadcopter kalman-filter"
"9609","Most accurate rotation representation for small angles","<p>Assume that I have a rigid body for which I know that it can rotate with respect to a global reference frame (which is considered fixed and already given) for only a few degrees of angle, so I can describe its rotation by using the small angle approximation. For this system, I would like to know if there is a rotation representation that offers more accuracy when compared with other representation methods.</p>

<p>The main representation methods that I considered are the euler angles and the pitch-yaw-roll transformation. To my perception, I think that pitch-yaw-roll representation is expected to be more accurate, since all the angles are expressed with respect to the initial coordinate frame. On the other hand, euler angles are defined on different frames, so I am not sure if the resulting angles will be really small.</p>

<p>To sum up, I know that the body can rotate for only a few degrees and I would like to know which coordinate representation is much probable to deliver the smallest angles, such that the small angle approximation is more valid.</p>

<p>It could also be the case that there is not a general answer (so it depends on the specific configuration) but still I haven't found anything about this topic on the related literature!</p>

<p><strong>Example</strong> (no small angle approx used): Assume I have a coordinate frame which describes a point in space by the following vector</p>

<p>$P2=\begin{bmatrix} 4 \\ 1 \\ 0.05 \end{bmatrix}$.</p>

<p>Given another coordinate frame which is rotated with respect to the previous one, the description of the same point is given by     </p>

<p>$P1=\begin{bmatrix} 3.8933 \\
    1.3566 \\
   -0.0630 \end{bmatrix}$.</p>

<p>Using Euler angles, I can find that the rotation matrix $R_{euler}$ is characterized by the angles $0.1,0.2,0.1$ rads, which correspond to the angle of rotation around z axis, the rotation around the resulting y axis and the rotation around the resulting z axis, respectively (these are basic stuff, it is explained in many books.). So I have that $P1=R_{euler} P2$.</p>

<p>Now I want to find the corresponding rotation matrix if I use the pitch-yaw-roll representation. Here I have to solve an optimization problem and the solution that I get (maximum error between P1 and the estimated P1 is $3 \times 10^{-8}$) delivers me the following angles</p>

<p>$\begin{bmatrix}  -0.0103   \\ 0.0257  \\  0.0902\end{bmatrix}$,</p>

<p>which correspond to the rotation around the x,y and z axis of the initial coordinate frame. </p>
","inverse-kinematics geometry rotation"
"9615","Orientation error with free rotations","<p>I am working on inverse kinematics for a 5DOF arm.  The tool is symmetric about its z-axis so we don't card about those rotations in the solution but we do care about the direction of the Z-axis.  in other words instead of the goal states orientation being $R = (N_d~ S_d ~A_d)$ we only care about $A_d$. How would I calculate the Orientation error (or adjust normal inverse kinematics) to account for this.  Setting $N_d$ and $S_d$ to zero or any value forces a particular orientation which may not be reachable.  for full 6DOF situations I have previously used the following equation for orientation error. </p>

<p>$$E_o = \frac{1}{2} (N_e(q) \times N_d + S_e(q) \times S_d + A_e(Q) \times A_d)$$</p>

<pre><code>Where $[N_e ~S_e ~A_e]$ is the rotation matrix of my current guess
and $[N_d~ S_d ~A_d]$ is the desired rotation matrix
</code></pre>

<p>is it sufficient to remove $N$ and $S$ from this equation giving</p>

<p>$$E_o = \frac{1}{2} (A_e(Q) \times A_d)$$</p>

<p>if not how else could I handle this situation?  </p>
","robotic-arm inverse-kinematics"
"9616","Motor upgrade to higher torque?","<p>I have assembled a 4WD car using kits I bought on ebay.</p>

<p>I have 4 motors similar to this one: <a href=""http://i.stack.imgur.com/7DFlDm.jpg"" rel=""nofollow""><img src=""http://i.stack.imgur.com/7DFlDm.jpg"" alt=""motor""></a>.</p>

<p>The description says:</p>

<blockquote>
  <ul>
  <li>Operating voltage: 3V~12VDC 
  (recommended operating voltage of about 6 to 8V)</li>
  <li>Maximum torque: 800gf cm min (3V)</li>
  <li>No-load speed: 1:48 (3V time)</li>
  <li>The load current: 70mA (250mA MAX) (3V)</li>
  <li>This motor with EMC, anti-interference ability. 
  The microcontroller without interference.</li>
  <li>Size: 7x2.2x1.8cm(approx)</li>
  </ul>
</blockquote>

<p>I am not too fond of the max speed I can reach, but I would be able to provide more power, because I have a 12V 2A battery onboard.</p>

<p>So far I have used 6V, because that seemed to be the safer voltage choice.</p>

<p>Has anybody tried successfully higher voltages, without wearing down the motor in few hours (I've read this can happen)?</p>

<p>Alternatively, can someone recommend replacement motors that would tolerate reliably a higher power envelope?</p>

<p>I would like to preserve the gearbox and replace only the motor, if possible.</p>

<p>I think I could fit a motor 2-4 mm longer (replacing the transparent strap which bonds it to the gearbox), if that makes any difference.</p>

<p>BTW, I'm making the assumption:</p>

<p>higher_voltage => higher_torque => higher_speed</p>

<p>but I'm not sure it's overall correct.</p>

<p>I expect that it would at least produce higher acceleration during the transients.</p>
","brushless-motor"
"9619","Extracting as many possible end configurations as possible","<p>I am trying to implement a path planner to generate a path that moves the robot from q_start to q_goal.   </p>

<p>Q_goal is extracted from a stereo camera mounted on the tool, from
which I extract x,y,z coordinates of the desired position, the rotation can be arbitrary. </p>

<p>The robot I am using is an industrial ur5 robot arm, the software I use is capable of performing Jacobian based inverse kinematics given a transformation matrix with rotation and translation. </p>

<p>my inverse kinematics provide me with only one solution, which is ok, but doesn't provide me flexibility for path planning...</p>

<p>How do I using inverse kinematics determine all possible q-configurations that fulfills my criteria of having the desired x,y,z coordinates? </p>
","robotic-arm inverse-kinematics"
"9620","Mobile robot path following using Model Predictive Control (MPC)","<p>I'am trying to implement a path following algorithm based on MPC (Model Predictive Control), found in this paper : <a href=""http://www2.imm.dtu.dk/pubdb/views/edoc_download.php/189/pdf/imm189.pdf"" rel=""nofollow"">Path Following Mobile Robot in the Presence of Velocity Constraints</a>  </p>

<p><strong>Principle:</strong> Using the robot model and the path, the algorithm predict the behavior of the robot over N future steps to compute a sequence of commands $(v,\omega)$ to allow the robot to follow the path without overshooting the trajectory, allowing to slow down before a sharp turn, etc.<br>
$v:$ Linear velocity<br>
$\omega:$ Angular velocity</p>

<p><strong>The robot:</strong> I have a non-holonomic robot like this one (Image extracted from the paper above) :<br>
<a href=""http://i.stack.imgur.com/K6MRY.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/K6MRY.png"" alt=""Non-holonomic robot with castor wheel. (Extracted from the paper above)""></a></p>

<p><strong>Here is my problem:</strong> Before implementing on the mobile robot, I'am trying to compute the needed matrices (using Matlab) to test the efficiency of this algorithm. At the end of the matrices computation some of them have dimension mismatch</p>

<p><strong>What I did:</strong><br>
For those interested, this calculation is from §4 (4.1, 4.2, 4.3, 4.4) p6-7 of the paper.  </p>

<blockquote>
  <h2>4.1 Model</h2>
  
  <p>$z_{k+1} =  Az_k + B_\phi\phi_k + B_rr_k$ (18) 
  with:<br>
  $A = \begin{bmatrix} 1 &amp; Tv \\ 0 &amp; 1 \end{bmatrix}$
  $B_\phi = \begin{bmatrix} {T^2\over2}v^2\\ Tv  \end{bmatrix}$
  $B_r = \begin{bmatrix} 0 &amp; -Tv \\ 0 &amp; 0 \end{bmatrix}$<br>
  $T$: sampling period<br>
  $v$: linear velocity<br>
  $k$: sampling index (i.e. $t= kT$)<br>
  $z_k:$ the state vector $z_k = (d_k, \theta_k)^T$ position and angle difference to the reference path
  $r_k:$ the reference vector $r_k = (0, \psi_k)^T$ with $\psi_k$ is the reference angle of the path at step k </p>
  
  <h2>4.2 Criterion</h2>
  
  <p>The predictive receding horizon controller is based on a minimization of the criterion<br>
  $J= \Sigma^N_{n=0} (\hat{z}_{k+n} - r_{k+n})^T Q(\hat{z}_{k+n} - r_{k+n}) + \lambda\phi^2_{k+n}$, (20)<br>
  Subject to the inequality constraint<br>
  $ P\begin{bmatrix} v_n \\ v_n\phi_n \end{bmatrix} \leq q,$<br>
  $n=0,..., N,$<br>
  where $\hat{z}$ is the predicted output, $Q$ is a weight matric, $\lambda$ is a scalar weight, and $N$ is prediction horizon.</p>
  
  <h2>4.3 Predictor</h2>
  
  <p>An n-step predictor $\hat{z}_{k+n|k}$ is easily found from iterating (18). Stacking the predictions $\hat{z}_{k+n|k},n = n,...,N$ in the vector $\hat{Z}$ yields<br>
  $\hat{Z} = \begin{bmatrix} \hat{z}_{k|k} \\ \vdots \\ \hat{z}_{k+N|k}\end{bmatrix} = Fz_k + G_\phi\Phi_k + G_rR_k$  (22)<br>
  with<br>
  $\Phi_k = \begin{bmatrix} \phi_k, \ldots, \phi_{k+N}\end{bmatrix}^T$,<br>
  $R_k = \begin{bmatrix} r_k, \ldots, r_{k+N}\end{bmatrix}^T$,<br>
  and<br>
  $F = \begin{bmatrix}I &amp; A &amp; \ldots &amp; A^N \end{bmatrix}^T$<br>
  $G_i = \begin{bmatrix} 0 &amp; 0 &amp; \ldots &amp; 0 &amp; 0 \\ B_i &amp; 0 &amp; \ldots &amp; 0 &amp; 0 \\ AB_i &amp; B_i &amp; \ddots &amp; \vdots &amp; \vdots \\ \vdots &amp; \ddots &amp; \ddots &amp; 0 &amp; 0 \\ A^{N-1}B_i &amp; \ldots &amp; AB_i &amp; B_i &amp; 0 \end{bmatrix}$  </p>
</blockquote>

<p>where index $i$ should be substituted with either $\phi$ or $r$</p>

<blockquote>
  <h2>4.4 Controller</h2>
  
  <p>Using the N-step predictor (22) simplifies the criterion (20) to
  $J_k = (\hat{Z}_k - R_k)^T I_q (\hat{Z}_k - R_k) + \lambda\Phi^T_k\Phi_k$, (23)
  where $I_q$ is a diagonal matrix of appropriate dimension with instances of Q in the diagonal. The unconstrained controller is found by minimizing (23) with respect to $\Phi$:<br>
  $\Phi_k = -L_zz_k - L_rR_k$, (24)<br>
  with<br>
  $L_z = (lambda + G^T_wI_qG_w)^{-1}G^T_wI_qF$
  $L_r = (lambda + G^T_wI_qG_w)^{-1}G^T_wI_q(Gr - I)$</p>
</blockquote>

<p>I'am trying to compute $\Phi_k = -L_zz_k - L_rR_k$ but the dimension of $L_r$ and $R_k$ does not match for matrix multiplication.</p>

<p>Parameters are :   </p>

<ul>
<li>$T=0.1s$</li>
<li>$N=10$</li>
<li>$\lambda=0.0001$</li>
<li>$Q=\begin{bmatrix} 1 &amp; 0 \\ 0 &amp; \delta \end{bmatrix}$ with $\delta=0.02$</li>
</ul>

<p>I get :<br>
$R_k$ a (11x2) matrix (N+1 elements of size 2x1, transposed)<br>
$G_w$ a (22x11) matrix<br>
$G^T_w$ a (11x22) matrix<br>
$I_q$ a (22x22) matrix<br>
$F$ a (22x2) matrix<br>
$G_r$ a (22x22) matrix    </p>

<p>so Lz computation gives (according to the matrix sizes)<br>
$L_z=(scalar + (11x22)(22x22)(22x11))^{-1} (11x22)(22x22)(22x22)$<br>
a (11x2) matrix.<br>
as $z_k$ is (2x1) matrix, doing $L_zz_k$ from (24) is fine.  </p>

<p>and Lr computation gives (according to the matrix sizes)
$L_r=(scalar + (11x22)(22x22)(22x11))^{-1} (11x22)(22x22)((22x22) - (22x22))$<br>
a (11x22) matrix.<br>
as $R_k$ is (11x2) matrix, doing $L_rR_k$ from (24) is not possible.<br>
I have a (11x22) matrix multiplicated by a (11x2) matrix.</p>

<p>I'm sure I'm missing something big here but unable to see what exactly.
Any help appreciated.</p>

<p>Thanks</p>
","mobile-robot navigation"
"9621","Electric vs. internal combustion engine for propulsion","<p>What are the main differences between electric motor and internal combustion engine for an ATV-sized mobile robot platform in terms of functionality, implementation difficulty (""RC"" conversion, ""electronic"" operation), durability and maintenance when used as an autonomous platform? A full sized ATV/UTV like Polaris Ranger (EV) is in question.</p>

<p>Are the advantages/disadvantages basically the same as the differences between electric and nitro RC cars or does the bigger scale adds something important to the game? I can think of the main differences like bigger range and faster ""refueling"" with IC and less maintenance with electric but I am interested in a detailed comparison.</p>

<p>The transmission for the IC engine is considered to be automatic.</p>

<p>EDIT: The fuel injection for IC is considered to be electronic (EFI) but I do not know whether that also means the ""electronic"" throttle (no mechanical wire as with carburetor?). Whatever the throttle may be I see the lag between its ""actuation"" and the engine running into higher RPM and giving more power/speed as the main disadvantage for IC control - however, it may probably be quite easy dealt with in software (by adding some timeout when checking desired RPM).</p>
","mobile-robot electronics engine electric propulsion"
"9622","inverse kinematics for 6 jointed robots","<p>I am a  uncertain about how to compute the right homogeneous transformation matrix to compute an inverse kinematic Q-configuration. </p>

<p>Looking at robot like this <a href=""http://i.stack.imgur.com/cuV2q.jpg"" rel=""nofollow""><img src=""http://i.stack.imgur.com/cuV2q.jpg"" alt=""enter image description here""></a></p>

<p>Where at the end of this robot I have a camera mounted on to it. </p>

<p>The purpose of my application is to make the robot follow an object, so basically tracking it.  The camera provide me with an X,Y,Z coordinate, which the position i want place my robot arm. </p>

<p><strong><em>First question - How do i set up the desired homogenous transformation matrix?</em></strong></p>

<p>The way i see it, I have 2 transformation matrices being <code>T_tool_base</code> and <code>T_world_tool</code> which become <code>T_world_base = (T_tool_base) (T_world_tool)</code></p>

<p>My question is that how do i compute my desired transformation matrix. 
I think i know how i should setup the transformation matrix for the camera which would be like this</p>

<p>T_world_tool = <code>0 0 0 x
               0 0 0 y
               0 0 0 z
               0 0 0 1</code></p>

<p>(Second question is regarding the rotation matrix, how do prescribe such that rotation in arbitrary as long the endpoint has the desired position in the world frame?)</p>

<p>but what should t_tool_base entail? should it entail the transformation of its current state or the desired transformation, and if so how do i extract the desired t_tool_base transformation?...</p>
","robotic-arm kinematics inverse-kinematics"
"9625","Bayesian filter for 2-D grid localizaton","<p>I have some data obtained from an experiment in terms of movements and observations with odometry and sensor data. My task is to find the probability mass on each of the grid cells after each set of motion and observation. I'm a bit lost in figuring out how to compute probability mass for each of the grid cell.
My odometry information is in terms of rotation, translation and rotation and my sensor information is in terms of range and bearing angle. 
How do I calculate the probability of robot present in each of the grid cell?</p>

<p>I have the formula for belief after motion as summation(P(x|u, x')xBel(x'))
How do I compute the motion model with noise?</p>
","localization filter"
"9629","Quadcopter refuses to fly when the Yaw PID component is added","<p>Good day,</p>

<p>I would like to ask why is it that when I add the Yaw control to my PID controller for each motor. The quadcopter refuses to take off or maintain its altitude. I am curently using a Cascaded PID controller for attitude hold using an Accelerometer, a Magnetometer and a Gyroscope, and a 40Hz Ultrasonic Sensor for Altitude Hold. Since the scope is indoor I have done away with the barometer due to its +-12m error. </p>

<p><strong>Resulting Response</strong></p>

<p>Without Yaw Control, the plot below shows the response of the quadrotor.
<a href=""http://i.stack.imgur.com/4WSGy.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/4WSGy.png"" alt=""enter image description here""></a></p>

<p>With Yaw Control, the plot below shows the response of the quadrotor.
<a href=""http://i.stack.imgur.com/ow2tB.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/ow2tB.png"" alt=""enter image description here""></a></p>

<p><strong>Debugging</strong></p>

<p>I found out that each of the outputs from each PID's give a too high of a value such that when summed together goes way over the PWM limit of 205 or Full Throttle.</p>

<ol>
<li><p>Without yawPID contribution
The limiter kicks in without damaging the desired response of the system thus is still able to fly albeit with oscillatory motion along the z axis or height
<a href=""http://i.stack.imgur.com/6w8rv.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/6w8rv.png"" alt=""enter image description here""></a>
<a href=""http://i.stack.imgur.com/xaSx3.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/xaSx3.png"" alt=""enter image description here""></a></p>

<ol start=""2"">
<li>With yawPID contribution
The added yaw components increases the sum of the PID's way above the limit thus the limiter compesates the excess too much resulting in an over all lower PWM output for all motors thus the quad never leaves the ground.
<a href=""http://i.stack.imgur.com/fli5D.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/fli5D.png"" alt=""enter image description here""></a>
<a href=""http://i.stack.imgur.com/9GvBM.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/9GvBM.png"" alt=""enter image description here""></a></li>
</ol>

<p>//Motor Front Left (1)
float motorPwm1 =  pitchPID + rollPID + yawPID + baseThrottle + baseCompensation;</p>

<p>//Motor Front Right (2)
float motorPwm2 =  pitchPID - rollPID - yawPID + baseThrottle + baseCompensation; </p>

<p>//Motor Back Left (3)
float motorPwm3 = -pitchPID + rollPID - yawPID + baseThrottle + baseCompensation; </p>

<p>//Motor Back Right (4)
float motorPwm4 = -pitchPID - rollPID + yawPID + baseThrottle + baseCompensation;</p></li>
</ol>

<p><strong>Background</strong></p>

<p>The PID parameters for the Pitch, Yaw and Roll were tuned individually meaning, the base throttle was set to a minimum value required for the quadcopter to be able to lift itself.</p>

<p>The PID parameters for the Altitude Sensor is tuned with the other controllers active (Pitch and Roll).</p>

<p><strong>Possible Problem</strong></p>

<ol>
<li>Limiter algorithm</li>
</ol>

<p>A possible problem is that the algorithm I used to limit the maximum and the minimum throttle value may have caused the problem. The following code is used to maintain the ratio of the motor values instead of limiting them. The code is used as a two stage limiter. In the 1st stage, if one motorPWM is less than the set baseThrottle, the algorithm increases each motor PWM value until none of them are below that. In the 2nd stage, if one motorPWM is more than the set maxThrottle, the algorithm decreases each motor PWM value until none of them are above that. </p>

<pre><code>//Check if PWM is Saturating - This method is used to fill then trim the outputs of the pwm that gets fed into the gpioPWM() function to avoid exceeding the earlier set maximum throttle while maintaining the ratios of the 4 motor throttles. 

float motorPWM[4] = {motorPwm1, motorPwm2, motorPwm3, motorPwm4};
float minPWM = motorPWM[0];
int i;
for(i=0; i&lt;4; i++){ // Get minimum PWM for filling
    if(motorPWM[i]&lt;minPWM){
        minPWM=motorPWM[i];
    }
}

cout &lt;&lt; "" MinPWM = "" &lt;&lt; minPWM &lt;&lt; endl;

if(minPWM&lt;baseThrottle){
    float fillPwm=baseThrottle-minPWM; //Get deficiency and use this to fill all 4 motors
    cout &lt;&lt; "" Fill = "" &lt;&lt; fillPwm &lt;&lt; endl;
    motorPwm1=motorPwm1+fillPwm;
    motorPwm2=motorPwm2+fillPwm;
    motorPwm3=motorPwm3+fillPwm;
    motorPwm4=motorPwm4+fillPwm;
}

float motorPWM2[4] = {motorPwm1, motorPwm2, motorPwm3, motorPwm4};
float maxPWM = motorPWM2[0];
for(i=0; i&lt;4; i++){ // Get max PWM for trimming
    if(motorPWM2[i]&gt;maxPWM){
        maxPWM=motorPWM2[i];
    }
}

cout &lt;&lt; "" MaxPWM = "" &lt;&lt; maxPWM &lt;&lt; endl;

if(maxPWM&gt;maxThrottle){
    float trimPwm=maxPWM-maxThrottle; //Get excess and use this to trim all 4 motors
    cout &lt;&lt; "" Trim = "" &lt;&lt; trimPwm &lt;&lt; endl;
    motorPwm1=motorPwm1-trimPwm;
    motorPwm2=motorPwm2-trimPwm;
    motorPwm3=motorPwm3-trimPwm;
    motorPwm4=motorPwm4-trimPwm;
}
</code></pre>

<p>This was obtained from pixhawk. However the difference is that they employ only upper bound compensation limiting, while mine also performs lower bound compensation limiting which may cause more saturation once it reaches the second stage.</p>

<p><a href=""http://i.stack.imgur.com/Rt7rZ.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/Rt7rZ.png"" alt=""enter image description here""></a>
From:<a href=""https://pixhawk.org/dev/mixing"" rel=""nofollow"">https://pixhawk.org/dev/mixing</a></p>

<ol start=""2"">
<li>Gains are set too high.</li>
</ol>

<p>It is also possible that I've set my P gains too high thus exceeding the max RPM limit of the motors causing the Limiter algorithm to overcompensate.</p>

<p><strong>Current PID Settings:</strong></p>

<p>The minimum motor value for the quad to lift itself is 160 while the maximum limit is 200 from the PWM time high of 2000ms </p>

<ol>
<li>Pitch (Cascaded P-PID controller)
Rate P = 0.07
Rate I = 0.03
Rate D = 0.0001
Stabilize P = 2</li>
<li>Roll (Cascaded P-PID controller)
Rate P = 0.09
Rate I = 0.03
Rate D = 0.0001
Stabilize P = 2</li>
<li>Yaw (Cascaded P-PID controller)
Rate P = 0.09
Rate I = 0.03
Rate D = 0.0001
Stabilize P = 2</li>
<li>Hover (Single loop PD controller)
P = 0.7
D = 35</li>
</ol>

<p><strong>Possible Solution</strong></p>

<p>I think I have set the PID parameters particularly the P or D gain too high that the computed sum of the outputs of the controller is beyond the limit. Maybe retuning them would help.</p>

<blockquote>
  <p>I would just like to ask if anyone has encountered this problem or if you have any suggestions. Thank you :) </p>
</blockquote>

<h2><strong>EDIT</strong></h2>

<p>I have added the plots of the response when the control loop is fast (500Hz) and Slow (300Hz)</p>

<blockquote>
  <p>500Hz: Does not fly
  <a href=""http://i.stack.imgur.com/OzU9n.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/OzU9n.png"" alt=""enter image description here""></a></p>
  
  <p>300Hz: Flies
  <a href=""http://i.stack.imgur.com/5MZ6j.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/5MZ6j.png"" alt=""enter image description here""></a></p>
</blockquote>
","quadcopter control pid raspberry-pi stability"
"9636","Regarding Kalman filter and estimating heading with magnetic compass","<p>I have trouble estimating the heading when close to the ""pivot"" point of the compass, and could use some input on how to solve it. I have set up my angles to be from 0-360 degrees during the testing but will be using radians (-pi, pi) from now on.</p>

<p>The setup is a differential robot with IMU, wheel encoders and a magnetic compass. </p>

<p>A complementary filter is used for fusing gyroZ and odo measurements to estimate the heading, and then correct it with a Kalman filter using the magnetic compass.</p>

<p>My problem occurs when the robot heading is close to -pi/pi .</p>

<p>The estimated heading is useless even though the robot is not even moving.</p>

<p>I am thinking this must be a very common problem and probably has a better solution than what I came up with, which was either re-initializing the integrator when crossing zero, adding 180 degrees each time the error is larger, or just ignoring the compass if the error is too large...</p>

<p>It's my first Kalman filter so I may have made a poor implementation if this is not a common issue...</p>

<p>Edit: trudesagen's solution solved my problem.</p>
","mobile-robot kalman-filter compass"
"9637","How do we write a STOP to a continuous Servo?","<p>I'm using processing to send strings to Arduino, using functions like </p>

<pre><code>else {
    int u=90;
    port.write(u+""z"");
}
</code></pre>

<p>on the processing side and in the Arduino side I'm using calls like </p>

<pre><code>  case 'z':
    z.write(v);
    v = 0;
    break;
  case 'L':
    z.write(0);
    //v = 0;
    break;
}
</code></pre>

<p>yet I can't get the servo to stop at all. How do I make it shut off?</p>

<p>If it was a regular servo I wouldn't even ask because that's easy but I write 0 or 90 or LOW and nothing, it just keeps spinning in one direction but when it meets one of the conditions in my statements it switches polarity/direction and that's good - I want that but I made this function to make it stop and it is not doing so, does anyone have any ideas ?</p>

<p>I am using a <a href=""https://www.parallax.com/sites/default/files/downloads/900-00008-Continuous-Rotation-Servo-Documentation-v2.2.pdf"" rel=""nofollow"">Parallax Continuous Rotation Servo</a>.</p>
","arduino"
"9640","Steadier wheels - Pin them or lock springs","<p>When running on a hard surface, the Create will shake sometimes during turns or acceleration.</p>

<p>Has anyone ever removed the springs or pinned the wheels in place so they can't move up and down?</p>
","irobot-create roomba"
"9642","What's the difference between a holonomic and a nonholonomic system?","<p>I was wondering if a 1D point mass (a mass which can only move on a line, accelerated by an external time-varying force, see <a href=""https://en.wikipedia.org/wiki/Double_integrator"" rel=""nofollow"">Wikipedia - Double integrator</a>) is a holonomic or a nonholonomic system? Why?</p>

<p>I think that it is nonholonomic since it cannot move in any direction in its configuration space (which is 1D, just the $x$ axis). E.g. if the point mass is moving at $$x=10$$ with a velocity of 100 m/s in positive $x$-direction it cannot immediately go to $$x=9.9$$ due to its inertia. However, I have the feeling that my thoughts are wrong...</p>

<p>The background is the following:</p>

<p>I am trying to understand what holonomic and nonholonomic systems are. What I found so far:</p>

<p><em>Mathematically</em>:</p>

<ul>
<li>Holonomic system are systems for which all constraints are integrable into positional constraints.</li>
<li>Nonholonomic systems are systems which have constraints that are nonintegrable into positional constraints. </li>
</ul>

<p><em>Intuitively</em>:</p>

<ul>
<li>Holonomic system where a robot can move in any direction in the configuration space. </li>
<li>Nonholonomic systems are systems where the velocities (magnitude and or direction) and other derivatives of the position are constraint. </li>
</ul>
","dynamics movement"
"9646","Forward kinematics with DH parameters","<p>I just started learning robotics at school and I have some problems to solve forward kinematics with DH parameters. I don't really understand how I can get them from the image. I would appreciate if somebody could help me with it.</p>

<p><a href=""http://i.stack.imgur.com/pT1cx.jpg"" rel=""nofollow""><img src=""http://i.stack.imgur.com/pT1cx.jpg"" alt=""Graph of forward kinematics with DH parameters""></a>. </p>
","forward-kinematics dh-parameters"
"9647","What rating Li-Po battery should I get for this configuration?","<p>I will have this configuration: </p>

<ul>
<li>A2212 Brushless Motor 1000KV - 4 each</li>
<li>ECS - 30A Electronic Speed Control (ESC) - 4 each </li>
<li>Propeller - 1045 Propeller CW &amp; CCW Pair 10 inch * 4.5 pitch</li>
<li>Arduino Mega - 2560 board</li>
<li>Raspberry Pi 3 </li>
<li>Open pilot CC3D flight controller </li>
</ul>

<p>I want to know what rating Li-Po battery should I get for this configuration.
The reason behind my asking here is because a simple google search is not able to satisfy me with an explanation...</p>

<p>Also, my weight will be 1.5 kg for the quadcopter, so I need a stable current discharge.</p>

<p>This is my first quadcopter, I am a Computer Science guy, so I have little knowledge of electronics, I'm learning, but need help...</p>
","quadcopter arduino raspberry-pi battery"
"9652","Smart Home Model with Raspberry Pi","<p>I'm still new to RPi and I am currently trying to do a smart home model. </p>

<p>I planned to use RPi only to control 5 servos (which will be controlling the open/close of the doors by setting the angle) and 5 LEDs. </p>

<p>Will I need to use an external circuit to supply the power for the servos or is it fine to just connect them to the RPi?</p>
","raspberry-pi rcservo"
"9654","How to know the payload of the chassis from its motors?","<p>I'm doing a mobile robot project with robotic arms, I wanted to buy a chassis for my robot that can carry enough weight, but many websites don't give definitive answers about maximum payload.</p>

<p>Is there is a way to figure this out just by knowing details about the motors?</p>
","mobile-robot"
"9657","Dynamic Model of a Manipulator","<p>I'm stuck on equation 4.30 of page 176 in<br>
<a href=""http://www.cds.caltech.edu/~murray/books/MLS/pdf/mls94-complete.pdf"" rel=""nofollow"">http://www.cds.caltech.edu/~murray/books/MLS/pdf/mls94-complete.pdf</a></p>

<p>This equation:<br>
$\frac {\partial M_{ij}} {\partial \theta_k} = \sum_{l=\max(i,j)}^n \Bigl( [A_{ki} \xi_i, \xi_k]^T A_{lk}^T {\cal M}_l' A_{lj} \xi_j + \xi_i^T A_{li}^T {\cal M}_l' A_{lk} [A_{kj} \xi_j, \xi_k] \Bigr)$  </p>

<p>seems impossible to process because it requires adding a 2x1 to a 1x2 matrix.
going by ROWSxCOLUMNS notation. Matrices M and A are 6x6 and $\xi$ is a 6x1, so how does this addition statement fit the rules of matrix addition?  This must be my mistake, I just don't see how.</p>
","dynamics matlab"
"9659","how to find maximum force of a robot joint","<p>I want to know if there is any equation that calculates the maximum force of a robot joint. The force that we should not exceed.</p>

<p>For example in human leg, if we apply a big external force to the knee, it will break. now how can i find the necessary force that will just make the leg move without breaking the knee.</p>

<p>I have a programme that generates robot morphologies randomly with different sizes, so I have to know the force to not exceed for each joint. I think this depend on weight, mass, inertia of each robot part.</p>

<p>I can not do this by trial and error because I have hundreds different morphologies.</p>

<p>This <a href=""https://www.youtube.com/watch?v=arEty0NlJds"" rel=""nofollow"">video</a> shows the behaviour of robot if I apply a big force. It is in Gazebo robotic simulator.</p>

<p>Thanks in advance!</p>
","simulation joint force"
"9662","Implementing an analytic version of an inverse kinematic","<p>People have recommended me implement an analytic version of inverse Jacobian solver, such that I won't be forced only the least square solution, but would have an local area of solution near to the one I desire. </p>

<p>I can't seem to implement it correctly, I mean how much does it differ from the least square inverse kinematics which I have implemented here?</p>

<pre><code>Eigen::MatrixXd jq(device_.get()-&gt;baseJend(state).e().cols(),device_.get()-&gt;baseJend(state).e().rows());
      jq = device_.get()-&gt;baseJend(state).e(); //Extract J(q) directly from robot


      //Least square solver - [AtA]⁻1AtB

      Eigen::MatrixXd A (6,6);
      A = jq.transpose()*(jq*jq.transpose()).inverse();



      Eigen::VectorXd du(6);
      du(0) = 0.1 - t_tool_base.P().e()[0];
      du(1) = 0 - t_tool_base.P().e()[1];
      du(2) = 0 - t_tool_base.P().e()[2];
      du(3) = 0;  // Should these be set to something if i don't want the tool position to rotate?
      du(4) = 0;
      du(5) = 0;

      ROS_ERROR(""What you want!"");
      Eigen::VectorXd q(6);
      q = A*du;


      cout &lt;&lt; q &lt;&lt; endl; // Least square solution - want a vector of solutions. 
</code></pre>

<p>I want a vector of solution - how do I get that?</p>

<p>the Q is related to this <a href=""http://robotics.stackexchange.com/questions/9672/how-do-i-construct-i-a-transformation-matrix-given-only-x-y-z-of-tool-position"">How do i construct i a transformation matrix given only x,y,z of tool position?</a></p>

<p>The robot being used is a UR5  - <a href=""https://smartech.gatech.edu/bitstream/handle/1853/50782/ur_kin_tech_report_1.pdf"" rel=""nofollow"">https://smartech.gatech.edu/bitstream/handle/1853/50782/ur_kin_tech_report_1.pdf</a></p>
","robotic-arm inverse-kinematics industrial-robot c++"
"9668","Directly observing lidar laser rays","<p>I am working with SICK lidars and have to mount/unmount them quite often on my robot. The mounting process is very tedious especially when it comes to making sure that the lidars are horizontal. I thought about using IR goggles (like the night vision ones) and some fog machine (like the one in nightclubs) in order to see the surface covered by the lidar's rotating laser ray. As a result I would expect to see something <a href=""https://www.youtube.com/watch?v=ufoDT2IDU58"" rel=""nofollow"">like this</a> but planar. </p>

<p>Before thinking about trying to get my hands on such hardware I wanted to ask:</p>

<p>Do sick laser have enough intensity to be observed by such goggles?
Does anybody tried such an approach?</p>
","laser lidar"
"9669","What type of actuator should I use?","<p>I need to find out if There is a way to get at least 60 Hz of linear Motion with at least 5 mm of stroke that I intend to make linear persistence of vision device(not rotating one)It must be small and light as possible. ( maybe 50 mm long and 10-15 mm diameter or around these) (less than 500 grams) The Load will be around 50 grams.  There are voice coils that is very expensive, can I use solenoids for instance or what do you recommend? </p>

<p>Thanks </p>
","actuator motion"
"9673","Is there a ""follow me"" Roomba/Create that works like a BEAMBot?","<p>The diagram below shows an old BEAMBot strategy:</p>

<p><a href=""http://youtu.be/DNj958sV0-U"" rel=""nofollow""><img src=""http://i.stack.imgur.com/AOmPx.png"" alt=""Block diagram of IR based following circuit""></a>   </p>

<p>Is there code or an example using this method? I would rather avoid OpenCV, ultrasonic, GPS etc. I just want the Roomba wheels to react as I go straight, turn left or right. Finally, I could add a front wheel on a servo and try having the Roomba turn with me. </p>

<p>Also has anybody added big, all terrain wheels to a Roomba to replace the originals?</p>
","control irobot-create roomba"
"9675","Suitable uC for atonomous robot","<p>I am going to build an autonomous robot with Kalman-filter for localization integrated from Lidar, encoder, IMU and GPS. I will also add obstacle avoidance while moving to the required position.</p>

<p>Is the ATmega32 8-bit suitable for that (or Arduino Mega) or do I have to use avr32, ARM, or PIC32 and which is better?</p>
","mobile-robot arduino localization microcontroller"
"9677","Kuka KR16L-2 robot simulation base and wrist rotation inconsistent with original robot","<p>The issue is regarding simulation of Kuka robot KR16L6-2 in <strong>MATLAB using Robotics toolkit by Peter Corke</strong>. I wish to simulate  the kinematic before passing a command to real robot for motion.</p>

<p>I have attached the DH-Parameters. Apart from this I have also tried many other combination of orientations but to no useful effect.</p>

<p><a href=""http://i.stack.imgur.com/S81q2.jpg"" rel=""nofollow""><img src=""http://i.stack.imgur.com/S81q2.jpg"" alt=""DH parameters and code""></a></p>

<p>The problems is that the robot base rotates counter-clockwise by default for positive increases in <code>Joint1</code>, while the original robot moves in the opposite direction. Similarly for the wrist roll, i.e. Joint 4, the direction of simulation is reversed.</p>

<p>In order to confirm that it's not my mistake only, I searched for similar ready made simulation software. Although it did not include the same KUKA robot, a similar variant (KUKA_KR_5_sixx_R650) was available. Hence, KUKA_KR_5_sixx_R650 had one set of motions for base and wrist in <strong><em>RoKiSim v1.7</em></strong> for positive increases in joint angle and reverse motion in <strong><em>roboanalyzerv7</em></strong>  .</p>

<p><a href=""http://i.stack.imgur.com/8vWCo.jpg"" rel=""nofollow""><img src=""http://i.stack.imgur.com/8vWCo.jpg"" alt=""Screenshots of Robot Simulation of KUKA_KR_5_sixx_R650""></a></p>

<p><strong><em>NOTE:</em></strong> Only the rotation of J1 (base) and J4 (Wrist Roll) are reversed
and I want to recreate the results of RoKiSim v1.7 in Matlab where rotations are similar to the real world robot spec provided by KUKA.</p>
","matlab industrial-robot simulation kuka"
"9680","Getting “rospack package not found error” in ROS","<p>I created a package in catkin workspace and put a publisher.py node inside the src directory of package which worked fine. Then i added another node subscriber.py node and used catkin_make to build. Now when I try to run any of the nodes or find package i am getting above error. Am I missing any step ?</p>

<p>Thanks.</p>
","ros"
"9682","What frames are supported by the Dynamixel XL-320 OLLO?","<p>I was recently looking into purchasing either a Dynamixel AX-12A or XL-320. The XL seems to use OLLO frames, which only seem to be available in a toy-like set. </p>

<p>I was wondering if there are any other frames available or if I should just get an AX-12?</p>
","servos walking-robot dynamixel"
"9683","Choosing the right Mecanum wheel","<p>I am part of my college robotics team which is preparing for Robocon 2017.</p>

<p>We have used Mecanum wheels in last Robocon competition, but we have faced huge slip and vibration. I have looked for all kinematic and dynamic formulas and all stuff about Mecanum wheels, but still can't get to a conclusion for my problem.</p>

<p><a href=""https://drive.google.com/file/d/0BwdLwLAVvVLGdFBLS3RSdlN4dXc/view"" rel=""nofollow"">Video of the problem</a></p>

<p>The robot is around 25 kg and the Mecanum wheel diameter is about 16 cm with 15 rollers (single type). Please help me why it happened like that!?</p>

<p>Also suggest me what to do now - Should I design a new Mecanum wheel or bring it from market? </p>

<p>If I should design, what parameters should I consider, and please help me how to design in CAD software like SolidWorks? And then, shall I give it to 3D printing?</p>

<p>If I should buy directly from market, where should I buy?</p>
","design wheel"
"9684","Cannot disable sleep in passive mode for iRobot Create 2","<p>I tried to disable sleep by pulsing the BRC pin low for one second every minute as suggested in the OI, but my Create 2 still goes to sleep after 5 minutes.</p>

<p>My firmware is r3_robot/tags/release-3.2.6:4975 CLEAN</p>

<p>The Create 2 is connected to an Arduino, and the BRC is driven by one of the Arduino pins.  I verified on a DMM that the voltage is indeed toggling.  I am able to both send and receive serial data between the Arduino and Create2.</p>

<p>Pseudo-code:</p>

<ol>
<li><p>Initialize roomba.  Connect serial at 115200 baud.  Toggle BRC: high for 200 ms, low for 200 ms, then high again.  Leave it high.</p></li>
<li><p>Ask roomba to stream sensor data in passive mode.  Wait 1 second after BRC toggle to give some extra time to wake-up.  Then send opcode 7 (reset), wait for reset message to complete by looking for the last few characters, then wait another second for good measure.  Next, send opcode 128 (start into passive mode), wait 100 ms to let opcode stick, then ask for stream of data (opcode 148 followed by number of packet IDs and the packet IDs themselves).</p></li>
<li><p>Main loop: Echo data from Create2 to the serial-USB output of the Arduino so that I can view the Create2 data.  The data sent by the Create2 look valid (good checksum) and are sent in the expected time interval of ~15 ms.  The main loop also toggles the BRC low for 1 second every minute.</p></li>
</ol>

<p>For the full gory details, the complete Arduino sketch is shown below</p>

<pre><code>const uint8_t brcPin = 2; // Must keep this low to keep robot awake
long last_minute = 0;
long minute = 0;

// Initialize roomba
void roomba_init()
{
  Serial3.begin(115200); // Default baud rate at power up
  while (!Serial3) {}    // Wait for serial port to connect

  // BRC state change from 1 to 0 = key-wakeup
  // keep BRC low to keep roomba awake
  pinMode(brcPin, OUTPUT);
  Serial.println(""BRC HIGH"");
  digitalWrite(brcPin, HIGH);
  delay(200);  // 50-500 ms

  Serial.println(""BRC LOW"");
  digitalWrite(brcPin, LOW);
  delay(200);

  Serial.println(""BRC HIGH"");
  digitalWrite(brcPin, HIGH);
  last_minute = millis()/60000;

  delay(1000);  // give some extra time to wake up after BRC toggle.

  Serial.println(""Opcode 7: reset robot"");
  Serial3.write(7);      // Reset robot
  // Discard roomba boot message
  // Last part of reset message has ""battery-current-zero 257""
  char c = 'x';
  Serial.println(""Gimme a z!"");
  while (c != 'z') {
    if (Serial3.available() &gt; 0) {c = Serial3.read(); Serial.write(c);}
  }
  Serial.println(""Gimme a e!"");
  while (c != 'e') {
    if (Serial3.available() &gt; 0) {c = Serial3.read(); Serial.write(c);}
  }
  Serial.println(""Gimme a r!"");
  while (c != 'r') {
    if (Serial3.available() &gt; 0) {c = Serial3.read(); Serial.write(c);}
  }
  Serial.println(""Gimme a o!"");
  while (c != 'o') {
    if (Serial3.available() &gt; 0) {c = Serial3.read(); Serial.write(c);}
  }
  // Flush remaining characters: 32 50 53 54 13 10 or "" 257\r\n""
  Serial.println(""Gimme a newline!"");
  while (c != 10) {
    if (Serial3.available() &gt; 0) {c = Serial3.read(); Serial.write(c);}
  }
  delay(1000);  // allow extra time for opcode 7 to stick

  Serial.println(""\nOpcode 128: start OI in passive mode"");
  Serial3.write(128);   // Start the Open Interface.  Passive mode. 
  delay(100);           // Allow some time for opcode 128 to stick (not sure if this is needed)
  Serial.println(""Opcode 148: stream data packets"");
  Serial3.write(148);   // Stream data packets (every 15 ms)
  Serial3.write(16);    //   Number of packet IDs
  Serial3.write(8);     //   Packet ID 8 = wall                       1 byte
  Serial3.write(9);     //   Packet ID 9 = cliff left                 1
  Serial3.write(10);    //   Packet ID 10 = cliff front left          1
  Serial3.write(11);    //   Packet ID 11 = cliff front right         1
  Serial3.write(12);    //   Packet ID 12 = cliff right               1
  Serial3.write(13);    //   Packet ID 13 = virtual wall              1
  Serial3.write(27);    //   Packet ID 27 = wall signal               2
  Serial3.write(28);    //   Packet ID 28 = cliff left signal         2
  Serial3.write(29);    //   Packet ID 29 = cliff front left signal   2
  Serial3.write(30);    //   Packet ID 30 = cliff front right signal  2
  Serial3.write(31);    //   Packet ID 31 = cliff right signal        2
  Serial3.write(41);    //   Packet ID 41 = velocity right            2
  Serial3.write(42);    //   Packet ID 42 = velocity left             2
  Serial3.write(43);    //   Packet ID 43 = encoder counts left       2
  Serial3.write(44);    //   Packet ID 44 = encoder counts right      2
  Serial3.write(45);    //   Packet ID 45 = light bumper              1
}

void setup() {
  // Open serial communications (through USB interface)
  // The serial output of the Create 2 is echoed from Serial3 to Serial
  // so that we can observe the Create 2 serial output on a computer.
  Serial.begin(115200);
  while (!Serial) {}   // Wait for serial port to connect
  Serial.println(F(""Starting roomba test...\n""));

  // Roomba serial commmunications
  Serial.println(F(""Initializing comm to Roomba\n""));
  roomba_init();
}

long low_start_time;
boolean brc_is_low;
void loop() {
  // Read from Serial3 and echo results to Serial
  if (Serial3.available()) {
    uint8_t b = Serial3.read();
    uint8_t checksum = 19;
    if (b==19) { // First byte of reply stream is 19
      Serial.print(""\nStart at "");
      Serial.println(millis());
      Serial.print(b); Serial.print("" "");
      while (Serial3.available() &lt; 43) {}  // Wait for rest of data (buffer is 64 bytes)
      for (int I=0; I&lt;43; I++) {
        b = Serial3.read();
        Serial.print(b); Serial.print("" "");
        checksum += b;
      }
      Serial.print(""Chksum "");
      Serial.println(checksum);  // 0 is good
    } else {
      // Probably an ascii message
      //Serial.write(b);
      Serial.print(b); Serial.print("" "");
    }
  }

  // Pulse BRC low every minute for 1 second
  long now = millis();
  long minute = now/60000;
  if (minute != last_minute) {
    Serial.println(""\n\nBRC LOW"");
    Serial.println(millis());
    digitalWrite(brcPin, LOW);

    last_minute = minute;
    low_start_time = now;
    brc_is_low = true;
  }

  // 1 s low pulse width
  if ((now &gt; low_start_time + 1000) &amp;&amp; brc_is_low) {
      Serial.println(""\n\nBRC HIGH"");
      Serial.println(millis());
      digitalWrite(brcPin, HIGH);
      brc_is_low = false;
  }  
}
</code></pre>
","irobot-create"
"9686","Performing inverse kinematics based on a displacement of the end effector?","<p>I think i have an simple problem, but can't my head around how i should resolve it...</p>

<p>My setup looks like this: </p>

<p><a href=""http://i.stack.imgur.com/fMAEU.jpg"" rel=""nofollow""><img src=""http://i.stack.imgur.com/fMAEUm.jpg"" alt=""enter image description here""></a>
<a href=""http://i.stack.imgur.com/BbdFh.jpg"" rel=""nofollow""><img src=""http://i.stack.imgur.com/BbdFhm.jpg"" alt=""enter image description here""></a></p>

<p>The grey box on end effector is supposed to be an camera, which measures a dx,dy,dz between the object and the camera. These are used to  position the camera such that dz between the object and the camera is equal to 0.5, and dx = dy = 0. </p>

<p>I know that I using inverse kinematics can determine the Q which positions it according the given rotation and position, but what if I only provide it a position only?</p>

<p>How do extract all Q that make dx = dy = 0, and dz = 0.5, while keeping the object in sight at all time?</p>

<p>An example could be if an object was placed just above the base (see second image), it should then find all possible configurations which in this case would consist of the arm rotating around the object, while the camera keeps the object in sight...</p>

<p><strong>Update</strong></p>

<p>I just realized a possible solution would be to create a sphere with the object in centrum  an radius of dz, and then use this sphere to extract all pairs of rotations and position... But how would one come by with such an solution?</p>
","robotic-arm inverse-kinematics stereo-vision"
"9691","Generate transformation matrices for rotating around a object?","<p>How do i compute all transformation matrices which places a robot endeffector at the shell of this sphere, with the end effector pointing toward the object in the center. </p>

<p><a href=""http://i.stack.imgur.com/fSeyk.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/fSeyk.png"" alt=""enter image description here""></a></p>

<p>I know at all time how far the object is relative to the endeffector, and radius of the sphere is the desired distance i want between the object and endeffector.  </p>

<p>I want by using inverse kinematics pan around this object in a sphere shaped trajectory. </p>

<p>Each transformation matrix should contain different positions on the sphere and the rotation should be oriented such that the arm looks at the object. </p>

<p>The position should be relative easy to compute, as i already know the distance to to object, and radius of the sphere. </p>

<p>But the rotation matrix for each position is still a mystery for me.  </p>
","robotic-arm rotation"
"9696","Vector Field Histogram: Is it possible to generate an occupancy grid without position feedback?","<p>I am currently working on an autonomous quadcopter project using stereo vision for obstacle detection. I am planning to use VFH+ for 2D trajectory planning, meaning movements of the quadcopter are only available on the X and Y axes and no movement is permitted along the Z axis or Altitude.</p>

<p>I currently have no methods implemented on position tracking. Accelerometers have been known to generate a lot of errors from integration. I tried to look for optical flow sensors for low computation needed however have found no luck as most of them are out of stock (probably due to company change (Agilent/Avago to Pixart)). </p>

<p>From what I understand, when the after the certainty/occupancy grid is constructed and reduced to a polar coordinate system, the robot's heading is aligned to the computed unblocked sector.</p>

<blockquote>
  <p>Is it possible to generate the occupancy grid without position feedback?</p>
</blockquote>

<p>I am planning to run the quadcopter at a slow constant velocity due to the data throughput of the raspberry pi stereo vision system of 2Hz. I plan to use the obtained angle direction from the VFH algorithm and align the quadcopter's heading.</p>

<p>The motor control scheme to be used for the quadcopter is mostly yaw and pitch based. Yaw for heading control and pitch for forward control.</p>

<p>Two raspberry pi b+'s are used. One for motor control which is currently running the PID control loop at 530Hz. Another for Stereo vision which is currently running at 2Hz.</p>
","quadcopter motion-planning stereo-vision vector-field-histogram vfh"
"9697","How to determine the angles between a UAV and a sphere","<p>I have an UAV modeled in three dimensions with let's say position coordinates $p_{uav} = (x_1,y_1,z_1)$ that is moving in a direction $d = (d_x,d_y,d_z)$ and a moving obstacle modeled as a sphere with known centre coordinates $p_{sph}=(x_2,y_2,z_2)$ and radius $ r_{sph}$. </p>

<p>If I have a plane $p$  in the direction of movement of the UAV that intersects the sphere, I want to be able to calculate the angles with respect to the vehicle's movement formed by the tangents to the sphere in the plane $ p$. In the figure, I would like to know how to calculate the angles $α_1$ and $α_2$.</p>

<p><a href=""http://i.stack.imgur.com/5vsYb.jpg"" rel=""nofollow""><img src=""http://i.stack.imgur.com/5vsYb.jpg"" alt=""Problem in three dimensions""></a></p>

<p>If it helps, what I am looking is an extension in three dimensions for this:</p>

<p><a href=""http://i.stack.imgur.com/cGR1n.jpg"" rel=""nofollow""><img src=""http://i.stack.imgur.com/cGR1n.jpg"" alt=""easier problem in 2d""></a></p>

<p>Which is a vehicle in two dimensions ;it is obviously an easier problem which requires only the centre of the circle. However I am not really sure how to make it work in 3D, as supposedly the plane can intersect the sphere at any two points, not necessarily the centre. </p>

<p>Thanks in advance for your help.</p>
","localization kinematics geometry"
"9700","Implementation of inverse kinematics solution in c++","<p>I am having some issue with implementing a least square solution of the inverse kinematics problem. </p>

<p>The q configuration I get are rather large, or makes no sense, so I was hoping someone here could help me find my error in my program. </p>

<pre><code>rw::math::Q pathPlanning::invKin(double dx, double dy , double dz)
{

    rw::kinematics::State state =  this-&gt;state;
    rw::math::Transform3D&lt;&gt; t_tool_base =  this-&gt;device.get()-&gt;baseTend(state);


    cout &lt;&lt; t_tool_base.R().e() &lt;&lt; endl;
    cout &lt;&lt; endl;
    cout &lt;&lt; t_tool_base.P().e() &lt;&lt; endl;
    cout &lt;&lt; endl;

    Eigen::MatrixXd jq(this-&gt;device.get()-&gt;baseJend(state).e().cols(), this-&gt;device.get()-&gt;baseJend(state).e().rows());
    jq =  this-&gt;device.get()-&gt;baseJend(state).e();


    //Least square solver - dq = [j(q)]T (j(q)[j(q)]T)⁻1 du  &lt;=&gt; dq = A*du
    Eigen::MatrixXd A (6,6);
    //A = jq.transpose()*(jq*jq.transpose()).inverse();
    A = (jq*jq.transpose()).inverse()*jq.transpose();

    std::vector&lt;rw::math::Transform3D&lt;&gt; &gt; out = sphere(dx,dy,dz);

    std::ofstream outfile;
    outfile.open(""q_conf.txt"", std::ios_base::app);

    for(unsigned int i = 0; i &lt;= out.size() ; ++i )
    {
        rw::math::Vector3D&lt;&gt; dif_p = out[i].P()-t_tool_base.P();

        Eigen::Matrix3d dif = out[i].R().e()- t_tool_base.R().e();
        rw::math::Rotation3D&lt;&gt; dif_r(dif);
        rw::math::RPY&lt;&gt; dif_rot(dif_r);

        Eigen::VectorXd du(6);
        du(0) = dif_p[0];
        du(1) = dif_p[1];
        du(2) = dif_p[2];

        du(3) = dif_rot[0];
        du(4) = dif_rot[1];
        du(5) = dif_rot[2];

        Eigen::VectorXd q(6);
        q = A*du;

        rw::math::Q q_current;
        q_current = this-&gt;device-&gt;getQ(this-&gt;state);
        rw::math::Q dq(q);
        rw::math::Q q_new = q_current+ dq;

        //cout &lt;&lt; jq &lt;&lt; endl;
        //cout &lt;&lt; endl;
        //std::string text = ""setQ{""  + to_string(q_new[0])  + "", "" + to_string(q_new[1]) + "", "" + to_string(q_new[2]) + "", "" + to_string(q_new[3]) + "", "" + to_string(q_new[4]) + "", "" + to_string(q_new[5]) + ""}"";
        //cout &lt;&lt; text &lt;&lt; endl;
        //outfile &lt;&lt; text &lt;&lt; endl;

    }

    rw::math::Q bla(6); //Just used the text file for debugging purposes,  Which why I just return a random Q config.
    return bla;
}


rw::math::Transform3D&lt;&gt; pathPlanning::transform(double obj_x, double obj_y, double obj_z, double sphere_x, double sphere_y ,double sphere_z)
{
    // Z-axis should be oriented towards the object.
    // Rot consist of 3 direction vector [x,y,z] which describes how the axis should be oriented in the world space.
    // Looking at the simulation the z-axis is the camera out. X, and Y describes the orientation of the camera.
    // The vector are only for direction purposes, so they have to be normalized....
    // TODO: case [0  0 -1]... Why is it happening at what can be done to undo it?

    rw::math::Vector3D&lt;&gt; dir_z((obj_x - sphere_x), (obj_y - sphere_y), (obj_z - sphere_z));
    dir_z = normalize(dir_z);
    rw::math::Vector3D&lt;&gt; downPlane(0.0,0.0,-1.0);
    rw::math::Vector3D&lt;&gt; dir_x = cross(downPlane,dir_z);
    dir_x = normalize(dir_x);
    rw::math::Vector3D&lt;&gt; dir_y = cross(dir_z,dir_x);
    dir_y = normalize(dir_y);

    rw::math::Rotation3D&lt;&gt; rot_out (dir_x,dir_y,dir_z);

    rw::math::Vector3D&lt;&gt; pos_out(sphere_x,sphere_y,sphere_z);

    rw::math::Transform3D&lt;&gt; out(pos_out,rot_out);
    return out;
}

std::vector&lt;rw::math::Transform3D&lt;&gt;&gt; pathPlanning::sphere(double dx, double dy, double dz)
{
    double r = 0.50; // Radius of the sphere -  set to 0.50 cm (TODO: has to be checked if that also is accurate)
    cout &lt;&lt; ""Create a sphere"" &lt;&lt; endl;

    double current_x = this-&gt;device-&gt;baseTend(this-&gt;state).P()[0];
    double current_y = this-&gt;device-&gt;baseTend(this-&gt;state).P()[1];
    double current_z = this-&gt;device-&gt;baseTend(this-&gt;state).P()[2];

    rw::math::Vector3D&lt;&gt; center(current_x + dx, current_y + dy , current_z + dz);

    // Formula for sphere (x-x0)²+(y-y0)²+(z-z0)²=r²
    // x: x = x_0 + rcos(theta)sin(phi)
    // y: y = y_0 + rsin(theta)sin(phi)
    // z: z = z_0 + rcos(phi)
    // Angle range: 0 &lt;= theta &lt;= 2M_PI ; 0 &lt;= phi &lt;= M_PI

    double obj_x = current_x + dx;
    double obj_y = current_y + dy;
    double obj_z = current_z + dz;

    ofstream positions;
    ofstream rotations_z;
    ofstream rotations_y;
    ofstream rotations_x;
    positions.open (""sphere_positions.csv"");
    rotations_z.open(""z_dir.csv"");
    rotations_y.open(""y_dir.csv"");
    rotations_x.open(""x_dir.csv"");
    std::vector&lt;rw::math::Transform3D&lt;&gt;&gt; out;

    int count = 32;
    for(double theta = 0; theta &lt;= 2*M_PI ; theta+=0.1 )
    {

        for(double phi = 0; phi &lt;= M_PI ; phi+=0.1)
        {

            double sphere_x = obj_x + r*cos(theta)*sin(phi);
            double sphere_y = obj_y + r*sin(theta)*sin(phi);
            double sphere_z = obj_z + + r*cos(phi);

            string text = to_string(sphere_x) + "" , "" + to_string(sphere_y)+ "" , "" + to_string(sphere_z);
            positions &lt;&lt; text &lt;&lt; endl;

            rw::math::Transform3D&lt;&gt; transformation_matrix = transform(obj_x,obj_y,obj_z,sphere_x,sphere_y,sphere_z);

            string text2 = to_string(transformation_matrix.R().e()(0,2)) + "" , ""  + to_string(transformation_matrix.R().e()(1,2)) + "" , "" + to_string(transformation_matrix.R().e()(2,2));
            string text1 = to_string(transformation_matrix.R().e()(0,1)) + "" , ""  + to_string(transformation_matrix.R().e()(1,1)) + "" , "" + to_string(transformation_matrix.R().e()(2,1));
            string text0 = to_string(transformation_matrix.R().e()(0,0)) + "" , ""  + to_string(transformation_matrix.R().e()(1,0)) + "" , "" + to_string(transformation_matrix.R().e()(2,0));

            rotations_z &lt;&lt; text2 &lt;&lt; endl;
            rotations_y &lt;&lt; text1 &lt;&lt; endl;
            rotations_x &lt;&lt; text0 &lt;&lt; endl;

            if(count == 32) //TODO: Why...... is this occuring?
            {
                //cout &lt;&lt; ""Theta: "" &lt;&lt; theta &lt;&lt; "" Phi: "" &lt;&lt; phi &lt;&lt; endl;
                //cout &lt;&lt; sphere_x &lt;&lt; "" , "" &lt;&lt; sphere_y &lt;&lt;"" , ""&lt;&lt; sphere_z &lt;&lt; endl;
                count = 0;
            }
            else
            {
                count++;
            }

            out.push_back(transformation_matrix);
        }
    }

    positions.close();
    rotations_z.close();
    rotations_y.close();
    rotations_x.close();
    cout &lt;&lt; endl;
    cout &lt;&lt;""Object at: "" &lt;&lt; obj_x &lt;&lt; "","" &lt;&lt; obj_y &lt;&lt; "","" &lt;&lt; obj_z &lt;&lt; endl;
    cout &lt;&lt; ""done "" &lt;&lt; endl;
    return out;
}        
</code></pre>

<p>What am I trying to do, I am trying to orbit a robot endeffector  around an object in the center. The trajectory of the endeffector is an sphere where the endeffector should always point in to the object.  The sphere function should compute all transformation matrices which move the robot arm to the different position on the sphere with a given rotation, and the inverse kinematics should compute all the different Q-states, given an <code>x,y,z</code> which is the actual displacement to the object itself.</p>

<p>I am not quite sure where my error could be at, but I think it might either be at <code>transform</code> function where I generate my desired transformation matrix, or in <code>invKin</code> where I create <code>du</code>, I think I might have made an mistake in creating <code>du(3)</code>,  <code>du(4)</code>, <code>du(5)</code></p>

<p>The libraries I've been using is Eigen, <a href=""http://www.robwork.dk/apidoc/nightly/rw/"" rel=""nofollow"">robwork</a> (basically all rw::) if anyone want to look syntax through. </p>

<p><strong>Update</strong></p>

<p>Based on @ghanimmukhtar I began checking for singularities for the jacobian.. Which seems in general supringsly low. I computed it for a list of random Q configurations which resulted into this...</p>

<pre><code>Determinant: -0.0577779
Determinant: -0.0582286
Determinant: 0.0051402
Determinant: -0.0498886
Determinant: 0.0209685
Determinant: 0.00372222
Determinant: 0.047645
Determinant: 0.0442362
Determinant: -0.0799746
Determinant: 0.00194714
Determinant: 0.0228195
Determinant: 0.096449
Determinant: -0.0339612
Determinant: -0.00365521
Determinant: -0.030022
Determinant: 0.021347
Determinant: 0.0413364
Determinant: 0.0041136
Determinant: -0.0151192
Determinant: 0.0682926
Determinant: -0.0657176
Determinant: 0.0915473
Determinant: -0.00516008
Determinant: -0.0394664
Determinant: -0.00469664
Determinant: 0.0494431
Determinant: -0.00156804
Determinant: -0.0402393
Determinant: -0.0141511
Determinant: 0.0203508
Determinant: -0.0368337
Determinant: -0.0313431
Determinant: -0.0566811
Determinant: -0.00766113
Determinant: -0.051767
Determinant: -0.00815555
Determinant: 0.0564639
Determinant: 0.0764514
Determinant: -0.0501299
Determinant: -0.00056537
Determinant: -0.0308103
Determinant: -0.0091592
Determinant: 0.0602148
Determinant: -0.0051255
Determinant: 0.0426342
Determinant: -0.0850566
Determinant: -0.0353419
Determinant: 0.0448761
Determinant: -0.0103023
Determinant: -0.0123843
Determinant: -0.00160566
Determinant: 0.00558663
Determinant: 0.0173488
Determinant: 0.0170783
Determinant: 0.0588363
Determinant: -0.000788464
Determinant: 0.052941
Determinant: 0.064341
Determinant: 0.00084967
Determinant: 0.00716674
Determinant: -0.0978426
Determinant: -0.0585773
Determinant: 0.038732
Determinant: -0.00489957
Determinant: -0.0460029
Determinant: 0.00269656
Determinant: 0.000600619
Determinant: -0.0408527
Determinant: -0.00115296
Determinant: 0.013114
Determinant: 0.0366423
Determinant: 0.0495209
Determinant: -0.042201
Determinant: -0.036663
Determinant: -0.103452
Determinant: -0.0119054
Determinant: 0.0692284
Determinant: -0.00717832
Determinant: 0.00729104
Determinant: 0.0126415
Determinant: -0.00515246
Determinant: -0.0556505
Determinant: 0.000670701
Determinant: -0.0545629
Determinant: 0.00251946
Determinant: 0.0405189
Determinant: 0.010928
Determinant: -0.00101032
Determinant: 0.0308612
Determinant: 0.0536183
Determinant: -0.0439223
Determinant: -0.0113453
Determinant: -0.0193872
Determinant: 0.0660165
Determinant: -0.00184695
Determinant: -0.106904
Determinant: 0.01246
Determinant: -0.00883772
Determinant: 0.0601036
Determinant: 0.0468602
Determinant: 0.0513812
Determinant: -0.000663089
Determinant: -0.00392395
Determinant: 0.0710837
Determinant: 0.0629583
Determinant: -0.0464579
Determinant: 0.0257618
Determinant: -0.0193227
Determinant: 0.00388693
Determinant: -0.02003
Determinant: 0.0191158
Determinant: -0.00159198
Determinant: -0.0702308
Determinant: -0.0242876
Determinant: -0.00934638
Determinant: -0.00221986
Determinant: -0.0268925
Determinant: 0.0596055
Determinant: -0.00925273
Determinant: -0.0167357
Determinant: 0.0596476
Determinant: -0.00515798
Determinant: -0.00324081
Determinant: -0.00321565
Determinant: 0.0669645
Determinant: -0.0342913
Determinant: -0.000342155
Determinant: -0.0104422
Determinant: -0.0410489
Determinant: -0.0246036
Determinant: 0.0208562
Determinant: -0.0692963
Determinant: 0.000839091
Determinant: -0.049308
Determinant: -0.0349338
Determinant: 0.0016057
Determinant: -0.00214381
Determinant: -0.0332965
Determinant: 0.0168007
Determinant: -0.0748581
Determinant: -0.00864737
Determinant: -0.0638044
Determinant: -0.00103911
Determinant: -0.00690918
Determinant: 0.000285789
Determinant: 0.0215414
Determinant: 0.0560827
Determinant: -0.0063201
Determinant: -0.00677609
Determinant: -0.00686829
Determinant: 0.0591599
Determinant: 0.0112705
Determinant: 0.0874784
Determinant: -0.0146124
Determinant: -0.0133718
Determinant: -0.0203801
Determinant: -0.0150386
Determinant: -0.102603
Determinant: -0.077111
Determinant: 0.021146
Determinant: 0.089761
Determinant: -0.0532867
Determinant: -0.0620632
Determinant: -0.0165414
Determinant: -0.0461426
Determinant: 0.00144256
Determinant: 0.00844777
Determinant: 0.0893306
Determinant: -0.0814478
Determinant: -0.0890507
Determinant: -0.0472091
Determinant: 0.0186799
Determinant: -0.00224087
Determinant: -0.0242662
Determinant: -0.00195303
Determinant: 0.014432
Determinant: 0.00185717
Determinant: -0.0354357
Determinant: -0.0427957
Determinant: -0.0380409
Determinant: 0.0627548
Determinant: 0.0397546
Determinant: 0.0570439
Determinant: 0.106265
Determinant: 0.0382001
Determinant: -0.0240826
Determinant: -0.0866264
Determinant: 0.024184
Determinant: 0.0841286
Determinant: -0.0303611
Determinant: -0.0337029
Determinant: -0.0202875
Determinant: 0.0643731
Determinant: -0.0475265
Determinant: -0.00928736
Determinant: -0.00373402
Determinant: 0.0636828
Determinant: 0.0122532
Determinant: 0.0398141
Determinant: -0.0563998
Determinant: -0.0778303
Determinant: 0.0164747
Determinant: 0.0314815
Determinant: 0.0744507
Determinant: -0.0897675
Determinant: 0.0260324
Determinant: -0.0734512
Determinant: 0.000234548
Determinant: -0.0238522
Determinant: -0.0849523
Determinant: 0.0204877
Determinant: -0.0715147
Determinant: 0.0703858
Determinant: -0.0142186
Determinant: -0.101503
Determinant: 0.03966
Determinant: 4.69111e-05
Determinant: 0.0394428
Determinant: 0.0409131
Determinant: 8.90995e-05
Determinant: -0.00841189
Determinant: -0.0671323
Determinant: 0.00805167
Determinant: -0.00292435
Determinant: 0.0507716
Determinant: 0.0493995
Determinant: 0.00629414
Determinant: -0.0428982
Determinant: -0.0446924
Determinant: 0.0776236
Determinant: 0.00440478
Determinant: -0.0463321
Determinant: -0.00247224
Determinant: -0.0199861
Determinant: 0.0267022
Determinant: 0.0184179
Determinant: 0.0104588
Determinant: 0.116535
Determinant: -0.0857382
Determinant: -0.0477216
Determinant: 0.0286968
Determinant: 0.0387932
Determinant: 0.042856
Determinant: -0.0964
Determinant: 0.0320456
Determinant: -0.0676327
Determinant: 0.0156632
Determinant: 0.0548582
Determinant: 0.0394791
Determinant: 0.0863353
Determinant: -0.0568753
Determinant: -0.00953039
Determinant: -0.0534666
Determinant: 0.0506779
Determinant: 0.00521034
Determinant: 0.0353338
Determinant: 0.0845463
Determinant: -0.00847695
Determinant: 0.015726
Determinant: -0.0648035
Determinant: 0.0170917
Determinant: 0.0045193
Determinant: -0.0195397
Determinant: 0.00630076
Determinant: -0.0137401
Determinant: 0.0209229
Determinant: 0.00382077
Determinant: -0.0588661
Determinant: -0.0923883
Determinant: -0.00726003
Determinant: -0.0411533
Determinant: 0.00544489
Determinant: 0.0101791
Determinant: 0.0903306
Determinant: -0.0590416
Determinant: -0.0377112
Determinant: -0.0150455
Determinant: 0.0793066
Determinant: 0.0425759
Determinant: -0.040728
Determinant: -0.0376792
Determinant: -0.0387703
Determinant: -0.0232208
Determinant: 0.0506747
Determinant: -0.0284409
Determinant: 0.000536999
Determinant: -0.0289103
Determinant: -0.00586449
Determinant: -0.0805586
Determinant: 0.0133906
Determinant: -0.00311773
Determinant: 0.0184798
Determinant: -0.00981978
Determinant: -0.0491601
Determinant: 0.0452526
Determinant: 0.00411708
Determinant: -0.0515142
Determinant: 0.0121114
Determinant: 0.00636972
Determinant: -0.0126048
Determinant: -0.0412662
Determinant: 0.00195264
Determinant: -0.0726478
Determinant: 0.0692254
Determinant: -0.0256477
Determinant: 0.0702529
Determinant: -0.0052493
Determinant: 0.0625172
Determinant: 0.00282606
Determinant: 0.0229033
Determinant: 0.0558893
Determinant: 0.0766217
Determinant: -0.00388679
Determinant: -0.0193821
Determinant: -0.00718189
Determinant: -0.0864566
Determinant: 0.0809026
Determinant: -0.0398232
Determinant: -0.00224801
Determinant: 0.0333072
Determinant: -0.0212002
Determinant: 0.00371396
Determinant: 0.0162035
Determinant: -0.0811845
Determinant: 0.0148128
Determinant: 0.0372953
Determinant: 0.00351286
Determinant: -0.00103575
Determinant: 0.0384813
Determinant: 0.00752738
Determinant: -0.0248252
Determinant: -0.106768
Determinant: -0.0192333
Determinant: -0.026543
Determinant: -0.0222608
Determinant: -0.0487862
Determinant: 0.00376402
Determinant: -0.0329469
Determinant: 0.00266775
Determinant: 0.0762491
Determinant: 0.0159609
Determinant: -0.0190175
Determinant: -0.0338969
Determinant: -0.0631867
Determinant: -0.0238901
Determinant: 0.107709
Determinant: -7.74935e-05
Determinant: -0.0468996
Determinant: 0.0462787
Determinant: 0.0387825
Determinant: 0.0753388
Determinant: -0.000279933
Determinant: 0.00638663
Determinant: -0.00458034
Determinant: 0.0185849
Determinant: -0.00543503
Determinant: -0.0520309
Determinant: -0.0234638
Determinant: 0.0593986
Determinant: -0.00036774
Determinant: 0.00960819
Determinant: -0.00685314
Determinant: -0.000176925
Determinant: 0.0207583
Determinant: -0.0337003
Determinant: -0.0534818
Determinant: 0.0142158
Determinant: -0.0728077
Determinant: 0.0246877
Determinant: -0.0660952
Determinant: -0.0466
Determinant: 0.0915457
Determinant: -0.00340539
Determinant: 0.00815076
Determinant: -0.0751806
Determinant: -0.00617677
Determinant: 0.0019761
Determinant: -0.0016673
Determinant: 0.0310364
Determinant: 0.0483121
Determinant: -0.00664964
Determinant: 0.0659273
Determinant: -0.019015
Determinant: 0.0087627
Determinant: 0.0267279
Determinant: 0.0253497
Determinant: 0.00246292
Determinant: -0.0684746
Determinant: -0.0234524
Determinant: -0.0197933
Determinant: 0.0120796
Determinant: -0.0192703
Determinant: 0.0853956
Determinant: 0.0388196
Determinant: -0.0599305
Determinant: -0.0626148
Determinant: 0.0258541
Determinant: -0.0341273
Determinant: 0.0972889
Determinant: -0.0306585
Determinant: 0.0188553
Determinant: 0.00247702
Determinant: -0.00368989
Determinant: -0.0951982
Determinant: 0.0113578
Determinant: 0.000762509
Determinant: -0.0225219
Determinant: 0.0414059
Determinant: -0.0244409
Determinant: -0.0425728
Determinant: 0.04275
Determinant: -0.0413427
Determinant: -0.00556264
Determinant: -0.0894398
Determinant: -0.0193197
Determinant: -0.00788038
Determinant: -0.00455421
Determinant: -0.0788177
Determinant: 0.0415381
Determinant: -0.0346766
Determinant: -0.0748027
Determinant: 0.0087688
Determinant: -0.0968796
Determinant: 0.0683526
Determinant: -0.00996678
Determinant: 0.00955922
Determinant: -0.0914706
Determinant: 0.0728304
Determinant: 0.0541784
Determinant: 0.0457072
Determinant: -0.0299529
Determinant: -0.0096473
Determinant: -0.0142643
Determinant: -0.0684794
Determinant: 0.00281004
Determinant: -0.03252
Determinant: -0.0144637
Determinant: 0.0294154
Determinant: 0.00574353
Determinant: -0.019569
Determinant: 0.00492446
Determinant: -0.0526394
Determinant: -0.000870143
Determinant: -0.0180984
Determinant: -0.0144104
Determinant: 0.0456077
Determinant: -0.0113433
Determinant: 0.00377549
Determinant: -0.0775854
Determinant: -0.0336789
Determinant: -0.0744995
Determinant: -0.0427397
Determinant: 0.0300061
Determinant: -0.0326518
Determinant: -0.0333735
Determinant: -0.0284057
Determinant: -0.00999835
Determinant: -0.0380404
Determinant: 0.00648521
Determinant: 0.0449298
Determinant: 0.0120318
Determinant: -0.0230653
Determinant: -0.00934067
Determinant: -0.0175326
Determinant: -0.0799447
Determinant: 0.0679027
Determinant: -0.00670324
Determinant: -0.0841748
Determinant: 0.0236213
Determinant: 0.0386624
Determinant: -0.0239495
Determinant: 0.076976
Determinant: -0.00997484
Determinant: 0.025157
Determinant: -0.0654046
Determinant: 0.0090564
Determinant: 0.00129045
Determinant: -0.105119
Determinant: 0.0976925
Determinant: -0.105149
Determinant: -0.0465851
Determinant: 0.00237453
Determinant: -0.0456927
Determinant: 0.0328236
Determinant: -0.0914691
Determinant: -0.0157904
Determinant: -0.00170804
Determinant: -0.014797
Determinant: 0.00464912
Determinant: -0.035118
Determinant: -0.0242306
Determinant: 0.0081405
Determinant: 0.0733502
Determinant: -0.0860252
Determinant: -0.0511219
Determinant: -0.0925647
Determinant: 0.0495087
Determinant: -0.0515914
Determinant: -0.044318
Determinant: 0.000900043
Determinant: 0.0632521
Determinant: 0.00957955
Determinant: 0.00598059
Determinant: 0.0179513
Determinant: 0.0952263
</code></pre>

<p><code>dx,dy,dz</code> is a is the distance between tcp and an object i want to keep in sight. The sphere is like a safety zone, but is mainly used to compute the orientation of the tool. </p>
","inverse-kinematics c++"
"9701","Designing Ackerman's Steering Principle for an autonomous robot","<p>I am working on a high speed autonomous robot (about 6-7 m/s), which does obstacle detection as well as senses traffic lights (I have used Raspberry Pi 3 and Arduino Uno).  </p>

<p>For the steering mechanism, I wanted to implement an Ackerman's steering. I've read about the principle and have understood its basics. Now to actually make the design, I am currently using switchboards, sold here in India, they are surprisingly strong, lightweight, waterproof(they are switchboards) and cheap. Now I got 1 big axle and the small axle cut out already, along with the two L-shaped pieces that join the 2 axles together... I'm just now confused as to how to connect the wheels to the axle and how to make them rotate along side it. The site won't let me upload any pics right now, I'll try again ASAP.</p>

<p>I have the switchboard, an electric drill and will to do anything to make this happen ( ;P ). I don't have access to a 3D Printer.</p>

<p>Any help would be greatly appreciated...  </p>

<p>P.S- And if you have any suggestions of your own, which might be better for my robot, feel free to share them, I'm just looking for a good steering method for my robot.</p>
","arduino raspberry-pi navigation steering"
"9702","How can I improve ZED Camera precision?","<p>I'm using Stereolabs ZED camera for my computer vision project. I did a small research about several sensors on the market and ultimately we decided to go with the ZED Camera. </p>

<p>However I'm finding that the precision of the camera isn't that great. And the Point Cloud takes too much storage space. Anyone found the same problems? And if so, how did you managed them?</p>

<p>Thank you!</p>
","computer-vision stereo-vision"
"9704","motor inertia tensor?","<p>In modeling dynamics of a robot ,in which servo motor is adjusted inside the link, there is a need to find inertia tensor of the motor itself,Right?</p>

<p><a href=""http://i.stack.imgur.com/qaVmL.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/qaVmL.png"" alt=""enter image description here""></a></p>

<p>So if it is needed how can i get the inertia tensor of motor since i couldn't find its solid works model having internal components,i mean gears and other stuff(with related specified materials)?</p>
","servomotor dynamics"
"9709","Is my servo fried?","<p>I got a new servo a few days back (RC Servo, Futaba FP-S148). I first tested it out with the Sweep sketch on Arduino, powering it with the Arduino 5v and GND pins only. It was working, just fine.</p>

<p>Today I was trying to use it in my robot and I tried powering it with 2 LiPo batteries (Samsung ICR16850 2200mAh, from an old laptop battery) connected in series, giving 8.32v. As soon as I connected my servo, it started rotating randomly, I had not connected it to my Arduino yet. I quickly took it out.</p>

<p>Next, I used a L7805 to get 5.13v regulated supply out of my batteries that I used earlier. When I connected my batteries to the servo, and the servo to the Arduino, uploaded the sketch, the servo started behaving rather strangely, it first did a complete turn and then stopped. Only a humming sound came from the servo. Strange thing is, whenever I connect one of my Multimeter leads to the power cables, the servo immediately turned in the opposite direction <strong>only as long as only lead was in contact with either the positive or negative wire</strong>.<br>
Otherwise, the servo just gives a humming sound.</p>

<p>Have I fried my servo? Or is it some other issue?</p>

<p><strong>UPDATE 1</strong><br>
I stripped down the servo and checked the motor. It is working fine, seems like this is a gear problem.</p>
","arduino battery rcservo"
"9711","Testbed for testing navigation algorithms","<p>I'm looking for a testbed (simulator or web-based interface that lets me to have control on a robot) for testing different routing and navigation algorithms. Is there such a system on the web?</p>
","navigation routing"
"9716","Quality check robot","<p>How to develop a robot based system to continuously monitor and check products for defeat which are moving on a conveyer belt using sensors and kick out the defect product from the queue?</p>
","microcontroller"
"9720","8 wheeled vehicle model","<p>I want the dynamic model for 8 wheeled robot. I expected to find it easily like the 4 wheeled bicycle model, but I couldn't.</p>

<p>Here is my effort only for rotation not for feedback.</p>

<p><a href=""http://i.stack.imgur.com/mbXU8.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/mbXU8.png"" alt=""8 wheel""></a>
From that I can calculate steering angle, but it was very messy to manage them all.</p>

<p>I need the model for controlling.</p>
","wheeled-robot dynamics motion robotc"
"9724","How to apply A bang-bang signal of amplitude 1 N and 1 s width as an input force to reproduce certain results in Matlab?","<p>I working on dynamic modeling and simulation of a mechanical system (overhead crane), after I obtained the equation of motion, in the form: $$ M(q)\ddot{q}+C(q,\dot{q})\dot{q}+G(q)=Q $$</p>

<p>All the matrices are know inertia, $ M(q)$, Coriolis-Centrifugal matrix $ C(q,\dot{q})$, and gravity $ G(q)$ as functions of the generalized coordinates $q$, and their derivatives $\dot{q}$.</p>

<p>I want to solve for $q$, using Matlab <em>ODE</em> (in m-file), I got the response for some initial conditions and zero input, but, I want to find the response, for the aforementioned control signal (<strong>A bang-bang signal of amplitude 1 N and 1 s width</strong>), I'm trying to regenerate some results from the literature, and what the authors of that work said, regrading the input signal is the following: ""A bang-bang signal of amplitude 1 N and 1 s width is used as an input force, applied at the cart of the gantry crane. A bang-bang force has a positive (acceleration) and negative (deceleration) period allowing the cart to, initially, accelerate and then decelerate and eventually stop at a target location."" I didn't grasp what do they mean by bang-bang signal, I know in Matlab we could have step input, impulse, ...etc. But bang-bang signal, I'm not familiar with. According to <a href=""https://en.wikipedia.org/wiki/Bang%E2%80%93bang_control"" rel=""nofollow"">this site</a> and <a href=""http://www.brown.edu/Departments/Engineering/Courses/En123/Lectures/FdbkBasic.html"" rel=""nofollow"">this</a> bang bang is a controller rather.</p>

<p>Could anyone suggest to me how to figure out this issue and implement this input signal? preferably in m-file.</p>

<p>The code I'm using is given bellows, two parts:</p>

<pre><code>function xdot = AlFagera(t,x,spec)
% xdot = zeros(8,1);
xdot = zeros(12,1); % to include the input torque

% % Crane Specifications
mp = spec(1);
mc = spec(2);
mr = spec(3);
L = spec(4);
J = spec(5);

g = 9.80;               % accelatrion of gravity (m/s^)

% % matix equations 
M11 = mr+mc+mp; M12 = 0; M13 = mp*L*cos(x(3))*sin(x(4)); M14 = mp*L*sin(x(3))*cos(x(4));
M21 = 0; M22 = mp+mc; M23 = mp*L*cos(x(3))*cos(x(4)); M24 = -mp*L*sin(x(3))*sin(x(4));
M31  = M13; M32 = M23; M33 = mp*L^2+J; M34 = 0;
M41 = M14; M42 = M24; M43 = 0; M44 = mp*L^2*(sin(x(3)))^2+J;
M = [M11 M12 M13 M14; M21 M22 M23 M24; M31 M32 M33 M34; M41 M42 M43 M44];

C11 = 0; C12 = 0; C13 = -mp*L*sin(x(3))*sin(x(4))*x(7)+mp*L*cos(x(3))*cos(x(4))*x(8);
C14 = mp*L*cos(x(3))*cos(x(4))*x(7)-mp*L*sin(x(3))*sin(x(4))*x(8);
C21 = 0; C22 = 0; C23 = -mp*L*sin(x(3))*cos(x(4))*x(7)-mp*L*cos(x(3))*sin(x(4))*x(8);
C24 = -mp*L*cos(x(3))*sin(x(4))*x(7)-mp*L*sin(x(3))*cos(x(4))*x(8); 
C31 = 0; C32 = 0; C33 = 0; C34 = -mp*L^2*sin(x(3))*cos(x(3))*x(8);
C41 = 0; C42 = 0; C43 = -C34; C44 = mp*L^2*sin(x(3))*cos(x(4))*x(7);

C = [C11 C12 C13 C14; C21 C22 C23 C24; C31 C32 C33 C34; C41 C42 C43 C44];
Cf = C*[x(5); x(6); x(7); x(8)];

G = [0; 0; mp*g*L*sin(x(3)); 0];

fx = 0; 

if t &gt;=1 &amp;&amp; t&lt;=2
fy = 1.*square(t*pi*2);
else fy = 0;
end

F =[fx; fy; 0; 0];     % input torque vector, 

xdot(1:4,1)= x(5:8);
xdot(5:8,1)= M\(F-G-Cf);
xdot(9:12,1) = F;
</code></pre>

<p>And:</p>

<pre><code>clear all; close all; clc;

t0 = 0;tf = 20;

x0 = [0.12 0.5 0 0, 0 0 0 0,0 0 0 0];  % initional conditions

% % spectifications
Mp = [0.1 0.5 1];      % variable mass for the payload
figure
plotStyle = {'b-','k','r'};
for i = 1:3
mp = Mp(i);
mc = 1.06; mr = 6.4;           % each mass in kg
L = 0.7; J = 0.005;            % m, kg-m^2 respe.
spec = [mp mc mr L J];
% % Call the the function
[t,x] = ode45(@(t,x)AlFagera(t,x,spec),[t0 :0.001: tf],x0);

legendInfo{i} = ['mp=',num2str(Mp(i)),'kg'];


fx = diff(x(:,9))./diff(t);
fy = diff(x(:,10))./diff(t);
tt=0:(t(end)/(length(fx)-1)):t(end); % this time vector

% to plot the cart positions in x and y direcitons
subplot(1,2,1)
plot(t,x(:,1),plotStyle{i})
axis([0 20 0 0.18]);
grid
xlabel('time (s)');
ylabel('cart position in x direction (m)');
hold on
legend(legendInfo,'Location','northeast')

subplot(1,2,2)
plot(t,x(:,2),plotStyle{i})
axis([0 20 0 1.1]);
grid
xlabel('time (s)');
ylabel('cart position in y direction (m)');
hold on
legend(legendInfo,'Location','northeast')

end

% to plot the input torque, (bagn-bang signal), just one sample
figure
plot(tt,fy)
grid
set(gca,'XTick',[0:20])
xlabel('time (s)');
ylabel('input signal, f_y (N)');
</code></pre>

<p>Furthermore, the results I'm getting and what I supposed to get are shown:
<a href=""http://i.stack.imgur.com/ha31n.jpg"" rel=""nofollow""><img src=""http://i.stack.imgur.com/ha31n.jpg"" alt=""This my output, for the give code""></a>
<a href=""http://i.stack.imgur.com/TTJH6.jpg"" rel=""nofollow""><img src=""http://i.stack.imgur.com/TTJH6.jpg"" alt=""This what I supposed to get""></a></p>

<p><strong>Major difficulties, initial conditions are not clearly stated in the paper, the input force direction, is only in y (which it should be), or it has different direction. I appreciate any help.</strong></p>

<blockquote>
  <p>the paper I'm trying to recreate is:
  R. M. T. Raja Ismail, M. A. Ahmad, M. S. Ramli, and F. R. M. Rashidi, “Nonlinear Dynamic Modelling and Analysis of a 3-D Overhead Gantry Crane System with System Parameters Variation.,” International Journal of Simulation–Systems, Science &amp; Technology, vol. 11, no. 2, 2010.
  <a href=""http://ijssst.info/Vol-11/No-2/paper2.pdf"" rel=""nofollow"">http://ijssst.info/Vol-11/No-2/paper2.pdf</a></p>
</blockquote>
","control robotic-arm dynamics matlab input"
"9727","How can my robot find its position in any given map without GPS, including when the initial point is not given?","<p>Consider this map</p>

<p><a href=""http://i.stack.imgur.com/VTUuP.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/VTUuP.png"" alt=""enter image description here""></a></p>

<p>The Contest arena shown in figure 1 consists of two sub arenas, both the sides are identical to each other and their scientists and safe zone locations are similar.</p>

<p>Each sub arena has 3 different colored rooms and a fourth shared room. Each robot will be placed at identical start locations, respective to their arena. These locations will be random and anywhere on the map.</p>

<p>Each room (other than the shared room) will have two entry and exit gates. Both of these gates will be open at all times. The robot can enter and exit from any gate it chooses.</p>
","localization"
"9728","How to check for a sharp angle with a line follower?","<p>I have the <a href=""http://www.makeblock.cc/mbot/"" rel=""nofollow"">mBot</a> robot and I want to program it to follow the line. So far it can pass any kind of line that is >90°.</p>

<p>I want it to be able to pass 90°-ish angles as well. Like this one:</p>

<p><a href=""http://i.stack.imgur.com/FGkQm.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/FGkQm.png"" alt=""&gt;90° angle""></a></p>

<p>The problem is that my mBot robot has only 2 line following sensors (they are 5 mm apart and the line is 2 cm wide) so I can't use just the sensors.</p>

<p><a href=""http://i.stack.imgur.com/qHQHt.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/qHQHt.png"" alt=""enter image description here""></a></p>

<p>Most of the times it just goes to the line and when it's supposed to turn it just misses the line (goes on the white) and goes back to get back on track. Once it's back on the black line it once again tries to go forward but goes on the white instead of taking a turn. This happens endlessly.</p>

<p>Sometimes it passes the angle by going back and forth and accidentally turning, but that's not even a workaround, let alone a solution.</p>

<p>Here's a test course of the first round of the competition.
<a href=""http://i.stack.imgur.com/pzf8y.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/pzf8y.png"" alt=""course""></a></p>

<p>My robot can pass this without a problem, but it gets stuck on this (poorly edited, sorry) course:</p>

<p><a href=""http://i.stack.imgur.com/IevUP.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/IevUP.png"" alt=""I don&#39;t currently have Photoshop installed. Sorry &#39;bout that.""></a></p>

<p>It can't pass the 20 block if the robot enters it from a 15 or 20 block (so basically it gets stuck if it's coming from an angle and hits a 90 degree turn).</p>

<p>The sensor value could be read as either 0, 1, 2 or 3 depending on what the robot currently sees:</p>

<p><a href=""http://i.stack.imgur.com/PNkB5.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/PNkB5.png"" alt=""sensor with all possible outcomes""></a></p>

<p>0 - on the line <br>
1 - on the right of the line <br>
2 - on the left of the line <br>
3 - not on the line <br></p>

<p>Pseudo code of my current program:</p>

<pre><code>loop forever:
    if (on the right of the line):
        turn_left()
    if (on the left of the line):
        turn_right()
    if (on the line):
        go_forward()
    if (not on the line):
        go_backwards()
</code></pre>

<p>So how would I go about taking such sharp turns?</p>
","arduino motor line-following"
"9729","Diffrence between Degrees of Freedom (DOF) and Degrees of Motion (DOM)","<p>Could anyone expain me shortly what is a diffrence between degrees of freedom (DOF) and degrees of motion (DOM)? I know that DOF is the number of independent movements a manipulation arm can make and robot system can have max 6 independent DOF and unlimited number of DOM but I do not distinguish them from each other.</p>
","manipulator theory"
"9738","How can a quadcopter be made to hover perfectly still?","<p>I need to get my drone flying still enough that I can rest a glass of water on it.</p>

<p>I've tried a few KK boards and APM 2.6 (3.1 software). I've balanced props, set PID settings, auto-trim / autotune and the drone still tends to inconsistently drift a little one way or another.</p>

<p>What is a plausible way to completely isolate drift?</p>
","quadcopter"
"9739","Quadcopter Flight Controller:Why does Using gyroscope data give better results?","<p>I have succeeded in making my first quadcopter from scratch with a readymade frame. I designed the flight controller myself with help from YMFC-3D youtube series of videos. <a href=""https://www.youtube.com/watch?v=2pHdO8m6T7c"" rel=""nofollow"">https://www.youtube.com/watch?v=2pHdO8m6T7c</a></p>

<p>But in the process, I discovered that using the euler angles or the 'ypr' values from the MPU6050 as the feeback to the PID loop makes it super difficult to tune the quadcopter and even then it doesn't fly great. </p>

<p>Whereas although not intuitive to me, using the gyroscope values with a complementary filter instantly made the quad respond much better and the tuning also was not too difficult.</p>

<p>Let me clearly define the response in both cases.</p>

<p>Using ypr values:-
+Always keeps overshooting or 'underreaching'
+Very small range of values that can let the quad fly stable
+Drastic Reactions to extreme values of P (Kp)values</p>

<p>Using gyro values:-
+Reaction is much more stable
+Tuning the PID was also simple
+ Even under high values of P(Kp) the quad might crash due to oscillations but not flip or react extremely</p>

<p>Below is a portion of the PID loop:</p>

<pre><code>//gyrox_temp is the current gyroscope output

gyro_x_input=(gyro_x_input*.8)+(gyrox_temp*0.2);//complementary filter

pidrate_error_temp =gyro_x_input - setpoint;//error value for PID loop

pidrate_i_mem_roll += pidrate_i_gain_roll * pidrate_error_temp;
//integral portion

pidrate_output_roll = pidrate_p_gain_roll * pidrate_error_temp + pidrate_i_mem_roll + pidrate_d_gain_roll * (pidrate_error_temp - pidrate_last_roll_d_error);
//output of the pid loop
/pidrate_p_gain_roll-Kp
//pidrate_i_gain_roll-Ki
//pidrate_d_gain_roll-Kd
//this output is given as the pwm signal to the quad plus throttle
</code></pre>
","quadcopter pid gyroscope"
"9741","Principle of virtual force - General help in understanding / explanation","<p>I'm an Electronics student taking a module in Robotics. </p>

<p>From the example,</p>

<p><a href=""http://i.stack.imgur.com/tg0sx.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/tg0sx.png"" alt=""Example from lecture notes""></a></p>

<p>I understand line 1 as the Jacobian is found from the time derivative of the kinematics equation and such relates joint angles to velocity.</p>

<p>I do not understand why the transpose has been taken on line 3 and how line 4 is produced.</p>
","kinematics jacobian"
"9751","Measuring vehicle's forward and lateral acceleration using a smartphone","<p>I want to measure the acceleration (forward and lateral separately) using an android smartphone device in order to be able to analyse the driving behavior. </p>

<p>My approach would be as follows:</p>

<h2>1. Aligning coordinate systems</h2>

<p><strong>Calibration (no motion / first motion):</strong>
While the car is stationary, I would calculate the magnitude of gravity using <code>Sensor.TYPE_GRAVITY</code> and rotate it straight to the z-axis (pointing downwards assuming a flat surface). That way, the <strong>pitch</strong> and <strong>roll</strong> angles should be near zero and equal to the angles of the car relativ to the world.</p>

<p>After this, I would start moving straight forward with the car to get a first motion indication using <code>Sensor.TYPE_ACCELEROMETER</code> and rotate this magnitude straight to the x-axis (pointing forward). This way, the <strong>yaw</strong> angle should be equal to the vehicle's heading relativ to the world.</p>

<p><strong>Update Orientation (while driving):</strong>
To be able to keep the coordinate systems aligned while driving I am going to use <code>Sensor.TYPE_GRAVITY</code> to maintain the roll and pitch of the system via</p>

<p><img src=""http://i.stack.imgur.com/4gm3G.gif"" alt=""enter image description here""></p>

<p><img src=""http://i.stack.imgur.com/Zoz9b.gif"" alt=""enter image description here""></p>

<p>where A_x,y,z is the acceleration of gravity. </p>

<p>Usually, the yaw angle would be maintained via <code>Sensor.ROTATION_VECTOR</code> or <code>Sensor.MAGNETIC_FIELD</code>. However, the reason behind not using them is because I am going to use the application also in <strong>electrical vehicles</strong>. The high amounts of volts and ampere produced by the engine would presumably make the accuracy of those sensor values suffer. Hence, the best alternative that I know (although not optimal) is using the GPS course to maintain the yaw angle.</p>

<h2>2. Getting measurements</h2>

<p>By applying all aforementioned rotations it should be possible to maintain an alignment between the smartphone's and vehicle's coordinate systems and, hence, giving me the pure forward and lateral acceleration values on the x-axis and y-axis.</p>

<h2><em>Questions:</em></h2>

<ul>
<li>Is this approach applicable or did I miss something crucial?</li>
<li>Is there an easier/alternative approach to this?</li>
</ul>
","sensors accelerometer gps"
"9752","E: Unable to locate package ros-jade-desktop-full","<p>I want to install ROS on my Xubuntu 16.04, Xenial Xerus. I have followed the ROS's site instruction: <a href=""http://wiki.ros.org/jade/Installation/Ubuntu"" rel=""nofollow"">http://wiki.ros.org/jade/Installation/Ubuntu</a>, and did the following: First, setup my sources.list:</p>

<p><code>sudo sh -c 'echo ""deb http://packages.ros.org/ros/ubuntu $(lsb_release -sc) main"" &gt; /etc/apt/sources.list.d/ros-latest.list'</code></p>

<p>Second, set up keys:</p>

<p><code>sudo apt-key adv --keyserver hkp://ha.pool.sks-keyservers.net:80 --recv-key 0xB01FA116</code></p>

<p>Then, make sure my package is up-to-date: </p>

<p><code>sudo apt-get update</code></p>

<p>Last, try to install ROS jade:</p>

<p><code>sudo apt-get install ros-jade-desktop-full</code></p>

<p>And get this error:</p>

<p><code>E: Unable to locate package ros-jade-desktop-full</code></p>

<p>Where did I go wrong, and how can I get ROS (any version is ok) running on my Xubuntu 16.04?</p>
","ros"
"9754","Are all Flight Controllers and Remote Controls using the same protocol?","<p>I'm about to start a project, where I'm sniffing data between remote controls and flight controllers on RC copters and doing stuff with that information.  Do all (or most) flight controllers use the same protocol to communicate with the remote controls, or does it vary based on which one you buy?  I would be testing on drones (DJI phantom and the like).  </p>

<p>So, my real question is:</p>

<p>If I want to write something to read the data, will I need to buy a different flight controller for each protocol used, or do they all use the same protocol, and I can just buy one flight controller, and the info I can get out will be the same for all types of flight controllers?</p>

<p>Also, are the protocols only spoken by the ground remote control and the flight controller?  Does the receiver care what protocol is being used, or is it just a middle man?</p>
","quadcopter radio-control research"
"9755","Dead Reckoning: Obtaining Position Estimation from Accelerometer Acceleration Integration","<p>Good day,</p>

<p>I have been reading papers about position integration from accelerometer readings.</p>

<p>I have consulted <a href=""http://perso-etis.ensea.fr/~pierandr/cours/M1_SIC/AN3397.pdf"" rel=""nofollow"">this paper from freescale</a> on how that is achievable and <a href=""http://diydrones.com/forum/topics/multi-rotors-the-altitude-yoyo-effect-and-how-to-deal-with-it"" rel=""nofollow"">this article regarding leaky integrators</a> to help in preventing accumulation of errors from integration.</p>

<p>I was testing this algorithm by moving the imu by approximately 0.1 meter. The algorithm does get it right at the instant it arrives at approx 0.1 meter however when left still at that position, the integrated position goes to zero.</p>

<p>It turns out the velocity readings become negative at a certain period after reaching 0.1 meters.</p>

<blockquote>
  <p>Does anyone have any suggestions in dealing with this error?</p>
</blockquote>

<p><strong>Plots</strong> (Red is the position, Blue is the velocity.)</p>

<p>The imu(accelerometer) was moved alternating positions 0 meters and 0.1 meters with a stop of approximately 3-5 seconds in between before moving to the next position</p>

<ol>
<li><p>Actual Data
<a href=""http://i.stack.imgur.com/bioEh.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/bioEh.png"" alt=""enter image description here""></a>
<a href=""http://i.stack.imgur.com/iJMmi.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/iJMmi.png"" alt=""enter image description here""></a></p></li>
<li><p>Desired Data output (Green - Desired position integration)
<a href=""http://i.stack.imgur.com/C85lq.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/C85lq.png"" alt=""enter image description here""></a></p></li>
</ol>

<p><strong>Code:</strong></p>

<pre><code>// Get acceleration per axis
float AccX = accelmagAngleArray.AccX;
float AccY = accelmagAngleArray.AccY;
float AccZ = accelmagAngleArray.AccZ;

AccX -= dc_offsetX;
AccY -= dc_offsetY;
AccZ -= dc_offsetZ;

//Calculate Current Velocity (m/s)
float leakRateAcc = 0.99000;
velCurrX = velCurrX*leakRateAcc + ( prevAccX + (AccX-prevAccX)/2 ) * deltaTime2;
velCurrY = velCurrY*leakRateAcc + ( prevAccY + (AccY-prevAccY)/2 ) * deltaTime2;
velCurrZ = velCurrZ*0.99000 + ( prevAccZ + (AccZ-prevAccZ)/2 ) * deltaTime2;
prevAccX = AccX;
prevAccY = AccY;
prevAccZ = AccZ;

//Discrimination window for Acceleration
if ((0.12 &gt; AccX) &amp;&amp; (AccX &gt; -0.12)){
  AccX = 0;
}

if ((0.12 &gt; AccY) &amp;&amp; (AccY &gt; -0.12)){
  AccY = 0;
}

//Count number of times acceleration is equal to zero to drive velocity to zero when acceleration is ""zero""
//X-axis---------------
if (AccX == 0){ //Increment no of times AccX is = to 0
    counterAccX++;    
}
else{ //Reset counter
    counterAccX = 0;
}

if (counterAccX&gt;25){ //Drive Velocity to Zero
velCurrX = 0;
    prevVelX = 0;
    counterAccX = 0;
}

//Y-axis--------------
if (AccY == 0){ //Increment no of times AccY is = to 0
    counterAccY++;    
}
else{ //Reset counter
    counterAccY = 0;
}

if (counterAccY&gt;25){ //Drive Velocity to Zero
    velCurrY = 0;
    prevVelY = 0;
    counterAccY = 0;
}

//Print Acceleration and Velocity
cout &lt;&lt; "" AccX = "" &lt;&lt; AccX ;// &lt;&lt; endl;
cout &lt;&lt; "" AccY = "" &lt;&lt; AccY ;// &lt;&lt; endl;
cout &lt;&lt; "" AccZ = "" &lt;&lt; AccZ &lt;&lt; endl;

cout &lt;&lt; "" velCurrX = "" &lt;&lt; velCurrX ;// &lt;&lt; endl;
cout &lt;&lt; "" velCurrY = "" &lt;&lt; velCurrY ;// &lt;&lt; endl;
cout &lt;&lt; "" velCurrZ = "" &lt;&lt; velCurrZ &lt;&lt; endl;

//Calculate Current Position in Meters
float leakRateVel = 0.99000;
posCurrX = posCurrX*leakRateVel + ( prevVelX + (velCurrX-prevVelX)/2 ) * deltaTime2;
posCurrY = posCurrY*leakRateVel + ( prevVelY + (velCurrY-prevVelY)/2 ) * deltaTime2;
posCurrZ = posCurrZ*0.99000 + ( prevVelZ + (velCurrZ-prevVelZ)/2 ) * deltaTime2;
prevVelX = velCurrX;
prevVelY = velCurrY;
prevVelZ = velCurrZ;

//Print X and Y position in meters
cout &lt;&lt; "" posCurrX = "" &lt;&lt; posCurrX ;// &lt;&lt; endl;
cout &lt;&lt; "" posCurrY = "" &lt;&lt; posCurrY ;// &lt;&lt; endl;
cout &lt;&lt; "" posCurrZ = "" &lt;&lt; posCurrZ &lt;&lt; endl;
</code></pre>
","quadcopter sensors localization integration dead-reckoning"
"9756","Step size in numerical differentiation","<p>I get position information and a corresponding timestamp from a motion tracking system (for a rigid body) at 120 Hz. The position is in sub-millimeter precision, but I'm not too sure about the time stamp, I can get it as floating point number in seconds from the motion tracking software. To get the velocity, I use the difference between two samples divided by the $\Delta t$ of the two samples:</p>

<p>$\dot{\mathbf{x}} = \dfrac{\mathbf{x}[k] - \mathbf{x}[k-1]}{t[k]-t[k-1]}$.</p>

<p>The result looks fine, but a bit noisy at times. A realized that I get much smoother results when I choose the differentiation step $h$ larger, e.g. $h=10$:</p>

<p>$\dot{\mathbf{x}} = \dfrac{\mathbf{x}[k] - \mathbf{x}[k-h]}{t[k]-t[k-h]}$.</p>

<p>On the other hand, peaks in the velocity signal begin to fade if I choose $h$ too large. Unfortunately, I didn't figure out why I get a smoother signal with a bigger step $h$. Does someone have a hint? Is there a general rule which differentiation step size is optimal with respect to smoothness vs. ""accuracy""?</p>

<p>This is a sample plot of one velocity component (blue: step size 1, red: step size 10):</p>

<p><a href=""http://i.stack.imgur.com/3aaOU.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/3aaOU.png"" alt=""Sample plot of step size 1 vs. step size 10.""></a></p>
","motion pose"
"9770","Mobile robot algorithm implementation error","<p>I am working in reproducing a robotics paper, first simulating it in MATLAB in order to implement it to a real robot afterwards. The robot's model is:</p>

<p>$$\dot{x}=V(t)cos\theta $$
$$\dot{y}=V(t)sin\theta$$
$$\dot{\theta}=u$$</p>

<p>The idea is to apply an algorithm to avoid obstacles and reach a determines target. This algorithm uses a cone vision to measure the obstacle's properties. The information required to apply this system is:</p>

<p>1) The minimum distance  $ d(t) $ between the robot and the obstacle (this obstacle is modelled as a circle of know radius $ R $).</p>

<p>2) The obstacle's speed  $ v_{obs}(t) $</p>

<p>3)The angles $ \alpha_{1}(t)$ and $ \alpha_{2}(t)$  that form the robot's cone vision, and</p>

<p>4) the heading $ H(t) $  from the robot to the target</p>

<p>First a safe distance  $ d_{safe}$  between the robot and the obstacle is defined. The robot has to reach the target without being closer than        $ d_{safe}$ to the obstacle.</p>

<p>An extended angle $ \alpha_{0} \ge arccos\left(\frac{R}{R+d_{safe}} \right) $  is defined, where $ 0 \le \alpha_{0} \le \pi $ </p>

<p>Then the following auxiliary angles are calculated:</p>

<p>$ \beta_{1}(t)=\alpha_{1}(t)-\alpha_{0}(t)$ </p>

<p>$ \beta_{2}=\alpha_{2}(t)+\alpha_{0}(t)$ </p>

<p>Then the following vectors are defined:</p>

<p>$ l_{1}=(V_{max}-V)[cos(\beta_{1}(t)),sin(\beta_{1}(t))]$ </p>

<p>$ l_{2}=(V_{max}-V)[cos(\beta_{2}(t)),sin(\beta_{1}(2))]$ </p>

<p>here $ V_{max}$  is the maximum robot's speed and  $ V $ a constant that fulfills  $ \|v_{obs}(t)\| \le V \le V_{max} $ </p>

<p>This vectors represent the boundaries of the cone vision of the vehicle</p>

<p>Given the vectors $ l_{1} $  and $ l_{2}$ , the angle $ \alpha(l_1,l_2)$  is the angle between $ l_{1}$  and $ l_{2} $  measured in counterclockwise direction, with $  \alpha \in (-\pi,\pi) $ . Then the function $f$ is </p>

<p>The evasion maneuver starts at time $t_0$. For that the robot find the index h:</p>

<p>$h = min|\alpha(v_{obs}(t_0)+l_j(t_0),v_R(t_0))|$</p>

<p>where $j={1,2}$ and $v_R(t)$ is the robot's velocity vector </p>

<p>Then, from the two vectors  $v_{obs}(t_0)+l_j(t_0)$ we choose that one that forms the smallest angle with the robot's velocity vector. Once h is determinded, the control law is applied:</p>

<p>$u(t)=-U_{max}f(v_{obs}(t)+l_h(t),v_R(t))$</p>

<p>$V(t)=\|v_{obs}(t)+l_h(t)\| \quad \quad (1)$ </p>

<p>This is a sliding mode type control law, that steers the robot's velocity  $v_R(t)$ towards a switching surface equal to the vector $v_{obs}(t)+l_h(t)$. Ideally the robot avoids the obstacle by surrounding it a </p>

<p>While the robot is not avoiding an obstacle it follows a control law:</p>

<p>$u(t)=0$</p>

<p>$V(t)=V_{max} \quad \quad  (2) $    </p>

<p>Hence the rules to switch between the two laws are:</p>

<p><strong>R10</strong> Switching from (2) to (1) occurs whenthe distance to the obstacle is equal to a constant C, which means when $d(t_0)=C$ and this distance is becoming smaller in time  i.e. $\dot{d(t)}&lt;0$</p>

<p><strong>R11</strong> Switching from (1) to (2) occurs when $d(t_*)&lt;1.1a_*$ and the vehicle is pointing towards the obstacle, i.e. $\theta(t_*)=H(T_*)$</p>

<p>where $a_*=\frac{R}{cos\alpha_0}-R $</p>

<p>Ideally the result should be similar to this</p>

<p>But I'm getting this instead</p>

<p>While I understand the theory there's obviously a flaw in my implementation that I haven't been able to solve. In my opinion the robot manages to avoid the obstacle but at certain point (in the red circle), the robot turns to the wrong side, making impossible the condition $H(t) = \theta(t) $ to be achieved.</p>

<p>I feel that I am not measuring properly the angle alpha between the $v_{obs}(t)+l_h(t)$ and $v_{R}(t)$ , because while debugging I can see that at certain point it stops switching between negative and positive values and become only positive, leading the robot's to the wrong side. It also seems to be related with my problem here: <a href=""http://robotics.stackexchange.com/questions/9386/"">Angle to a circle tangent line</a></p>

<p><a href=""http://i.stack.imgur.com/BDYtU.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/BDYtU.png"" alt=""angles""></a></p>
","mobile-robot kinematics matlab geometry"
"9772","Is this a singularity or incorrect implementation of inverse kinematics?","<p>I at moment trying to compute the Q configuration that moves my robot from it current state described by this transformation matrix.
with rotation</p>

<pre><code>0.00549713  0.842013  -0.539429  
0.999983 -0.00362229 0.00453632 
0.00186567 -0.539445 -0.842019
</code></pre>

<p>and position as this:</p>

<pre><code>-0.0882761
-0.255069
 0.183645
</code></pre>

<p>To this rotatation </p>

<pre><code>    0  0.942755 -0.333487
    1         0         0
    0 -0.333487 -0.942755
</code></pre>

<p>and this position</p>

<pre><code>8.66654
19.809
115.771
</code></pre>

<p>Due to the drastic change in the Z direction, I thought i could split the path between start and end into small chunks by creating data inbetween by interpolating, and compute the inverse kinematics for each of these small position.  Problem is that the output i am getting is pretty large.. Which making me suspect that some of the output might be wrong. The simulation i am using constrains the rotation to 360 degrees.. I think something goes wrong.. </p>

<p>The only reason I could think would do this, would be if the jacobian i am was using had singularities... Which why i assumed that i was running into singualarity issue.. </p>

<pre><code>setQ{22.395444, 319.402231, 90.548314, -228.989836, -295.921218, -336.808799}
setQ{8.209388, 362.472468, 108.618073, -232.346755, -299.935844, -334.929518}
setQ{8.479842, 399.892521, 127.432982, -234.017882, -303.852583, -335.063821}
setQ{8.224516, 362.232497, 108.666778, -232.319554, -299.899932, -334.928688}
setQ{7.718908, 286.832458, 71.150606, -228.913831, -291.982659, -334.658147}
setQ{7.468625, 249.092444, 52.400638, -227.206436, -288.018036, -334.522738}
setQ{7.220023, 211.325766, 33.656081, -225.496018, -284.049424, -334.387237}
setQ{-6.134091, -2538.260148, -1283.375216, -96.331289, 7.920957, -324.531125}
setQ{-6.261661, -2577.946595, -1301.730132, -94.403263, 12.176863, -324.388990}
setQ{-6.634286, -2697.165915, -1356.762411, -88.601053, 24.968521, -323.962029}
setQ{-6.991781, -2816.625206, -1411.745985, -82.771641, 37.796090, -323.534239}
setQ{-7.334148, -2936.324468, -1466.680853, -76.915029, 50.659572, -323.105620}
setQ{-7.661386, -3056.263702, -1521.567017, -71.031215, 63.558965, -322.676171}
setQ{-8.642914, -3457.794271, -1704.169136, -51.222052, 106.816303, -321.238686}
setQ{-8.988457, -3619.153075, -1777.058457, -43.213761, 124.230964, -320.661112}
setQ{-9.382564, -3821.451508, -1868.048346, -33.135395, 146.089069, -319.937071}
setQ{-9.528439, -3902.557525, -1904.406419, -29.082892, 154.860242, -319.646810}
setQ{-9.667591, -3983.770196, -1940.742846, -25.018300, 163.647376, -319.356179}
setQ{-9.734645, -4024.416527, -1958.902942, -22.981471, 168.046928, -319.210726}
setQ{-9.986053, -4187.268484, -2031.489209, -14.803929, 185.685040, -318.627992}
setQ{-10.210564, -4350.547057, -2103.988889, -6.578030, 203.386994, -318.043783}
setQ{-10.312734, -4432.346324, -2140.206259, -2.446947, 212.261912, -317.751125}
setQ{-10.453381, -4555.245201, -2194.491727, 3.772345, 225.604215, -317.311448}
setQ{-10.496902, -4596.264820, -2212.576060, 5.851488, 230.059630, -317.164705}
setQ{-10.538741, -4637.311102, -2230.654980, 7.933652, 234.519035, -317.017869}
setQ{-10.617377, -4719.483658, -2266.796587, 12.107048, 243.449816, -316.723922}
setQ{-10.812941, -4966.641247, -2375.091527, 24.699772, 270.337923, -315.839868}
setQ{-10.839651, -5007.927501, -2393.121742, 26.809138, 274.833240, -315.692203}
setQ{-10.888029, -5090.579998, -2429.165939, 31.036936, 283.835844, -315.396596}
</code></pre>

<p><code>setQ</code> is just a function for my simulation, the numbers are the actual Q values starting from 0 - 5. (I am using a 6 jointed robot (UR5))</p>

<p><strong>Update</strong></p>

<p>I am using a sphere to compute my desired transformation matrix.. The idea is that i want my arm be on this sphere, point inward to the center. </p>



<pre><code>std::vector&lt;Transform3D&lt;&gt;&gt; pathPlanning::sphere(double dx, double dy, double dz)
{
    double r = 5.0; // Radius of the sphere -  set to 5.0 cm (TODO: has to be checked if that also is accurate)
    cout &lt;&lt; ""Create a sphere"" &lt;&lt; endl;

    double current_x = this-&gt;device-&gt;baseTframe(this-&gt;toolFrame,this-&gt;state).P()[0];
    double current_y = this-&gt;device-&gt;baseTframe(this-&gt;toolFrame,this-&gt;state).P()[1];
    double current_z = this-&gt;device-&gt;baseTframe(this-&gt;toolFrame,this-&gt;state).P()[2];


    // Formula for sphere (x-x0)²+(y-y0)²+(z-z0)²=r²
    // x: x = x_0 + rcos(theta)sin(phi)
    // y: y = y_0 + rsin(theta)sin(phi)
    // z: z = z_0 + rcos(phi)
    // Angle range: 0 &lt;= theta &lt;= 2M_PI ; 0 &lt;= phi &lt;= M_PI

    double obj_x = current_x + dx;
    double obj_y = current_y + dy;
    double obj_z = current_z + dz;

    std::vector&lt;Transform3D&lt;&gt;&gt; out;

    int count = 32;

    for(double azimuthal = 0; azimuthal &lt;= M_PI ; azimuthal+=0.01 )
    {

        for(double polar = 0.35; polar &lt;= M_PI-0.35 ; polar+=0.01 )
        {

            double sphere_x = obj_x + r*cos(azimuthal)*sin(polar);
            double sphere_y = obj_y + r*sin(azimuthal)*sin(polar);
            double sphere_z = obj_z + + r*cos(polar);

            //string text = to_string(sphere_x) + "" , "" + to_string(sphere_y)+ "" , "" + to_string(sphere_z);
            //positions &lt;&lt; text &lt;&lt; endl;

            Transform3D&lt;&gt; transformation_matrix = transform(obj_x,obj_y,obj_z,sphere_x,sphere_y,sphere_z);

            if(0.1&lt;(transformation_matrix.P()[0] - current_x) || 0.1&lt;(transformation_matrix.P()[1] - current_y) || 0.1&lt;(transformation_matrix.P()[2] - current_z))
            {
                cout &lt;&lt; ""Interpolate: "" &lt;&lt; endl;

                std::vector&lt;Transform3D&lt;&gt;&gt; transformation_i = invKin_LargeDisplacement(transformation_matrix);
                out.insert(out.end(),transformation_i.begin(),transformation_i.end());
                cout &lt;&lt; out.size() &lt;&lt; endl;
                cout &lt;&lt; ""only returning one interpolation onto the sphere!"" &lt;&lt; endl;

                return transformation_i;
            }
            else
            {
                cout &lt;&lt; ""OK"" &lt;&lt; endl;
                out.push_back(transformation_matrix);

            }


            if(count == 32) //TODO: Why...... is this occuring?
            {
                //cout &lt;&lt; ""Theta: "" &lt;&lt; theta &lt;&lt; "" Phi: "" &lt;&lt; phi &lt;&lt; endl;
                //cout &lt;&lt; sphere_x &lt;&lt; "" , "" &lt;&lt; sphere_y &lt;&lt;"" , ""&lt;&lt; sphere_z &lt;&lt; endl;
                count = 0;
            }
            else
            {
                count++;
            }
        }
    }

    return out;
}
</code></pre>

<p>This function provides me with the point on the sphere, which is use to create my rotation matrix using <code>transform</code>.</p>

<pre><code>Transform3D&lt;&gt; pathPlanning::transform(double obj_x, double obj_y, double obj_z, double sphere_x, double sphere_y ,double sphere_z)
{
    // Z-axis should be oriented towards the object.
    // Rot consist of 3 direction vector [x,y,z] which describes how the axis should be oriented in the world space.
    // Looking at the simulation the z-axis is the camera out. X, and Y describes the orientation of the camera.
    // The vector are only for direction purposes, so they have to be normalized....
    // TODO: case [0  0 -1]... Why is it happening at what can be done to undo it?
    cout &lt;&lt; ""inside Transform"" &lt;&lt; endl;
    cout &lt;&lt; obj_x &lt;&lt; "","" &lt;&lt; sphere_x &lt;&lt; "" ; ""  &lt;&lt; obj_y &lt;&lt; "" , "" &lt;&lt; sphere_y  &lt;&lt;"" ; ""&lt;&lt; obj_z &lt;&lt; "" , "" &lt;&lt; sphere_z  &lt;&lt; endl;
    Vector3D&lt;&gt; dir_z((obj_x - sphere_x), (obj_y - sphere_y), (obj_z - sphere_z));
    //Vector3D&lt;&gt; dir_z((sphere_x-obj_x), (sphere_y - obj_y), (sphere_z-obj_z));
    dir_z = normalize(dir_z);
    Vector3D&lt;&gt; downPlane(0.0,0.0,-1.0);
    Vector3D&lt;&gt; dir_x = cross(downPlane,dir_z);
    dir_x = normalize(dir_x);
    Vector3D&lt;&gt; dir_y = cross(dir_z,dir_x);
    dir_y = normalize(dir_y);
    Rotation3D&lt;&gt; rot_out (dir_x,dir_y,dir_z);  // [x y z]

    Vector3D&lt;&gt; pos_out(sphere_x,sphere_y,sphere_z);

    Transform3D&lt;&gt; out(pos_out,rot_out);
    cout &lt;&lt; ""desired: "" &lt;&lt; out &lt;&lt; endl;

    return out;
}
</code></pre>

<p>The transform basically computes the rotation matrix. The math is based on the on this <a href=""http://robotics.stackexchange.com/questions/9691/generate-transformation-matrices-for-rotating-around-a-object"">post</a> by @Ben, which is an answer to a similar problem i am having..      </p>

<p><strong>Update</strong></p>

<p>Error with the rotation matrix was due to the polar coordinate being 0 => sin(0) = 0. </p>

<p>I made this plot displaying the determinant of the jacobian, while i compute the inverse kinematics for the large displacement. For each inverse kinematics iteration, I set the robot to the new q_i and use that as current and continue computing until i reach the end configuration. </p>

<p><a href=""http://i.stack.imgur.com/DBLWm.jpg"" rel=""nofollow""><img src=""http://i.stack.imgur.com/DBLWm.jpg"" alt=""Determinant of the different jacobians""></a></p>

<p>It seems that alot of them goes toward a singularity or in general a pretty low number..</p>

<p><strong>Update</strong></p>

<p>Again i think the singularities might be the culprit here.. </p>

<pre><code>determinant: 0.0424284
Q{13.0099, -46.6613, -18.9411, 2.38865, 5.39454, -4.53456}
determinant: -0.0150253
Q{47.1089, -0.790356, 6.89939, -2.725, -1.66168, 11.2271}
determinant: -0.0368926
Q{15.7475, 8.89658, 7.78122, -2.74134, -5.32446, 1.11023}
determinant: -0.0596228
Q{180.884, 66.3786, 17.5729, 9.21228, -14.9721, -12.9577}
determinant: -0.000910399
Q{5426.74, 5568.04, -524.078, 283.581, -316.499, -67.3459}
determinant: -0.0897656
Q{16.6649, -37.4239, -34.0747, -16.5337, -3.95636, -7.31064}
determinant: -0.00719097
Q{-1377.14, 167.281, -125.883, -10.4689, 179.78, 56.3877}
determinant: 0.0432689
Q{22.2983, -10.1491, -15.0894, -4.41318, -2.07675, -3.48763}
determinant: -0.0430843
Q{82.6984, -39.02, -24.5518, 13.6317, 4.17851, -14.0956}
determinant: -0.0137243
Q{425.189, -9.65443, 20.9752, 7.63067, 25.4944, -52.4964}
</code></pre>

<p>Everytime i compute a new Q I set the robot in that state, and perform inverse kinematics from that state.. Q is the joint angles for the 6 joints. </p>

<p><strong>Update</strong></p>

<p>Interpolation is done by lineary dividing the path from start to end into a specified amount of of data points.  </p>

<p><a href=""http://i.stack.imgur.com/gnHbZ.jpg"" rel=""nofollow""><img src=""http://i.stack.imgur.com/gnHbZ.jpg"" alt=""enter image description here""></a></p>

<p>This plot shows  each tranformation matrices generated from the interpolation and with their the position part plotted. The red dots is the path (every 1000th position). The blue ball is the object in want to track, and green dots represents the sphere.. As I am only doing this for the first point on the sphere, it only hits one point on the sphere, which is the top point, which the plot also shows. </p>

<p>Rotation doesn't show that much change, which also makes sense based difference between the current and desired rotations. </p>

<p><strong>Update</strong></p>

<p>My InvKin Implementation for LargeDisplacements:</p>

<pre><code>std::vector&lt;Q&gt; pathPlanning::invKin_largeDisplacement(std::vector&lt;Transform3D&lt;&gt;&gt; t_tool_base_desired_i)
{

     Device::Ptr device_backup = this-&gt;device;  //Read in device parameter
     WorkCell::Ptr workcell_backup = this-&gt;workcell; //Read in workcell parameter
     State state_backup = this-&gt;state;
     std::vector&lt;Q&gt; output;

     for(unsigned int i = 0; i&lt;t_tool_base_desired_i.size();  ++i)
     {
         Transform3D&lt;&gt; T_tool_base_current_i = device_backup-&gt;baseTframe(this-&gt;toolFrame,state_backup); //Read in Current transformation matrix

         Eigen::MatrixXd jq(device_backup-&gt;baseJframe(this-&gt;toolFrame,state_backup).e().cols(), this-&gt;device.get()-&gt;baseJframe(this-&gt;toolFrame,state_backup).e().rows());

         jq =  this-&gt;device.get()-&gt;baseJframe(this-&gt;toolFrame,state_backup).e(); // Get the jacobian for current_configuration

         //Least square solver - dq = [j(q)]T (j(q)[j(q)]T)⁻1 du  &lt;=&gt; dq = A*du
         Eigen::MatrixXd A (6,6);

         //A = jq.transpose()*(jq*jq.transpose()).inverse();
         A = (jq*jq.transpose()).inverse()*jq.transpose();

         Vector3D&lt;&gt; dif_p = t_tool_base_desired_i[i].P()-T_tool_base_current_i.P();  //Difference in position

         Eigen::Matrix3d dif = t_tool_base_desired_i[i].R().e()- T_tool_base_current_i.R().e(); //Differene in rotation
         Rotation3D&lt;&gt; dif_r(dif); //Making a rotation matrix the the difference of rotation
         RPY&lt;&gt; dif_rot(dif_r);    //RPY of the rotation matrix. 

         Eigen::VectorXd du(6); //Creating du
         du(0) = dif_p[0];
         du(1) = dif_p[1];
         du(2) = dif_p[2];

         du(3) = dif_rot[0];
         du(4) = dif_rot[1];
         du(5) = dif_rot[2];

         Eigen::VectorXd q(6);
         q = A*du; // computing dq

         Q q_current;
         q_current = this-&gt;device-&gt;getQ(this-&gt;state);
         Q dq(q); 
         Q q_new = q_current+ dq; // computing the new Q angles
         output.push_back(q_new); store it in the output vector
         device_backup-&gt;setQ(q_new,state_backup); //Set the robot to the calculated state. 
     }
     return output;
}
</code></pre>

<p>I am pretty sure that my interpolation works, as the plot shows.  My inverse kinematics on the other hand not so sure..</p>

<p><strong>Update</strong></p>

<p>@Chuck mentions in his answer that it would be a good idea to check the core functionality, which might shed some light on what could be going wrong. </p>

<p>I tried it with an inv.kin function i know would work, which didn't return any result, which make me doubt whether my transformation function i create is accurate?</p>

<p>The robot simulation is the one shown above..  The  <code>Transform</code> function shown above, is the function which i use to compute my desired, and provide my inverse kinematics..  Is something incorrectly setup?
<a href=""http://i.stack.imgur.com/UOYcL.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/UOYcL.png"" alt=""enter image description here""></a></p>

<p><strong>Update</strong></p>

<p>@Chuck came up with an different approach to my problem, which only has 3 DOF, being the position.  I choose change track, and peform a simple inverse kinematics given a distance dx,dy,dz.. Which for some reason isn't working quite good for me? even for small differences... </p>

<p>Here is my code:</p>

<pre><code>    std::vector&lt;Q&gt;pathPlanning::invKin(double dx, double dy , double dz)
{

    kinematics::State state =  this-&gt;state;
    Transform3D&lt;&gt; t_tool_base_current =  this-&gt;device.get()-&gt;baseTframe(this-&gt;toolFrame,state);

    cout &lt;&lt;""Current: ""&lt;&lt; t_tool_base_current.P().e()&lt;&lt; endl;

    Vector3D&lt;&gt; P_desired(0.000001+t_tool_base_current.P().e()[0],t_tool_base_current.P().e()[1],t_tool_base_current.P().e()[2]);
    cout &lt;&lt;""Desired: "" &lt;&lt;P_desired &lt;&lt; endl;

    Transform3D&lt;&gt; t_tool_base_desired(P_desired,t_tool_base_current.R());
    Eigen::MatrixXd jq(this-&gt;device.get()-&gt;baseJframe(this-&gt;toolFrame,state).e().cols(), this-&gt;device.get()-&gt;baseJframe(this-&gt;toolFrame,state).e().rows());
    jq =  this-&gt;device.get()-&gt;baseJframe(this-&gt;toolFrame,state).e();


    //Least square solver - dq = [j(q)]T (j(q)[j(q)]T)⁻1 du  &lt;=&gt; dq = A*du
    Eigen::MatrixXd A (6,6);

    //A = jq.transpose()*(jq*jq.transpose()).inverse();
    A = (jq*jq.transpose()).inverse()*jq.transpose();

    Vector3D&lt;&gt; dif_p = t_tool_base_desired.P()-t_tool_base_current.P();
    cout &lt;&lt;""difference: "" &lt;&lt;dif_p &lt;&lt; endl;

    Eigen::VectorXd du(6);
    du(0) = dif_p[0];
    du(1) = dif_p[1];
    du(2) = dif_p[2];

    du(3) = 0;
    du(4) = 0;
    du(5) = 0;

    Eigen::VectorXd q(6);
    q = A*du;

    Q q_current;
    q_current = this-&gt;device-&gt;getQ(this-&gt;state);
    Q dq(q);
    Q q_new = q_current+ dq;
    std::vector&lt;rw::math::Q&gt; output;
    if(!collision(q_new))
    {
        output.push_back(q_new);
    }
    else
    {
        cout &lt;&lt; endl;      
        cout &lt;&lt; q_new &lt;&lt; endl;
    }

    return output;
}
</code></pre>

<p>which outputs this</p>

<pre><code>Current: -0.000799058
-0.282
0.99963
Desired: Vector3D(-0.000789058, -0.282, 0.99963)
difference: Vector3D(1e-05, 0, 0)
setQ{1.559142, 110474925659325248.000000, -1834.776226, 55426871347211368.000000, 0.068436, 88275880260745.328125}
</code></pre>

<p><code>setQ</code> is the state which moves the robot to the desires state.. 
Either is something wrong with my implementation, or it is a singularity..</p>

<p>Especially because i am not moving it that much (0.00001)!!!</p>

<p><strong>Updates</strong></p>

<p>I think I have solved the mystery.. It must be the sphere function which creates points that outside the reach of the robot.!! </p>
","robotic-arm inverse-kinematics"
"9776","How do I compute the inverse kinematics given a desired transformation matrix?","<p>I am at the moment trying to implement an inverse kinematics function which function is to take a desired transformation matrix, and the current transformation matrix, and compute the Q states that is needed to move my robot arm from current state to end state. </p>

<p>I have already written the code, but since my simulation isn't showing the right path, or what I would expect it to be, this makes me unsure as to whether my implementation is correct.  Could someone comment on my implementation and maybe spot an error?</p>

<pre><code>std::vector&lt;Q&gt; pathPlanning::invKin_largeDisplacement(std::vector&lt;Transform3D&lt;&gt;&gt; t_tool_base_desired_i)
{

    for(unsigned int i = 0; i&lt;t_tool_base_desired_i.size();  ++i)
    {
        Transform3D&lt;&gt; T_tool_base_current_i = device_backup-&gt;baseTframe(this-&gt;toolFrame,state_backup);
        Eigen::MatrixXd jq(device_backup-&gt;baseJframe(this-&gt;toolFrame,state_backup).e().cols(), this-&gt;device.get()-&gt;baseJframe(this-&gt;toolFrame,state_backup).e().rows());
        jq =  this-&gt;device.get()-&gt;baseJframe(this-&gt;toolFrame,state_backup).e();


        //Least square solver - dq = [j(q)]T (j(q)[j(q)]T)⁻1 du  &lt;=&gt; dq = A*du
        Eigen::MatrixXd A (6,6);
        //A = jq.transpose()*(jq*jq.transpose()).inverse();
        A = (jq*jq.transpose()).inverse()*jq.transpose();

        Vector3D&lt;&gt; dif_p = t_tool_base_desired_i[i].P()-T_tool_base_current_i.P(); // Difference in position between current_i and desired_i

        Eigen::Matrix3d dif = t_tool_base_desired_i[i].R().e()- T_tool_base_current_i.R().e(); // Difference in rotation between current_i and desired_i

        Rotation3D&lt;&gt; dif_r(dif); //Construct rotation matrix
        RPY&lt;&gt; dif_rot(dif_r); // compute RPY from rotation matrix

        //Jq*dq = du
        Eigen::VectorXd du(6);
        du(0) = dif_p[0];
        du(1) = dif_p[1];
        du(2) = dif_p[2];

        du(3) = dif_rot[0];
        du(4) = dif_rot[1];
        du(5) = dif_rot[2];

        Eigen::VectorXd q(6);
        q = A*du; // Compute change dq

        Q q_current;
        q_current = this-&gt;device-&gt;getQ(this-&gt;state); // Get Current Q
        Q dq(q);
        Q q_new = q_current+ dq; // compute new Q by adding dq
        output.push_back(q_new); // Pushback to output vector
        device_backup-&gt;setQ(q_new,state_backup); //set current state to newly calculated Q.

    }
    return output;
}
</code></pre>

<p>Example of output: </p>

<pre><code>Q{-1.994910, -94.421754, -123.448429, 15.218864, 6.602184, -13.742988}
Q{2627.867315, -2048.863588, -51.340574, 287.654959, 270.187026, 258.581800}
Q{12941.812459, -536.870516, -294.362593, -2145.963577, -31133.660814, -4742.343433}
Q{32.044799, -14.220020, -14.312226, -12.444921, 12.269179, -24.393637}
Q{125.537278, 28.626924, -55.646716, -20.945348, 17.536762, -2.656717}
Q{9.514525, -107.455064, -17.009190, -15.245588, -0.960273, -2.010570}
Q{8.255582, -3.010934, -4.882207, -1.369533, 0.848644, 1.175172}
Q{208.655993, -28.443465, -64.413952, -3.129896, 13.063806, -6.042187}
Q{-73.706483, -20.381540, -5.306434, -1.204419, -4.035149, 21.806934}
Q{10.003481, 10.867394, 13.256192, -6.491445, -1.711469, 2.896646}
Q{24.890626, -72.265307, -94.886507, 12.327304, -4.425786, 4.188531}
Q{7.111258, 31.500732, -0.111033, -20.434697, 5.302118, 1.781690}
Q{477.993581, 659.221820, 19.819916, -88.627757, 65.850191, -77.267367}
Q{-30.672145, -53.496243, -18.170871, 83.648574, 48.311796, -28.015005}
Q{-36.677982, -15.908633, 17.751008, 0.995766, -0.500259, 9.409435}
Q{114246.358249, -10664.813432, -75.904830, 462.907904, 7992.514723, -18484.319327}
Q{83.827086, -75.899321, -38.576446, 37.266068, 47.843725, 39.096061}
Q{-119.682661, -774.773093, -251.969174, 23.212110, -42.662580, 53.247454}
Q{98.608881, -28.013383, 132.896921, 17.121488, 36.916894, -14.627180}
Q{-11519.051453, 5761.564318, -364.916044, -1188.567128, -2582.813750, -462.784007}
Q{54802.605226, 40971.776641, 10204.739981, -654.963987, -244.277958, -8618.970216}
Q{-21.334047, -14.314134, 17.714174, 2.463993, 0.963385, 5.304530}
</code></pre>
","robotic-arm inverse-kinematics c++"
"9783","Perspective n Point - RPnP algorithm","<p>I need to caculate the pose of a camera using an image of an artificial landmkark. For this porpouse I am trying to use the Perspective n Point approach so I can calculate it using the intrinsic camera matrix, the world coordinates of the landmark (I am using 4 points) and its projection in the image.</p>

<p>There are some algorithms to solve this (PnP, EPnP, RPnP, etc) and I am trying to use the RPnP. I have found an implementation of this here:
<a href=""http://xuchi.weebly.com/rpnp.html"" rel=""nofollow"">http://xuchi.weebly.com/rpnp.html</a></p>

<p>I used this code but I am having some problems because I can't obtain the correct pose.</p>

<p>I am using the P.Corke's Robotics Toolbox for MATLAB to create a CentraCamera with a known pose and calculating the projection of the landmark in this camera, but the rotation and translation that the RPnP returns me is not the same as I defined before.</p>

<p>Anyone has used this RPnP algorithm to solve that kind of problems?</p>
","computer-vision cameras 3d-reconstruction"
"9786","How do the PID parameters (Kp, Ki, and Kd) affect the heading of a differential driving robot when they are increased individually?","<blockquote>
  <p><strong>Question:</strong> A PID controller has three parameters Kp, Ki and Kd which could affect the output performance. A differential driving robot is controlled by a PID controller. The heading information is sensed by a compass sensor. The moving forward speed is kept constant. The PID controller is able to control the heading information to follow a given direction. Explain the outcome on the differential driving robot performance when the three parameters are increased individually. </p>
</blockquote>

<p>This is a question that has come up in a past paper but most likely won't show up this year but it still worries me. It's the only question that has me thinking for quite some time. 
I'd love an answer in simple terms. Most stuff i've read on the internet don't make much sense to me as it goes heavy into the detail and off topic for my case.  </p>

<p><strong>My take on this:</strong></p>

<p>I know that the <strong>proportional</strong> term, Kp, is entirely based on the error and that, let's say, double the error would mean doubling Kp (applying proportional force). This therefore implies that increasing Kp is a result of the robot heading in the wrong direction so Kp is increased to ensure the robot goes on the right direction or at least tries to reduce the error as time passes so an increase in Kp would affect the robot in such a way to adjust the heading of the robot so it stays on the right path.</p>

<p>The <strong>derivative</strong> term, Kd, is based on the rate of change of the error so an increase in Kd implies that the rate of change of error has increased over time so double the error would result in double the force. An increase by double the change in the robot's heading would take place if the robot's heading is doubled in error from the previous feedback result. Kd causes the robot to react faster as the error increases. </p>

<p>An increase in the <strong>integral</strong> term, Ki, means that the error is increased over time. The integral accounts for the sum of error over time. Even a small increase in the error would increase the integral so the robot would have to head in the right direction for an equal amount of time for the integral to balance to zero. </p>

<p><em>I would appreciate a much better answer and it would be great to be confident for a similar upcoming question in the finals.</em> </p>
","pid wheeled-robot differential-drive"
"9797","What is the wheel base distance of the create2?","<p>What is the wheel base distance that should be used for the create2 to calculate angle? I have seen 230.8mm in code samples but the manual seems to indicate 235.0 mm.</p>
","irobot-create roomba"
"9798","How to compensate the brushless DC motor for voltage drop?","<p>I am working on a quadcopter project based on Arduino board, my system is powered by a 4S LiPo battery (14.8V) but the motors behave differently as the battery voltage drops, when discharging. </p>

<p>Is there any way that I can make the motors behave the same until a minimum value of, say, 5 volts?</p>

<p>My current system works fine at the range from 14.8 to 10 volts, but below that I can't even hover.</p>
","quadcopter"
"9801","Low latency control from a laptop","<p>Lets say that I needed to send sensor readings in increments of 100 bytes from a micro controller to a laptop with sub 2 ms latencies in real time (the data needs to be processed and acted upon immediately (to control a robot)). What interfaces would one use? </p>

<p>FTDI usb-serial converters aren't an option because they introduce 5-10 ms latencies both ways. PCI cards are an option though.</p>
","low-latency laptop"
"9802","All-in-one GNSS localization solution (hardware+software)","<p>Is there something like an all-in-one satellite based localization solution that would contain both hardware and software to do GNSS localization for robotics? I mean a package that would also contain an IMU, would fuse it with GPS and filter the result accordingly and then provide a software API to query for location/speed etc.</p>

<p>I am interested rather in some affordable solution but is there some professional hardware too?</p>

<p>I am trying to implement this for my mobile robot and I realize that a smartphone-grade GPS (Samsung J5) gives me better preliminary results than an u-blox eval board (<a href=""http://www.csgshop.com/product.php?id_product=221"" rel=""nofollow"">this NEO-M8T</a> with integrated antenna and ground plane) - I wonder why, I guess Android may fuse the IMU and have better readings even with worse antenna?</p>
","localization software gps gnss hardware"
"9803","TCP Communication with PCDuino","<p>I'm working on a robot that is controlled by an xbox controller connected to a windows computer and commands are sent to a pcduino through a tcp connection. I have it working by sending a string of 1's and 0's to tell the pcduino which motors to turn on. I'm trying to optimize it by just sending an int and using bit masks to make the decisions on the pcduino but I can't get the pcduino to receive the int correctly. I tested the windows function sending the command with sokit and its sending the correct values but the pcduino is receiving the same number even when the commands are changing.</p>

<p>This is what its doing:</p>

<p>Windows          -> PCDuino</p>

<p>command = 1      -> sendBuff = 73932</p>

<p>cmdstring = 1    -> n = 1</p>

<hr>

<p>command = 1025   -> sendBuff = 73932</p>

<p>cmdstring = 1025 -> n = 4</p>

<hr>

<p>My windows functions are:</p>

<pre><code>bool Client::Send(char * smsg)
{
    int iResult = send(ConnectSocket, smsg, strlen(smsg), 0);

    if (iResult == SOCKET_ERROR)
    {
        std::cout &lt;&lt; ""Sending Message has failed: "" &lt;&lt; WSAGetLastError() &lt;&lt; ""\n"";
        Stop();
        return false;
    }
    return true;
}
</code></pre>

<hr>

<pre><code>    bool sendCommand()
{
    cmdbuffer &lt;&lt; command;
    cmdstring = cmdbuffer.str();

    if (!client-&gt;Send((char *)cmdstring.c_str()))
    {
        std::cout &lt;&lt; ""Disconnected from Server. Press Enter to Exit"";
        std::cin.ignore();
        std::cin.get();
        return false;
    }
    return true;
}
</code></pre>

<hr>

<p>PCDuino Loop Function</p>

<pre><code>void loop()
{
    recBuff = 0;
    deviceFlag = 0;

    //Read Socket

/******************************************************************************/

    read(connfd, sendBuff, strlen(sendBuff));
    recBuff = atoi(sendBuff);

/******************************************************************************/

    //Set Current Device to Receive Instructions From
    checkAuto(recBuff, currDevice);

    //Find Current Device of Command
    deviceFlag = checkDevice(recBuff);

    //If Current Device and Set Device are the Same Parse Command
    if (deviceFlag == currDevice)
    {
        parseHex(recBuff);
    }
    usleep(50000);
}
</code></pre>

<hr>

<p>I have a printf after the read call and that's where I am getting the 73932 number. I think I have everything you guys need but if there's anything else I need to add let me know. I'm stumped...I don't know if its just a casting problem or what.</p>

<h2>Update 1</h2>

<p>What I have before everything the setup and loop functions on the PCduino run is:</p>

<pre><code>int listenfd = 0, connfd = 0;
int n;
struct sockaddr_in serv_addr;
char sendBuff[1025];
time_t ticks;
</code></pre>
","communication c++ c"
"9819","Dji Wookong-M - To unstable to take off","<p>I've built a quadcopter using the Dji Wookong-M. As of a couple of weeks ago I have been able to get everything to work except for one small thing. When I throttle up the Drone tends to flip to the side. I have tested all the motors over and over again and I know that they are spinning the right direction and that I have the right props on the right motors. I tested on both grass and concrete but both times it flipped. It starts to flip once the throttle is past 50%. I don't know if it is catching or if something is off balance although I don't think this is the problem since the quadcopter tips different directions almost every time. If any one could tell me what is wrong I would appreciate it a lot since my project is due in 2 1/2 weeks.</p>

<p>Thanks in Advance</p>
","quadcopter"
"9822","Intropection ROS objects using python client library","<p>How can I see attributes and methods of ROS objects ? Can I use inspect module of Python ?</p>

<p>Like in python I can use dir(), type() commands.</p>
","ros python"
"9825","Stability of PID values update function for quadrotor","<p>A reviewer of the last paper I sent replied me that the it is very dangerous to update a PID with next kind of formula (paper is about quadrotor control):</p>

<p>$$
K_p (t + 1) = K_p (t)+e(t) (μ_1 (Pe(t)) + μ_4 (Pe(t)))
$$</p>

<p>$Pe(t)$ is the % relationship between the desired angles and the real angles, and $e(t)$ is the difference between those angles. $μ_1$ and $μ_4$ are the membership functions of a fuzzy function. I think that the reviewer is talking about the time increment update rather than the fuzzy usage and specific formula.</p>

<p>How can stability of this formula be tested, please?</p>

<p>EDIT: </p>

<p>membership functions are represented in following graph:
<a href=""http://i.stack.imgur.com/rJ3vy.jpg"" rel=""nofollow""><img src=""http://i.stack.imgur.com/rJ3vy.jpg"" alt=""enter image description here""></a></p>

<p>$e(t)$ is not the absolute difference between angles, just the difference. It can be negative</p>
","quadcopter control pid"
"9826","Angular momentum of rimless wheel in Passive Dynamic Walking","<p>In Tad McGeer's work, <a href=""https://www.google.com.tw/url?sa=t&amp;rct=j&amp;q=&amp;esrc=s&amp;source=web&amp;cd=1&amp;cad=rja&amp;uact=8&amp;ved=0ahUKEwjz9KOX5NTMAhWEj5QKHbauD9YQFgggMAA&amp;url=http%3A%2F%2Fruina.tam.cornell.edu%2Fresearch%2Fhistory%2Fmcgeer_1990_passive_dynamic_walking.pdf&amp;usg=AFQjCNG1l9gbWCJgcd5mlARdYvOcD123cg&amp;sig2=cE3ne4MxgyCi_9lnRHT3Bw"" rel=""nofollow"">Passive Dynamic Walking</a>, in 1990, he mentions the rimless wheel model, which is used to approximate the bipedal locomotion. I can't understand why the angular momentum is written as follows.</p>

<p>$H^-=(\cos 2\alpha_0+r^2_{gyr})ml^2\Omega^-$</p>

<p>I have the following questions:</p>

<ol>
<li><p>Isn't the angular momentum be $I*\omega$, $m^2l\Omega$ as the paper's notation?</p></li>
<li><p>If $\alpha_0$ is $\frac{\pi}{2}$ and $r_{gyr}$ approaches to 0, shouldn't the angular momentum before impact, $H^-$, be negative? Then how the conservation goes?</p></li>
</ol>
","wheel walking-robot"
"9828","Brushless motor from RC car won't spin with even small resistance","<p>I recently bought a RC car kit and after 10 minutes it stopped going. </p>

<p>When I throttle, I can see the motor trying to spin but it will just grind and get hot quite fast.</p>

<p>The motor does move if I disconnect it from the big gear, but not as fast as it did when new and it will still get very hot. Also, I can stop it with my fingers with a very slight touch.</p>

<p>I don't know anything about motors or ESCs, so I'm not sure if my problem is the motor or the ESC.  Did I burn it out?</p>
","brushless-motor esc radio-control"
"9832","What are the biggest challenges to build an highly performant robotic hand?","<p>When looking at the robotic hands made by researchers that are said to be rather close to a real human hand, they can easily cost tens of thousands of dollars.  </p>

<p>What makes them so much expensive? Sure there are lots of joints where parts must move, but it's still hard to figure out how it can cost so much even with highly precise servomotors.  </p>

<p>What is so much expensive when trying to build a humanoid hand? How can we make it less expensive? What do these expensive hands can do, that a diy cheap hand project can't?  </p>

<p>Thank you.</p>
","humanoid"
"9839","Which mechanical device could repeatedly present an ID tag to a card-reader","<p>I'm trying to build a test-automation robotic arm which can repeatedly present an ID-tag (such as RFID or NFC card or fob) to a card reader.</p>

<p>I suspect our reader fails either (a) after hundreds of presentations or due to fast presentations or (b) at a specific <em>moment</em> in the reader duty cycle.</p>

<p>The tag needs to move in a well-controlled manner:</p>

<ol>
<li>Quickly present the card, </li>
<li>Pause (mark)</li>
<li>Quickly remove the card,</li>
<li>Pause (space)</li>
<li>Repeat at 1.</li>
</ol>

<p>I'm calling the present/remove sequence the mark-space ratio for simplicity.</p>

<p>The tests I want to perform involve varying (a) the frequency and (b) the mark-space ratio, to (a) stress-test and  (b) boundary-test the re-presentation guard times built into the reader to debounce presentations.</p>

<p>The guard times are around 400ms, response around 100ms, so I need something that can move in and out of a 5-10cm range quickly and repeat within those sorts of timescales. </p>

<p>The distance the card needs to move depends on the reader model, as they have different field ranges. I want to get through the edge of the field quickly to avoid any inconsistencies in testing.</p>

<p>I'm able to do any programming (professional) and simple electromechanical design and build (ex-professional, now hobbyist). I only need to build one, it doesn't have to be particularly robust, but it does need to be fairly accurate with regard to the timings to do the second test.</p>

<p>What I've done so far:</p>

<p>I've built one version already using a Raspberry Pi, GPIO, a stepper motor with an aluminium arm screwed to a wheel.  It works, but it's a bit jerky and too slow, even with a 30cm arm to amplify the motion. It will probably do for the repeat test, but it's not time-accurate enough to do the timing tests.</p>

<p>My other design ideas were: </p>

<ul>
<li>Servo (are these also slow?)</li>
<li>Solenoid (fast, but too limited range? and might cause EM?)</li>
<li>Motor (too uncontrollable, and will require too much mechanical work for me)</li>
<li>Rotating drum (fast, stable, but cannot control mark-space ratio)</li>
</ul>

<p>I'm not a electro-mechanical design expert, so I'm wondering if I'm missing an electrical device or mechanical design which can do this more easily.</p>
","robotic-arm raspberry-pi stepper-motor industrial-robot automation"
"9840","The velocity profile of my robot is fluctuating","<p>I am presently doing a robotics project. I am using USARSIM (Urban Search and Rescue Simulation) to spawn a robot. I am trying to create different behaviors, like: </p>

<ul>
<li>goal following behavior; </li>
<li>obstacle avoidance behavior, and; </li>
<li>wall following behavior for my robot. </li>
</ul>

<p>I first generate the robots in USARSIM. Then I specify a goal location to the robot and provide it with a speed. The robot then moves to the goal location at the specified speed. USARSIM provides me the (x, y, z) coordinates of the vehicle at every time stamp. Based on the the coordinates received, I am trying to calculate the instantaneous speed of the robot at every time stamp. The instantaneous speed graph is fluctuating a lot. </p>

<p>In a specific case, I am providing the robot with 0.2 m/s. The velocity profile is shown below. I am unable to understand the reason behind it.</p>

<p><a href=""http://i.stack.imgur.com/731c5.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/731c5.png"" alt=""The velocity profile of my robot""></a></p>

<p>Here are some observations that I have made. </p>

<ol>
<li>As I increase the speed of the robot, the variations are decreasing.</li>
<li>Suppose, I provide a straight trajectory to the robot, it doesn't follow the straight trajectory. Does it explain why my velocity profile is fluctuating a lot ? </li>
</ol>

<p><a href=""http://i.stack.imgur.com/i7DHF.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/i7DHF.png"" alt=""Graph of &quot;Provided trajectory&quot; versus &quot;Followed trajectory&quot;""></a></p>

<p>Please let me know if any one can provide me a possible explanation for the variance in my velocity profile.</p>
","mobile-robot"
"9842","Generalized Voronoi Diagram","<p>I need to compute the Voronoi diagram for a map with some obstacles but I can't find any pseudo-code or example in MATLAB.</p>

<p>The ""voronoi"" function in MATLAB works with points, but in this case the obstacles are polygons (convex and non-convex). You can see the map in the attached image.</p>

<p><a href=""http://i.stack.imgur.com/XmMdq.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/XmMdq.png"" alt=""enter image description here""></a></p>

<p>Because the obstacles are polygons I found that the Voronoi algorithm needed is the GVD (Generalized Voronoi Diagram).</p>

<p>Can anyone help with code or examples on internet explaining how to compute this GVD?</p>
","mobile-robot motion-planning geometry"
"9845","Path planning vs. linear interpolation?","<p>I at moment trying to convince myself that what I need is a simple path planning algorithm, instead of linearly interpolating between a current and a desired state. </p>

<p>I am at moment working with an robot arm (UR) with an camera mounted on to its TCP.  The application i am trying to create is a simple ball tracking application which tracks the movement of the ball, while always keeping the object within sight. </p>

<p>which meant that I needed some form of path planning algorithm which plans the path between my current state and the desired state. The path should should be such that the ball is always kept in focus while the arm moves to the desired state.</p>

<p>But then I began question myself whether it was a bit overkill, and whether a simple straight line interpolation wouldn't suffice?.. I am actual not sure what form of benefit i would have by choosing a pathplanner, than a simple interpolation..</p>

<p>Interpolation would also generate the path I desire, so why choose a pathPlanner at all?</p>

<p>would someone elaborate?
It should be noted that obstacle avoidance is also a part of the task, which could cause trouble for a straight line interpolating. </p>
","robotic-arm"
"9848","Path planning for visual servoing","<p>I am at moment trying to implement a visual servoing application. 
the robot I am using is a UR5, and TCP has a stereo camera mounted on to it. The idea is to move the end effector according to the object being tracked. </p>

<p>The path-planning algorithm for this system should comply with some rules. </p>

<ol>
<li>The path which it creates should be collision free, and always keep the object being tracked at sight at all time. </li>
</ol>

<p>Having a path that keeps the object in sight has been bit of problem.  Sometime will the end effector rotate around itself, messing up  measurements taken and thus the tracking itself. </p>

<ol start=""2"">
<li>It should be able to maneuver away from static obstacles. </li>
</ol>

<p><strong>A Possible solution?</strong></p>

<p>I thought of a possible solution. Since my current state and desired state is defined by two different sphere, A possible solution would be to create a straight line between each center of each sphere, and between the current position and desired position, such that a straight path in between could be computed easily.  which always keeps itself oriented toward the object. Problems is that I am not sure how I should handle collision here.</p>

<p><a href=""http://i.stack.imgur.com/kyJdi.jpg"" rel=""nofollow""><img src=""http://i.stack.imgur.com/kyJdi.jpg"" alt=""""></a></p>

<p><strong>Update</strong>
  Or use it as a heuristic for a heuristic based path planning? </p>
","robotic-arm motion-planning algorithm"
"9850","Holonomic and Non-holonomic UAV's: Gliders vs Quadcopters","<p>Good day I would just like to ask if a fixed wing aircraft such as a glider(without thrust capability therefore needs external forces such as air flow to move constraining its movement) can be considered a non-holonomic system considering the fact that it cannot move freely compared to a quadcopter that is holonomic.</p>

<p><strong>I found this information from:</strong> <a href=""http://robotics.stackexchange.com/questions/9642/whats-the-difference-between-a-holonomic-and-a-nonholonomic-system"">What&#39;s the difference between a holonomic and a nonholonomic system?</a></p>

<p>Mathematically:</p>

<p>Holonomic system are systems for which all constraints are integrable into positional constraints.</p>

<p>Nonholonomic systems are systems which have constraints that are nonintegrable into positional constraints.</p>

<p>Intuitively:</p>

<p>Holonomic system where a robot can move in any direction in the configuration space.</p>

<p>Nonholonomic systems are systems where the velocities (magnitude and or direction) and other derivatives of the position are constraint.</p>
","quadcopter control uav glider"
"9851","How do I evaluate the minimum requirements of the processor and camera for a visual SLAM robot?","<p>I would like to build a visual SLAM robot (just for self-learning purpose) but I get frustrated how I know which processor and camera should be used for visual SLAM. </p>

<p>First, for the processor, I have seen three articles, which shows different systems are used for implementing their SLAM algorithm:</p>

<ol>
<li><p>Implementing SLAM algorithm (however it uses ultrasonic sensor rather than visual sensor) in Raspberry Pi (processing power is only 700 MHz) in <a href=""http://www.academia.edu/11312106/Implementing_Odometry_and_SLAM_Algorithms_on_a_Raspberry_Pi_to_Drive_a_Rover"" rel=""nofollow"">Implementing Odometry and SLAM Algorithms on a Raspberry Pi to Drive a Rover</a>  </p></li>
<li><p>I have also seen that Boston Dynamics use Pentium CPU, PC104 stack and QNX OS for their Big Dog project, <a href=""http://www.bostondynamics.com/img/BigDog_Overview.pdf"" rel=""nofollow"">BigDog Overview
November 22, 2008</a></p></li>
<li><p>Then, I also found a project uses a modern XILINX Zynq-7020 System-on-Chip (a device that combines FPGA resources with a dual ARM Cortex-A9 on
a single chip), for a Synchronized Visual-Inertial Sensor System, in <a href=""http://ieeexplore.ieee.org/xpl/login.jsp?tp=&amp;arnumber=6906892&amp;url=http%3A%2F%2Fieeexplore.ieee.org%2Fxpls%2Fabs_all.jsp%3Farnumber%3D6906892"" rel=""nofollow"">A synchronized visual-inertial sensor system with FPGA pre-processing for accurate real-time SLAM</a></p></li>
</ol>

<p>But after reading those, I have no clue how they end up with those decisions to use those kinds of processors, stacks or even OSes for their project. Is there a mathematical way, or a general practice, to evaluate the minimum requirement of the system (as cheap and as power efficient as possible) for an algorithm to run? </p>

<p>If not, how could I know what processor or system I have to prepare for a visual SLAM robot? If there is no simple answer, it is also cool if you can recommend something I could read to have a good start.</p>

<p>Secondly, I also cannot find clear information which camera I should use for a visual SLAM robot. I also have no idea how they evaluate the minimum requirement of the camera. I found a lot of papers saying they use RGB-D camera but when I Google to find one, there are very few commercially available. The one I found is Xtion Pro Live from ASUS Global (for $170 which is quite affordable for me), but they are out of stock. Are there any practice I can choose a suitable camera system for visual SLAM too? </p>

<p>Sorry if my question is too long. I feel that choosing the system and camera looks like a thing that requires a lot of experience and background knowledge. So rather than direct suggestions, it is cool if you have some ideas/recommended resources for me to learn the general ways people make such decisions in general or in similar projects, if any.</p>
","slam"
"9852","Vector Field Histogram: (VFH) Certainty value","<p>Good day I am currently trying to implement a vector field histogram algorithm. May I ask if anyone knows what the certainty value pertains to? From my understanding it is like a point system for a cell to increment its certainty every sensor read. Is this right? Thank you.</p>
","quadcopter motion-planning vfh"
"9853","Maximum distance for robotic arm throwing","<p>I have a <a href=""http://www.robotnik.eu/robotics-arms/robotis-manipulator/"" rel=""nofollow"">6DOF robotic arm</a> which I am using to throw a ball. Each joint can achieve a maximum velocity of 30 RPM (180 deg/s). I have been trying to generate joint angles manually and feeding them to see how far I can throw the ball until now. This has shown me that it's like less than 2 meters. </p>

<p>But I feel that I may not be combining the motions of the various motors in order get better throwing distance. I wanted to know if there is a simple way of theoretically determining the maximum distance I can throw. I read a few papers that appear very complicated, I do not need a very accurate value, just an estimate so that I decide whether I should move to a different arm. </p>
","robotic-arm"
"9856","Cosine interpolation between two transformation matrices?","<p>Is it possible to perform cosine interpolation between two transformation matrices?</p>

<p>It make sense for the translation part, but how about the rotational part?</p>
","robotic-arm stereo-vision"
"9863","Configuration space obstacle - calculating collision","<p>I need to calculate the configuration space obstacle to planning a path with a mobile robot. The idea is to divide the obstacles and the robot in triangles and test whether is there any triangle from the robot colliding with any triangle from the obstacles.</p>

<p>The approach to solve this is to test this between two triangles each time so I need to look if any of the 6 edges (3 for each triangle) divide the triangles so 3 vertex from one of them lie in one side and the other 3 vertex lie on the other side of the line.</p>

<p>I wrote some code to calculate the line equation (y = m*x + b) and I think it is correct, but I am having problems when the line is vertical (this means that m = -Inf) because MATLAB gives me a NaN when I calculate the equation for it. I am not sure how to handle this.</p>

<p>Here you can see a snippet from the code where I test the 3 edges from the 
robot triangle:</p>

<pre><code>for i = 1:1:3

    vertex_x = P1(edge(i,:),1);
    vertex_y = P1(edge(i,:),2);
    m = (vertex_y(2) - vertex_y(1))/(vertex_x(2) - vertex_x(1));
    b = -m*vertex_x(1) + vertex_y(1);

    for j = 1:1:6   % For each vertex...
        pto = listaVertices(j,:);
        if (m*pto(1) + b &gt; pto(2))
            % Vertex lies below the edge...
            cont1 = cont1 + 1;
        elseif (m*pto(1) + b &lt; pto(2))
            % Vertex lies above the edge...
            cont2 = cont2 + 1;
        else
            % Vertex lie inside the edge...
            % Do nothing
        end
    end 

    % 3 vertex on one side and 1 on the others side means they do not
    % collide. Two of the vertex always lie inside the line (the two vertex 
    % of each edge).

    if (cont1 == 1 &amp;&amp; cont2 == 3) || (cont1 == 3 &amp;&amp; cont2 == 1)
        flag_aux = false;   % Do not collide...
    end
    % Reset the counters for the 3 next edges...
    cont1 = 0;
    cont2 = 0;

end
</code></pre>

<p>Anyone could help with this issue?</p>
","mobile-robot motion-planning matlab"
"9870","Quadcopter starts at max speed","<p>I am new to Quadcopters. Just recently started with </p>

<ul>
<li>4, 1000kV motors</li>
<li>2 ESC with 490Hz / 2 ESC with 50 Hz (got it from eBay, just found from software)</li>
<li>CC3D OpenPilot (now LibrePilot)</li>
<li>FlySky FS-T6 (6 channel)</li>
</ul>

<p>Problem:</p>

<ol>
<li><p>When I configure and calibrate with LibrePilot software, motors run fine with radio, slow on min and fast on max. But as soon as I remove the USB cable and runs it directly from radio, suddenly on min they go to max speed.</p></li>
<li><p>I have calibrated manually also with ESC cable in receiver with radio, it works perfectly during process and when cable move to CC3D and runs again, shows same behaviour. </p></li>
<li><p>I have calibrated the motors directly, using only the LibrePilot configuration software and it works fine while connected through USB cable. </p></li>
</ol>
","quadcopter calibration"
"9871","convert toolframe coordinate to world frame coordinates?","<p>I am not sure how i should explain this, I am looking for a way to plot the trajectory an robot arm.  An object is seen from the toolFrame frame, but how do I plot the position of each joint, such that the frame they uses are the same.</p>

<p>One way would be to use the world frame as reference, but how would i plot the position of the object related to the world frame?</p>
","kinematics matlab visualization"
"9876","Connect 3S Li-Po Battery to 4 ESC","<p>I have 4 ESC with Male XT60 Connector and the Battery has 3 Male JST connectors.</p>

<p>I want to connect all 4 ESC to this one battery. I will have the following connectors: </p>

<blockquote>
  <p>XT60 Male to XT60 Female JST Female</p>
</blockquote>

<p>But this can connect only one ESC to the battery. How can I connect all 4 ESC to the battery.</p>

<p>I know last option is soldering, which I want to avoid as I am CSE guy.</p>
","quadcopter battery esc connector"
"9882","Madgwick sensor function algorithm: two issues","<p>I am studying the popular Madgwick algorithm for IMU, and stumbled with two issues:</p>

<ol>
<li><p>In magnetic distortion compensation, he used the following:</p>

<pre><code>hx = mx * q0q0 - _2q0my * q3 + _2q0mz * q2 + mx * q1q1 + _2q1 * my * q2 + _2q1 * mz * q3 - mx * q2q2 - mx * q3q3;
hy = _2q0mx * q3 + my * q0q0 - _2q0mz * q1 + _2q1mx * q2 - my * q1q1 + my * q2q2 + _2q2 * mz * q3 - my * q3q3;
_2bx = sqrt(hx * hx + hy * hy);
</code></pre></li>
</ol>

<p>Why is this assigned to <code>_2bx</code> but not <code>_bx</code>? By formula (46) in his article, it is <code>_bx</code>. Is my understanding correct?</p>

<ol start=""2"">
<li><p>The raw gyro information is used as:</p>

<pre><code>qDot1 = 0.5f * (-q1 * gx - q2 * gy - q3 * gz);
qDot2 = 0.5f * (q0 * gx + q2 * gz - q3 * gy);
qDot3 = 0.5f * (q0 * gy - q1 * gz + q3 * gx);
qDot4 = 0.5f * (q0 * gz + q1 * gy - q2 * gx);
</code></pre></li>
</ol>

<p>Shall the bias drift compensation done in this step? Assuming we track the bias with his formula (48), the bias value shall be applied here as:</p>

<pre><code>gx = gx - gyro_bias_x; gy = gy - gyro_bias_y;gz = gz - gyro_bias_z;
qDot1 = 0.5f * (-q1 * gx - q2 * gy - q3 * gz);
qDot2 = 0.5f * (q0 * gx + q2 * gz - q3 * gy);
qDot3 = 0.5f * (q0 * gy - q1 * gz + q3 * gx);
qDot4 = 0.5f * (q0 * gz + q1 * gy - q2 * gx);
</code></pre>

<p>Is my understanding correct?</p>
","sensors imu calibration"
"9890","iRobot Create 2: Has anyone used the emss iRobot Create framework to control the create 2?","<p>I am having trouble using the emss Interface to connect to the iRobot Create 2?
Can I use that framework for Create 2 or is that strictly made for Create 1?
Sorry in advance I'm very new to the robotics field. </p>

<p><a href=""http://emssframework.sourceforge.net/emssframework_Main_Page"" rel=""nofollow"">http://emssframework.sourceforge.net/emssframework_Main_Page</a></p>
","irobot-create software roomba"
"9891","How to programme an iRobot Create using a serial to USB cable?","<p>I am trying to programme my iRobot Create using a Serial to USB cable. I have connected the Serial end in the cargo bay connector port of the iRobot. I am using a software called Realterm (<a href=""http://realterm.sourceforge.net/index.html#Display_Formats"" rel=""nofollow"">http://realterm.sourceforge.net/index.html#Display_Formats</a>) to send commands to the iRobot. I have set the correct Baud Rate and other parameters. I downloaded the required driver from <a href=""http://www.ftdichip.com/Drivers/VCP.htm"" rel=""nofollow"">http://www.ftdichip.com/Drivers/VCP.htm</a> .
Inspite of all this, my iRobot is just not responding to the commands.</p>
","irobot-create"
"9892","Issues with Running Multiple Instructions in Sequence","<p>I tried to use Microsoft Robotics Dev Studio (sample 4) to write a code that was able for robot to go with a square path by just one clicked. However, there is one problem.
When I try to put DriveDistanceRequest and RotateDegreesRequest in a loop. It would only execute the last request. The problem is that Arbiter.Choice within the DriveDistance is activated immediately as soon as the drive operation starts. Did anyone have this kind of problem before? If so, how do I solve it? If no, how am I able to fix this problem? Thanks your so much.</p>

<p>//-----------------------------------------------------------------------</p>

<p>//  This file is part of Microsoft Robotics Developer Studio Code Samples.
//</p>

<p>//  Copyright (C) Microsoft Corporation.  All rights reserved.
//</p>

<p>//  $File: RoboticsTutorial4.cs $ $Revision: 22 $</p>

<p>//-----------------------------------------------------------------------</p>

<p>using Microsoft.Ccr.Core;</p>

<p>using Microsoft.Ccr.Adapters.WinForms;</p>

<p>using Microsoft.Dss.Core;</p>

<p>using Microsoft.Dss.Core.Attributes;</p>

<p>using Microsoft.Dss.ServiceModel.Dssp;</p>

<p>using Microsoft.Dss.ServiceModel.DsspServiceBase;</p>

<p>using System;</p>

<p>using System.Collections.Generic;</p>

<p>using System.Security.Permissions;</p>

<p>using xml = System.Xml;</p>

<p>using drive = Microsoft.Robotics.Services.Drive.Proxy;</p>

<p>using W3C.Soap;</p>

<p>using Microsoft.Robotics.Services.RoboticsTutorial4.Properties;</p>

<p>using Microsoft.Robotics.Services.Drive.Proxy;</p>

<p>using System.ComponentModel;</p>

<p>namespace Microsoft.Robotics.Services.RoboticsTutorial4
{</p>

<pre><code>[DisplayName(""(User) Robotics Tutorial 4 (C#): Drive-By-Wire"")]
[Description(""This tutorial demonstrates how to create a service that partners with abstract, base definitions of hardware services."")]
[DssServiceDescription(""http://msdn.microsoft.com/library/bb483053.aspx"")]
[Contract(Contract.Identifier)]
public class RoboticsTutorial4 : DsspServiceBase
{
    [ServiceState]
    private RoboticsTutorial4State _state = new RoboticsTutorial4State();

    [ServicePort(""/RoboticsTutorial4"", AllowMultipleInstances=false)]
    private RoboticsTutorial4Operations _mainPort = new RoboticsTutorial4Operations();

    [Partner(""Drive"", Contract = drive.Contract.Identifier, CreationPolicy = PartnerCreationPolicy.UseExisting)]
    private drive.DriveOperations _drivePort = new drive.DriveOperations();
    private drive.DriveOperations _driveNotify = new drive.DriveOperations();

    public RoboticsTutorial4(DsspServiceCreationPort creationPort) :
            base(creationPort)
    {
    }

    #region CODECLIP 02-1
    protected override void Start()
    {
        base.Start();

        WinFormsServicePort.Post(new RunForm(StartForm));

        #region CODECLIP 01-5
        _drivePort.Subscribe(_driveNotify);
        Activate(Arbiter.Receive&lt;drive.Update&gt;(true, _driveNotify, NotifyDriveUpdate));
        #endregion
    }
    #endregion

    #region CODECLIP 02-2
    private System.Windows.Forms.Form StartForm()
    {
        RoboticsTutorial4Form form = new RoboticsTutorial4Form(_mainPort);

        Invoke(delegate()
            {
                PartnerType partner = FindPartner(""Drive"");
                Uri uri = new Uri(partner.Service);
                form.Text = string.Format(
                    Resources.Culture,
                    Resources.Title,
                    uri.AbsolutePath
                );
            }
        );

        return form;
    }
    #endregion

    #region CODECLIP 02-3
    private void Invoke(System.Windows.Forms.MethodInvoker mi)
    {
        WinFormsServicePort.Post(new FormInvoke(mi));
    }
    #endregion


    /// &lt;summary&gt;
    /// Replace Handler
    /// &lt;/summary&gt;
    [ServiceHandler(ServiceHandlerBehavior.Exclusive)]
    public virtual IEnumerator&lt;ITask&gt; ReplaceHandler(Replace replace)
    {
        _state = replace.Body;
        replace.ResponsePort.Post(DefaultReplaceResponseType.Instance);
        yield break;
    }

    [ServiceHandler(ServiceHandlerBehavior.Concurrent)]
    //stop
    public virtual IEnumerator&lt;ITask&gt; StopHandler(Stop stop)
    {
        drive.SetDrivePowerRequest request = new drive.SetDrivePowerRequest();
        request.LeftWheelPower = 0;
        request.RightWheelPower = 0;

        yield return Arbiter.Choice(
            _drivePort.SetDrivePower(request),
            delegate(DefaultUpdateResponseType response) { },
            delegate(Fault fault)
            {
                LogError(null, ""Unable to stop"", fault);
            }
        );
    }

    //forward
    #region CODECLIP 01-3 
    [ServiceHandler(ServiceHandlerBehavior.Concurrent)]
    //forward
    public virtual IEnumerator&lt;ITask&gt; ForwardHandler(Forward forward)
    {
        if (!_state.MotorEnabled)
        {
            yield return EnableMotor();
        }
        // movement speed
        // This sample sets the power to 75%.
        // Depending on your robotic hardware,
        // you may wish to change these values.
        drive.SetDrivePowerRequest request = new drive.SetDrivePowerRequest();
        request.LeftWheelPower = 0.5;
        request.RightWheelPower = 0.5;

        yield return Arbiter.Choice(
            _drivePort.SetDrivePower(request),
            delegate(DefaultUpdateResponseType response) { },
            delegate(Fault fault)
            {
                LogError(null, ""Unable to drive forwards"", fault);
            }
        );
    }
    #endregion

    [ServiceHandler(ServiceHandlerBehavior.Concurrent)]
    // backup speed
    public virtual IEnumerator&lt;ITask&gt; BackwardHandler(Backward backward)
    {
        if (!_state.MotorEnabled)
        {
            yield return EnableMotor();
        }

        drive.SetDrivePowerRequest request = new drive.SetDrivePowerRequest();          
        request.LeftWheelPower = -0.6;
        request.RightWheelPower = -0.6;

        yield return Arbiter.Choice(
            _drivePort.SetDrivePower(request),
            delegate(DefaultUpdateResponseType response) { },
            delegate(Fault fault)
            {
                LogError(null, ""Unable to drive backwards"", fault);
            }
        );
    }

    [ServiceHandler(ServiceHandlerBehavior.Concurrent)]
    // left turn speed
    public virtual IEnumerator&lt;ITask&gt; TurnLeftHandler(TurnLeft turnLeft)
    {
        if (!_state.MotorEnabled)
        {
            yield return EnableMotor();
        }

        drive.SetDrivePowerRequest request = new drive.SetDrivePowerRequest();
        request.LeftWheelPower = -0.5;
        request.RightWheelPower = 0.5;

        yield return Arbiter.Choice(
            _drivePort.SetDrivePower(request),
            delegate(DefaultUpdateResponseType response) { },
            delegate(Fault fault)
            {
                LogError(null, ""Unable to turn left"", fault);
            }
        );
    }

    [ServiceHandler(ServiceHandlerBehavior.Concurrent)]
    // right turn speed
    public virtual IEnumerator&lt;ITask&gt; TurnRightHandler(TurnRight forward)
    {
        if (!_state.MotorEnabled)
        {
            yield return EnableMotor();
        }

        drive.SetDrivePowerRequest request = new drive.SetDrivePowerRequest();
        request.LeftWheelPower = 0.5;
        request.RightWheelPower = -0.5;

        yield return Arbiter.Choice(
            _drivePort.SetDrivePower(request),
            delegate(DefaultUpdateResponseType response) { },
            delegate(Fault fault)
            {
                LogError(null, ""Unable to turn right"", fault);
            }
        );
    }

    #region CODECLIP 01-4
    private Choice EnableMotor()
    {
        drive.EnableDriveRequest request = new drive.EnableDriveRequest();
        request.Enable = true;

        return Arbiter.Choice(
            _drivePort.EnableDrive(request),
            delegate(DefaultUpdateResponseType response) { },
            delegate(Fault fault)
            {
                LogError(null, ""Unable to enable motor"", fault);
            }
        );
    }
    #endregion

    #region CODECLIP 01-6
    private void NotifyDriveUpdate(drive.Update update)
    {
        RoboticsTutorial4State state = new RoboticsTutorial4State();
        state.MotorEnabled = update.Body.IsEnabled;

        _mainPort.Post(new Replace(state));
    }
    #endregion


    // Here is where I had change the code.
    #region Test Code (Creating Path)
    [ServiceHandler(ServiceHandlerBehavior.Concurrent)]
    public virtual IEnumerator&lt;ITask&gt; PathHandler(StartPath path)
    {
        if (!_state.MotorEnabled)
        {
            yield return EnableMotor();
        }

        for(int i=1; i&lt;3; i++)
        {
                if(i == 1)
                {
                    drive.DriveDistanceRequest distance = new drive.DriveDistanceRequest();
                    distance.Power = 1;
                    distance.Distance = 1;

                    yield return Arbiter.Choice(
                        _drivePort.DriveDistance(distance),
                        delegate(DefaultUpdateResponseType response) { },
                        delegate(Fault fault)
                        {
                            LogError(null, ""Unable to turn left"", fault);
                        }
                    );

                }
                else if(i == 2)
                {
                    drive.RotateDegreesRequest rotate = new drive.RotateDegreesRequest();
                    rotate.Power = 1;
                    rotate.Degrees = 90;

                    yield return Arbiter.Choice(
                        _drivePort.RotateDegrees(rotate),
                        delegate(DefaultUpdateResponseType response) { },
                        delegate(Fault fault)
                        {
                            LogError(null, ""Unable to turn left"", fault);
                        }
                    );
                }

        }

    }
    #endregion
}
</code></pre>

<p>}</p>
","mobile-robot control irobot-create mrds"
"9893","Computing the Jacobian Matrix -- chain rule?","<p>I am learning about robot kinematics and the Jacobian matrix, and I'm trying to understand how to compute the Jacobian matrix given a kinematic chain, such as a robot arm. I understand the theory behind the Jacobian matrix, but I'm not sure actually how it would be calculated in practice.</p>

<p>So, let's say that I have a 7 DoF robot arm, with 7 joints and 6 links between the joints. I know how to compute the transformation matrix between each joint, and by applying forward kinematics, I know the pose of the end effector for any configuration of joint angles. To calculate this, I have written some code which stores each transformation matrix, and then multiplies them in series to create the transformation matrix between the first joint and the end effector.</p>

<p>However, how do I now go about computing the Jacobian matrix? My solution so far, is to write down each transformation matrix by hand, then multiply them all by hand, to yield the overall transformation matrix, with respect to the joint angles. I could then differentiate this to create the Jacobian matrix. The problem with this though, is that the maths becomes very, very complicated as I move along the kinematic chain. By the end, there are so many terms as a result of the multiple matrix multiplications, that it just becomes so tedious doing this by hand.</p>

<p>Is there a better way to do this? In the case of calculating the forward kinematics, I didn't have to do it by hand, I just wrote some code to multiply the individual matrices. But when I want the Jacobian matrix, it seems like I need to compute the derivative of the overall transformation matrix after it has been computed, and so I need to do this by hand. What's the standard solution to this? Is it something to do with the chain rule for differentiation...? I'm not sure exactly how this applies here though...</p>

<p>Thank you!</p>
","robotic-arm kinematics inverse-kinematics jacobian manipulator"
"9894","Using Accelerometer, Gyroscope and any sensor to track speed, position,","<h1>Problem</h1>

<p>Currently working on reverse engineering this application <a href=""http://www.zepp.com/en-us/baseball/smart-coach/"" rel=""nofollow"">zepp.com/baseball</a>. This is a wearable device that can track a users </p>

<ul>
<li>speed</li>
<li>positional tracking</li>
<li>when the object makes contact with another one </li>
<li>3-D Rendering</li>
</ul>

<p>Currently using an accelerometer and gyroscope to get the yaw, pitch, and roll(orientation) of the device, but do not know how to use that information to calculate speed, or if the device has collided with another object?</p>
","imu accelerometer precise-positioning"
"9895","Estimating the displacement of a drone in three dimensions","<p>Assuming a drone is in two dimension, it has to predict its future position by calculating its future displacement:</p>

<p><a href=""http://i.stack.imgur.com/eifIG.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/eifIG.png"" alt=""enter image description here""></a></p>

<p>For a real quad-rotor, <strong>why should we not only estimate the displacement of a robot in three dimensions but also the change of orientation of the robot, its linear velocity and its angular velocity?</strong></p>
","quadcopter motion-planning uav"
"9899","VFH+ (Vector Field Histogram+) : Is it possible to choose a candidate sector without a set goal point?","<p>Good day</p>

<p>I am currently implementing the VFH algorithm. </p>

<blockquote>
  <p>Is it possible to configure the algorithm such that a reactionary motion is generated at the presence of an obstacle? </p>
</blockquote>

<p>I have been able to generate the obstacle map, primary polar histogram and the binary polar histogram.</p>

<blockquote>
  <p>How does one prioritize a sector to pass through?</p>
</blockquote>

<p>I have seen an implementation in labview where in it is possible to implement a simple vector field histogram path planning without any goal points <a href=""http://zone.ni.com/reference/en-XX/help/372983F-01/lvrobovi/advanced_vfh/"" rel=""nofollow"">here</a></p>
","mobile-robot motion-planning mapping c++ vfh"
"9904","Solving Inverse Kinematics with Gradient Descent","<p>I am trying to implement my own inverse kinematics solver for a robot arm. My solution is a standard iterative one, where at each step, I compute the Jacobian and the pseudo-inverse Jacobian, then compute the Euclidean distance between the end effector and the target, and from these I then compute the next joint angles by following the gradient with respect to the end effector distance.</p>

<p>This achieves a reasonable, smooth path towards the solution. However, during my reading, I have learned that typically, there are in fact multiple solutions, particularly when there are many degrees of freedom. But the gradient descent solution I have implemented only reaches one solution.</p>

<p>So my questions are as follows:</p>

<ol>
<li><p>How can I compute all the solutions? Can I write down the full forward kinematics equation, set it equal to the desired end effector position, and then solve the linear equations? Or is there a better way?</p></li>
<li><p>Is there anything of interest about the particular solution that is achieved by using my gradient descent method? For example, is it guaranteed to be the solution that can be reached the fastest by the robot?</p></li>
<li><p>Are there cases when the gradient descent method will fail? For example, is it possible that it could fall into a local minimum? Or is the function convex, and hence has a single global minimum?</p></li>
</ol>
","robotic-arm kinematics inverse-kinematics jacobian"
"9916","What's the difference between the term ""pose estimation"" and ""visual odometry""?","<p>I'm reading a paper:</p>

<blockquote>
  <p>Choi C, Trevor A J B, Christensen H I. RGB-D edge detection and
  edge-based registration[C]//Intelligent Robots and Systems (IROS),
  2013 IEEE/RSJ International Conference on. IEEE, 2013: 1568-1575.</p>
</blockquote>

<p>which refers: </p>

<blockquote>
  <p>Visual features such as corners, keypoints, edges, and color are
  widely used in computer vision and robotic perception for applications
  such as object recognition and <strong>pose estimation</strong>, <strong>visual odometry</strong>, and SLAM</p>
</blockquote>

<p>I previously assume <strong>pose estimation</strong> to be roughly equal to <strong>visual odometry</strong>, yet the text above seems to deny.</p>

<p>So what's their difference? I didn't find much info from google. IMHO, it seems pose estimation is estimating the pose of moving object with the camera static, while visual odometry is estimating the pose of camera in a static(mostly) scene, is that precise enough?</p>
","slam odometry pose"
"9918","Tring to run 12 V DC geared motor using Samsung Li Ion ICR16850 batteries","<p>I am trying to run this <a href=""http://www.amazon.in/Johnson-geared-Specially-Robowar-competitions/dp/B010V2O6CO?ie=UTF8&amp;psc=1&amp;redirect=true&amp;ref_=oh_aui_detailpage_o01_s00"" rel=""nofollow"">motor</a>. </p>

<p>Using the batteries stated in the title. The motor requires 12 V and I am supplying 11.98V to the motor, through a motor driver. After a while, the motor keeps slowing down and the battery voltage drops down to 5-6 V, but after I remove the battery from the motor driver it again shows 11.9V.</p>

<p>Is this battery capable enough to run my motors, or do I need a new one?</p>
","motor battery"
"9923","Change Message Interval ArduPilot","<p>I am using Mavlink protocol (in c++) to communicate with the ArduPilotMega, I am able to read messages such as <a href=""https://pixhawk.ethz.ch/mavlink/#ATTITUDE"" rel=""nofollow"">ATTITUDE</a> for example.<br>
I am currently getting only 2Hz (message rate) and I would like to increase it. I found out that I should use <a href=""https://pixhawk.ethz.ch/mavlink/#MESSAGE_INTERVAL"" rel=""nofollow"">MESSAGE_INTERVAL</a> in order to change it, and that I probably need to use the command <code>MAV_CMD_SET_MESSAGE_INTERVAL</code> to set it.</p>

<p>So my question is, how do I send that command using mavlink in c++?</p>

<p>I tried doing this with the code below but it did not work, I guess that I should use the command that I mentioned above but I don't know how.</p>

<pre><code>mavlink_message_t command;
mavlink_message_interval_t interval;

interval.interval_us = 100000;
interval.message_id = 30;

mavlink_msg_message_interval_encode(255, 200, &amp;command, &amp;interval);
p_sensorsPort-&gt;write_message(command);
</code></pre>

<p><strong>Update:</strong> I also tried this code below, maybe I am not giving it the right system id or component id.</p>

<pre><code>mavlink_message_t command;
mavlink_command_long_t interval;

interval.param1 = MAVLINK_MSG_ID_ATTITUDE;
interval.param2 = 100000;
interval.command = MAV_CMD_SET_MESSAGE_INTERVAL;
interval.target_system = 0;
interval.target_component = 0;

mavlink_msg_command_long_encode(255, 0, &amp;command, &amp;interval);
p_sensorsPort-&gt;write_message(command);
</code></pre>

<p>Maybe I am missing something about the difference between <code>target_system</code>, <code>target_component</code> and <code>sysid</code>, <code>compid</code>. I tried few values for each but nothing worked. Is there any ack that will be able to tell me if it even got the command? </p>
","quadcopter c++ ardupilot mavlink"
"9925","VFH (Vector Field Histogram+): Obtaining the Primary Polar Histogram","<p>Good day</p>

<p>Note: I have found out that my code works. I have placed a minor explanation to be further expounded.</p>

<p>I have been having trouble obtaining the right directional output from my implementation. I noticed that every time I put an obstacle at the right, it gives left, it gives the right steering direction, the problem is with the presence of a left obstacle where it still tends to steer itself towards that obstacle. I have checked the occupancy map generated using matlab and was found to be correct. I couldn't pinpoint what is exactly wrong with my code for I have been debugging this for almost a week now and was hoping if someone can see the error I cannot.</p>

<p>Here is my code implementation:</p>

<pre><code>//1st:: Create Occupancy Grid from Data-------------------------------------------------
// &gt; Cell Size/Grid Resolution  = 5meters/33 cells = 0.15meters each = 15cm each
// &gt; Grid Dimension = 5meters by 5meters / 33x33 cells //Field of view of robot is 54 degrees
//or 31 meters horizontal if subject is 5 meters away
// &gt; Robot Width = 1meter 100cm
// &gt; Because the focal length of the lens is roughly the same as the width of the sensor,
// it is easy to remember the field of view: at x meters away, you can see about x meters horizontally,
// assuming 4x3 stills mode. Horizontal field of view in 1080p video mode is 75%
// of that (75% H x 55% V sensor crop for 1:1 pixels at 1920x1080).

//Converting the Position into an Angle--------------------------------------------
//from:
//  A. https://decibel.ni.com/content/docs/DOC-17771
//  B. ""USING THE SENSOR KINECT FOR LANDMARK"" by ANDRES FELIPE ECHEVERRI GUEVARA
//1. Get angle
// &gt; Each pixel from the image represents an angle
// &gt; angle = ith pixel in row * (field of view in degrees/number of pixels in row)
// &gt; field of view of Pi camera is 54 degrees horizontal
//2. Convert Polar to Cartesian
// &gt; x = z*cos(angle)
// &gt; y = z*sin(angle)

int arrOccupancyGrid[33][33];
float matDepthZ[33][33];
int robotPosX = 0;
int robotPosY = 0;
int xCoor=0; //Coordinates of Occupancy Map
int yCoor=0;
int xPosRobot=0; //Present cordinates of robot
int yPosRobot=0;
float fov = 54; // 54 degrees field of view in degrees must be converted to radians
float nop = 320; //number of pixels in row
int mapDimension = 33; // 33by33 array or 33*15cm = 5mby5m grid
int mapResolution = 15; //cm

//Limit max distance measured
/*
for(i=0; i&lt; nop ;i++){
    if(arrDepthZ.at(i)&gt;500){
        arrDepthZ.at(i) = 500;
    }
}
*/

for (i=0 ; i &lt; nop; i++){
    //Process data/Get coordinates for mapping
        //Get Angle
        int angle = ((float)(i-160.0f) * ((float)fov/(float)nop)); //if robot is centered at zero add -160 to i
        //cout &lt;&lt; ""arrDepthZ "" &lt;&lt; arrDepthZ.at(i) &lt;&lt; endl;
            //cout &lt;&lt; ""i "" &lt;&lt; i &lt;&lt; endl;
            //cout &lt;&lt; ""fov "" &lt;&lt; fov &lt;&lt; endl;
            //cout &lt;&lt; ""nop "" &lt;&lt; nop &lt;&lt; endl;
            //cout &lt;&lt; ""angle "" &lt;&lt; i * (fov/nop) &lt;&lt; endl;
        arrAngle.push_back(angle);
        //Get position X and Y use floor() to output nearest integer
        //Get X --------
        xCoor = (arrDepthZ.at(i) / mapResolution) * cos(angle*PI/180.0f); //angle must be in radians because cpp
        //cout &lt;&lt; ""xCoor "" &lt;&lt; xCoor &lt;&lt; endl;
        arrCoorX.push_back(xCoor);
        //Get Y --------
        yCoor = (arrDepthZ.at(i) / mapResolution) * sin(angle*PI/180.0f); //angle must be in radians because cpp
                //cout &lt;&lt; ""yCoor "" &lt;&lt; yCoor &lt;&lt; endl;
        arrCoorY.push_back(yCoor);
    //Populate Occupancy Map / Cartesian Histogram Grid

        if((xCoor &lt;= 33) &amp;&amp; (yCoor &lt;= 33)){ //Condition Check if obtained X and Y coordinates are within the dimesion of the grid
            arrOccupancyGrid[xCoor][yCoor] = 1; //[increment] equate obstacle certainty value of cell by 1
            matDepthZ[xCoor][yCoor] = arrDepthZ.at(i);
        }

        //cout &lt;&lt; ""arrCoorX.size()"" &lt;&lt; arrCoorX.size() &lt;&lt; endl;
        //cout &lt;&lt; ""arrCoorY.size()"" &lt;&lt; arrCoorY.size() &lt;&lt; endl;

}

for (i=0 ; i &lt; arrCoorX.size(); i++){
  file43 &lt;&lt; arrCoorX.at(i) &lt;&lt; endl;
}

for (i=0 ; i &lt; arrCoorY.size(); i++){
  file44 &lt;&lt; arrCoorY.at(i) &lt;&lt; endl;
}

for (i=0 ; i &lt; arrDepthZ.size(); i++){
  file45 &lt;&lt; arrDepthZ.at(i) &lt;&lt; endl;
}

//------------------------- End Create Occupancy Grid -------------------------


//2nd:: Create 1st/Primary Polar Histogram ------------------------------------------------------
//1. Define angular resolution alpha
// &gt; n = 360degrees/alpha;
// &gt; set alpha to 5 degrees resulting in 72 sectors from 360/5 = 72 ///// change 180/5 = 35
//2. Define number of sectors (k is the sector index for sector array eg kth sector)
// &gt; k=INT(beta/alpha), where beta is the direction from the active cell
//to the Vehicle Center Point (VCP(xPosRobot, yPosRobot)). Note INT asserts k to be an integer

cout &lt;&lt; ""2nd:: Create 1st/Primary Polar Histogram"" &lt;&lt; endl;

//Put this at the start of the code away from the while loop ----------------
int j=0;
int sectorResolution = 5; //degrees 72 sectors, alpha
int sectorTotal = 36; // 360/5 = 72 //// change 180/5 = 36
int k=0; //sector index (kth)
int maxDistance = 500; //max distance limit in cm
//vector&lt;int&gt;arrAlpha; //already initiated

float matMagnitude[33][33]; //m(i,j)
float matDirection[33][33]; //beta(i,j)
float matAngleEnlarge[33][33]; //gamma(i,j)
int matHconst[33][33]; //h(i,j) either = 1 or 0

float robotRadius = 100; //cm
float robotSafeDist = 50; //cm
float robotSize4Sector = robotRadius + robotSafeDist;

for (i=0; i&lt;sectorTotal; i++){
    arrAlpha.push_back(i*sectorResolution);
}
//---------end initiating sectors----------

//Determine magnitude (m or matMagnitude) and direction (beta or matDirection) of each obstacle vector
//Modify m(i,j) = c(i,j)*(a-bd(i,j)) to m(i,j) = c(i,j)*(dmax-d(i,j)) from sir Lounell Gueta's work (RAL MS)
//Compute beta as is, beta(i,j) = arctan((yi-yo)/(xi-xo))
//Enlarge robot and compute the enlargment angle (gamma or matAngleEnlarge)
int wew =0;
int firstfillPrimaryH = 0; //flag for arrayPrimaryH storage
for (k=0; k&lt;sectorTotal; k++){
    for (i=0; i&lt;mapDimension; i++){
        for (j=0; j&lt;mapDimension; j++){
            //cout &lt;&lt; ""i"" &lt;&lt; i &lt;&lt; ""j"" &lt;&lt; j &lt;&lt; ""k"" &lt;&lt; k &lt;&lt; endl;
            //cout &lt;&lt; ""mapDimension"" &lt;&lt; mapDimension &lt;&lt; endl;
            //cout &lt;&lt; ""sectorTotal"" &lt;&lt; sectorTotal &lt;&lt; endl;
            //Compute magnitude m, direction beta, and enlargment angle gamma
            matMagnitude[i][j] = (arrOccupancyGrid[i][j])*( maxDistance-matDepthZ[i][j]); //m(i,j)
            //cout &lt;&lt; ""matMagnitude[i][j]"" &lt;&lt;  (arrOccupancyGrid[i][j])*( maxDistance-matDepthZ[i][j]) &lt;&lt; endl;
            matDirection[i][j] = ((float)atan2f( (float)(i-yPosRobot), (float)(j-xPosRobot))*180.0f/PI); //beta(i,j)
            //cout &lt;&lt; ""matDirection[i][j]"" &lt;&lt; ((float)atan2f( (float)(i-yPosRobot), (float)(j-xPosRobot))*180.000/PI) &lt;&lt; endl;
            //cout &lt;&lt; ""matDepthZ[i][j]"" &lt;&lt; matDepthZ[i][j] &lt;&lt; endl;
            if(matDepthZ[i][j] == 0){ //if matDepthZ[i][j] == 0; obstable is very far thus path is free, no enlargement angle
                matAngleEnlarge[i][j] = 0; //gamma(i,j)
                //cout &lt;&lt; ""matAngleEnlarge[i][j]"" &lt;&lt; 0 &lt;&lt; endl;
            }
            else{ //if matDepthZ[i][j] &gt; 0 there is an obstacle so compute enlargement angle
                matAngleEnlarge[i][j] = asin( robotSize4Sector / matDepthZ[i][j])*180/PI; //gamma(i,j)
                //cout &lt;&lt; ""matAngleEnlarge[i][j]"" &lt;&lt; asin( robotSize4Sector / matDepthZ[i][j])*180.0f/PI &lt;&lt; endl;
            }

            wew = k*sectorResolution; //k*alpha
            //cout &lt;&lt; ""wew"" &lt;&lt; k*sectorResolution &lt;&lt; endl;
            //Check if magnitude is a part of the sector
            if ( ((matDirection[i][j]-matAngleEnlarge[i][j]) &lt;= wew) &amp;&amp; (wew &lt;= (matDirection[i][j]+matAngleEnlarge[i][j])) ){
                matHconst[i][j]=1; //Part of the sector
                //cout &lt;&lt; ""Part of the sector ---------------------------------------------------------------"" &lt;&lt; endl;
                //cout &lt;&lt; ""matHconst[i][j]=1"" &lt;&lt; matHconst[i][j] &lt;&lt; endl;
            }
            else{
                matHconst[i][j]=0; //Not Part of the sector
                //cout &lt;&lt; ""Not Part of the sector"" &lt;&lt; endl;
                //cout &lt;&lt; ""matHconst[i][j]=0"" &lt;&lt; matHconst[i][j] &lt;&lt; endl;
            }
            //Compute primary polar histogram Hp(k)
            //cout &lt;&lt; ""firstfillPrimaryH"" &lt;&lt; firstfillPrimaryH &lt;&lt; endl;
            if (firstfillPrimaryH==0){ //If first fill at sector
                //cout &lt;&lt; ""matMagnitude[i][j]"" &lt;&lt; matMagnitude[i][j] &lt;&lt; endl;
                //cout &lt;&lt; ""matHconst[i][j]"" &lt;&lt; matHconst[i][j] &lt;&lt; endl;
                float temp = matMagnitude[i][j]*matHconst[i][j];
                //cout &lt;&lt; ""matMagnitude[i][j]*matHconst[i][j]"" &lt;&lt; temp &lt;&lt; endl;
                arrPrimaryH.push_back(temp);
                firstfillPrimaryH=1; //Trigger flag
                //cout &lt;&lt; ""arrPrimaryH kth"" &lt;&lt; arrPrimaryH.at(k) &lt;&lt; endl;
            }
            else{ //If sector filled previously
                arrPrimaryH.at(k) = arrPrimaryH.at(k)+(matMagnitude[i][j]*matHconst[i][j]);
                //cout &lt;&lt; ""arrPrimaryH kth"" &lt;&lt; arrPrimaryH.at(k) &lt;&lt; endl;
            }

        }
    }
    firstfillPrimaryH=0; //Reset flag
}
</code></pre>
","mobile-robot motion-planning vfh path-planning"
"9926","Calculate the uncertainty of a 6-dof pose for graph-based SLAM","<p>This question is strongly related to my other question over <a href=""http://robotics.stackexchange.com/questions/9129/how-to-compute-the-error-function-in-graph-slam-for-3d-poses/9137#9137"" title=""here"">here</a>.</p>

<p>I am estimating 6-DOF poses $x_{i}$ of a trajectory using a graph-based SLAM approach. The estimation is based on 6-DOF transformation measurements $z_{ij}$ with uncertainty $\Sigma_{ij}$ which connect the poses. </p>

<p>To avoid singularities I represent both poses and transforms with a 7x1 vector consisting of a 3D-vector and a unit-quaternion:</p>

<p>$$x_{i} = \left( \begin{matrix} t \\ q \end{matrix} \right)$$</p>

<p>The optimization yields 6x1 manifold increment vectors </p>

<p>$$ \Delta \tilde{x}_i = \left( \begin{matrix} t \\ log(q) \end{matrix} \right)$$</p>

<p>which are applied to the pose estimates after each optimization iteration:</p>

<p>$$ x_i \leftarrow x_i \boxplus \Delta \tilde{x}_i$$</p>

<p>The uncertainty gets involved during the hessian update in the optimization step:</p>

<p>$$ \tilde{H}_{[ii]} += \tilde{A}_{ij}^T \Sigma_{ij}^{-1} \tilde{A}_{ij} $$</p>

<p>where </p>

<p>$$ \tilde{A}_{ij} \leftarrow A_{ij} M_{i} = \frac{\partial e_{ij}(x)}{\partial x_i} \frac{\partial x_i \boxplus \Delta \tilde{x}_i}{\partial \Delta x_i} |_{\Delta \tilde{x}_i = 0}$$</p>

<p>and</p>

<p>$$ e_{ij} = log \left( (x_{j} \ominus x_{i}) \ominus z_{ij} \right) $$</p>

<p>is the error function between a measurement $z_{ij}$ and its estimate $\hat{z}_{ij} = x_j \ominus x_i$. Since $\tilde{A}_{ij}$ is a 6x6 matrix and we're optimizing for 6-DOF $\Sigma_{ij}$ is also a 6x6 matrix.</p>

<hr>

<p>Based on IMU measurements of acceleration $a$ and rotational velocity $\omega$ one can build up a 6x6 sensor noise matrix</p>

<p>$$ \Sigma_{sensor} = \left( \begin{matrix} \sigma_{a}^2 &amp; 0 \\ 0 &amp; \sigma_{\omega}^2 \end{matrix} \right) $$</p>

<p>Further we have a process model which integrates acceleration twice and rotational velocity once to obtain a pose measurement.</p>

<p>To properly model the uncertainty both sensor noise and integration noise have to be considered (anything else?). Thus, I want to calculate the uncertainty as</p>

<p>$$ \Sigma_{ij}^{t} = J_{iterate} \Sigma_{ij}^{t-1} J_{iterate}^T + J_{process} \Sigma_{sensor} J_{process}^T$$</p>

<p>where $J_{iterate} = \frac{\partial x_{i}^{t}}{\partial x_{i}^{t-1}}$ and $J_{process} = \frac{\partial x_{i}^{t}}{\partial \xi_{i}^{t}}$ and current measurement $\xi{i}^{t} = [a,\omega]$.</p>

<p>According to this formula $\Sigma_{ij}$ is a 7x7 matrix, but I need a 6x6 matrix instead. I think I have to include a manifold projection somewhere, but how?</p>

<hr>

<p>For further details take a look at the following publication, especially at their algorithm 2:</p>

<p>G. Grisetti, R. Kümmerle, C. Stachniss, and W. Burgard, “A tutorial on graph-based SLAM,” IEEE Intelligent Transportation Systems Maga- zine, vol. 2, no. 4, pp. 31–43, 2010.</p>

<hr>

<p>For a similar calculation of the uncertainty take a look at the end of section III A. in:</p>

<p>Corominas Murtra, Andreu, and Josep M. Mirats Tur. ""IMU and cable encoder data fusion for in-pipe mobile robot localization."" Technologies for Practical Robot Applications (TePRA), 2013 IEEE International Conference on. IEEE, 2013.</p>

<hr>

<p>.. or section III A. and IV A. in:</p>

<p>Ila, Viorela, Josep M. Porta, and Juan Andrade-Cetto. ""Information-based compact Pose SLAM."" Robotics, IEEE Transactions on 26.1 (2010): 78-93.</p>
","slam ekf jacobian"
"9927","graph-based SLAM optimization fails with numeric error","<p>I am implementing a graph-based SLAM system which works fine, i.e. it converges, if I assume a constant covariance matrix $\Sigma_{ij}$ for all my constraints. However, if I model my $\Sigma_{ij}$ more realistically, i.e. with increasing entries for progress in time (see <a href=""http://robotics.stackexchange.com/questions/9926/calculate-the-uncertainty-of-a-6-dof-pose-for-graph-based-slam"">my other question</a>) it fails after one successful iteration, i.e. after solving the system once but before applying the pose increments $\Delta \tilde{x}_{ij}$ to the poses $x_{ij}$ for the first time. I use the cholesky based sparse solver implementation of Eigen which fails with a numeric error.</p>

<p>Of course, the failure might occur due to a bug in my implementation. But maybe there's also a problem about representing the math with computers, e.g. some overflow in double representation or something similar. Could anybody comment on this assumption, please?</p>

<hr>

<p>My implementation makes use of manifolds and is based on algorithm 2 in:</p>

<p>G. Grisetti, R. Kümmerle, C. Stachniss, and W. Burgard, “A tutorial on graph-based SLAM,” IEEE Intelligent Transportation Systems Maga- zine, vol. 2, no. 4, pp. 31–43, 2010.</p>
","slam"
"9932","How to split tasks between interrupts and the main loop on a bare metal controller?","<p>I'm working on a robotics project where I have 3 services running. I have my sensor DAQ, my logic ISR (motor controller at 20kHz) and my EtherCAT slave controller.</p>

<p>DAQ and EtherCAT run in the idle and the logic runs during an interrupt. The logic does some calculations and controls the motor. The EtherCAT service (kinda like CANbus) runs together with my DAQ in the idle loop. I can not run the DAQ in the interrupt because that leaves me with less than 100ns for the EtherCAT service to run.</p>

<p>I'm not sure whether this is the right way to do this especially considering all the scary things i've read regarding data corruption when using interrupts.</p>

<p>Does anyone have some nice ideas on how to handle these services?</p>

<p>I'm running all my code on a Zynq 7020 (on the ARM Cortex) and it's written in C++.</p>

<p>Here is an example of my code:</p>

<pre><code>/**
 * Get all sensor data
 */
void Supervisor::communication(void) {
    // Get all the sensors data
    dispatchComm.getData(&amp;motorAngle, &amp;motorVelocity, &amp;jointAngle, &amp;springAngle, &amp;tuningParameter);

}

/**
 * Run all the logic
 */
void Supervisor::logic(void) {

    dispatchLogic.calculate(tuningParameter, motorAngle, motorVelocity, jointAngle, springAngle);

    dispatchLogic.getData(&amp;angle, &amp;magnitude);

    // Dispatch values to the motor drive
    dispatchComm.setMotorDriveSetpoint(angle, magnitude);
    dispatchComm.writeToPiggyback((uint32_t) (tuningParameter), motorAngle, motorVelocity);
}
</code></pre>
","c++ interrupts"
"9935","How to find Friction or Viscous force b (nmsec) in DC motor","<p>PLease guide me 
How to find Friction or Viscous force b (nmsec) in DC motor for a particlar speed.
The motor is connected with a gear and the ration is 26:1
I want to find for 200 rpm and the motor no load speed is 4900rpm
please guide me</p>
","quadcopter wheeled-robot brushless-motor stepper-motor"
"9936","Path planning - Quadtree decomposition (cell decomposition)","<p>I need to solve a path planning problem using a cell decomposition method, more precisely the quadtree decomposition. I need to do it in MATLAB so I would like to know whether there is any exmaple or code I could to make some tests.</p>

<p>I read about the <code>qtdecomp</code> MATLAB function but I couldn't do anything useful. In addition I would need to decompose the cells until a path is found and this functionality is not in the previous function.</p>

<p>Anyone knows how could I program this quadtree decomposition?</p>
","mobile-robot matlab path-planning"
"9937","Do you have to stop first when switching direction for proper encoding readings?","<p>Since the encoder is square wave not quadrature, do you have to stop first before changing directions for proper measurements?</p>

<p>In other words, if you are commanding along in one direction at some low speed like 50mm/s or less and want to change direction to -50mm/s, would you first need command it to zero and wait for the encoder to read 0 speed, and then command the reverse direction, in order to get as accurate as possible encoder readings?</p>
","irobot-create roomba"
"9941","Suggestion for correct battery pack","<p>I am trying to run 2 12V Geared DC motors which has No-load current = 800 mA(Max), Load current = upto 9.5 A(Max). Runtime to be atleast 3-4 hours.<br>
The motor takes about 10-12 V for operation.<br>
I need a proper battery pack for these, but how can I determine the specs I should go for?</p>
","motor battery"
"9946","Quadcopter: X-Y Velocity PID Controller","<p>Good day,</p>

<p><strong>Introduction</strong></p>

<p>I am currently working on an autonomous quadcopter project. I have currently implemented a cascaded PID controller consisting of two loops. The inner rate loop takes the angular velocity from the gyroscope as measurements. The outer stabilize/angle loop takes in angle measurements from the complementary filter (gyroscope + accelerometer angles). </p>

<p><strong>Question:</strong></p>

<p>I would like to ask if it is effective to cascade a Lateral Velocity (X and Y - axis) PID controller to the the Angle Controller (Roll and Pitch) to control drift along the X-Y plane. For the outermost PID controller, the setpoint is 0 m/s with the measured velocities obtained from integrating linear accelerations from the accelerometer. This then controls the PID controller responsible for the Pitch (if Y velocity PID) and Roll (if X velocity PID).</p>
","quadcopter control pid raspberry-pi stability"
"9947","Starting out, dissertation project using computer controlled drone","<p>For my final year in Computer Science university I will be doing a dissertation that includes controlling a drone through computer and communication with an onboard camera for computer vision.</p>

<p>The first step is obtaining a drone that suits my needs, and I have no clue how to go about it. Basically what is needed is a drone that will be able to communicate with a computer both for its movement and to ""stream"" the video to the computer for analysis. </p>

<p>So, would I go for a store bought drone, a rasperry pi or some other microcontroller based one etc. What do I need to take into consideration etc?</p>

<p>P.S. the project is going to be based indoors, so I don't need crazy range, or very powerf</p>
","quadcopter control microcontroller computer-vision"
"9951","Why do series elastic actuators have more accurate and stable force control?","<p>The other day, somebody was telling me about a robot in their lab, and they mentioned that it has ""series elastic"" actuators. But after doing a bit of Googling, I'm still not sure as to what this means, and have been unable to find a simple explanation. It seems that it is something to do with the link between the actuator and the load having a spring-like quality to it, but this is rather vague...</p>

<p>In any case, the what I am really interested in is the advantages and disadvantages of series elastic actuators. Specifically, I have read that one of the advantages is that it allows for ""more accurate and stable force control"". However, this appears counter-intuitive to me. I would have thought that if the link between the actuator and the load was more ""springy"", then this would lower the ability to have accurate control over the force send to the load, because more of this force would be stored and dissipated in the spring, with less directly transferred to the load.</p>

<p>So: Why do series elastic actuators have ""more accurate and stable force control""?</p>
","robotic-arm actuator dynamics torque"
"9953","kalman filter with redundant sensors","<p>Suppose I have one robot with two 3D position sensors based on different physical principles and I want to run them through a Kalman filter. I construct an observation matrix two represent my two sensors by vertically concatenating two identity matrices.</p>

<p>$H = \begin{bmatrix} 1&amp;0&amp;0\\0&amp;1&amp;0\\0&amp;0&amp;1\\1&amp;0&amp;0\\0&amp;1&amp;0\\0&amp;0&amp;1 \end{bmatrix}$ $\hspace{20pt}$
 $\overrightarrow x = \begin{bmatrix} x\\y\\z \end{bmatrix}$</p>

<p>so that </p>

<p>$H \overrightarrow x = \begin{bmatrix} x\\y\\z\\x\\y\\z \end{bmatrix}$</p>

<p>which represents both sensors reading the exact position of the robot. Makes sense so far. The problem comes when I compute the innovation covariance</p>

<p>$S_k = R + HP_{k|k-1}H^T$</p>

<p>Since </p>

<p>$H H^T = \begin{bmatrix}
 1 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 1 \\
 0 &amp; 1 &amp; 0 &amp; 1 &amp; 0 &amp; 0 \\
 0 &amp; 0 &amp; 1 &amp; 0 &amp; 1 &amp; 0 \\
 0 &amp; 1 &amp; 0 &amp; 1 &amp; 0 &amp; 0 \\
 0 &amp; 0 &amp; 1 &amp; 0 &amp; 1 &amp; 0 \\
 1 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 1 \\
 \end{bmatrix}$ </p>

<p>then, no matter what $P$ is, I'm going to wind up with $x$ innovations from the first sensor being correlated to $z$ innovations from the second, which seems intuitively wrong, if I'm interpreting this right. </p>

<p>Proceeding from here, my gain matrix ($K = P_{k|k-1} H^T S_k^{-1}$) winds up doing some pretty odd stuff (swapping rows and the like) so that, when updating a static system ($A = I_3, B = [0]$) with a constant measurement $\overrightarrow z = [1,0,0]$ I wind up with a predicted state $\hat x = [0,0,1]$.</p>

<p>If I separate the sensors and update the filter with each measurement separately, then $H H^T = I_3$, and I get sensible results.</p>

<p>I think I am confused about some technical points in one or more of these steps. Where am I going wrong? Does it not make sense to vertically concatenate the observation matrices?</p>

<p>I suppose that I could just set the off-diagonal 3x3 blocks of $S_k$ to 0, since I know that the sensors are independent, but is there anything in the theory that suggests or incorporates this step?</p>
","kalman-filter"
"9954","Using FRI and ATICombinedDAQFT simultaneously in visual studio","<p>I am trying to the force control experiment with KUKA LWR IV. I have a mini45 ATI force sensor. Our data acquisition board is PCIe-6320 which is not supported using Linux and it seems that I should use windows. One of the Lab guys has previously tried to use ""FRI"" and ""ATICombinedDAQFT"" simultaneously in windows but he was not hopeful. Two libraries did not work with each other.
Did anybody do the same experiment in KUKA? Or at least encounter the same problem with another sensor and FRI? It is noteworthy that our force sensor does not have ""NET box"" to use ""UDP"". I am a bit in a hurry and I really appreciate your suggestions.</p>

<p>Thanks.</p>
","force-sensor kuka"
"9955","I'm looking for hands-on experience with different types of leg and hip designs for walking robots","<p>I'm looking to find out, How do human-like legs compare to chicken legs and four-leg systems. In terms of cost, performance, speed, strength and accuracy.</p>

<p>I'm interested in things like speed, agility, turning radius, complexity and cost.</p>

<p>For a design large enough for a person to ride, rider fatigue is also important -- how do they compare in terms of achievable ride smoothness, vibration, and so on?</p>

<p>Are there quantitative benefits of 3 DOF hip joints, compared to 2 DOF?</p>

<p>I realize other factors will come into play as well, such as actuators, joint designs and control systems.
However, my interest at the moment is how basic leg designs compare to one another.</p>

<p>Edit: I'm looking for someone who has used these mechanisms first hand.</p>
","kinematics walking-robot"
"9956","Need help in implementing EKF based SLAM","<p>I just started learning about slam and I have been trying to simulate a robot moving around a set of landmarks for the past 3 days. The landmarks have known correspondences. </p>

<p>My problem is, if I add motion noise to the covariance matrix in the prediction step, the robot starts to behave very weirdly. If I don't add motion noise in the prediction step, the robot will move around perfectly. I have been trying to figure out why this is happening for 3 days now but cannot find anything wrong with my code.</p>

<p>I have attached a link to github which has all the files pertaining to my project. In the folder named 'octave' the file 'prediction_step' and 'correction_step' contains code for the prediction and correction steps respectively. The ekf_slam file is the main loop which calls the above two functions.</p>

<p>My github repository also contains 3 videos which correspond to robot with no motion noise, robot with motion noise and another video which shows how the robot should ideally go about.</p>

<p>Please help me in figuring out what is wrong with my code in 'prediction_step' and 'correction_step'.</p>

<p>Link to my github repository: <a href=""https://github.com/harishsatishchandra/EKF-SLAM.git"" rel=""nofollow"">please click here</a> </p>
","slam ekf"
"9962","Linearize a non linear system","<p>How do I linearize the following system using taylor series expansion:<br/>
$\dot x = v cos\theta \\ \dot y = v sin\theta \\ \dot \theta = u$
<br/>
Here, $\theta$ is the heading direction of my robot, measured counter clockwise with respect to $x$ axis.<br/>
$v$ is the linear velocity of the robot,<br/>
$u$ is the angular velocity of the robot.</p>
","mobile-robot localization"
"9963","Distance calculation with two robots and two obstacles","<p>I have a problem with two robots and two obstacles in a space. Each robot can communicate its measurements to the other and can measure angles and distances.
The two obstacles in the environment are identical to each other.</p>

<p>Each robot can see both obstacles but not each other. Therefore have angle theta 1 and 2 combined with distance 1 and 2. Can the distance between the two robots be calculated?</p>

<p><a href=""http://i.stack.imgur.com/QvtCP.jpg"" rel=""nofollow""><img src=""http://i.stack.imgur.com/QvtCP.jpg"" alt=""Problem Set up""></a></p>

<p>So far I have placed circles with radius of the measured distance over each landmark (triangles in my workings), this provides 4 possible positions for each robot. Red and black circles correspond to robot 1 and blue and green to robot 2. Using the relative size of the angle measurements I can discount two of these positions per robot.
This still leaves me with two possible positions for each robot shown with the filled or hashed circles.
<a href=""http://i.stack.imgur.com/a6xPJ.jpg"" rel=""nofollow""><img src=""http://i.stack.imgur.com/a6xPJ.jpg"" alt=""Workings so far""></a></p>

<p>Is it possible to calculate which side the robot is to the landmarks and the distance between each other?</p>

<p>Robot 1 only has the two measurements of angle and distance and can therefore assign an id to each obstacle, but when information is transmitted to robot 2, robot 2 does not know which obstacle will have been designated an id of 1 or 2.</p>
","localization kinematics"
"9965","How to get manufactured part from CAD file?","<p>I am working through the book Learning Robotics using Python, which is for Python programmers who want to learn some robotics. Chapter 2 shows you how to use LibreCAD to design the plates and poles that form the chassis of the turtlebot-like robot. For instance, the base plate looks like this:</p>

<p><a href=""http://i.stack.imgur.com/1pGlY.png""><img src=""http://i.stack.imgur.com/1pGlY.png"" alt=""enter image description here""></a></p>

<p>Then, there is nothing more about it but suddenly in a later chapter there is a picture of the fully manufactured plates assembled into a chassis, and the author acts as if this should just be something we know how to do:</p>

<p><a href=""http://i.stack.imgur.com/mNsbC.png""><img src=""http://i.stack.imgur.com/mNsbC.png"" alt=""enter image description here""></a></p>

<p>How did he do that? We have these CAD drawings, and suddenly there are these plates that were manufactured via some magical process the book never discusses, he never gives things like tolerances, the material that they are supposed to be made out of, etc., the kinds of things discussed here: </p>

<p><a href=""http://www.omwcorp.com/how-to-design-machined-parts.html"">http://www.omwcorp.com/how-to-design-machined-parts.html</a></p>

<p>I know nothing about this stuff, in terms of how to go from CAD design specs to getting an actual part manufactured. What kinds of companies should I use, what is a reasonable price to expect, what is the process? </p>

<p>In general, how do I go from CAD design to manufactured item? Do I find a local machine shop that specializes in robotics, bring my CAD drawings, and work with them to try to build the parts?</p>

<p>I am totally a noob, I hope this isn't a question like this:</p>

<p><a href=""http://blog.stackoverflow.com/2010/11/qa-is-hard-lets-go-shopping/"">http://blog.stackoverflow.com/2010/11/qa-is-hard-lets-go-shopping/</a></p>
","design software manufacturing chassis hardware"
"9967","How to further understand the computed torque model controller","<p>For the following controller what do $q_{des}$ and $q_{act}$ stand for? Also, what is the general principle of this controller? <a href=""http://i.stack.imgur.com/hHZUI.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/hHZUI.png"" alt=""enter image description here""></a></p>

<p>Thanks!</p>
","control microcontroller torque"
"9969","augmenting room magnetic field for smartphone sensors","<p>is it possible to enhance (or redirect) the earth's magnetic field in a room or house so that one can write a small program that makes smartphones with hall-effect sensors detect more reliably in which direction they are pointing?</p>

<p>I presume a fridge magnet won't do the job...</p>
","sensors hall-sensor"
"9971","What software to use to send (OI) commands to Create 2. Using windows laptop and supplied Create 2 cable?","<p>I read most of the iRobot Create 2 Open Interface (OI). It says send these serial commands to the Create 2 to get it to do the described action, but no suggestion of what software to use to send these serial commands through the USB interface.  I did install the FTDI Drivers to enable the USB to serial connection.  Question: What serial software should I use to communicate with Create 2?  Is there a tool to verify that the supplied usb to serial cable supplied with Create 2 is functioning and if the Create 2 is functioning? (I did a reset on Create 2 using Spot and Dock buttons)</p>
","irobot-create serial"
"9972","Issue using Digit LEDs Raw (op code 163) on Create2","<p>If I understand <a href=""https://cdn-shop.adafruit.com/datasheets/create_2_Open_Interface_Spec.pdf"" rel=""nofollow"">the manual</a>, each leg in each of the 7 segment displays is labeled with a letter A-G.  These letters then map to specific bits in a byte - 1 byte for each of the 4 displays.  Setting a bit turns on the corresponding leg while not setting it leaves it off.</p>

<p>With this understanding, I tried to turn on all the A segments by sending</p>

<pre><code>[163][1][1][1][1]
</code></pre>

<p>Instead of the A segment in each display turning on, the displays all showed a 1. Further testing shows that if I send the numbers 1-9 for any of the displays, they will display the number sent.  Sending the number 10 or greater turns on various combinations of the segments.</p>

<p>I was able to activate individual segments with the following numbers:</p>

<pre><code>63 G
64 A
65 B
66 C
67 D
68 E
69 F
</code></pre>

<p>However, I haven't been able to determine how the bytes sent affect the individual segments. Either I don't understand the manual or <strong>Digit LEDs Raw</strong> does not work as the manual specifies.</p>

<p><strong>UPDATE 03JUNE2016</strong></p>

<p>I have confirmed this behavior exists in the following firmware versions:</p>

<ul>
<li>r3-robot/tags/release-3.4.1:5858 CLEAN</li>
<li>r3_robot/tags/release-3.2.6:4975 CLEAN</li>
</ul>
","irobot-create"
"9978","Can too much input current destroy my motor driver?","<p>I have a 18 V rated driver I'm using to drive two 12 V DC gear motors using my Arduino. I bought a new battery pack which is rated 3300 mAh 25C, 11.1V making the total current input 82.5 A. My driver is rated for 7 V min and 18 V max, no current rating is given.</p>

<p>My motors are 12V max current under load is 9.5 A.</p>

<p>So just to be sure, can using this battery destroy my motor driver?</p>

<p>This is the <a href=""https://onedrive.live.com/redir?resid=EEABAA7CDCB08F75!40200&amp;authkey=!AGgKQkCY0LNj7v8&amp;ithint=file%2Cpdf"" rel=""nofollow"">datasheet</a>.</p>
","motor battery current"
"9985","Is there an algorithm using the Kinect depth image (not the point cloud) for registration?","<p>I know given the intrinsics <code>fx, fy, cx, cy</code> (where <code>fx, fy</code> are the horizontal and vertical focal length, and <code>(cx, cy)</code> is the location of principal point of the camera if pinhole camera model assumed) of an Kinect depth camera(or other range sensor?), a depth pixel <code>px=(u, v, d)</code> (<code>(u, v)</code> is the pixel coordinate, <code>d</code> is the depth value) can be converted to a 3D point <code>p</code>:</p>

<p><code>p=(x, y, z)
x=(u-cx)/fx*d
y=(v-cy)/fy*d
z=d
</code>
so that a depth image can be converted to a point cloud, and indeed, a depth Image represents a unique point cloud physically.</p>

<p>SLAM systems e.g. KinectFusion use such point clouds for ICP based registration to obtain camera pose at each time and then fuse new point cloud to the previously reconstructed model.</p>

<p>However, my mentor told me that depth Image cannot be inveribly converted to a point cloud since it's 2D->3D mapping with ambiguity (which I disagree), and he claims that I should use the depth Image at time (i-1) and (i) for registration, not the derived point cloud.</p>

<p>（If I have to obey my mentor's order) I've been reading papers and found one using Gradient Descent to solve camera pose <code>(tx, ty, tz, qw, qx, qy, qz)</code>:</p>

<blockquote>
  <p>Prisacariu V A, Reid I D. PWP3D: Real-time segmentation and tracking
  of 3D objects[J]. International journal of computer vision, 2012,
  98(3): 335-354.</p>
</blockquote>

<p>which uses <strong>RGB</strong> Images and a known model for pose estimation. However, I've <strong>NEVER</strong> found a paper (e.g., KinectFusion and other later RGB-D SLAM algorithms) deals with depth data just as plane image but not point cloud for registration. So could someone give me some hint (papers or opensource code) about: </p>

<p><em>How to do depth image registration without converting them to point clouds</em>?</p>
","localization slam kinect"
"9986","Need help with motors for high load","<p>I need to select motors for the wheeled robot I'm planning to build. My requirements are:</p>

<ul>
<li>robot should be able to carry 35 kg</li>
<li>platform size around 20 cm x 35-40 cm</li>
<li>speed = 6 mph or 3m/s</li>
<li>planning to use 4 driven wheels, but can increase if not enough.</li>
</ul>

<p>Other parameters like diameter of the wheels, acceleration are not fixed. Please help me to calculate required torque and RPM of the motors. It would be great if you can provide equations. Or post any lessons related to this problem.</p>
","motor design wheeled-robot torque"
"9987","How to compile arm compatible binary from an x86 precompiled library on a pc host to be run on an arm target?","<p>I am using a library precompiled on x86 on my pc (x86_64). Does there exist any toolchain to compile the x86 library and in the end generate an executable for armv7l ubuntu?</p>
","arm linux"
"9988","Instantaneous velocity calculation from accelerometer?","<p>I am trying derive velocity from accelerometer (MPU9250 in sensor-tag board). I saw lot of blogs which talk about noise and exact estimation problems. I started seeing velocity derivation (integration of accelerometer data over time) yielding me towards ramp because of noise presence in MPU9250.</p>

<p>Is the velocity can be estimated only by accelerometer or we need assistance of another sensor such as GPS or gyroscope, etc..</p>

<p>Please let me know as I see my velocity calculations never converge at all.
Also I have limitation in compute power, so Kalman filter kind of estimation techniques is difficult to implement. Can you please suggest whether I am in right direction or not.</p>
","sensors accelerometer"
"9994","Why wouldn't my robot stop?","<p>I am working on an Arduino based robot which engages a braking mechanism detecting anything in front. I was using an ultrasonic sensor, to detect obstacles which worked well while the robot was on my table (i.e under construction). But when I ran it on the ground, it doesn't stops and crashes.</p>

<p>The robot is programmed as if anything is detected 50 cm ahead if the robot, the braking mechanism stops the wheels. But when testing, the robot just wouldn't stop.</p>

<p>My robot is running at an average 7.5m/s . Thinking that doppler's effect might have rendered my sensor useless, I tried a little IR sensor I had lying around (range 25 cm approx), but that didn't work as well.</p>

<p>What am I doing wrong here?</p>
","arduino motor ultrasonic-sensors"
"10002","Control of WMR (Wheeled Mobile Robot) in 3D","<p>I've implemented SMC (Sliding Mode Controller) on WMR in both X-Y and X-Z plane. 
Now i want to combine both of these to control WMR in 3D. For this purpose I'm trying to use resultant vector of simulation in XY plane and track that resultant vector in XZ plane as value of X in previously designed code.  Tracking control of resultant vector is shown in figure 1 while Vector sum decomposed in rectangular coordinates after simulation is shown in figure 2. </p>

<p>Am I going wrong?  </p>

<p>What other tecniques can I apply to do 3D control of vehicle using Sliding Mode Controller.</p>

<p>Can i reduce the time delay offset? I've implemented right equations for SMC tracking controller equations but simulation does not gives exact results.These equations work well for control of vehicle in two dimensions (X-Z plane).</p>

<p><a href=""http://i.stack.imgur.com/iio7h.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/iio7h.png"" alt=""Tracking control of resultant vector""></a></p>

<p><a href=""http://i.stack.imgur.com/z3qaK.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/z3qaK.png"" alt=""Vector sum decomposed in rectangular coordinates after simulation""></a></p>
","wheeled-robot matlab simulation"
"10003","How to go around in a circle?","<p>I have the mBot robot and I'm trying to get it to go to the other side of a cylindral obstacle. </p>

<p>Something like this:</p>

<p><a href=""http://i.stack.imgur.com/5cuP9.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/5cuP9.png"" alt=""Robot&#39;s path""></a></p>

<p>What I know:</p>

<ul>
<li>Radius of the cylinder - r</li>
<li>Robot's distance from the cylinder</li>
<li>Wheel thickness - 1.5 cm</li>
<li>Distance between the middle of each wheel - 11.5 cm</li>
</ul>

<p>How would I achieve the above path?</p>

<p>The only thing I saw was this <a href=""http://stackoverflow.com/questions/26368200/myro-moving-robot-in-a-circle-based-on-the-radius-a-user-inputs"">SO question</a> that says: </p>

<blockquote>
  <p>The distance between the left and right wheel of the robot is 6
  inches.</p>
  
  <p>So the left wheel should travel at a distance of 2(pi)(radius+6)</p>
  
  <p>And the right wheel should travel at a distance of 2(pi) (radius-6)</p>
</blockquote>

<p>The problem with my robot is that you can't tell it to go 20cm to the right, nor can you tell it to turn 90 degrees to the right.</p>

<p>All you can do is set each motor's speed 0-255, so there's not way to put it in the formula <em>disatance = time x speed</em>.</p>

<p>I assume I have to set each motor's speed to a different value so they would go in a circle of radius x and then just exit at the half of the circle (like shown in the picture)</p>
","arduino wheeled-robot"
"10004","Solving Inverse Kinematics with Non-Linear Least Squares","<p>I want to write my own inverse kinematics solver, and I have been recommended to use Google's Ceres Solver to help. Now, according to the documentation, Ceres Solver is usually used for non-linear least squares problems (<a href=""http://ceres-solver.org/nnls_tutorial.html"" rel=""nofollow"">http://ceres-solver.org/nnls_tutorial.html</a>). This minimises the sum of squared differences between the measured and target values, over all data. What I am confused about, is how this relates to inverse kinematics.</p>

<p>In inverse kinematics, with the example of a robot arm, the goal is to determine what joint angles the robot should be positioned in, in order to reach a target end effector pose. There exists a single equation which determines the end effector pose, given the set of joint angles, and we want to determine the parameters in that equation (the joint angles).</p>

<p>But how does this relate to the least-squares problem, where there are multiple measurements? Is the problem I am trying to solve essentially the same, except that the number of measurements is one? And in that case, is using Ceres Solver's non-linear least squares solver really necessary?</p>

<p>Thanks!</p>
","robotic-arm inverse-kinematics"
"10006","Build a simple robot to learn ROS","<p>I am a beginner to ROS and I wanted to know if I could build a simple robot to learn ROS. 
I currently have the following components available:</p>

<ol>
<li>Arduino Uno</li>
<li>Simple two wheeled robot chassis</li>
<li>Some motors</li>
<li>L293D motor driver</li>
<li>Some ultrasonic sensors</li>
<li>Some infrared sensors</li>
</ol>
","arduino slam ros beginner ultrasonic-sensors"
"10007","How to produce a continuous variation of a discontinuous function?","<p>I have a differential equation that connects the ""velocity"" of a point in the FOV of a camera with the velocities of a robot's joints, that is $$\dot s=J(s) \dot q$$ where s is a vector with the $x$,$y$ coordinates of the point in the FOV, $J$ is the interaction matrix and $q$ is the vector of the joint positions. </p>

<p>If I have a certain point whose velocity I am tracking and this point remains in the FOV, then $\dot s$ is well defined. But if I change this point online, that is at the time instant $t$ I have point $s_t$ and at the time instant $t+dt$ I have the point $s_{t+dt}$, then $\dot s$ is not defined.</p>

<p>Can I create a filter to produce a continuous variation of $\dot s$? If not, what can I do?</p>

<p>More specifically, I want to perform occlusion avoidance. In order to do this I want to compute the minimum distance of each feature point of my target object from the possibly occluding object. But, obviously, this distance can be discontinuous due to the fact that another possibly occluding object can appear in the FOV nearer to my target than the previously measured.  </p>
","cameras visual-servoing filter"
"10011","Stabilising an inverted pendulum","<p>With the problem of stabilising an inverted pendulum on a cart, it's clear that the cart needs to move toward the side the pendulum leans. But for a given angle $\theta$, how much should the cart move, and how fast? Is there a theory determining the distance and speed of the cart or is it just trial and error? I've seen quite a few videos of inverted pendulum, but it's not clear how the distance and speed are determined.</p>
","mobile-robot sensors accelerometer gyroscope"
"10014","Design in the robotics world","<p>Apologies if this isn't really the right place to be asking, but I was wondering whether third party design firms are ever contracted to design industrial and or consumer robots? </p>

<p>If not is it something that is usually done in house, and who within an org would usually take care of this process?</p>

<p>Thanks.</p>
","design"
"10018","What is the sensor equation for odometry in a Kalman Filter?","<p>I would like to use an Extended Kalman Filter for the localization of a wheeled robot. With this filter I would like to do sensor fusion between 2 encoder sensors and an IMU with gyro and accelero sensors. 
The only method I can find for the odometry data to blend in the filter, is to add this as input [uk] to the system xk = f(xk-1,uk).
I would like to add the odometry data as normal measurement data (so in the zk vector). But then I need measurement equations g(xt). </p>

<p>This was what I had:
Nkleft = ((T*n0)/(2*pi*r))<em>sqrt(xv²+yv²)+((T</em>n0*b)/(2*pi*r))<em>tauv + n
Nkright = ((T</em>n0)/(2*pi*r))<em>sqrt(xv²+yv²)-((T</em>n0*b)/(2*pi*r))*tauv + n</p>

<p>with: Nkleft/Nkright = number of odometry pulses during sample period
      T = sample time
      n0 = total pulses in one wheelspeed sensor
      r = wheel radius
      xv = speed in x-direction
      yv = speed in y-direction
      tauv = angular velocity
      n = sensor noise</p>

<p>But when I test my implementation, I don't get the results I expect, so I think there is something wrong in these equations. </p>

<p>Does anyone know an example where the odometry is inserted as a sensor measurement, so not in the system equation? </p>
","kalman-filter odometry"
"10019","Should I use gyro or encoders for robot moving in straight line?","<p>I've recently succeeded in building my first collision-avoidance Arduino robot with 2 DC motors, and it works pretty well. However, it doesn't move in a straight line yet, when it should. I'm now studying which method should I implement to make the robot go straight. I've heard about using IMU or encoders with feedback control. I pretty much understand how to use the encoders, but I'm not sure about the gyro. Should I use just one of those or a combination of them?</p>
","mobile-robot arduino control gyroscope"
"10023","What is the most realistic grasping simulator?","<p>I am looking for a physics simulator which can accurately model a robot hand picking up an object. The main requirement is for accuracy / realism, rather than speed. It needs to be able to model soft bodies, such as the rubber ""skin"" on robotic finger tips. It also needs to be a dynamics engine, such that the object is actually moved around by the hand, modelling effects such as slippage.</p>

<p>From the research I have already done, there are two good candidates. First, GraspIt! (<a href=""http://graspit-simulator.github.io/"" rel=""nofollow"">http://graspit-simulator.github.io/</a>). This is open-source, and specifically designed for grasping, rather than physics simulation in general. Second, MuJoCo (<a href=""http://www.mujoco.org/"" rel=""nofollow"">http://www.mujoco.org/</a>). This is a more general simulator, is a commercial product, and has been adopted by some big names such as DeepMind.</p>

<p>I have tried using the Bullet physics engine for robot grasping simulation, but soon realised that this was not going to be strong enough, because Bullet is really designed for games, and hence sacrifices realism for speed. However, I'm much more interested in something which is as realistic as possible, even if the computation is slow.</p>

<p>Does anyone have any suggestions as to how I can proceed? Anybody with any experience with GraspIt! or MuJoCo?</p>

<p>Thanks!</p>
","robotic-arm simulator simulation"
"10026","Raspberry pi 3 location in a set field, no gps","<p>I'm developing a project which involves a raspberry pi 3 remote control rover and I need to know the exact location of the raspberry pi rover in a set field.</p>

<p>Let's say I have four logs, one in each corner of the square field (the goal right now is to extend this to any shape field, any number of corners), every of them equipped with (some kind of wave technology) that allows me to triangulate the position (based on signal intensity) of the raspberry pi rover.</p>

<p>The distance between logs should not be bigger than 30m (~100feet) and there is no line of sight guaranted.</p>

<p>The question is: Which kind of technology should I use, infrared, wifi, bluetooth, radio, ultrasound, etc? or, is there any better approach to this problem?</p>
","wireless"
"10029","Damping vs Friction","<p>I am using a physics simulator to simulate a robot arm. For a revolute joint in the arm, there are two parameters which need to be specified: damping, and friction. If a torque is applied to the joint, both the damping and the friction seem to reduce the resultant force on the torque. But what is the difference between the damping and the friction?</p>
","robotic-arm dynamics"
"10033","First build - Quadcopter , need help deciding hardware and connections","<p>I am building my first drone,</p>

<blockquote>
  <p>Objective: - Need to control drone by wifi on phone or laptop using
  ground station software of Openpilot</p>
</blockquote>

<p>I have a Arduino 2560 , cc3d Openpilot flight controller , raspberry pi with wifi bluetooth in built...</p>

<p>Now i am not able to understand , how to go forward , should i connect arduino with openpilot cc3d flight controller , or raspberry pi directory with cc3d flight controller ....</p>

<p>Do i really need arduino 2560 now ?</p>

<p>also how to connect r pi with cc3d flight controller , and how to mock PWM signals ?</p>
","quadcopter arduino raspberry-pi"
"10034","S-curve motion profile: discontinuous acceleration profile in a multipoint trajectory","<p>I am trying to implement an s-curve motion profile to reduce the effects of the jerk on a mobile robot.  I had succeeded in calculating the equations of the trajectory in case of a point-to-point trajectory. 
My problem is in the case of multipoint trajectory. First I introduce to my robot a start position and stop position with initial speed, max speed, max acceleration and max jerk. Then, while he is running, I introduce a new stop position and I re-calculate the equations of the trajectory. When I generated the trajectory, I found that the acceleration profile becomes null suddenly.
What should I do to fixe this problem? </p>

<p><a href=""http://i.stack.imgur.com/r5IAd.jpg"" rel=""nofollow""><img src=""http://i.stack.imgur.com/r5IAd.jpg"" alt=""enter image description here""></a></p>
","mobile-robot"
"10035","Handling of a 4WD robot frame as a 2 wheel differential drive","<p>I have a '<a href=""http://www.dfrobot.com/index.php?route=product/product&amp;product_id=261#.V1f3lPmLSUk"" rel=""nofollow"">Baron</a>' robot frame with 4 static wheels, all driven by a motor. At the moment I'm thinking of handling it like a 2 wheel differential drive. Left and right wheels would receive the same signal. Actually you can interpret it as a tank on caterpillars, exept there is no link between the two tires. 
Does anyone have a different idea about this? </p>

<p>Ps: The purpose of the robot will be to know it's exact location. I will use a kalman filter (EKF) to do sensor fusion of the odometry and an IMU with accelero, gyro and magnetometer. So in the kalman filter I add the odometry model of a differential drive robot.</p>
","differential-drive"
"10040","Rotate 3d vector value into a single axis using a rotation quaternion","<p>I want to rotate the whole value of a 3d vector into one axis using quaternion rotations.</p>

<p>The reason behind is that I want to align the X and Y Axis of my smartphone with the X and Y Axis of my vehicle in order to detect lateral and longitudinal acceleration separated on these two axis. Therefore I want to detect the first straight acceleration of the car and rotate the whole acceleration value into the heading axis (X-Axis) of the phone assuming a straight forward motion.</p>

<p>How do I achieve this?</p>

<p><img src=""http://i.stack.imgur.com/ioaYJ.png"" alt=""enter image description here"">
<img src=""http://i.stack.imgur.com/4VVDF.png"" alt=""enter image description here""></p>
","sensors accelerometer rotation"
"10045","What information an IMU gives to a drone?","<p>An Inertial Measurement Unit (IMU) is an important sensor used in aerial robotics. A typical IMU will contain an accelerometer and a rate gyroscope. Which of the following information does a robot get from an IMU? </p>

<ul>
<li>Position</li>
<li>Orientation</li>
<li>Linear velocity</li>
<li>Angular velocity</li>
<li>Linear acceleration</li>
<li>Angular acceleration</li>
</ul>

<p>I don't think it gets its orientation information from IMU. The last time I took the test, I said that all but the first two are true. I failed.</p>
","imu"
"10048","Double/Triple inverted pendulum always on a cart?","<p>All of the examples of keeping a double/triple inverted pendulum balanced using a PID controller I've seen seem to be on a cart. Like this one <a href=""https://www.youtube.com/watch?v=cyN-CRNrb3E"" rel=""nofollow"">https://www.youtube.com/watch?v=cyN-CRNrb3E</a></p>

<p>How come the PID controller always controls a cart rather than a servo that holds the first pendulum? The second/third pendulum could be connected loosely on the first pendulum and the PID controller controls the first pendulum. Is it because servos tend to be too slow or are there other reasons?</p>
","motor mechanism servos"
"10049","Question about sampling of proposal distribution in gmapping algorithm","<p>I'm trying to reimplement the gmapping algorithm (which is based on the paper by Grisetti et al. 2007) for my own purposes and therefore would like to understand in detail both the algorithm and the default parameter values people use.</p>

<p>To my understanding, gmapping uses a proposal distribution for each particle whose moments are determined by sampling around a scan-matching estimate. My questions are:</p>

<ol>
<li><p>How many samples $K$ does the standard gmapping implementation use to estimate the mean and covariance of the proposal distribution? </p></li>
<li><p>The samples $\{x_k\}_{k=1}^K$ are drawn from $x_k \sim \{x_j|x_j - \hat{x}^{(i)}| &lt; \Delta\}$, where $\hat{x}^{(i)}$ is the scan-matching estimate of particle $i$. How is $\Delta$ determined? How much does this parameter matter?</p></li>
</ol>

<p>I couldn't find the values they used neither in their paper nor in their implementation of gmapping at openslam.org. Any pointers regarding the practical significance of these parameters is highly appreciated!</p>
","slam particle-filter"
"10050","What does Simultaneous Localization And Mapping (SLAM) software do?","<p>I took a course to have a better understanding of drones and their design. At the end of the course there was a test question that I got wrong and I would like to understand why. </p>

<p>I was supposed to select the choices that best describe SLAM.</p>

<p>and the possible answers were:</p>

<ol>
<li>Estimates the location of features in the environment? </li>
<li>Controls the robot's flight through the environment?</li>
<li>Causes the robot to avoid obstacles in the environment?</li>
<li>Navigate in a cluttered environment?</li>
<li>Estimates the position and orientation of the robot with respect to
    the environment?</li>
</ol>

<p>At first I knew that at least 3 and 4 were right because I watched a drone doing these things. I also thought that the last answer was linked to these two so I said yes to it too. Finally, I thought that the only thing that was still controlled by the user would be the flight...</p>

<p>Yet I failed again... Therefore <strong>what does Simultaneous Localization And Mapping (SLAM) software do?</strong></p>
","slam"
"10052","Position Control vs Velocity Control vs Torque Control","<p>Please can somebody explain to me the difference between Position Control, Velocity Control, and Torque Control? Specifically, I am thinking in terms of a robot arm. I understand that position control tries to control the position of the actuators, such that the error signal is the difference between the current position and the desired position. Velocity control is then trying to control the velocity of each actuator, and torque control is trying to control the torque of each actuator.</p>

<p>However, I don't understand why these are not all the same thing. If you want to send a robot arm to a certain position, then you could use position control. But in order to move an actuator to a certain position, you need to give it a velocity. And in order to give it a velocity, you need to give it a torque. Therefore, whether the error is in position, velocity, or torque, it always seems to come back to just the error in torque. What am I missing?</p>
","control kinematics dynamics roboti-arm"
"10053","Load pre-built occupancy map in Gazebo","<p>I have built an occupancy map using the gmapping package in ROS for a (real) hallway. Now I want to use this map in Gazebo so that I can simulate my robot in this environment. It seems that Gazebo can only support the map built from its own models, not from an external occupancy map. Is there any way that I can use an occupancy map in Gazebo or some other simulators? It is easier to obtain an occupancy map from real world than building a physical world in a simulator...</p>
","mapping gazebo occupancygrid"
"10056","Choice of a motor for robotic arm","<p>This is my first post here, so hello all. I really hope I can learn a lot from you guys.</p>

<p>I am trying to build a robotic arm to carry an object and put it inside of different boxes that are placed in different fixed locations.</p>

<p>I found a few robotic arms that can do it, but I am still trying to find the right motor for the job. I read a lot on-line about the different motors, but I am not sure which on to pick. Since the boxes are located in fixed places, the motors have to move in a precise way, so, according to my research, Servo motors are the ones I should use.  </p>

<p>Since it is a low budget project (I am college student), I wasn't sure which motor to choose (there are a lot of servo motors out there). I found several Servo motors on-line, for example , <a href=""https://www.adafruit.com/products/1404?gclid=CjwKEAjwp-S6BRDj4Z7z2IWUhG8SJAAbqbF35BtbqoBtiPJUrmAZsWO4AWTCzDTnAvgGPHldha_-nBoC237w_wcB"" rel=""nofollow"">Analog Feedback Servo</a>, and I was wondering what is the best servo motor I can buy for a really low cost project? I think I can spend about 10-20$ per motor (I need 5 motors).</p>

<p>I already have an Rpi and I know that pin 18 is the PWM pin that controls the motor's precision movement, but before I purchase a PWM controller and additional motors I need to run some testing to find how precise the motor is.</p>

<p>By the way, how can I calculate the amount of weight the motor can handle?</p>

<p>Any ideas and information will be greatly appreciated.</p>

<p>Thank you</p>
","robotic-arm raspberry-pi servomotor python"
"10059","In how many ways can a six propellers drone fly or rotate?","<p>I thought it were twelve ways:</p>

<ul>
<li>Six for each ways between two propellers</li>
<li>Six others for each rotation on these ways.</li>
</ul>

<p>But according to Vijay Kumar Dean of Penn Engineering , it seems that I was wrong...</p>

<p>Then I read <a href=""http://www.hindawi.com/journals/mpe/2013/673525/"" rel=""nofollow"">this article  about modeling and robust trajectory tracking control for a novel Six-Rotor Unmanned Aerial Vehicle</a> and <a href=""http://lup.lub.lu.se/luur/download?func=downloadFile&amp;recordOId=4359940&amp;fileOId=4359943"" rel=""nofollow"">this one about navigation and autonomous control of a Hexacopter in indoor environments</a> but was never able to find such an information.</p>

<p>I then guessed that 3 of the rotors could go one direction and three others into another which would add 6 other ways for rotating and therefore 6 others for simply flying but that is only a guess.</p>

<p><a href=""http://i.stack.imgur.com/TuWzD.jpg"" rel=""nofollow""><img src=""http://i.stack.imgur.com/TuWzD.jpg"" alt=""Six propellers drone""></a></p>
","multi-rotor"
"10060","Motor Choice given size constraint and load requirement","<p>Good day everyone :)</p>

<p>I am an undergraduate student working on a project involving the use of high torque small-sized DC motors for use in designing a person following trolley bag. Where in the problem is to use small sized motors but still maintain the usability and efficiency in carrying loaded luggages.</p>

<p>I have been looking for motors in local stores as well as in RS components as well as Element 14. However I am not sure if my choices are the right fit as I am at a loss on what specifications to look for when selecting a particular motor for this application. I have also tried to look for motors used in current products that can be used in my application such as todays electric skate boards but unfortunately have no luck on finding their suppliers.</p>

<blockquote>
  <p>Basically the question I would like to ask  is what specifications or calculations can I perform to select the proper motors given size constraints and weight carrying requirments. Or does anyone have suggestions on common motors that are already normally used for this application. My target maximum load capacity is 20kg.</p>
</blockquote>

<p>Thank you!</p>
","mobile-robot motor gearing"
"10063","Rostock Delta Robot 3D Printer Degrees of Freedom (DOF)","<p>What is the degrees of freedom (DOF) of the Rostock delta robot 3d printer (delta mechanism that consists of three prismatic joints)?</p>

<p>Here's the link to the delta mechanism I'm referring to:
<a href=""https://www.youtube.com/watch?v=AYs6jASd_Ww"" rel=""nofollow"">https://www.youtube.com/watch?v=AYs6jASd_Ww</a>.</p>

<p>Thanks in advance for your help!</p>
","kinematics inverse-kinematics actuator manipulator 3d-printing"
"10070","Tracked robots dimensioning","<p>I'm designing a tank tracked robot. I would like to know how do we calculate the minimum height difference between the sprocket wheel axis and the boogie wheel axis( if the maximum height of any obstacle is 16 cm)?</p>
","design mechanism tracks"
"10072","How to make a robot?","<p>For instance, how would you hook up a electric pump communicate with a motherboard? Let's say I buy a electric pump, I hook it up to some sort of metal structure that if the pump is turned on it moves the metal structure, <strong>how would I hook up the pump to my motherboard so that I can program it?</strong> </p>
","control motor robotic-arm microcontroller machine-learning"
"10076","problem with vex updating","<p>Hi I am having a problem with my vex robot updating system.  Right now I am using VEX IQ Firmare Update and my mac states that everything is up to date.  However when I look online there is a new update out.  I can not use the radios for the controller because I can't update the brain.</p>
","vex"
"10077","How are industrial robotics components purchased?","<p>For hobbyists, you go to a store to buy products. The prices for these products are all clearly listed in the store catalog, and you can easily search for parts by lowest price or read customer reviews of the products.</p>

<p>For industrial engineers building complex machines, how do they buy components? Or don't they worry about cost, and leave it to their employer to eat the cost as a part of doing their line of work?</p>

<p>Is it possible to ""shop around"" for low-cost engineering components?</p>

<p>&nbsp;</p>

<p>It is unclear to me how someone building a robot on their own as a small one-man startup can make the step from the world of toy robots, to larger and more industrial robotic components.</p>

<p>Most of the non-hobbyist stuff is hidden away and not exposed to the world. While a product catalog might be available, there are no prices listed for anything.</p>

<p>For larger industrial components, there does not seem to be any realistic way to shop around for the lowest price or best value, since pricing for much of the big stuff is basically unavailable.</p>

<p>&nbsp;</p>

<p>For me personally, I am interested in trying to build my own powered exoskeleton on a middle class American income, so I can't afford to be paying 1000 bucks for a single electrohydraulic proportioning servo valve, when I'll need probably 50 of them. But shopping around for low cost ones is basically impossible as far as I can determine, because pricing info is generally not available or searchable from the majority of manufacturers.</p>
","industrial-robot"
"10087","Task space to joint motion space conversion","<p>I am the moment trying to read and understand this paper <a href=""http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.591.8314&amp;rep=rep1&amp;type=pdf"" rel=""nofollow"">Task Constrained Motion Planning in Robot Joint Space</a> but seem to have a hard time understanding the math.  </p>

<p>The paper describes how to perform task constrained motion planning in cases where a frame is constrained to a specific task.<br>
the problem the paper tackles is when sampling in joint space, randomized planners typically produce samples that lie outside the constraint manifold. the method  they proposed methods use a specified motion constrain vector to formulate a distance metric in task space and project samples within a tolerance distance of the constrain.   </p>

<p>Given the this I am seem to a bit confused on some simple terms they define in this paper. </p>

<p>Examples. How is a task space coordinate defined ? what information does it have?</p>

<p>they compute the $$\Delta x = T_e^t(q_s)$$ which is transformation matrix of the end effector with respect to the task frame. </p>

<p>What I don't get is why the end effector? and why the end effector with respect to the task frame?</p>

<p>Secondly.
Later in the paper they write down an expression that relates the task space to the joint space motion. They do it using the Jacobian, but seem to miss explaining (in my opinion) what $E(q_s)$ actually do. </p>

<p>$$J(q_s) = E(q_s)J^t(q_s)$$</p>

<p>What is said about it in the paper is that </p>

<blockquote>
  <p>Given the configuration $q_s$, instantaneous velocities have a linear
  relationship $E(q_s)$</p>
</blockquote>

<p>why the need of instaneous? what is the definition of an instantaneous component? how does it differ from the information given by the jacobian?</p>

<p>Basically i don't understand how and why the mapping is as it is?.. </p>
","robotic-arm motion-planning jacobian"
"10088","Send Quad copter control signals from Arduino or Raspberry PI to receiver on Open pilot CC3d","<p>I have open Pilot CC3D attached to a receiver which can accept signals from a RC transmitter after configuration through GCS ..</p>

<blockquote>
  <p>Objective is to - Send these control signals ( YAW/THROTTLE/ROLL/PITCH
  ) from R Pi or through Arduino ( whichever is best to transmit
  signals) by using a simple Radio transmitter , like 433 mhz TX,</p>
</blockquote>

<p>I want to use a USB based Game controller , which will be connected to Raspberry PI and then it can transmit signals through radio to receiver...</p>

<p>Is this feasible  or need to be scrapped...</p>
","quadcopter arduino raspberry-pi radio-control"
"10089","Locking the yaw direction of a laser pointer","<p>I have a laser pointer on a handle grip and I'm trying to keep the laser pointer's yaw direction, which can rotate at around 10deg/s. So I have the laser pointer on a stepper motor and an accelerometer/gyro in the handle, but what's a good way for maintaining its yaw direction? Could I simply turn the shaft according to the accelerometer/gyro's yaw readings or is control theory (PID) needed?</p>

<p>That is, if my stepper makes 4096 steps/rev, one gives 0.0879 deg. If the handle is turned by, say, 0.879 deg, then turn 10 steps in reverse (instantaneously). Would this be jerky and PID be needed?</p>

<p>Any thought appreciated.</p>
","sensors stability"
"10092","Arduino or Raspberry Pi?","<p>I want to make a <strong>object tracking quadcopter</strong> for a project. While I'm using the <strong>Arduino Mega 2560</strong> as the <strong>flight controller</strong>, I was thinking of using an <strong>additional offboard microcontroller/board</strong> for getting data from the onboard camera,which would then send an appropriate command to the onboard Arduino.</p>

<p>I was hoping someone could <strong>provide clarification on the advantages/disadvantages of doing object tracking with either choice.</strong></p>

<p>Thanks!   </p>
","quadcopter arduino raspberry-pi"
"10096","How can I charge a 11,1 volt LiPo akku?","<p>I didn't found any modules to charge my <a href=""https://www.amazon.de/gp/product/B00FY2URC0/ref=pd_lpo_sbs_dp_ss_2?pf_rd_p=556245207&amp;pf_rd_s=lpo-top-stripe&amp;pf_rd_t=201&amp;pf_rd_i=B005EPV0Y6&amp;pf_rd_m=A3JWKAKR8XB7XF&amp;pf_rd_r=K07V2ANNG3VMBEKKZDGV"" rel=""nofollow"">11,1 volt LiPo akku</a>, only for 3,7 volt with 5 volt power supply. How can I handle that with a micro-USB connector on my robotplatform?</p>
","arduino power lithium-polymer"
"10098","Storing a 3D map","<p>I am trying to build a 3D map using two cameras. I have found the coordinates of all the objects.</p>

<p>What is the best way to store this data.I would also like to display this map later. My range will be 5-6 meters(500-600 cms). The accuracy I have managed to achieve is within 1 cm.</p>

<p>I have used openCV and python on a laptop.I would like to shift to a Raspberry Pi later if the Pi can perform well enough.</p>

<p>My main issue is how can store this 3D Map.</p>
","cameras mapping 3d-reconstruction opencv"
"10099","Hubsan x4 drone camera recording black","<p>I'm unsure if this is the correct community to ask this question (vs StackExchange Electronics or Aviation, for example), but I recently purchased a <a href=""http://rads.stackoverflow.com/amzn/click/B00D8UO8G6"" rel=""nofollow"">Hubsan x4 HD video drone from Amazon.</a></p>

<p>This is my second Hubsan drone so I am already familiar with using the recording feature. However, after every recording, the recordings are the correct length, with the correct audio, but the image is black. I tried formatting the micro SD, using different micro SDs, reading up on forums, etc. but nothing seems to do the trick.</p>

<p>Is mine defective, or has someone had this issue and has been able to solve it?</p>

<p>Thanks</p>
","quadcopter cameras"
"10101","Ultrasonic Sensor through a column","<p>I am trying to measure the height of water inside a column. The column is 50mm in dia and 304mm long. I have mounted the sensor just above the column.</p>

<p><a href=""http://i.stack.imgur.com/c2yzF.jpg"" rel=""nofollow""><img src=""http://i.stack.imgur.com/c2yzF.jpg"" alt=""enter image description here""></a></p>

<p>To measure the accuracy of the sensor, I filled the column up to a known value (a) and got the average sensor reading (b). a+b should give me the height of the sensor from the base of the column.</p>

<p>Repeating this for different values of a, I got very different values for (a+b). see attached chart.</p>

<p><a href=""http://i.stack.imgur.com/OwfU5.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/OwfU5.png"" alt=""enter image description here""></a></p>

<p>My question is</p>

<ol>
<li>Is the sensor expected to have error of this order? </li>
<li>Is my setup of confining the sensor through a column producing such errors. </li>
<li>Any other ideas to get the water column height. Please note that during the actual test, the water inside will be oscillating (up and down).I am thinking of making a capacitive sensor using aluminium foil. Water will work as the dielectric and the level of water will determine the capacitance.</li>
</ol>

<p>P.S. I also did some open tests (not through a column) to get the distance of a fixed object, and it was quite accurate.</p>

<p>Any help is appreciated.</p>

<p>Arduino Code</p>

<pre><code>#include &lt;NewPing.h&gt;

#define TRIGGER_PIN  7  // Arduino pin tied to trigger pin on the ultrasonic sensor.
#define ECHO_PIN     8  // Arduino pin tied to echo pin on the ultrasonic sensor.
#define MAX_DISTANCE 200 // Maximum distance we want to ping for (in centimeters). Maximum sensor distance is rated at 400-500cm.

double DB_ROUNDTRIP_CM = 57.0;

NewPing sonar(TRIGGER_PIN, ECHO_PIN, MAX_DISTANCE); // NewPing setup of pins and maximum distance.

void setup() {
  Serial.begin(9600); // Open serial monitor at 115200 baud to see ping results.
}

void loop() {
  delay(100);
  unsigned int uS = sonar.ping();
  double d = uS / DB_ROUNDTRIP_CM;


  Serial.println(d);
}
</code></pre>
","arduino ultrasonic-sensors"
"10103","Is there a way to combine and sync two 2K cameras @ 90fps with ICs","<p>I am searching for a way to minimize the size of a stereo vision module and cannot find any ICs that will combine and sync two MIPI CSI-2 (4 lane) data streams without an FPGA and too much code.  there was one online (MAX7366A 3D Video Combiner/Synchronizer with two MIPI CSI-2 Input and one MIPI CSI-2 output) but the product is not publicly available.   Does anyone have knowledge of an arrangement of ICs that I could try?.</p>
","computer-vision"
"10106","How to use the opcode to start?","<p>I am new to robotics.Recently came into contact code.So the teacher let me use the Serial port app for Android to enter the opcode.But the robot did not have any reaction.</p>

<hr>

<p>I use the Communications Cable with Adapter in android phone.
App Use [DroidTerm: USB Serial port]</p>

<p>Serial Port Settings
Baud: 115200   (19200 also used)
Data bits: 8
Parity: None
Stop bits: 1
Flow control: None</p>

<hr>

<p>I try to enter opcode, but no response.--> enter:128,135,134.....
But it did not show any reaction to the phone and robot.
I hope according to Opcode instructions to control robots to make the specified action.</p>
","irobot-create"
"10110","Visualizing raw accelerometer and gyro data","<p>I have an arduino wired to an MPU6050 breakout board. The arduino continuously collects accelerometer and gyroscope data from the MPU6050 and calculates angle and velocity. </p>

<p>Simply plotting the vector components (x,y,z) of this data does not allow one to reason about the motion of the sensor or robot. It's possible, though not easy, to do sanity checks (Is the sensor oriented as expected? Is gravity working?). But it's very difficult to look at a x,y,z plot of accelerometer log data and imagine what the robot did for instance. </p>

<p>I was wondering if there is some sort of tool or Python library to visualise accelerometer and gyro, or IMU data? (I'm looking for something like this- <a href=""https://youtu.be/6ijArKE8vKU"" rel=""nofollow"">https://youtu.be/6ijArKE8vKU</a>)</p>
","arduino accelerometer gyroscope visualization"
"10115","SLAM - odometry motion model","<p>I am making a project with a 4 wheeled differential robot to make visual SLAM using a stereo rig. I have some encoders to measure de displacement and the steering angle of the robot and I want to use the <strong>odometry motion model</strong> in the fastSLAM algorithm.</p>

<p>To use the odometry motion model you need to calculate the values it needs from the odometry reading (incremental encoders), $u_t=(\bar{x}_{t-1},\bar{x_t})$ where $\bar{x}_{t-1}=(\bar{x}\&gt;\bar{y}\&gt;\bar{\theta})$ and $\bar{x}_t=(\bar{x}'\&gt;\bar{y}'\&gt;\bar{\theta}')$ are the previous and the current pose extracted from the odometry of the vehicle.</p>

<p>My question is about how to obtain those values from the encoders. I guess that in this case I would need to obtain the equations from the geometric model for the differential robot:</p>

<p>$D_L=\frac{2\cdot\pi \cdot R_L}{N_c}\cdot N_L$</p>

<p>$D_R=\frac{2\cdot\pi \cdot R_R}{N_c}\cdot N_R$</p>

<p>$D_T=\frac{D_L+D_R}{2}$</p>

<p>$\Delta\theta=\frac{D_L-D_R}{L}$</p>

<p>where $D_L$ is the advance of the left wheel, $D_R$ is the advance of the right wheel, $R_L$ is the lecture from the left encoder, $R_R$ the lecture from the right encoder, $N_C$ the total number of pulses of the encoder type, $D$ is the total distance achieved by the robot and $\theta$ the angle steered. $L$ is the distance between the wheels.</p>

<p>Using those equations is possible to obtain the pose in every time step:</p>

<p>$\bar{x}_{t}=\bar{x}_{t-1}+D\cos(\theta_{t-1})$</p>

<p>$\bar{y}_{t}=\bar{y}_{t-1}+D\sin(\theta_{t-1})$</p>

<p>$\bar{\theta}_{t}=\bar{\theta}_{t-1}+\Delta\theta$</p>

<p>So those last are the values I need to inject to the modometry motion model and then add gaussian noise to them.
Am I right? Or is there another way of computing the pose from odometry for a differential 4-wheel robot?</p>
","mobile-robot slam odometry movement"
"10117","How to preprogram iRobot Create 2","<p>I want to program a set path for the iRobot to follow without having to be tethered all the time to my computer. What is the best way to do that? </p>
","programming-languages"
"10118","How to decide the torque of the motor and the gearbox ratio for a robotic arm?","<p>How to decide the torque of the motor and the gearbox ratio for a (say 6 DOF) robotic arm, having a 5 kg payload capacity for instance. I am mainly concerned about the inertial mismatch. How do I calculate it? Are there any other factors that I should consider? </p>
","motor robotic-arm torque manipulator"
"10119","How to create a model for temperature control?","<p>I have a heated compartment, inside which, there is another object heated up by independent heater. I want to control temperatures of both chamber and the object. </p>

<p>I could achieve this by simple PID (or PI) controllers for both chamber and object, but I would like to try more thoughtful approach :) I have two temperature sensors, and two PWM outputs for heaters. How do I identify a model for an object I want to control?   </p>
","control pid automation"
"10125","What kind of torque is needed for a small 5-6 axis robotic arm?","<p>I'm new to robotics and I'm looking to make a 5-6 axis robotic arm out of stepper motors but I honestly dont know how much torque I should have for each part. Below I have described in more detail what my current plan is but I'm really not sure as to how much I really should be spending on each of these joints.</p>

<p>My general plan for this project was to make a arm that when fully extended would only be around 40-50(max) cm long. It would be consisted of light weight aluminum and I am hoping for it to weigh only a couple of pounds when done.<br>
Anyway here is my current list of actuators for each of the joints:<br>
(Bottom = 1, Top = 6)  </p>

<p>1st Joint, (I cant actually post the link because I don't have enough rep, but this is what it is called on amazon: Nema 23 CNC Stepper Motor 2.8A)  </p>

<p>2nd and 3rd <a href=""http://rads.stackoverflow.com/amzn/click/B00PNEQKC0"" rel=""nofollow"">Joints</a>  </p>

<p>4th, 5th and 6th <a href=""http://rads.stackoverflow.com/amzn/click/B00PNEQI7W"" rel=""nofollow"">Joints</a>  </p>

<p>My real questions is, is this overkill or is it not enough for what I'm really trying to make. I really don't need it to be able to pick up a lot of weight, at most 1 to 2 kilos but I highly doubt I will ever be picking up anything more than that. Anyway I just wanted to see if this was sufficient enough for my project... I know this isn't really the best place to ask but I really need some help because I am new to this and I don't want to throw money where I don't need to. Thanks in advanced ;)</p>
","robotic-arm stepper-motor actuator"
"10127","PID with position and velocity goal?","<p>I'm trying to design a control system for a robot that tracks moving object. Thus I want to robot to match the position and velocity state of the object. I don't want robot to simply to arrive at the position, but I want to arrive at the position with the same velocity of the object. </p>

<p>Object velocity and position data will be provided externally.</p>

<p>I'm not sure if a traditional PID controller (with velocity controls) with just a position based error is enough. Wouldn't position only state goal result in tracking that is always lagging behind?</p>

<p>Is PID what I want or should I be looking at something else like trajectory controls?</p>
","pid"
"10129","Disable MAVLINK Heartbeat Using Telemetry","<p>I am using an APM 2.6 that is connected to an Odroid USB via Telemetry port (UART to USB). I am trying to get MAVLINK messages without the need to keep sending a HEARTBEAT message.  </p>

<p>If I switch to USB connection (not telemetry) I get MAVLINK messages continuously  without sending HEARTBEAT messages to the APM. I want to be able to do the same using the Telemetry port.</p>

<p>Is there any place in the firmware (ArduCopter) code that I can change? Or maybe just a parameter?</p>

<p>I am using 57600 Baud-Rate, I tried also 115200. And the USB cable is not connected.</p>
","quadcopter ardupilot mavlink"
"10130","Detect physical touch/hit","<p>I'm making a target to an outdoor robot competition.</p>

<p>The target should detect if some of the robot got touched or got an hit   automatically. and the target can get hit 360 degree. </p>

<p>I'm searching for the perfect sensor to detect an hit, without get false positive from a wind.</p>

<p>My option right now are:
1- ultrasonic sensor (bad coverage)
2- tilt sensor  (bad FP rate)
3- wooden conductive </p>

<p>I would like to know if someone has other ideas (that affordable - less than 30$ dollar per target might be o.k)</p>

<p>Edit: the target is static, and just waiting to a robot to touch it.</p>

<p>Edit: The specs are:</p>

<p>1- The target dimension is 1 meter height, 0.5 meter width , 0.3 depth.</p>

<p>2- To trigger the target ,the robot should be around 10 centimeter long to any point of the target surface.</p>

<p>3-To trigger the target the robot needs to get close up to 10 centimeter or even press with around 1 Newton force. the robot might even throw an object that satisfy the previous condition.</p>

<p>4-Detection must be only from intentional touch.</p>

<p>5-Wooden conductive is trigger because a human is Electrically conductive. this might not be the option when we throw an object.</p>

<p>6- Target will be placed outdoor, so the sensor need to be wind-resistance (not extreme wind condition- just around 20-25 km/h)</p>

<p>7- I  prefer a sensor that detect touch (more than proximity)because it might make my solution more cheap and reliable(in factor of amount sensors as i estimate).</p>

<p>Thanks.
Guy</p>
","arduino sensors force-sensor"
"10132","What is the best way to plug in more than 3 stepper motors into a Arduino Uno board?","<p>I'm developing a 5 axis robotic arm with stepper motors and I am getting around to ordering most of my parts. I plan on using 5 <a href=""http://rads.stackoverflow.com/amzn/click/B00RCTW5SM"" rel=""nofollow"">EasyDriver</a> shields to drive all of my motors. I am also planning on using just a basic arduino uno board to go with it. So here are my questions:<br>
Is there any alternative instead of buying a ton of Easy Drivers and connecting all of them to a single board?<br>
And if there isn't, then how would the setup look to use more than 3 stepper motors? This is the most <a href=""http://cdn.instructables.com/F8Z/AAO2/I3KBOWBE/F8ZAAO2I3KBOWBE.MEDIUM.jpg"" rel=""nofollow"">useful picture</a> I found, however It only shows 3 and while I know I could plug in a 4th I am unsure whether I could plug in a 5th.</p>
","stepper-motor"
"10136","iRobot Create2: Granularity of drive control?","<p>I own an iRobot create2 on which I am planning to implement a control algorithm. After playing with the different drive commands, I noticed that changing the desired velocity values marginally doesn't seem to do anything.</p>

<p>Even the Drive PWM command that ranges from -255 to 255 seems to have an internal granularity that is bigger than 1.</p>

<p>In <a href=""https://www.youtube.com/watch?v=U4_abg9LDLw"" rel=""nofollow"">this video</a> the create seems to change its driving direction nearly seamlessly, which I am not able to reproduce with the described behavior. </p>

<p>Does anyone have any suggestions?</p>
","control motor irobot-create"
"10148","What's new about drone technology?","<p>In recent years, we've heard a lot about <strong><em>drones</em></strong>. What new <em>technology</em> is enabling these new devices? Why are they making news?</p>

<p>When I was a kid, we used to call those things (similar to what we now call drones) <strong><em>remote control</em></strong> or <strong><em>RC</em></strong>. But, apparently it's not just a nomenclature change because we now have new FCC regulations, commercial applications like <a href=""https://www.amazon.com/b?node=8037720011"" rel=""nofollow"">Amazon Prime Air</a>, news of military applications and I still see RC labeled devices in the stores.</p>

<p>So, what is this, new <em>drone</em> stuff all about? And why now? What new technology has recently emerged that enables these devices?</p>
","untagged"
"10150","How does ODE determine contact points in Gazebo?","<p>I was looking at the contact points for the Atlas in the DRCsim package. Each foot has 4 contact points at each vertex of the rectangle. I'd like to know how these points are determined. I've tried looking at the ODE code, but C++ isn't my strong suit so I had some difficulty figuring out what was going on. What I understand is that ODE compares the geometries one by one however it's not possible to compare all points so it only compare a select few points. What I'm trying to understand is what basis are those particular points selected? Why does the Atlas have the 4 contacts set up the way they are, and not some additional points on the heel? Can I add them myself?</p>

<p>Thanks.</p>
","gazebo"
"10153","APM 2.6 response only once after REQUEST_DATA_STREAM","<p>I am trying to get mavlink messages from APM 2.6 via telemetry that is connected to my Ordoid U3 USB port. 
I am able to read messages when I send the REQUEST_DATA_STREAM message, but it sends them only once, I want to be able to get them continuously without needing to send the request again.</p>

<p>Any ways to solve this?</p>
","quadcopter communication ardupilot mavlink"
"10154","Bldc motors erratic behavior with Arduino program","<p>I've been making my own quadcopter flight controller using <strong>Arduino Mega</strong>. This is the sample code I wrote in order to <strong>test the esc timers and motors</strong>:</p>

<pre><code>byte channelcount_1, channelcount_2, channelcount_3, channelcount_4;  
int receiverinput_channel_1, receiverinput_channel_2, receiverinput_channel_3, receiverinput_channel_4, start;  
unsigned long channel_timer_1, channel_timer_2, channel_timer_3, channel_timer_4, current_time, esc_looptimer;  
unsigned long zero_timer, timer_1, timer_2, timer_3, timer_4;  
void setup() {  
  // put your setup code here, to run once:  
  DDRC |= B11110000; //Setting digital pins 30,31,32,33 as output  
  DDRB |= B10000000;; //Setting LED Pin 13 as output  
  //Enabling Pin Change Interrupts  
  PCICR |= (1 &lt;&lt; PCIE0);  
  PCMSK0 |= (1 &lt;&lt; PCINT0); //Channel 3 PIN 52  
  PCMSK0 |= (1 &lt;&lt; PCINT1); //Channel 4 PIN 53  
  PCMSK0 |= (1 &lt;&lt; PCINT2); //Channel 2 PIN 51  
  PCMSK0 |= (1 &lt;&lt; PCINT3); //Channel 1 PIN 50  
  //Wait till receiver is connected
  while (receiverinput_channel_3 &lt; 990 || receiverinput_channel_3 &gt; 1020 || receiverinput_channel_4 &lt; 1400) {  
    start++;  
    PORTC |= B11110000;  
    delayMicroseconds(1000); // 1000us pulse for esc  
    PORTC &amp;= B00001111;  
    delay(3); //Wait 3 ms for next loop  
    if (start == 125) { // every 125 loops i.e. 500ms  
      digitalWrite(13, !(digitalRead(13))); //Change LED status  
      start = 0; //Loop again  
    }  
  }  
  start = 0;  
  digitalWrite(13, LOW); //Turn off LED pin 13  
  zero_timer = micros();  
}  
void loop() {  
  // put your main code here, to run repeatedly:  
  while (zero_timer + 4000 &gt; micros());  
  zero_timer = micros();  
  PORTC |= B11110000;  
  channel_timer_1 = receiverinput_channel_3 + zero_timer; //Time calculation for pin 33  
  channel_timer_2 = receiverinput_channel_3 + zero_timer; //Time calculation for pin 32  
  channel_timer_3 = receiverinput_channel_3 + zero_timer; //Time calculation for pin 31  
  channel_timer_4 = receiverinput_channel_3 + zero_timer; //Time calculation for pin 30  
  while (PORTC &gt;= 16) //Execute till pins 33,32,31,30 are set low  
  {  
    esc_looptimer = micros();  
    if (esc_looptimer &gt;= channel_timer_1)PORTC &amp;= B11101111; //When delay time expires, pin 33 is set low  
    if (esc_looptimer &gt;= channel_timer_2)PORTC &amp;= B11011111; //When delay time expires, pin 32 is set low  
    if (esc_looptimer &gt;= channel_timer_3)PORTC &amp;= B10111111; //When delay time expires, pin 31 is set low  
    if (esc_looptimer &gt;= channel_timer_4)PORTC &amp;= B01111111; //When delay time expires, pin 30 is set low  
  }  
}  
//Interrupt Routine PCI0 for Receiver  
ISR(PCINT0_vect)  
{  
  current_time = micros();  
  //Channel 1  
  if (PINB &amp; B00001000)  
  {  
    if (channelcount_1 == 0 )  
    {  
      channelcount_1 = 1;  
      channel_timer_1 = current_time;  
    }  
  }  
  else if (channelcount_1 == 1 )  
  {  
    channelcount_1 = 0;  
    receiverinput_channel_1 = current_time - channel_timer_1;  
  }  
  //Channel 2  
  if (PINB &amp; B00000100)  
  {  
    if (channelcount_2 == 0 )  
    {  
      channelcount_2 = 1;  
      channel_timer_2 = current_time;  
    }  
  }  
  else if (channelcount_2 == 1)  
  {  
    channelcount_2 = 0;  
    receiverinput_channel_2 = current_time - channel_timer_2;  
  }  

  //Channel 3  
  if (PINB &amp; B00000010)  
  {  
    if (channelcount_3 == 0 &amp;&amp; PINB &amp; B00000010)  
    {  
      channelcount_3 = 1;  
      channel_timer_3 = current_time;  
    }  
  }  
  else if (channelcount_3 == 1)  
  {  
    channelcount_3 = 0;  
    receiverinput_channel_3 = current_time - channel_timer_3;  
  }  
  //Channel 4  
  if (PINB &amp; B00000001) {  
    if (channelcount_4 == 0 )  
    {  
      channelcount_4 = 1;  
      channel_timer_4 = current_time;  
    }  
  }  
  else if (channelcount_4 == 1)  
  {  
    channelcount_4 = 0;  
    receiverinput_channel_4 = current_time - channel_timer_4;  
  }  
}
</code></pre>

<p>However, my issue here is that the <strong>bldc motors i'm using don't work smoothly when connected to the Arduino.</strong> They <strong>erratically stop and even change direction of rotation</strong> at the same throttle input. I've tested them by connecting them directly to the transmitter and they work fine there with perfect rotation and speed. Can someone please help me out and tell me where I might be going wrong? </p>

<p>EDIT: I do realize posting the entire Arduino code might be overkill, but I've been trying to solve this problem for three days (as of 22nd June,16) and I really do hope someone can point out any improvements/corrections in my code.</p>
","quadcopter arduino brushless-motor esc"
"10159","iCreate 2 with Arduino (Just getting going)","<p>I am new to the iRobot Create 2 but I do know a thing or two about the Arduino (don't assume too much though). However, in this case, I am beyond stumped over what I am sure is something simple but is somehow not obvious to me. Three people have confirmed my wiring from the Create 2 to the Arduino to be correct and the code I have looks similar to many examples that I have seen on this forum. However, I cannot get my Create 2 to do ANYTHING. I am not at all sure what is wrong and I am starting to wonder if the robot is even receiving commands let alone doing anything with them. Is there anything wrong with this code and can anybody suggest a way to verify that the robot is receiving data (since it does not beep or provide return messages)? Thank you.</p>

<p>EDIT (06/24 01:10 EST); Updated code (with a few notes).</p>

<pre><code>#########################
#include &lt;SoftwareSerial.h&gt;
#include &lt;SPI.h&gt;

int baudPin = 17;
int i;
int ledPin = 13;
int rxPin = 19;
int txPin = 18;

unsigned long baudTimer = 240000; // 4 minutes
unsigned long thisTimer = 0;
unsigned long prevTimer = 0;

SoftwareSerial Roomba(rxPin, txPin);

void setup() {
  pinMode(baudPin, OUTPUT);
  pinMode(ledPin, OUTPUT);
  pinMode(rxPin, INPUT);
  pinMode(txPin, OUTPUT);

  // I have tired communicating with both baud rates (19200 and 115200).
  // When trying the 115200 baud, I set ""i&lt;=0;"" in the loop below since
  // the pulse does not need to be sent.
  Roomba.begin(19200);
  Serial.begin(115200);
  delay(2000);

  // I hooked up an LED in series with the baudPin so that it would turn
  // off when low thus giving me some kind of visual confirmation that a
  // pulse is being sent. See additional note in loop() below.
  for (i = 1; i &lt;= 3; i++) {
    digitalWrite(baudPin, HIGH);
    delay(100);
    digitalWrite(baudPin, LOW);
    delay(500);
    digitalWrite(baudPin, HIGH);
  }

  // I know this might not be the right way to send data to the robot,
  // but I was fiddling with this while trying to figure out a separate
  // problem regarding the TX/RX lines which I am putting off until I
  // get the baud issue straightened out.
. /*
    int sentBytes = Roomba.write(""128"");
    Serial.print(sentBytes);
    Serial.print(""\n"");
  */

  i = 0;
}

void loop() {
  thisTimer = millis();

  // The LED that I have hooked up in series with the baudPin blinks
  // when the pulse is low, thus indicating that a pulse is being sent.
  // However, it only seems to wake the robot when it is asleep. If the
  // robot is already awake when the pulse is sent, it has no affect and
  // the robot will fall asleep a minute later.
  if (thisTimer - prevTimer &gt; baudTimer) {
    prevTimer = thisTimer;
    i = 10;
    Serial.print(""Sending pulse...\n"");
    digitalWrite(baudPin, LOW);
    delay(500);
    digitalWrite(baudPin, HIGH);
  }

  /*
    i++;
    Serial.print(prevTimer);
    Serial.print("" --&gt; "");
    Serial.print(thisTimer);
    Serial.print("" --&gt; "");
    Serial.print(i);
    Serial.print(""\n"");
    delay(1000);
  */
}
#########################
</code></pre>
","arduino irobot-create"
"10167","Getting I2C sensor output from Ardupilot to Arduino","<p>I am trying to get the airspeed for ArduPlane from Erle Brain 2 ( Ardupilot) through its I2C port, and send it to Arduino. </p>

<p>What I have discovered:
There already exists I2C_driver.cpp, and I can use this to send data, by using the functions in Arduplane.cpp. However, I am lost on how to implement the sending part, as in how I use the functions? The functions like write accept arguments of address, length, data. How do I know that? And do I send the data in binary?</p>

<p>Any help will be really appreciated! </p>

<p>Thanks!</p>
","arduino ardupilot i2c"
"10169","What are the prerequisites for learning ROS?","<p>It is helpful in robotics to first learn about ""Linux kernel development"" or ""device driver development in Linux"" before I start learning ROS? I know C and JAVA! In brief, I want to know any prerequisites which are essential to understand ROS better.</p>
","ros linux"
"10172","Covering Up Ultrasonic Sensor","<p>I'm using a basic trig/echo <strong>Ultrasonic Sensor</strong> with an <strong>Arduino Uno</strong>. I get accurate readings until I <em>cover the sensor</em> at which point I receive very large numbers. Why is this?</p>

<p><strong>Program</strong></p>

<pre><code>int trigPin = 8;
int echoPin = 9;
float pingTime;
float targetDistance;
const float speedOfSound = 776.5; // mph

void setup() {
  Serial.begin(9600);

  pinMode(trigPin, OUTPUT);
  pinMode(echoPin, INPUT);

}

void loop() {
  digitalWrite(trigPin, LOW);
  delayMicroseconds(2000);
  digitalWrite(trigPin, HIGH);
  delayMicroseconds(15);
  digitalWrite(trigPin, LOW);
  delayMicroseconds(10);

  pingTime = pulseIn(echoPin, HIGH);
  pingTime /= 1000000; // microseconds to seconds
  pingTime /= 3600; // hours
  targetDistance = speedOfSound * pingTime; // miles
  targetDistance /= 2; // to from target (averaging distance)
  targetDistance *= 63360; // miles to inches

  Serial.print(""distance: "");
  Serial.print(targetDistance);
  Serial.println("""");

  delay(100);
}
</code></pre>

<p><strong>Example Output</strong></p>

<p>I moved my hand from 10"" away until I cover the sensor</p>

<pre><code>10.20 distance: // my hand is 10"" away from the sensor
10.01 distance:
9.51 distance:
8.71 distance:
7.85 distance:
6.90 distance:
5.20 distance:
4.76 distance:
3.44 distance:
2.97 distance:
1.65 distance:
1211.92 distance: // my hand is now pressed up against the sensor
1225.39 distance:
1197.16 distance:
1207.43 distance:
1212.66 distance:
1204.60 distance:
</code></pre>

<h1>EDIT</h1>

<p>I changed the amounts from <code>inches</code> to <code>milimeters</code> to get a more precise reading. I held the sensor ~100mm from a granite counter-top and quickly lowered it until the tabletop covered the front of the sensor.</p>

<pre><code>distance: 103.27 // 100mm from tabletop
distance: 96.50
distance: 79.84
distance: 76.72
distance: 62.66
distance: 65.78
distance: 54.85
distance: 47.04
distance: 44.95
distance: 38.71
distance: 28.81
distance: 25.69
distance: 27.08
distance: 25.17
distance: 27.77
distance: 22.04 // sensor continues toward table but values start to increase when they would logically decrease ??
distance: 23.95
distance: 26.73
distance: 28.81
distance: 46.52
distance: 2292.85 // sensor is now flush against tabletop
distance: 2579.59
distance: 2608.75
distance: 2595.56
distance: 2591.57
distance: 2583.75
distance: 2569.87
distance: 2570.91
distance: 2600.07
distance: 30579.64 // extreme high &amp; low values with sensor is same place against tabletop
distance: 37.66
distance: 30444.43
distance: 37.66
distance: 30674.23
distance: 38.71
</code></pre>
","arduino ultrasonic-sensors"
"10178","Switch activated by a microcontroller","<p>I'm working on a project where I'm using a voltage that is higher than what most microcontrollers can handle. I'm looking for a kind of switch that will connect a power source to an electromagnet and all of this controlled by my microcontroller. I also thought about using a potentiometer to control the speed of two high voltage DC motors via my microcontroller so please tell me if this is a good idea aswell. <br />
Thanks for your time
<br />
Zakary</p>
","arduino microcontroller"
"10187","Proper naming of PID regulators","<p>I was wondering either there is any special naming for regulators that:</p>

<ul>
<li>Outputs unit is the same as inputs, ie. velocity [m/s] as input and velocity as output [m/s].</li>
<li>Outputs unit is different than inputs, ie. position as input [m], velocity as output [/m/s]</li>
</ul>

<p>I would appreciate all help.</p>
","pid"
"10190","Can a robot or mechanical part be programmed to exert a specific force","<p>So I was thinking about projectiles that don't need a propellant like gunpowder I've seen coils gun but that's a little out my way. I was wondering if I know the force required to propel a object could I program a robot to exert that force to propel the object the same way (in a linear propelled fashion).</p>
","force-sensor"
"10192","Create2 incremental encoder rollover method","<p>I have never yet had the Create2's incremental encoder rollover but want to write my code to be prepared for this to happen and test it. When the encoder rolls past 32767 (14.5m), does it rollover to -32768 and count there or start at 0 again and count up from there?</p>

<p>One other odd thing but not a big deal. When I reset the Create2, the first value is 1 not 0. </p>
","irobot-create roomba"
"10193","What is LIADAR alternative for indoor RC CAr","<p>I am new to robotics and working on autonomous RC car for indoor purpose only.
I was wondering how can I detect expected collision (I am planning to put dummy cars near by RC car).</p>

<p>Please guide me if there is any other alternative or any one has worked on similar project.</p>

<p>reference:<a href=""http://robotics.stackexchange.com/questions/4583/what-are-some-low-cost-alternatives-for-lidar"">What are some low cost alternatives for lidar?</a></p>

<p>Thanks,
Sandy</p>
","mobile-robot"
"10195","Starting out: Arduino vs Raspberry Pi drone","<p>For my dissertation project, I will have to use a drone. What it will do is look for an object in a closed space.</p>

<p>What I will most deffinately need are:</p>

<ul>
<li>A camera</li>
<li>Sensors to avoid collision</li>
<li>PC communication: 

<ul>
<li>stream video </li>
<li>receive directions (the pc will control the drone, give it directions etc)</li>
</ul></li>
</ul>

<p>Now I'm wondering what would be the best platform to build on considering these requirements and I have absolutely no idea what's going on in the microcontroller world. So I don't know which of the two has more shields and whatnot that would be suitable for my needs.</p>
","quadcopter arduino raspberry-pi"
"10196","Using Quaternions to feed a quadcopter PID stabilizing controller to avoid Gimbal lock","<p>I am trying to control my F450 dji quadcopter using a PID controller. From my IMU, I am getting the quaternions, then I convert them to Euler's angles, this is causing me to have the Gimbal lock issue. However, is there a way that I directly use the quaternions to generate my control commands without converting them to Euler's angle?</p>

<p>This conversation <a href=""http://robotics.stackexchange.com/questions/3137/how-to-use-quaternions-to-feed-a-pid-quadcopter-stabilization-loop/4483#4483?newreg=e2857a59984a4df59536d648f360dec1"">here</a> discusses a similar issue but without mentioning a clear answer for my problem.</p>

<p>The three errors so far I am trying to drive to 0 are:</p>

<pre><code>double errorAlpha  = rollMaster  - rollSlave;
double errorTheta  = pitchMaster - pitchSlave;
double errorPsi    = yawMaster   - yawSlave;
</code></pre>

<p>where the Master generates the desired rotation and the Slave is the IMU.</p>

<p>UPDATE:</p>

<p>Here are some pieces of my code:</p>

<p>Getting the current and the reference quaternions for bot the Master and the Slave from the <strong>ROTATION_VECTOR</strong>:</p>

<pre><code>/** Master's current quaternion */
double x   = measurements.get(1);
double y   = measurements.get(2);
double z   = measurements.get(3);
double w   = measurements.get(4);

/** Slave's current quaternion */
double xS  = measurements.get(5);
double yS  = measurements.get(6);
double zS  = measurements.get(7);
double wS  = measurements.get(8);

/** Master's Reference quaternion */
double x0  = measurements.get(9);
double y0  = measurements.get(10);
double z0  = measurements.get(11);
double w0  = measurements.get(12);

/** Slave's Reference quaternion.
 *  If the code has not been initialized yet, save the current quaternion
 *  of the slave as the slave's reference orientation. The orientation of
 *  the slave will henceforth be computed relative to this initial
 *  orientation.
 */
if (!initialized) {
    x0S = xS;
    y0S = yS;
    z0S = zS;
    w0S = wS;
    initialized = true;
}
</code></pre>

<p>Then I want to know the orientation of the current quaternion relative to the reference quaternion for both the Master and the Slave. </p>

<pre><code>/**
     * Compute the orientation of the current quaternion relative to the
     * reference quaternion, where the relative quaternion is given by the
     * quaternion product: q0 * conj(q)
     *
     * (w0 + x0*i + y0*j + z0*k) * (w - x*i - y*j - z*k).
     *
     * &lt;pre&gt;
     * See: http://gamedev.stackexchange.com/questions/68162/how-can-obtain-the-relative-orientation-between-two-quaternions
     * http://www.mathworks.com/help/aerotbx/ug/quatmultiply.html
     * &lt;/pre&gt;
     */
    // For the Master
    double wr = w * w0 + x * x0 + y * y0 + z * z0;
    double xr = w * x0 - x * w0 + y * z0 - z * y0;
    double yr = w * y0 - x * z0 - y * w0 + z * x0;
    double zr = w * z0 + x * y0 - y * x0 - z * w0;

    // For the Slave
    double wrS = wS * w0S + xS * x0S + yS * y0S + zS * z0S;
    double xrS = wS * x0S - xS * w0S + yS * z0S - zS * y0S;
    double yrS = wS * y0S - xS * z0S - yS * w0S + zS * x0S;
    double zrS = wS * z0S + xS * y0S - yS * x0S - zS * w0S;
</code></pre>

<p>Finally, I calculate the Euler angles:</p>

<pre><code>/**
     * Compute the roll and pitch adopting the Tait–Bryan angles. z-y'-x"" sequence.
     *
     * &lt;pre&gt;
     * See https://en.wikipedia.org/wiki/Rotation_formalisms_in_three_dimensions#Quaternion_.E2.86.92_Euler_angles_.28z-y.E2.80.99-x.E2.80.B3_intrinsic.29
     * or  http://nghiaho.com/?page_id=846
     * &lt;/pre&gt;
     */
    double rollMaster  =  Math.atan2(2 * (wr * xr + yr * zr), 1 - 2 * (xr * xr + yr * yr));
    double pitchMaster =  Math.asin( 2 * (wr * yr - zr * xr));
    double yawMaster   =  Math.atan2(2 * (wr * zr + xr * yr), 1 - 2 * (yr * yr + zr * zr));
</code></pre>

<p>and I do the same thing for the Slave. </p>

<p>At the beginning, the reference quaternion should be equal to the current quaternion for each of the Slave and the Master, and thus, the relative roll, pitch and yaw should be all zeros, but they are not!</p>
","quadcopter pid stability"
"10200","Create 2 Reading Sensor Values","<p>I am trying to solve some Create 2 sensor reading problem that I am having when I came across @NBCKLY's posts (<a href=""http://robotics.stackexchange.com/questions/7215/arduino-create-2-reading-sensor-values"">Part 1</a> and <a href=""http://robotics.stackexchange.com/questions/7229/irobot-create-2-encoder-counts"">Part 2</a>) that I believe are exactly what I am looking for. I copied his code from the original post into my project and updated the code from the second post as best as I could interpret...but something is not going according to plan.</p>

<p>For example, I am printing the angle to my serial monitor (for now) but I am constantly getting a value of 0 (sometimes 1).</p>

<p>Can @NBCKLY or anybody please check out this code and tell me what I'm doing wrong? I would appreciate it. Thank you very much.</p>

<pre><code>int baudPin = 2;
int data;
bool flag;
int i;
int ledPin = 13;
int rxPin = 0;
signed char sensorData[4];
int txPin = 1;

unsigned long baudTimer = 240000;
unsigned long prevTimer = 0;
unsigned long thisTimer = 0;



void drive(signed short left, signed short right) {
  Serial.write(145);
  Serial.write(right &gt;&gt; 8);
  Serial.write(right &amp; 0xFF);
  Serial.write(left &gt;&gt; 8);
  Serial.write(left &amp; 0xFF);
}

void updateSensors() {
  Serial.write(149);
  Serial.write(2);
  Serial.write(43); // left encoder
  Serial.write(44); // right encoder
  delay(100);

  i = 0;

  while (Serial.available()) {
    sensorData[i++] = Serial.read();
  }

  int leftEncoder = int((sensorData[0] &lt;&lt; 8)) | (int(sensorData[1]) &amp; 0xFF);
  int rightEncoder = (int)(sensorData[2] &lt;&lt; 8) | (int)(sensorData[3] &amp; 0xFF);
  int angle = ((rightEncoder * 72 * 3.14 / 508.8) - (leftEncoder * 72 * 3.14 / 508.8)) / 235;

  Serial.print(""\nAngle: "");
  Serial.print(angle);
  Serial.print(""\n"");
}



void setup() {
  pinMode(baudPin, OUTPUT);
  pinMode(ledPin, OUTPUT);
  pinMode(rxPin, INPUT);
  pinMode(txPin, OUTPUT);

  delay(2000);
  Serial.begin(115200);

  digitalWrite(baudPin, LOW);
  delay(500);
  digitalWrite(baudPin, HIGH);
  delay(100);

  Serial.write(128);
  Serial.write(131);
  updateSensors();
  drive(50, -50);
}



void loop() {
  thisTimer = millis();

  if (thisTimer - prevTimer &gt; baudTimer) {
    i = 0;
    prevTimer = thisTimer;
    digitalWrite(baudPin, LOW);
    delay(500);
    digitalWrite(baudPin, HIGH);
    Serial.print(""Pulse sent...\n"");
  }

  updateSensors();
}
</code></pre>

#

<p>What I am asking is why do I only get an angle of rotation of 0 or 1 degrees when the robot is moving in a circle. The angle should be incrementing while the robot is moving. </p>

<p>The output I am getting on the serial monitor shows a line of what looks like garble which I assume is supposed to be the bytes sent back from the Create which is followed by ""Angle: 0 (or 1)"" What I was expecting to see was an increasing angle value. (1,2,3...360, and so on).</p>
","arduino sensors irobot-create"
"10202","The 3 key components of a robot are controller, servo, and reducer. Can someone give us an ""official"" explanation of what they do respectively?","<p>I've googled a lot but wasn't able to find official definitions of these 3 parts. Maybe the explanations of <a href=""http://www.seattlerobotics.org/guide/servos.html"" rel=""nofollow"">servo</a> and <a href=""http://open-robotics.com/what-is-the-definition-of-a-robot-controller/"" rel=""nofollow"">controller</a> are good enough, but I'm still trying to look for a more ""official"" one.</p>

<p>Any ideas?</p>

<p>Thanks,</p>

<p>snakeninny</p>
","microcontroller servos"
"10210","Is this the right way to do motor mixing with PID outputs for a quadcopter?","<p>These are the motor mixing formulas I've written for my quadcopter's flight controller (Arduino-Mega) and I was wondering if its all right to use all three (roll-pitch-yaw) in each of the esc's signals. </p>

<pre><code>esc1 = throttle - pid_output_pitch + pid_output_roll - pid_output_yaw;  
esc2 = throttle + pid_output_pitch + pid_output_roll + pid_output_yaw;  
esc3 = throttle + pid_output_pitch - pid_output_roll - pid_output_yaw;  
esc4 = throttle - pid_output_pitch - pid_output_roll + pid_output_yaw;
</code></pre>
","quadcopter arduino pid esc"
"10213","Triangulation from calibrated stereo rig","<p>I am using a stereo rig to do SLAM, calibrated using the MATLAB Calibration Tool. I need to compute the 2D coordinates of a landmark using the observation model obtained from triangulation (the images are rectified). </p>

<p>The equations obtained from triangulation are the ones presented in the blue box <a href=""https://www.cs.auckland.ac.nz/courses/compsci773s1t/lectures/773-GG/topCS773.htm"" rel=""nofollow"">here</a>. Because I am doing SLAM in 2D the coordinates I need to use are $Z_p$ and $X_p$. The parameters needed to compute those values are $f$, $T$ and $disparity (x_L - x_R)$.</p>

<p>After doing the calibration intrinsics matrices $K_L$ and $K_R$ are obtained and a common intrinsic matrix for the stereo rig is calculated from $K = 1/2*(K_L +K_R)$ so I get the parameters needed in triangulation from  this common matrix.</p>

<p>The focal length is supplied from the manufacturer, and for my Logitech C170 is 2.3mm. The baseline $T$ from the calibration is 78.7803 mm. To compute the disparity I am obtaining SURF points and using RANSAC to discard the outliers so I get x coordinates from both rectified images.</p>

<p>The problem is that with those values I can't obtain correct values for $Z_p$ and $X_p$ and I am not sure why or where I am doing the wrong step. Anyone can help with this? Are those the correct steps to do triangulation from rectified stereo images?</p>

<p><strong>EDIT:</strong> My stereo rig looks like the figure I attach:<a href=""http://i.stack.imgur.com/1gpVj.jpg"" rel=""nofollow""><img src=""http://i.stack.imgur.com/1gpVj.jpg"" alt=""Figure 1""></a></p>

<p>If you compare the coordinates system with the one used in the link before is easy to see that my $X_r$ corresponds to the $Z_p$ from the link and the $Y_r$ corresponds to $X_p$, so the equations to calculate the distance using triangulation and with the coordinate system of the figure are:</p>

<p>$x_r=\frac{fb}{x_L-x_R}$</p>

<p>$y_r = \frac{(x_L-p_x)b}{x_L-x_R}-\frac{b}{2}$ </p>

<p>Being $f$ the focal length, $b$ the baseline, $p_x$ the x coordinate of the central point and $x_L-x_R$ the disparity. The $X_r$ $Y_r$ coordinate system is situated between the two cameras, so this is the meaning of the $\frac{b}{2}$ displacement in the equations.</p>

<p><strong>Calibration</strong> </p>

<p>To obtain the cameras calibration I am using the <a href=""http://es.mathworks.com/help/vision/ug/stereo-camera-calibrator-app.html"" rel=""nofollow"">Stereo Camera Calibrator Toolbox</a> with the chessboard pattern.</p>

<p>After calibration, I made some tests using MATLAB functions <em>triangulate</em> and <em>reconstructScene</em> to know whether the parameteres are well calculated. The distances I obtained using this functions (which use the stereoParams object created by the calibrator) works well and I obtain distances very similar to the actual ones. So I suposse the calibration works well.</p>

<p>The problem, as I explained before, is when I try to calculate the distances using the equations $x_r$ and $y_r$ because I am not sure how to obtain the common matrix $K$ for the stereo rig (the calibrator gives one intrinsic matrix for each camera, so you have two matrices).</p>

<p>The value of the baseline given from calibration make sense, I made a measurement with a ruler and gives me 78 mm approximately.</p>

<p>The $f$ value I assume should be in pixels but here again the calibration gives an $f_x$ and $f_y$ value so I am not sure which one should I use.</p>

<p>Those are the intrinsic matrices I obtain:</p>

<p><strong>Left</strong>: 
$\begin{pmatrix} 672.6879&amp;-0.7752&amp;282.2488\\0&amp;674.3705&amp;240.1287\\0&amp;0&amp;1 \end{pmatrix}$</p>

<p><strong>Right</strong>: $\begin{pmatrix} 681.7049&amp;0.0451&amp;331.2612\\0&amp;681.8235&amp;246.1209\\0&amp;0&amp;1\end{pmatrix}$</p>

<p>Being the parameters of $K$: $\begin{pmatrix}f_x&amp;s&amp;p_x\\0&amp;f_y&amp;p_y\\0&amp;0&amp;1\end{pmatrix}$</p>
","mobile-robot slam computer-vision stereo-vision"
"10215","Piezo sensors and multiplexers","<p>I got asked to make some sort of trigger pads for the foot section of an organ working over midi to a electric piano and my friend wants it to be pressure sensitive so we can program in the note velocity when he's not using the organ sound.</p>

<p><a href=""https://www.youtube.com/watch?v=DuariiHWJQg"" rel=""nofollow"">https://www.youtube.com/watch?v=DuariiHWJQg</a></p>

<p>That is what I try to achive. I want the pads to not just be on/off but also be able to control the velocity of the midi note.</p>

<p>Im planning to use a Adruino Uno with a MUX Shield II from Mayhew Labs to get 36 analog inputs. Not exactly sure on the wiring yet but have looked at some guides and videos on google to get a feel for how it can be made. </p>

<p>All these 36 piezo-""sensors"" is planned to register how hard you push the pedals and then send out a MIDI signal with a specific note correspondig to the pedal, and velocity to the electric piano so you can control the low notes with your feet.</p>

<p><a href=""http://www.thomann.de/se/clavia_nord_pedal_key_27.htm"" rel=""nofollow"">http://www.thomann.de/se/clavia_nord_pedal_key_27.htm</a></p>

<p>Just like that but more pedals and a lot cheaper. </p>

<p>Will the Arduino be able to read the analog output of the piezo sensor even though it's going through a multiplexer?</p>
","control electronics"
"10220","Quadcopter that can carry heavy things?","<p>So, while I was out drinking with a couple of my friends, one of us said something like 'man, wouldn't it be cool if the beer just came to us?' and that got me thinking.</p>

<p>We all have seen some crazy things people do with quadcopters (or polycopters even), but would it be possible (and not too expensive) to build a quadcopter that could carry, say, a crate of beer? (16-20kg)</p>

<p>I'm a bit of a tinkerer and I've built some minor things with rasp. pi's before but never tried myself at a quadcopter, because they are quite a big piece of work, but being able to fly a crate of beer right in front of me would be pretty awesome.</p>

<p>That aside, how strong would such a quadcopter have to be? In terms of motors, propellers, battery &amp; frame. I'm a complete noob when it comes to RPM and the like, so I wouldn't even know where to begin. I have, of course, read through most of the available tutorials on the internet, but they don't answer my question of <strong>what exactly to look for</strong> when I want my quadcopter to be able to carry something specific.</p>
","quadcopter raspberry-pi"
"10221","Should I use or not EKF for Baro-Acc altitude estimation?","<p>I've recently implemented a kalman filter to estimate altitude for a small robot with an IMU+Baro sensor mounted on it.</p>

<p>My objective is to get max precision I can have, using this two sensor, with small computing power that a MCU can provide me. I've tuned my filter and it seems to work pretty well.</p>

<p>Can I obtain a significant improvement using an Extended Kalman Filter instead of a normal Kalman Filter and if it worth time to implement it?</p>

<p>More in detail, since this request is too specific for each application, if a Model function that use Baro and Accel as states should be linearized and used in a EKF and if this can improve data reliability compared to a simply KF?</p>
","kalman-filter accelerometer ekf"
"10224","landmark extraction algorithm","<p>Hi the Landmark are very used in SLAM , what are the Algorithme those be used to Extract them , and how robot can diferentiate the landmark , if they detecte one in point A at Xt and another in Xt+1 how the robot can know if Its the same or not ?
Sorry for my bad english :/</p>
","slam ekf lidar ransac"
"10227","Are there any others alternatives for PID controllers for line following robots?","<p>Are there any better/ advanced ways of steering a line following robot other than pid controller? If so what are them? </p>
","pid line-following steering"
"10229","How to select dc motors for a line following robot?","<p>What are the criteria to consider when ordering dc motors for a line following robot?</p>

<p>Is there a way to calculate the torque required?</p>
","motor line-following"
"10233","Generate synthetic accelerometer data based on (x,y,z) coordinate","<p>I would like to create a simulation model (basically a signal generator) which will allow me to generate the 3 output signals of an accelerometer based on 3 location input signals (x,y and z). I would like a more realistic model of the data produced by an accelerometer (with some noise and bias offsets).</p>

<p>How can I convert the series of points into a simulated accelerometer output?</p>

<p>Specifically:</p>

<p>I have a series of positions which describe a trajectory in 3D space...If an accelerometer was moving along the trajectory described by the series of positions, I am interested in knowing (simulating!) the data that the accelerometer would produce as the result of moving along the described trajectory. </p>

<p>I could just calculate the 2nd derivative of the trajectory, but that would probably be too ideal. I am looking for a model which is more realistic. </p>
","accelerometer simulation"
"10237","What does internal sparking in a motor mean?","<p>My Arduino + Raspberry Pi robot was working fine in the morning. I tested it, it ran perfectly, and then I switched it off. </p>

<p>Now in the evening when I'm trying to run it again, with the same batteries and everything, it just doesn't move!</p>

<p>I stripped it down to the motor compartment and found that when I try to run my main motor, I can see sparks through the translucent plastic on the back.</p>

<p>Does that mean my motor is gone?</p>
","arduino motor raspberry-pi battery"
"10239","What are some pitfalls of an ultrasonic sensor?","<p>I'm using a <a href=""http://www.micropik.com/PDF/HCSR04.pdf"" rel=""nofollow"">HC-SR04 sensor</a> to detect obstacles. What are the pitfalls with an ultrasonic sensor?</p>

<p>Here are a couple I've found during my testing:</p>

<ul>
<li>The signal can bounce off of one wall to another and then get picked up, distorting latency</li>
<li>Absorbent materials sometimes don't bounce the signal back</li>
<li>Check the datasheet for supported range (min/max)</li>
</ul>
","ultrasonic-sensors"
"10241","CC3D PWM control signal characteristic (to be simulated by Raspberry PI)","<p>My goal is to control drone by Raspberry PI. The Raspberry PI uses camera and OpenCV, sends control commands to AVR microcontroller which will generate the PWM control signal. Meaning that it will simulate pilot with transmitter-receiver setup.</p>

<p>In other words (to make it more clear). Raspberry tells the Atmega8 that the drone needs to go more forward. Atmega8 generates custom PWM signals on 8 pins. Those signals are sent directly to CC3D pins responsible for roll, pitch etc. Atmega8 replaces controller receiver in this setup. It generates signal not based on user input but on what Raspberry tells it.</p>

<p>In order to do that I need the parameters (period, voltage etc.) of the PWM signal that CC3D accepts to properly simulate it. I have found this topic:</p>

<p><a href=""http://robotics.stackexchange.com/questions/8965/cc3d-replacing-rc-emitter-with-an-rpi?newreg=1af0c139fc9f44bab93b847d3d906fdf"">CC3D - Replacing RC emitter with an RPi</a></p>

<p>He has the same problem as I do and he found the solution. Unfortunately I can't send pm and I can't comment because I'm new to the site... so basically there is no way for me to contact him.</p>

<p>So any help would be appreciated.</p>
","quadcopter arduino raspberry-pi uav avr"
"10247","Doosan lynx 220 where to find inputs and outputs","<p>I want to add an robot to my machine the Doosan lynx 220 lsy. For that I need this inputs and output:</p>

<p>INPUTS:
Cycle start,
Chuck1 open,
Chuck1 close,
Chuck2 open,
Chuck3 close</p>

<p>OUTPUTS:
Cycle finished,
Check chuck1 opend,
Check chuck1 closed,
Check chuck2 opend,
Check chuck2 closed</p>

<p>I already found a book where i found those inputs and outputs but it justs says for example Cycle start (SB373). I cant find these number on the i/o board or anywhere else.
Can someone help me to find my listed outputs?</p>
","cnc"
"10248","Compass sensor for robot","<p>What's an appropriate compass sensor to use on a robot?</p>

<p>There are a ton of cheap digital compass sensors, and I was thinking of using an MPU9250 combined accel/gyro/magnetometer as a compass, but I'm finding these are terribly unreliable and need constant calibration via the ""wave in a figure 8 pattern"" method whenever it gets near other electronics or small magnets, which a robot obviously won't be able to do. Is there a digital compass technology that mimics traditional compasses that requires little to no calibration, appropriate for installation on a robot?</p>
","magnetometer compass"
"10249","Fence avoidance for manually controlled robot","<p>I'm trying to find known techniques for keeping a manually controlled robot within a known polygon fence. More specifically, a pilot controls a robot by issuing desired velocity vectors, and the autopilot adjusts the velocity so that the distance to any boundary is always at least the stopping distance of the robot.</p>

<p>My goal is to implement a system that:</p>

<ol>
<li>Tries to follow the pilot's desired velocity as closely as possible.</li>
<li>Is robust to changes in position and desired velocity. At a minimum, I want the velocity to change <strong>continuously</strong> with respect to the position of the robot and desired velocity of the pilot. Informally, this means that sufficiently small changes in the position or desired velocity of the pilot induce arbitrarily small changes in the velocity.</li>
</ol>

<p>The second point is particularly important. Suppose that the policy were to find the intersection with the boundary in the direction of the desired velocity and slow down smoothly to that point. The below figure depicts a couple of scenarios in which this would <em>not</em> be continuous. In this figure, the black lines represent the fence boundary, the red dot is the position of the robot, and the blue line is the desired velocity of the pilot. In figure (a), a small perturbation of the position to the left will cause a large increase in allowed velocity because the desired velocity will intersect the far edge instead of the near edge. In figure (b), a small clockwise rotation of the velocity vector will result in a large decrease in allowed velocity because the desired velocity will intersect the near edge instead of the far edge.</p>

<p><a href=""http://i.stack.imgur.com/yDbWx.jpg"" rel=""nofollow""><img src=""http://i.stack.imgur.com/yDbWx.jpg"" alt=""enter image description here""></a></p>

<p>I have searched for relevant papers, but most of the papers I've seen have dealt with fully autonomous obstacle avoidance. Moreover, I haven't seen any papers address the robustness/continuity of the system.</p>

<h2>:EDIT:</h2>

<p>The robot knows its own location and the location of the boundary at all times. I also have some equations for maximum velocity that allow a smooth ramp-down to a single line boundary (though I'd be interested in seeing a better one). I would like the velocity limits to be continuous in the position and desired velocity of the pilot.</p>

<p>I want to continuously throttle the user's input such that a minimum safe distance between the robot and the boundary is maintained, but see the figure that I added to the question. The hard part (I think) is to make sure that small changes in position (e.g. due to sensor noise) or small changes in desired velocity (e.g. due to pilot noise) don't cause huge changes in what the autopilot allows.</p>

<p>I want continuity because I think it will provide a much nicer experience for the pilot while still enforcing the fence boundary. There is a trade-off with optimally but I think this is worth it. Even though the physical world smoothes any discontinuities in velocity, big changes could still cause large jerk which will be somewhat disturbing to the pilot. The goal is to not have the autopilot introduce large oscillations not intended by the pilot.</p>

<p>This Will be implemented On a physical system that has sensors that provide an estimation of position, and the boundary shape is known and is unchanging. The actual system that I'm targeting is a quadcopter.</p>
","control geometry reference-request"
"10252","Obstacle Avoidance while Navigating","<p>I need some ideas for strategies or algorithms to apply on these strategies to perform obstacle avoidance while navigating.</p>

<p>At the moment I'm doing offline path planning and obstacle avoidance of known obstacles with an occupancy grid. And running the A* algorithm over the created matrix. After that my robot follows along the resulting trajectory. This is done by splitting the whole trajectory into sub-path. The robot adjust it's heading to the new target and follows the straight line. The robot is controlled by a fuzzy logic controller to correct deviations from the ideal line (steering) and adjusting the velocity according to the steering action and distance to the target. So far so good. And it's working very well.</p>

<p>As sensor system, I solely use the Google Project Tango (Motion Tracking and Area Learning for proper path following). Now I want to use the depth perception capability of the device. Getting the appropriate depth information and extracting a possible obstacle is done with a quite simple strategy. The robot analyses the depth information in front of the robot and if any object is in between the robot and the target point of the sub-path, an obstacle must be there.</p>

<p>Now I'm wondering how to bypass this obstacle most efficiently. The robot is only aware of the height and width of the obstacle, but has no clue about the depth (only the front of the obstacle is scanned). Feeding the occupancy grid with this new obstacle and running again the A* algorithm is not effective, because of the missing depth. One possible strategy I could imagine is estimating a depth of the length of the grid cell, re-plan and continue the navigation. If the robot faces the same obstacle again, the depth is increased by the size of one additional grid cell length. But I think this is extremely ineffective. </p>

<p>The requirement is to only use the Google Project Tango and no additional sensors, such as ultrasonic to sense the sides.</p>

<p><strong>Update 1</strong></p>

<p>The first picture illustrates the given trajectory from the path planning (orange). The gray and blue data points are the sensed obstacles in front of the robot. The notch behind the blue obstacle is actually the wall, but is shadowed by the blue obstacle. Image 2 shows the same scene just from a different perspective.</p>

<p>The issue I have to treat is, how to optimally bypass the blue obstacle even I don't know how deep it is. Driving to the left and to the right only to capture better data points (to generate a 3D model) is not possible. </p>

<p><a href=""http://i.stack.imgur.com/w7OqT.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/w7OqT.png"" alt=""Front perspective""></a>
<a href=""http://i.stack.imgur.com/tDx3G.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/tDx3G.png"" alt=""Side perspective""></a></p>

<p><strong>Update 2</strong>
Yes, I'm using a depth sensor, the one integrated in Google Project Tango. It's a visual measurement. A infra-red laser beams a grid onto the objects and a RGB-IR camera capture these information and evaluates the appropriate depth information.</p>
","control motion-planning algorithm"
"10253","Steering using different speeds in DC motors or using a servo?","<p>I am trying to assess the pros and cons of steering a robot car using different speeds of 2 or more DC motors versus using a servo and a steering mechanism? From your experience which is better in terms of:</p>

<ul>
<li>Steering accuracy (e.g. prompt responsiveness or skidding while on higher speeds)</li>
<li>Efficiency in electrical power consumption</li>
<li>Durability and maintenance</li>
<li>Control complexity (coding and electronics)</li>
</ul>

<p>I researched and understood how both approaches work, but I need some practical insight to select the most suitable approach. Any hint or research direction is appreciated.</p>
","motor servos steering"
"10256","Dynamic torque simulation for a 6 DOF robotic arm","<p>I am working on a 6 DOF robotic arm(industrial manipulator). I have the basic structural specs (dimensions, weights etc) for the links and joints with me. </p>

<p>Basically, I want to simulate both <em>static torque</em>(Due to the weight of the arm) and <em>dynamic torque</em>(due to the accelerating joint's motion) <strong>Torque</strong> that the joints will need to bear for a given set of motions. </p>

<p>I have looked on the web and found tools like the ROS-MoveIt Visualiser, Gazebo, V-REP which let me visually see a robotic arm and simulate the position logic and external factors like collisions etc. But I have been unable to simulate/calculate dynamic torque values from these tools.</p>

<p>Ideally, I'd want to define a fixed motion of the end effector(i.e. move the robot between 2 positions) and measure the Torque(both static and dynamic) during that particular move.</p>

<p>These torque values are essential for selecting the optimum motors and gearboxes for my design and payload.</p>
","robotic-arm dynamics torque simulation manipulator"
"10259","Find object using only distance","<p>I'm working on a extremely simple robot (very first project) that attempts to find the source of a Bluetooth signal. There are two motors that drive the platform and each has an encoder. We've already used a Kalman filter to calculate the approximate distance to the Bluetooth beacon within reasonable error.</p>

<p>I worked out a manual solution using some trig that solves the problem in theory, but it fails if there is any error (For example, it attempts to turn 73 degrees, but turns 60).</p>

<p>My question is how can I reasonably drive the motors based on the encoder data to continuously minimize the distance to the signal? Furthermore, is there a generic solution to problems like these? (I guess you might call it a stochastic ""Hotter/Colder"" problem)</p>

<p>Thanks in Advance.</p>
","raspberry-pi wheeled-robot"
"10265","How does a QuadCopter Startup work ? Will they tune every copter before releasing to market?","<p>We know that a quadcopter needs to be tuned to its perfect PID values to minimise the pitch, roll , yaw errors and etc., Before releasing to the market will they tune every unit and ship it ? Or a any different algorithm is used which doesn’t require any tuning ? Because every motor/ESC or a chassis will not be exactly same, which will add to the noise. </p>
","quadcopter"
"10267","Path planning of 2 arm 4dof Robot","<p>I am working on path planning for a 2 arm 4dof (2 dof for each arm) robot. I am currently using a centralised planning methodology (considering the multi robot system as a single one with higher dof, 4 in this case) and A* algorithm to find the shortest path. The problem with this algorithm is its high computation time.Is there any way to reduce the computation time while still obtaining the shortest route ?</p>

<p>Note:decentralised path planning is not good enough for my case.</p>
","robotic-arm motion-planning path-planning"
"10272","Stereo Vision Using Compute Module: Pi camera synchronization","<p>Good day,</p>

<p>I am currently working on an obstacle avoiding UAV using stereo vision to obtain depth maps. I noticed that the quadcopter would sometimes not steer to the correct direction.</p>

<p>I am using the <strong>Raspberry Pi Compute Module IO board which comes with two CSI ports</strong> used with two v1 Pi Cameras.</p>

<p>Issue</p>

<p>I soon found out that due to the latency between the cameras, the left and the right images are not in sync thus the errors in the depth map result.</p>

<p>Steps taken:</p>

<p>I noticed the image blur when moving the cameras around so I adjusted the shutter speed by setting the UV4l/raspicam driver. With the shutter speed, I also tried to increase the framerate as I've read, it improves the latency issue. In my code which uses the opencv library, I used the grab() and retrieve() commands to replace the read() command so that the frames from both cameras is grabbed at the nearest time possible however it didn't help much.</p>

<p>Does anyone know any possible solutions?</p>
","computer-vision stereo-vision c++ opencv"
"10277","What is the thread/screw size for the iRobot Create 2 internal screw bosses described in the Open Interface Spec doc","<p>In the “iRobot_Roomba_600_Open_Interface_Spec.pdf” provided for the iRobot Create 2, there is a section titled “Roomba Internal Screw Boss Locations”.  It states that “Screws may be replaced with threaded standoffs.”</p>

<p>Does anyone know what screw/thread size of standoffs should be used to match the screw threads?</p>

<p>(I saw another similar thread but the only solution listed was to re-thread the holes, which I would like to avoid if at all possible.)</p>

<p>Thanks!</p>
","irobot-create"
"10284","Mounting a gimbal BLDC motor","<p>I'm trying to build my own motorised camera gimbal using a BLDC like <a href=""http://www.aliexpress.com/store/product/2pcs-Ipower-brushless-motor-2804-100T-hollow-shaft-free-shipping/1755685_32355221325.html"" rel=""nofollow"">this</a>, where the shaft is hollow. Does anyone know how the camera platform should be mounted? Should a shaft be somehow pressed into the hole?</p>

<p>Any thought appreciated.</p>
","brushless-motor"
"10285","Using a six wire stepper motor with l298n","<p>i am using a l298n IC and (not a driver shield) and an arduino.
I would like to know how to use the IC with the arduino to run a six wire stepper motor.</p>

<p>Apparently i am new to electronics.Can i have a detailed explaination for wiring the IC connections on the breadboard and the arduino.</p>

<p>Thanks</p>
","stepper-motor"
"10286","Question about what motor to use for opening window","<p>first off, just to be transparent, I'm a total newbie when it comes to DC motors (and pretty much anything robotic). </p>

<p>I've got a couch that's right up to a window with the lever type openings (anderson windows). With the couch, I have no clearance to turn the lever to open it. Given I've replaced most of my house switches/outlets with home automatable ones, I figured I'd see if I can build myself a small motor that I can automate to open these also. To be absolutely honest, I've got no clue where to start. I have no problem with coding the automation part, but I don't even know what kind of motors to look for that would be able to turn my knob (or rather how to actuate the thing my knob connects to)...</p>

<p>Help!</p>

<p>Thanks :)</p>
","motor"
"10294","Lidar problems in a multi-robot setup","<p>Consider multiple mobile bases driving around in some area. In order to get meaningful data from the lidar of each base, the sensors should be mounted as horizontal as possible. Due to safety regulations, the lidars should also be mounted at a height of 15 cm from the floor. When I checked the data sheet of SICK lidars, it shows that all models use the wavelength 904 nm. Does that mean that mobile bases equipped with lidars with a coplanar scan lines will end up mutually blinding each other? </p>

<p>If it is the case, how is this problem solved? (I don't consider tilting the lidars a solution as it defeats the purpose of having ""2D"" lidars where even if the tilting angle is known, what the lidar observes becomes dependent on the robot's pose and distance from eventual obstacles)</p>
","sensors lidar rangefinder"
"10295","CompressedImage to an Image in a node","<h1>Update</h1>

<p>Hey I have the following subscriber on Nvidia TX1 board running on an agricultural robot. we have the following issue with subscribing to Sensor_msgs::Compressed:</p>

<pre><code>ImageConverter(ros::NodeHandle &amp;n) :  n_(n), it_(n_)
{
  image_pub_ = it_.advertise(""/output_img"",1);

  cv::namedWindow(OPENCV_WINDOW);
  image_transport::TransportHints TH(""compressed"");
  image_sub_compressed.subscribe(n,""/Logitech_webcam/image_raw/compressed"",5,&amp;ImageConverter::imageCallback,ros::VoidPtr(),TH);
}
</code></pre>

<p>And the callback function</p>

<pre><code>void imageCallback(const sensor_msgs::CompressedImageConstPtr&amp; msg)
</code></pre>

<p>When I compile this I get an error:</p>

<pre><code>from /home/johann/catkin_ws/src/uncompressimage/src/publisher_uncompressed_images.cpp:1:
/usr/include/boost/function/function_template.hpp: In instantiation of ‘static void boost::detail::function::function_void_mem_invoker1&lt;MemberPtr, R, T0&gt;::invoke(boost::detail::function::function_buffer&amp;, T0) [with MemberPtr = void (ImageConverter::*)(const boost::shared_ptr&lt;const sensor_msgs::CompressedImage_&lt;std::allocator&lt;void&gt; &gt; &gt;&amp;); R = void; T0 = const boost::shared_ptr&lt;const sensor_msgs::Image_&lt;std::allocator&lt;void&gt; &gt; &gt;&amp;]’:
/usr/include/boost/function/function_template.hpp:934:38:   required from ‘void boost::function1&lt;R, T1&gt;::assign_to(Functor) [with Functor = void (ImageConverter::*)(const boost::shared_ptr&lt;const sensor_msgs::CompressedImage_&lt;std::allocator&lt;void&gt; &gt; &gt;&amp;); R = void; T0 = const boost::shared_ptr&lt;const sensor_msgs::Image_&lt;std::allocator&lt;void&gt; &gt; &gt;&amp;]’
/usr/include/boost/function/function_template.hpp:722:7:   required from ‘boost::function1&lt;R, T1&gt;::function1(Functor, typename boost::enable_if_c&lt;boost::type_traits::ice_not&lt;boost::is_integral&lt;Functor&gt;::value&gt;::value, int&gt;::type) [with Functor = void (ImageConverter::*)(const boost::shared_ptr&lt;const sensor_msgs::CompressedImage_&lt;std::allocator&lt;void&gt; &gt; &gt;&amp;); R = void; T0 = const boost::shared_ptr&lt;const sensor_msgs::Image_&lt;std::allocator&lt;void&gt; &gt; &gt;&amp;; typename boost::enable_if_c&lt;boost::type_traits::ice_not&lt;boost::is_integral&lt;Functor&gt;::value&gt;::value, int&gt;::type = int]’
/usr/include/boost/function/function_template.hpp:1069:16:   required from ‘boost::function&lt;R(T0)&gt;::function(Functor, typename boost::enable_if_c&lt;boost::type_traits::ice_not&lt;boost::is_integral&lt;Functor&gt;::value&gt;::value, int&gt;::type) [with Functor = void (ImageConverter::*)(const boost::shared_ptr&lt;const sensor_msgs::CompressedImage_&lt;std::allocator&lt;void&gt; &gt; &gt;&amp;); R = void; T0 = const boost::shared_ptr&lt;const sensor_msgs::Image_&lt;std::allocator&lt;void&gt; &gt; &gt;&amp;; typename boost::enable_if_c&lt;boost::type_traits::ice_not&lt;boost::is_integral&lt;Functor&gt;::value&gt;::value, int&gt;::type = int]’
/home/johann/catkin_ws/src/uncompressimage/src/publisher_uncompressed_images.cpp:27:126:   required from here
</code></pre>

<p>The red error statement was:</p>

<pre><code>/usr/include/boost/function/function_template.hpp:225:11: error: no match for call to ‘(boost::_mfi::mf1&lt;void, ImageConverter, const boost::shared_ptr&lt;const sensor_msgs::CompressedImage_&lt;std::allocator&lt;void&gt; &gt; &gt;&amp;&gt;) (const boost::shared_ptr&lt;const sensor_msgs::Image_&lt;std::allocator&lt;void&gt; &gt; &gt;&amp;)’
           BOOST_FUNCTION_RETURN(boost::mem_fn(*f)(BOOST_FUNCTION_ARGS));
</code></pre>

<p>I am not using BOOST, and searching around hasn't helped me solve it</p>
","ros c++ opencv"
"10296","RVIZ Transform error Base_link and Camera_link","<p>I am working on a differential drive robot with two motor wheels with encoders and caster wheels. The robot also has a intel realsense depth camera.</p>

<p>When I launch RVIZ : Thee Global option > Fixed frame is set to Base_link and shows all the transforms for the differential driver nodes. But an error appears for the Depth camera nodes with message saying :</p>

<p>No transform from Camera_depth_frame to baselink</p>

<p>No transform from Camera_depth_optical_frame to baselink</p>

<p>No transform from Camera_link to baselink</p>

<p>No transform from Camera_rgb_frame to baselink</p>

<p>If I change the Global option > fixed frame to Camera_link I can see all the transforms for the depth camera but now the differential drive transforms are now not available</p>

<p>Hope you can help.</p>
","ros"
"10297","12 volt input to 5 volt ouput of Arduino","<p>I accidentally ended up supplying 12 v to the Arduino 5v output pin instead of the Vin pin. Does that mean that I can't use the 5v output pin anymore i.e. its fried?</p>
","arduino"
"10301","what books do you suggest for a beginner like me ?","<p>i am studying bacholar of dental surgery but have intrest in learning this subjet so tell me about a good book to read.</p>
","mobile-robot"
"10312","Difference between 3D Camera(using IR projection) and Stereo Camera?","<p>I am currently busy with a final year project which requires me to track people walking through a doorway.
I initially thought this may be possible using a normal camera and using some motion detection functions given in OpenCV, I have however come to the conclusion that the the camera is mounted too low for this to work effectively.(Height shown in the image below)</p>

<p><a href=""http://i.stack.imgur.com/CZbkH.jpg"" rel=""nofollow""><img src=""http://i.stack.imgur.com/CZbkH.jpg"" alt=""enter image description here""></a></p>

<p>I have now been looking into using a 3D camera or a stereo camera to try and get around this problem.</p>

<p>I have seen similar examples where a Kinect(from Xbox 360) has been used to generate a depth map which is then processed and used to do the tracking, this was however done from a higher vantage point, and I found that the minimum operating range of the Kinect is 0.5m.</p>

<p>From what I have found, the Kinect uses an IR projector and receiver to generate its depth map, and have been looking at the Orbbec Astra S which uses a similar system and has a minimum working distance of 0.3m.</p>

<p>My question now:</p>

<p>What exactly would the difference be between the depth maps produced by a 3D camera that uses an IR projector and receiver, and a stereo camera such as the DUO/ZED type options?</p>

<p>I am just looking for some insight from people that may have used these types of cameras before</p>

<p>On a side note, am i going about this the right way? or should i be looking into Time of Flight Cameras instead? </p>

<p>----EDIT----:</p>

<p>My goal is to count the people moving into and out of the train doorway. I began this using OpenCV, initially with a background subtraction and blob detection method. This only worked for one person at a time and with a test video filmed at a higher vantage point as a ""blob-merging"" problem was encountered as shown in the left image below.</p>

<p>So the next method tested involved an optical flow method using motion vectors obtained from OpenCV's dense optical flow algorithm.
From which i was able to obtain motion vectors from the higher test videos and track them as shown in the middle image below, because of the densely packed and easily detected motion vectors it was simple to cluster them.</p>

<p>But when this same system was attempted with footage taken from inside a train at a lower height, it was unable to give a consistant output. My thoughts of the reasons for this was because of the low height of the camera, single camera tracking is able to function when there is sufficient space between the camera and the top of the person. But as the distance is minimized, the area of the frame that the moving person takes up becomes larger and larger, and the space to which the person can be compared is reduced (or atleast that is how I understand it). Below on the right you can see how in the image the color of the persons clothing is almost uniform, Optical flow is therefore unable to detect it as motion in both cases.</p>

<p><a href=""http://i.stack.imgur.com/i8Rvu.jpg"" rel=""nofollow""><img src=""http://i.stack.imgur.com/i8Rvu.jpg"" alt=""enter image description here""></a></p>

<p>I only started working with computer vision a few months ago so please forgive me if I have missed some crucial aspects.</p>

<p>From what i have seen from research, most commercial systems make used of a 3D cameras, stereo cameras or Time-of-Flight cameras, but I am unsure as to how the specifics of each of these would be best suited for my application.</p>
","computer-vision cameras stereo-vision"
"10314","OpenRAVE output torques and simulation timestep","<p>I'm using OpenRAVE to simulate a quadruped, in order to get an idea of torque requirements. </p>

<p>To get started I made a single DOF, single link pendulum to test controllers etc out on.
I've whipped up an inverse dynamics based PD controller using ComputeInverseDynamics(), which I set the outputs using SetDOFTorques(). I then set a desired position, with the desired velocity being zero. This all appears to work well and I can start the simulation, with the pendulum driving up to the desired position and settling. 
My concern is the value of the output torques. My pendulum is modeled as a simple box of length 1, mass manually set to 1, with a COM of 0.5.
When I run my simulation, I output the gravity component from ComputeInverseDynamics(). This gives 4.9NM, which matches up with hand calculated torques I expect from the pendulum (eg the static case) when it is driven to the desired position (from down to horizontal).
But the output torques to SetDOFTorques() are much higher and vary depending what I set the simulation timestep to.
If I maintain a controller update rate of 0.001 seconds, then for a simulation update of 0.0001 seconds, my output torque is approximately 87NM. If I alter the simulation timestep to 0.0005 seconds, keeping the controller rate the same the output torques drop down to about 18NM.</p>

<p>As an experiment I removed the inverse dynamics controller and replaced it with a plain PD controller, but I still see large output torques.</p>

<p>Can anyone shed some light on this? It's very possible I'm missing something here!</p>

<p>Thanks very much</p>

<p>Edits:
I'm adding the main section of my code. There is no trajectory generation, really. I'm just trying to get to a fixed static position.
In the code, if I keep dt fixed, and alter env.StartSimulation(timestep=0.0001), I get the issues popping up.</p>

<pre><code>with env:
    robot = env.GetRobots()[0]
    robot.GetLinks()[0].SetStatic(True)
    env.StopSimulation()
    env.StartSimulation(timestep=0.0001)

dt = 0.001
w = 100
eta = 5
Kp = [w*w]
Kv = [2*eta*w]
# Desired pos, vel and acc 
cmd_p = [3.14/2]
cmd_v = [0]
cmd_a = [0]

while True:    
    with env:
        torqueconfiguration, torquecoriolis, torquegravity = robot.ComputeInverseDynamics([1],None,returncomponents=True)
        err_p = cmd_p - robot.GetDOFValues()
        err_v = cmd_v - robot.GetDOFVelocities()

        # ID Controller
        M = compute_inertia_matrix(robot, robot.GetDOFValues())
        a_cmd = (Kp*err_p + Kv*err_v + cmd_a)
        taus = torquegravity + torquecoriolis +  M.dot(a_cmd.transpose()).transpose()

        # Just PD(ish) controller
        #taus = Kp*err_p - Kv*robot.GetDOFVelocities()

        with robot:
            robot.SetDOFTorques(taus,False)     # True = use limits
            print (taus, torquegravity+torquecoriolis, a_cmd, M.dot(a_cmd.transpose()).transpose())
    time.sleep(dt)


# https://scaron.info/teaching/equations-of-motion.html
def compute_inertia_matrix(robot, q, external_torque=None):
    n = len(q)
    M = np.zeros((n, n))
    with robot:
        robot.SetDOFValues(q)
        for (i, e_i) in enumerate(np.eye(n)):
            m, c, g = robot.ComputeInverseDynamics(e_i, external_torque, returncomponents=True)
            M[:, i] = m
    return M



&lt;?xml version=""1.0"" encoding=""utf-8""?&gt;
&lt;Robot name=""Pendulum""&gt;
    &lt;RotationAxis&gt;0 1 0 90&lt;/RotationAxis&gt; &lt;!-- makes the pendulum vertical --&gt;
    &lt;KinBody&gt;
        &lt;!-- &lt;Mass type=""mimicgeom""&gt;&lt;density&gt;100000&lt;/density&gt;&lt;/Mass&gt; --&gt;
        &lt;Body name=""Base"" type=""dynamic""&gt;
            &lt;Translation&gt;0.0  0.0  0.0&lt;/Translation&gt;
            &lt;Geom type=""cylinder""&gt;
                &lt;rotationaxis&gt;1 0 0 90&lt;/rotationaxis&gt;
                &lt;radius&gt;0.3&lt;/radius&gt;
                &lt;height&gt;0.02&lt;/height&gt;
                &lt;ambientColor&gt;1. 0. 0.&lt;/ambientColor&gt;
                &lt;diffuseColor&gt;1. 0. 0.&lt;/diffuseColor&gt;
            &lt;/Geom&gt;
            &lt;mass type=""custom""&gt;
                &lt;!-- specify the total mass--&gt;
                &lt;total&gt;5.0&lt;/total&gt;
                  &lt;!-- specify the 3x3 inertia matrix--&gt;
                  &lt;!--&lt;inertia&gt;2 0 0 0 3 0 0 0 5&lt;/inertia&gt; --&gt;
                  &lt;!-- specify the center of mass (if using ODE physics engine, should be 0)--&gt;
                &lt;com&gt;0.1 0.0 0.0&lt;/com&gt;
                &lt;/mass&gt; 
        &lt;/Body&gt;
        &lt;Body name=""Arm0"" type=""dynamic""&gt;
            &lt;offsetfrom&gt;Base&lt;/offsetfrom&gt;
            &lt;!-- translation and rotation  will be relative to Base --&gt;
            &lt;Translation&gt;0 0 0&lt;/Translation&gt;
            &lt;Geom type=""box""&gt;
                &lt;Translation&gt;1 0 0&lt;/Translation&gt;
                &lt;Extents&gt;1 0.1 0.1&lt;/Extents&gt;
                &lt;ambientColor&gt;1. 0. 0.&lt;/ambientColor&gt;
                &lt;diffuseColor&gt;1. 0. 0.&lt;/diffuseColor&gt;
            &lt;/Geom&gt;
            &lt;mass type=""custom""&gt;
                &lt;!-- specify the total mass--&gt;
                &lt;total&gt;1.0&lt;/total&gt;
                  &lt;!-- specify the 3x3 inertia matrix--&gt;
                  &lt;!--&lt;inertia&gt;2 0 0 0 3 0 0 0 5&lt;/inertia&gt; --&gt;
                  &lt;!-- specify the center of mass (if using ODE physics engine, should be 0)--&gt;
                &lt;com&gt;0.5 0.0 0.0&lt;/com&gt;
                &lt;/mass&gt;             
        &lt;/Body&gt;
        &lt;Joint circular=""true"" name=""Joint0"" type=""hinge""&gt;
            &lt;Body&gt;Base&lt;/Body&gt;
            &lt;Body&gt;Arm0&lt;/Body&gt;
            &lt;offsetfrom&gt;Arm0&lt;/offsetfrom&gt;
            &lt;weight&gt;0&lt;/weight&gt;
            &lt;axis&gt;0 0 1&lt;/axis&gt;
            &lt;maxvel&gt;100&lt;/maxvel&gt;
            &lt;resolution&gt;1&lt;/resolution&gt;
        &lt;/Joint&gt;
    &lt;/KinBody&gt;
&lt;/Robot&gt;
</code></pre>

<p>Here is some data for dt = 0.001 and env.StartSimulation(timestep=0.0001)</p>

<p>In this data,</p>

<ul>
<li>taus is the torque command to the simulation, </li>
<li>torquegravity+torquecoriolis is returned from the inverse dynamics </li>
<li>a_cmd is the controller command and</li>
<li>M*a_cmd is the command after being multiplied by the mass matrix</li>
</ul>

<p>The gravity and coriolis parts appear to be correct for steady state, where it should be about 4.9NM</p>

<pre><code>taus, torquegravity+torquecoriolis, a_cmd, M*a_cmd
 3464.88331508,  0.48809828,  5329.83879509,  3464.39521681
 330.67177959,  1.47549936,  506.45581573,  329.19628023
-785.91806527,  2.45531014, -1212.88211601, -788.37337541
-1065.4689484,  3.23603844, -1644.16151823, -1068.70498685
-1027.47479809,  3.80261774, -1586.58063974, -1031.27741583
-877.83110127,  4.18635604, -1356.94993433, -882.01745731
-707.25108627,  4.4371714, -1094.9050118, -711.68825767
-554.34483533,  4.6006198, -859.91608481, -558.94545512
-432.22314217,  4.70818921, -672.20204828, -436.93133138
-327.797496,  4.7768792, -511.65288492, -332.5743752
-240.77203429,  4.82021019, -377.83422228, -245.59224448
-172.18942128,  4.84807059, -272.3653721, -177.03749186
-117.58895761,  4.86591166, -188.39210657, -122.45486927
-74.51920719,  4.87743369, -122.14867828, -79.39664088
-39.91183436,  4.88473444, -68.91779816, -44.7965688
-12.82321495,  4.88971433, -27.25066043, -17.71292928
 8.45349476,  4.89281357,  5.47797105,  3.56068118
 25.35468725,  4.89489884,  31.47659755,  20.45978841
 38.84080509,  4.896309,  52.22230167,  33.94449609
 48.72668147,  4.89724689,  67.42989936,  43.82943458
 56.78552877,  4.89790152,  79.82711885,  51.88762725
 65.515892,  4.89836756,  93.25772991,  60.61752444
 68.81359264,  4.89867903,  98.33063633,  63.91491362
 73.86961896,  4.89891052,  106.10878221,  68.97070844
 76.67416578,  4.89907489,  110.42321674,  71.77509088
 79.62549808,  4.89919702,  114.96354008,  74.72630105
 85.17343708,  4.89928669,  123.49869291,  80.27415039
 85.13686188,  4.89934963,  123.44232654,  80.23751225
 85.75675034,  4.89939931,  124.39592466,  80.85735103
 86.55192592,  4.89943807,  125.61921208,  81.65248785
 86.39672231,  4.89946802,  125.38039121,  81.49725429
 87.4299925,  4.89949202,  126.97000073,  82.53050048
 87.42776523,  4.8995098,  126.96654682,  82.52825543
 87.15472709,  4.8995251,  126.54646461,  82.255202
 86.97240783,  4.89953825,  126.26595319,  82.07286958
 86.98023044,  4.89954905,  126.27797137,  82.08068139
 86.75364661,  4.89955809,  125.92936696,  81.85408852
 86.9853716,  4.89956526,  126.28585591,  82.08580634
 88.01679721,  4.89957062,  127.8726563,  83.1172266
 89.2610231,  4.89957348,  129.78684557,  84.36144962
 88.47969399,  4.89957495,  128.58479851,  83.58011903
 88.77623594,  4.89957711,  129.04101359,  83.87665884
 90.87280518,  4.89957739,  132.2665043,  85.9732278
 88.9513552,  4.89957707,  129.3104279,  84.05177813
 89.14100099,  4.89957773,  129.60218964,  84.24142327
</code></pre>

<p>And here is some data for dt = 0.001 and env.StartSimulation(timestep=0.0005)</p>

<pre><code>taus, torquegravity+torquecoriolis, a_cmd, M*a_cmd
-313.62240349,  0.98927261, -484.01796324, -314.61167611
-242.03525463,  2.00886997, -375.45249938, -244.0441246
-199.82226305,  2.79259699, -311.71516928, -202.61486003
-190.02605484,  3.39367572, -297.56881625, -193.41973056
-162.08293067,  3.8525617, -255.28537288, -165.93549237
-125.84847045,  4.17559368, -200.03702174, -130.02406413
-103.89936813,  4.40068949, -166.61547326, -108.30005762
-82.32305905,  4.5566127, -133.66103347, -86.87967175
-64.56801352,  4.66415211, -106.51102404, -69.23216563
-49.68124446,  4.73812107, -83.72210081, -54.41936553
-37.91265825,  4.78890663, -65.6947152, -42.70156488
-27.99189838,  4.82374208, -50.48560071, -32.81564046
-19.81225948,  4.84762415, -37.9382825, -24.65988362
-12.55978349,  4.8636252, -26.80524414, -17.42340869
-6.89165107,  4.87470983, -18.10209369, -11.7663609
-3.13313345,  4.88256746, -12.33184754, -8.0157009
 0.69831646,  4.88796162, -6.44560793, -4.18964516
 3.86277859,  4.89166745, -1.58290594, -1.02888886
 6.12163439,  4.8941598,  1.88842245,  1.22747459
 8.58189707,  4.89593332,  5.67071346,  3.68596375
 9.1580546,  4.89712981,  6.55526891,  4.26092479
 11.81854706,  4.89798468,  10.64701905,  6.92056238
 12.40540565,  4.89856409,  11.54898701,  7.50684156
 14.04109075,  4.89897979,  14.06478609,  9.14211096
 14.39924399,  4.89926951,  14.61534535,  9.49997448
 14.98060951,  4.89947252,  15.50944153,  10.08113699
 16.08890875,  4.89961544,  17.2142974,  11.18929331
 16.01955973,  4.89971637,  17.10745133,  11.11984337
 17.06493791,  4.89978831,  18.71561478,  12.16514961
 17.35364328,  4.89983976,  19.15969772,  12.45380352
 17.62239334,  4.89987688,  19.57310225,  12.72251646
 17.84455913,  4.89990387,  19.91485424,  12.94465525
 17.43825648,  4.89992362,  19.28974286,  12.53833286
 17.58436934,  4.89993826,  19.51450935,  12.68443108
 17.70571012,  4.8999492,  19.70117065,  12.80576093
 18.40852272,  4.89995746,  20.78240808,  13.50856525
 18.49492461,  4.89996372,  20.91532445,  13.59496089
 18.56575802,  4.89996852,  21.02429154,  13.6657895
 18.62430693,  4.89997223,  21.11436108,  13.7243347
 16.54216482,  4.89997511,  17.91106109,  11.64218971
 18.71146936,  4.89997747,  21.24844907,  13.81149189
 18.13316504,  4.89997923,  20.35874741,  13.23318581
 18.77330006,  4.89998067,  21.34356829,  13.87331939
</code></pre>

<p>Despite the differences in torque command (a_cmd) I still get similar performance, in that the arm drives to the right position fairly quickly.
As another experiment I set the initial position to pi/2 and just fed back the gravity term to the torque output. My understanding of this is that the arm should float, ala a gravity compensation sort of thing. But it just drops as if a small torque is applied.
Thanks again! </p>
","robotic-arm torque"
"10322","is it possible to get all possible solutions of inverse kinematics of a 6 DOF arm?","<p>I would like to know if there is any way to get <strong>all the possible solutions</strong> <strong>of inverse kinematics of a 6 DOF robotic arm</strong>?
I have found some good Matlab codes but gives only one solution like in Peter corke's book .
Thank you in advance. </p>
","inverse-kinematics"
"10324","When was the first time a robot killed a human?","<p>Scott Adams, creator of Dilbert, recently shared an <a href=""http://scottadams-tttt.tumblr.com/post/147119485801/top-tech-258-killer-robots"" rel=""nofollow"">article</a> about a robot the police used to kill a suspect by detonating a bomb in close range.</p>

<p>This made me wonder -- when was the first time a robot took a human life?</p>

<p>Good comments were made on this which leads me to clarify that I mean a <em>pureposeful</em> taking of life. I shy away from the term ""murder"" because that involves legal concepts, but I mean an intentional killing.</p>

<p>An interesting subdivision would be between robots under active human direction (""remote control"") and those with a degree of autonomy.</p>

<p><a href=""http://i.stack.imgur.com/SDeV4.jpg"" rel=""nofollow""><img src=""http://i.stack.imgur.com/SDeV4.jpg"" alt=""killer robot""></a></p>
","mobile-robot"
"10325","Is horsepower related to torque in electric motors?","<p>Is torque related to size or power at all in electric motors? And what about gas motors too?</p>

<p>I have a go kart that is 2.5hp and it's 50cc and its about 1ft x 2ft x 1ft in size. I also see online there are .21 cubic inch gas motors for R/C cars that are also 2.5hp, the difference being that the R/C motor spins at 32k rpm while the go-kart motor spins at 12k rpm. If I were to put a gear reduction on the R/C motor, would it preform more or less the same as the go kart motor? Why is there a size difference?</p>

<p>Same for electric motors. I can buy an RC car electric motor that's 10hp and the size of a pop can. The CNC machine at work has a 10hp motor the size of a 5 gal bucket. Again, the only difference is the RPM.</p>

<p>If I were to reduce both setups so they spun at the same RPM, would they preform the same?</p>

<p>The only reasons I could think of is 1. Cooling and 2. RPM control (For PID loops and sensors)</p>
","motor power torque engine"
"10326","SLAM with iRobot Create 2","<p>I have an iRobot Create 2 and have been working with it and have gotten to the point where I can control it via Bluetooth. This is great but I also want it to be able to be autonomous and navigate itself room to room for example. Are there any SLAM iRobot tutorials or any other materials you'd recommend for autonomous navigation?</p>
","mobile-robot slam irobot-create"
"10330","Localising a robot placed at an unknown position in a known environment","<p>I am a third-year electrical engineering student and am working on an intelligent autonomous robot in my summer vacations.</p>

<p>The robot I am trying to make is supposed to be used in rescue operations. The information I would know is the position of the person (the coordinates of the person in a JSON file) to be rescued from a building on fire. I would also know the rooms of the building from a map, but I don't know where the robot may be placed inside the building to start the rescue operation.</p>

<p>That means I have to localise the robot placed at an unknown position in a known environment, and then the robot can plan its path to the person who has to be rescued. But, since this is not my domain I would like you to guide me on what is the best method for localising given that I can use an IMU ( or gyro, accelerometer, magnetometer) and ultrasonic sensors to do the localising job. I cannot use a GPS module or a camera for this purpose. </p>

<p>I, however, do know how to do path planning.</p>

<p>As far as my research on the Internet is concerned I have found a method called ""Kalman filtering"" that maybe can do the localising job. But there are I think some other filtering methods as well. Which one should I use? Or is there any other simpler/better method out there of which I don't know yet?</p>

<p>I am also attaching the map of the building which is known to me.</p>

<p><a href=""http://i.stack.imgur.com/QDoUf.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/QDoUf.png"" alt=""enter image description here""></a></p>

<p>Edit:</p>

<p>The terrain is flat, and I would like to know where the robot is on the map like at coordinate 0,4 etc.</p>
","mobile-robot localization imu accelerometer gyroscope"
"10334","Autonomous Navigation without Distance Sensors","<p>I'm doing a project with the iRobot Create 2. I want it to be able to map out a room and navigate to a point for example. My problem is that the robot doesn't have any distance sensors. What it can do is detect if there is an obstacle ahead of it or not (0 or 1) and it can measure how far it has traveled in millimeters. Any good techniques out there or best to buy an IR sensor?</p>
","mobile-robot irobot-create"
"10335","Typical laser scanner noise values","<p>I am building an application that executes graphSLAM using datasets recorded in a simulated environment. The dataset has been produced in MRPT using the <a href=""http://www.mrpt.org/list-of-mrpt-apps/application_gridmapnavsimul/"" rel=""nofollow"">GridMapNavSimul</a> application. To simulate the laserScans one can issue the bearing and range error standard deviation of the range finder. </p>

<p>Currently I am using a dataset recorded with range_noise = 0.30m, bearing_noise = 0.15deg. Am I exaggerating with these values? Could somebody provide me with typical values for these quantities? Do laser scanner manufacturers provide these values?</p>

<p>Thanks in advance,</p>
","slam laser rangefinder"
"10348","quaternion implementation","<p>I am trying to implement quaternions and i am using CC2650 sensortag board from TI. This board has MPU9250 from invensense which has Digital Motion Processor (DMP) in it. This DMP gives quaternion, but for my understanding i implemented my own quaternion. I used Gyroscope and acceleorometer values coming out of DMP (which are calibrated ) to calculate angle of rotation. I feed this angle, in 3 directions (x,y,z), to my quaternion. I am not able to match my quaternion values with DMP quaternion values. In fact it's way off, so wondering what I have done wrong.</p>

<p>Following are detailed steps that i did :</p>

<p>1)  Tapped Gyro sensor values from function “read_from_mpl”.</p>

<p>2)  Converted gyro values in to float by diving by 2^16. As gyro values are in Q16 format.</p>

<p>3)  Now used Gyro values of 3 axis and found out resultant using formula : 
<em>Gr = sqrt(Gx^2+Gy^2+Gz^2)</em>
     <em>Where Gx,Gy and Gz are Gyro values along x-axis,y-axis and z-axis respectively.</em></p>

<p>4)  Now Angle is derived using above found resultant Gr by : 
*Angle = Gr*1/sample_rate*
       <em>Where sample_rate is found using API call ,mpu_get_sample_rate(&amp;sample_rate)</em></p>

<p>5)  This Angle is fed to angle_to_quater function which basically converts angle to axis and then quaternion multiplication.</p>

<pre><code>/* Angle to axis and quaternion multiplication: */
temp.w = cos((Angle*1.0/RAD_TO_DEG)/2);
temp.x = sin((Angle*1.0/RAD_TO_DEG)/2);
temp.y = sin((Angle*1.0/RAD_TO_DEG)/2);
temp.z = sin((Angle*1.0/RAD_TO_DEG)/2);
temp.x = temp.x *gyro_axis[0];//gyro_axis[0]=Gx
temp.y = temp.x *gyro_axis[1]; //gyro_axis[0]=Gy
temp.z = temp.x *gyro_axis[2]; //gyro_axis[0]=Gz
/* quaternion multiplication and normalization */
res = quat_mul(*qt,temp);
quat_normalize(&amp;res);
*qt = res;*   
</code></pre>

<p>6)  I also added  doing angle calculations from accelerometer as follows : Here also accelerometer is converted to float by dividing by 2^16, as acceleorometer values also in Q16 format.</p>

<pre><code>*//acc_data[0]-&gt;Ax, acc_data[1]-&gt;Ay, acc_data[2]-&gt;Az
temp = (acc_data[0]*acc_data[0]) + (acc_data[1]*acc_data[1]);
acc_angle[0]=atan2(acc_data[2],temp)*RAD_TO_DEG;
temp = (acc_data[1]*acc_data[1]) + (acc_data[2]*acc_data[2]);
acc_angle[1]=atan2(acc_data[0],temp)*RAD_TO_DEG;
temp = (acc_data[1]*acc_data[1]) + (acc_data[0]*acc_data[0]);
acc_angle[2]=atan2(acc_data[1],temp)*RAD_TO_DEG;*
</code></pre>

<p>*Find resultant angle of this also as :</p>

<pre><code>inst_acc_angle = (sqrt(acc_angle[0]*acc_angle[0] + acc_angle[1]*acc_angle[1] + acc_angle[2]*acc_angle[2]));*
</code></pre>

<p>7)  Then complimentary filter is :
*FinalAngle = 0.96*Angle + 0.04*inst_acc_angle;
This Final Angle is fed to step 5 to get quaternion.*</p>

<p>Quaternion multiplication is done as below and then normailized to get new quaternion (q). </p>

<p>quater_mul :</p>

<pre><code>q3.w = -q1.x * q2.x - q1.y * q2.y - q1.z * q2.z + q1.w * q2.w;
q3.x =  q1.x * q2.w + q1.y * q2.z - q1.z * q2.y + q1.w * q2.x;
q3.y = -q1.x * q2.z + q1.y * q2.w + q1.z * q2.x + q1.w * q2.y;
q3.z =  q1.x * q2.y - q1.y * q2.x + q1.z * q2.w + q1.w * q2.z;
</code></pre>

<p>quat_normalize:</p>

<pre><code>double mag = pow(q-&gt;w,2) + pow(q-&gt;x,2) + pow(q-&gt;y,2) + pow(q-&gt;z,2);
mag = sqrt(mag);
q-&gt;w = q-&gt;w/mag;
q-&gt;x = q-&gt;x/mag;
q-&gt;y = q-&gt;y/mag;
q-&gt;z = q-&gt;z/mag;
</code></pre>

<p>When i check my quaternion values with DMP, they are WAY off. Can you please provide some insights in to what could be wrong here. </p>

<p><strong>Source code :</strong></p>

<pre><code>acc_data[0]=data[0]/65536.0;
acc_data[1]=data[1]/65536.0;
acc_data[2]=data[2]/65536.0;
double temp = (acc_data[0]*acc_data[0]) + (acc_data[1]*acc_data[1]);
acc_angle[0]=atan2(acc_data[2],temp)*RAD_TO_DEG;
temp = (acc_data[1]*acc_data[1]) + (acc_data[2]*acc_data[2]);
acc_angle[1]=atan2(acc_data[0],temp)*RAD_TO_DEG;
temp = (acc_data[1]*acc_data[1]) + (acc_data[0]*acc_data[0]);
acc_angle[2]=atan2(acc_data[1],temp)*RAD_TO_DEG;*

gyro_rate_data[0]=data[0]/65536.0;
gyro_rate_data[1]=data[1]/65536.0;
gyro_rate_data[2]=data[2]/65536.0;

float inst_angle = (sqrt(gyro_rate_data[0]*gyro_rate_data[0] +  gyro_rate_data[1]*gyro_rate_data[1] + gyro_rate_data[2]*gyro_rate_data[2]));
gyro_rate_data[0] = gyro_rate_data[0]/inst_angle;
gyro_rate_data[1] = gyro_rate_data[1]/inst_angle;
gyro_rate_data[2] = gyro_rate_data[2]/inst_angle;
inst_angle = inst_angle *1.0/sam_rate;
float inst_acc_angle = (sqrt(acc_angle[0]*acc_angle[0] + acc_angle[1]*acc_angle[1] + acc_angle[2]*acc_angle[2]));
inst_angle = WT*inst_angle + (1.0-WT)*inst_acc_angle;

angle_to_quat(inst_angle,gyro_rate_data,&amp;q);

/* The function for angle to quaterinion and multiplication,normalization */
void angle_to_quat(float Angle,float *gyro_axis,struct quat *qt)
{
    struct quat temp;
    struct quat res;
    temp.w = cos((Angle*1.0/RAD_TO_DEG)/2);
    temp.x = sin((Angle*1.0/RAD_TO_DEG)/2);
    temp.y = sin((Angle*1.0/RAD_TO_DEG)/2);
    temp.z = sin((Angle*1.0/RAD_TO_DEG)/2);
    temp.x = temp.x *gyro_axis[0];
    temp.y = temp.x *gyro_axis[1];
    temp.z = temp.x *gyro_axis[2];
    res = quat_mul(*qt,temp);
    quat_normalize(&amp;res);
    *qt = res;
}
</code></pre>

<p><a href=""http://i.stack.imgur.com/w98QK.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/w98QK.png"" alt=""enter image description here""></a></p>

<p>This variation is coming when i am keeping my device stationary.</p>

<p>Y-Axis : Resultant of all 3 gyro axis.</p>

<p>X-axis : The number of samples. (have not converted them to time)</p>

<p>Sample_rate is 3Hz.</p>
","sensor-fusion"
"10350","Q Learning And Kohonen Maps For Line Follower Robot","<p>I'm trying to build a line follower robot and I'm interested in predicting the curves on the track.
I have 8 binary sensor array(qre1113).
My goal is to make a system that it can generalize what it learned about the curves and give me predictions about where should be at the line to pass it as fast as possible.</p>

<p>How can I integrate a system like Q learning and how can I train it?
And also how can I combine this system with a Type C PID controlller ?</p>

<p><a href=""https://github.com/michalnand/motoko_after_math_linefollower/blob/master/doc/ossconf_2015_paper/paper.pdf"" rel=""nofollow"">There is a paper about it ot you are willing to explain</a></p>

<p>This is a important project for me and i am kinda running on clock so quick help would be appreciated</p>
","differential-drive"
"10357","PID Gains: Drop in control loop rate, need to retune?","<p>Good Day,</p>

<p>I am working on an autonomous quadcopter. May I ask if there is a significant difference if my control loop dropped from 500Hz to 460Hz due to added lines of code that would require retuning of the PID gains? And if retuning is required, is it correct to assume that only the I and D gains should be retweaked since they are the only constants which are time dependent? Thank you :)</p>
","quadcopter mobile-robot control pid stability"
"10367","Battery damaged?","<p>Could you please see the attached battery images and tell me if it is safe to continue using this battery or should I discard it?</p>

<p><a href=""http://i.stack.imgur.com/1VaJe.jpg"" rel=""nofollow""><img src=""http://i.stack.imgur.com/1VaJe.jpg"" alt=""enter image description here""></a></p>
","battery lithium-polymer"
"10369","What are the specifications of the digital compass used in iPhone 6S","<p>What are the specifications of the digital compass used in the iPhone 6S?
I am trying to measure yaw angle using the magnetometer.  I observed the magnetometer/digital compass in the iPhone is really very stable. The north direction is always the same, while the magnetometer I am using (or the magnetometer used in Nexus) needs to be calibrated again and again to function properly.</p>

<p>I found that the digital compass <strong>AK8963C</strong> is used in the iPhone 6, but it needs calibration.  So I am not sure what is inside iPhone 6S because it works without a calibration procedure.</p>
","imu sensor-fusion magnetometer"
"10371","Understanding and correct drift when using BreezySLAM (aka tinySLAM / CoreSLAM)","<p>I was looking for a Python implementation of SLAM and stumbled upon <a href=""https://github.com/simondlevy/BreezySLAM"" rel=""nofollow"">BreezySLAM</a> which implements <a href=""https://openslam.org/tinyslam.html"" rel=""nofollow"">tinySLAM</a> aka CoreSLAM. </p>

<p>My robot is equipped with the <a href=""https://www.hokuyo-aut.jp/02sensor/07scanner/urg_04lx_ug01.html"" rel=""nofollow"">hokuyo urg-04lx-ug01</a>. </p>

<p>I have odometry hence passing it to the updater: </p>

<pre><code>self.slam.update(ls_array, (dxy_mm, dtheta_deg, dt));
</code></pre>

<p>As I <strong>start</strong> moving the robot starts discovering <strong>room A</strong> and then <strong>room B &amp; C</strong> already the map seems to have rotated. I come back to room A and return the initial pose <strong>end=start</strong> using the same path. Now I noticed <strong>room A</strong> has significantly rotated in relation to the other room. Consequently the map isn't correct at all, neither is the path travelled by the robot. </p>

<ol>
<li>Wasn't the SLAM supposed to store and keep the boundaries for the first room it discovered?</li>
<li>Why this rotation may be happening?</li>
<li>How could I try to troubleshoot this issue with the data I have collected (odometry, calculated position, liDAR scans)?</li>
<li>Can I tune SLAM to do a better job for my robot?</li>
</ol>

<p>SLAM is pretty new to me, so please bear with me, any pointers on literature that may clarify and moderate my expectations of what SLAM can do.</p>

<p><a href=""http://i.stack.imgur.com/vsidl.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/vsidl.png"" alt=""enter image description here""></a></p>

<h1>Extra</h1>

<p>... and <a href=""https://www.youtube.com/watch?v=aUkBa1zMKv4"" rel=""nofollow"">Here the best video I found to understand particle filter</a></p>
","localization slam mapping"
"10377","Need help regarding odometry using Encoder motor and raspberry pi","<p>I am doing project on odometry using raspberry pi. I know that encoder motor will tell me how much distance my robot has covered, but I have no idea ho to implement completely. I just need guideline about which steps to follow. Till now I have interfaced motor with raspberry pi and counted the number of rotation. I have questions as follow?</p>

<p>How to plot map of odometry using which language and library?</p>

<p>If you know anything, just give me guideline about steps to follow.</p>
","motor raspberry-pi odometry"
"10378","Robot positioning using IMU quaternion data?","<p>I want to use a MPU9150 to give me the position (XY) and heading (angle) of a wheeled robot. This MPU9150 from invensense has a Digital Motion Processor in it which can give me a quaternion. 
But how do I convert this quaternion data to an XY-coordinate and an angle so I can plot the position of my vehicle?</p>
","wheeled-robot imu sensor-fusion"
"10379","battery question for DW558 Explorer (small quadrocopter)","<p>I recently bought a DW558 Quadrocopter (<a href=""http://www.gearbest.com/rc-quadcopters/pp_110531.html"" rel=""nofollow"">http://www.gearbest.com/rc-quadcopters/pp_110531.html</a>).</p>

<p>After few minutes, the battery is dead. Which is understandable since the battery is so tiny. It is a 3.7V 250mAh battery, included with the Quadrocopter. I was thinking about buying spare batteries for it, and I have few questions about this:</p>

<ul>
<li>1: Can I buy any kind of battery 3.7V 250mAh of the same size or is there any other property I have to pay attention?</li>
<li>2: Can I buy batteries of 3.7V and 350mAh (100 more than the included battery) and expect my Quadrocopter to be more ""energic""? Is it bad to buy batteries with more mAh ?</li>
<li>2b: If I buy few 3.7V 350mAh batteries, will I be able to charge them with the same charger I got with my 3.7V 250 mAh batteries or do I have to buy a specific charger for these too?</li>
</ul>

<p>(these are the batteries I want to buy, any comment is greatly appreciated: 350mAh batteries x5 <a href=""http://www.gearbest.com/rc-quadcopter-parts/pp_196991.html"" rel=""nofollow"">http://www.gearbest.com/rc-quadcopter-parts/pp_196991.html</a> and/or 4x 250mAh batteries + charger <a href=""http://www.gearbest.com/rc-quadcopter-parts/pp_331372.html"" rel=""nofollow"">http://www.gearbest.com/rc-quadcopter-parts/pp_331372.html</a>)</p>

<p>Thank you very much for your input. I think I just discovered my new hobby and I can't wait to have my spare batteries!</p>
","quadcopter battery"
"10382","Need help regarding development of Extended Kalman Filter for sensor-data fusion of odometry and IMU data","<p>I'm trying to develop an Extended Kalman Filter (EKF) for the positioning of a wheeled vehicle. I have a '<a href=""http://www.dfrobot.com/index.php?route=product/product&amp;product_id=261#.V53MFvmLSUk"" rel=""nofollow"">Baron</a>' robot frame with 4 static wheels, all driven by a motor. On the 2 rear wheels I have an encoder. I want to fuse this odometry data with data from an '<a href=""https://www.sparkfun.com/products/retired/11486"" rel=""nofollow"">MPU9150</a>' 9 DOF IMU. </p>

<p>This is my mathlab code for the what I call <strong>'medium-size' EKF</strong>. This uses data from encoders, accelerometer in x and y axis and gyroscope z-axis.</p>

<h1>Medium-size EKF</h1>

<blockquote>
  <p><strong>Inputs:</strong>   x: ""a priori"" state estimate vector (8x1)<br/>
           t: sampling time [s]<br/>
           P: ""a priori"" estimated state covariance vector (8x8)<br/>
           z: current measurement vector (5x1) (encoder left; encoder right; x-acceleration, y-acceleration, z-axis gyroscope)<br/>
  <strong>Output:</strong>   x: ""a posteriori"" state estimate vector (8x1)<br/>
           P: ""a posteriori"" state covariance vector (8x8)<br/></p>
  
  <p><strong>State vector x:</strong> a 8x1 vector $\begin{bmatrix} x \rightarrow X-Position In Global Frame \\ \dot x \rightarrow Speed In X-direction Global Frame \\ \ddot x \rightarrow Acceleration In X-direction Global Frame \\ y \rightarrow Y-Position In Global Frame \\ \dot y \rightarrow Speed In Y-direction Global Frame \\ \ddot y \rightarrow Acceleration In Y-direction Global Frame \\ \theta \rightarrow Vehicle Angle In Global Frame \\ \dot \theta \rightarrow Angular Speed Of The Vehicle \end {bmatrix}$</p>
  
  <p><strong>Measurement vector z:</strong> <br/>
  a 5x1 vector $\begin{bmatrix} \eta_{left} \rightarrow Wheelspeed Pulses On Left Wheel \\ \eta_{right} \rightarrow Wheelspeed Pulses On Right Wheel \\ \dot \theta_z \rightarrow GyroscopeMeasurementInZ-axisVehicleFrame \\ a_x \rightarrow AccelerometerMeasurementX-axisVehicleFrame \\ a_y \rightarrow AccelerometerMeasurementY-axisVehicleFrame \end {bmatrix}$</p>
</blockquote>

<pre><code>function [x,P] = moodieEKFmedium(x,t,P,z,sigma_ax,sigma_ay,sigma_atau,sigma_odo,sigma_acc,sigma_gyro)

% Check if input matrixes are of correct size
[rows columns] = size(x);
if (rows ~= 8 &amp;&amp; columns ~= 1)
    error('Input vector size incorrect')
end
[rows columns] = size(z);
if (rows ~= 5 &amp;&amp; columns ~= 1)
    error('Input data vector size incorrect')
end

% Constants
n0 = 16;
r = 30;
b = 50;

Q = zeros(8,6);
Q(3,3) = sigma_ax;
Q(6,6) = sigma_ay;
Q(8,8) = sigma_atau;
%[Q(1,8),Q(3,6),Q(6,3)] = deal(small);

dfdx = eye(8);
[dfdx(1,2),dfdx(2,3),dfdx(4,5),dfdx(5,6),dfdx(7,8)] = deal(t);
[dfdx(1,3),dfdx(4,6)] = deal((t^2)/2);

dfda = zeros(6,6);
[dfda(3,3),dfda(6,6),dfda(8,8)] = deal(1);

dhdn = eye(5,5);

R = zeros(5,5);
[R(1,1),R(2,2)] = deal(sigma_odo);
R(3,3) = sigma_gyro;
[R(4,4),R(5,5)] = deal(sigma_acc);
%[R(2,1),R(1,2)] = deal(small);


% Predict next state
% xk = f(xk-1)
xtemp = zeros(8,1);
xtemp(1) = x(1) + t*x(2)+((t^2)/2)*x(3);
xtemp(2) = x(2) + t*x(3);
u1 = normrnd(0,sigma_ax);
xtemp(3) = x(3) + u1;

xtemp(4) = x(4) + t*x(5)+((t^2)/2)*x(6);
xtemp(5) = x(5) + t*x(6);
u2 = normrnd(0,sigma_ay);
xtemp(6) = x(6) + u2;

xtemp(7) = x(7) + t*x(8);
u3 = normrnd(0,sigma_atau);
xtemp(8) = x(8) + u3;

x = xtemp

% Predict next state covariance
% Pk = dfdx * Pk-1 * transpose(dfdx) + dfda * Q * transpose(dfda)
P = dfdx * P * transpose(dfdx) + dfda * Q * transpose(dfda);

% Calculate Kalman gain
% Kk = P * transpose(dhdx) [dhdx * P + dhdn * R * transpose(dhdn)]^-1
dhdx = zeros(5,8);
if(x(2) == 0 &amp;&amp; x(5) == 0)
    [dhdx(1,2),dhdx(2,2)] = deal(0);
    [dhdx(1,4),dhdx(2,4)] = deal(0);
else
    [dhdx(1,2),dhdx(2,2)] = deal(((t*n0)/(2*pi*r))*(x(2)/sqrt(x(2)^2+x(5)^2)));
    [dhdx(1,4),dhdx(2,4)] = deal(((t*n0)/(2*pi*r))*(x(5)/sqrt(x(2)^2+x(5)^2)));
end
%[dhdx(1,2),dhdx(2,2)] = deal(((t*n0)/(2*pi*r))*(x(2)/sqrt(x(2)^2+x(5)^2)));
%[dhdx(1,4),dhdx(2,4)] = deal(((t*n0)/(2*pi*r))*(x(5)/sqrt(x(2)^2+x(5)^2)));
dhdx(1,6) = (t*n0*b)/(2*pi*r);
dhdx(2,6) = -(t*n0*b)/(2*pi*r);

dhdx(4,3) = sin(x(7));
dhdx(4,6) = -cos(x(7));
dhdx(4,7) = (x(3)*cos(x(7)))+(x(6)*sin(x(7)));

dhdx(5,3) = cos(x(7));
dhdx(5,6) = sin(x(7));
dhdx(5,7) = (-x(3)*sin(x(7)))+(x(6)*cos(x(7)));

Kk = P * transpose(dhdx) * (dhdx * P * transpose(dhdx) + dhdn * R * transpose(dhdn))^(-1)

% Update state
H = zeros(5,1);
n1 = normrnd(0,sigma_odo);
H(1) = (((t*n0)/(2*pi*r))*sqrt(x(2)^2+x(4)^2))+(((t*n0*b)/(2*pi*r))*x(6)) + n1;
n2 = normrnd(0,sigma_odo);
H(2) = (((t*n0)/(2*pi*r))*sqrt(x(2)^2+x(4)^2))-(((t*n0*b)/(2*pi*r))*x(6)) + n2;
n3 = normrnd(0,sigma_gyro);
H(3)= x(8) + n3;
n4 = normrnd(0,sigma_acc);
H(4)=(x(3)*sin(x(7))-(x(6)*cos(x(7))))+n4;
n5 = normrnd(0,sigma_acc);
H(5)=(x(3)*cos(x(7))+(x(6)*sin(x(7))))+n5;

x = x + Kk*(z-H)

% Update state covariance
P = (eye(8)-Kk*dhdx)*P;
</code></pre>

<p>end</p>

<p>This is the filter in schematic :
<a href=""http://i.stack.imgur.com/hwxer.jpg"" rel=""nofollow""><img src=""http://i.stack.imgur.com/hwxer.jpg"" alt=""enter image description here""></a></p>

<p>These are the state transition equations I use : 
$$\ x_{t+1} = x_{t} + T \cdot \dot x_{t} + \frac{T^{2}}{2} \cdot \ddot x_{t}$$
$$\ \dot x_{t+1} = \dot x_{t} + T \cdot \ddot x_{t} $$
$$\ \ddot x_{t+1} = \ddot x_{t} + u_{1} $$
$$\ y_{t+1} = y_{t} + T \cdot \dot y_{t} + \frac{T^{2}}{2} \cdot \ddot y_{t}$$
$$\ \dot y_{t+1} = \dot y_{t} + T \cdot \ddot y_{t} $$
$$\ \ddot y_{t+1} = \ddot y_{t} + u_{2} $$
$$\ \dot \theta_{t+1} = \dot \theta_{t} + T \cdot \ddot \theta_{t} $$
$$\ \ddot \theta_{t+1} = \ddot \theta_{t} + u_{3} $$</p>

<p>These are the observation equations I use :</p>

<p>$$\ \eta_{left} = \frac{T \cdot n_{0}}{2 \cdot \pi \cdot r} \cdot \sqrt{\dot x^{2} + \dot y^{2}} + \frac{T \cdot n_{0} \cdot b}{2 \cdot \pi \cdot r} \cdot \dot \theta + n_{1}$$
$$\ \eta_{right} = \frac{T \cdot n_{0}}{2 \cdot \pi \cdot r} \cdot \sqrt{\dot x^{2} + \dot y^{2}} - \frac{T \cdot n_{0} \cdot b}{2 \cdot \pi \cdot r} \cdot \dot \theta + n_{2}$$
$$\ \dot \theta_{z} = \dot \theta + n_{3}$$
$$\ a_{x} = \ddot x \sin \theta - \ddot y \cos \theta + n_{4}$$
$$\ a_{y} = \ddot x \cos \theta + \ddot y \sin \theta + n_{5}$$</p>

<h1>Small-size EKF</h1>

<p>I wanted to test my filter, therefore I started with a smaller one, in which I only give the odometry measurements as input. This because I know that if I always receive the same amount of pulses on the left and right encoder, than my vehicle should be driving a straight line. </p>

<blockquote>
  <p><strong>Inputs:</strong>   x: ""a priori"" state estimate vector (6x1)<br/>
           t: sampling time [s]<br/>
           P: ""a priori"" estimated state covariance vector (6x6)<br/>
           z: current measurement vector (2x1) (encoder left; encoder right)<br/>
  <strong>Output:</strong>   x: ""a posteriori"" state estimate vector (6x1)<br/>
           P: ""a posteriori"" state covariance vector (6x6)<br/></p>
  
  <p><strong>State vector x:</strong> a 6x1 vector $\begin{bmatrix} x \rightarrow X-Position In Global Frame \\ \dot x \rightarrow Speed In X-direction Global Frame  \\ y \rightarrow Y-Position In Global Frame \\ \dot y \rightarrow Speed In Y-direction Global Frame \\  \theta \rightarrow Vehicle Angle In Global Frame \\ \dot \theta \rightarrow Angular Speed Of The Vehicle \end {bmatrix}$</p>
  
  <p><strong>Measurement vector z:</strong>
  a 2x1 vector $\begin{bmatrix} \eta_{left} \rightarrow Wheelspeed Pulses On Left Wheel \\ \eta_{right} \rightarrow Wheelspeed Pulses On Right Wheel \end {bmatrix}$</p>
</blockquote>

<pre><code>% Check if input matrixes are of correct size
[rows columns] = size(x);
if (rows ~= 6 &amp;&amp; columns ~= 1)
    error('Input vector size incorrect')
end
[rows columns] = size(z);
if (rows ~= 2 &amp;&amp; columns ~= 1)
    error('Input data vector size incorrect')
end

% Constants
n0 = 16;
r = 30;
b = 50;

Q = zeros(6,6);
Q(2,2) = sigma_ax;
Q(4,4) = sigma_ay;
Q(6,6) = sigma_atau;
%[Q(1,8),Q(3,6),Q(6,3)] = deal(small);

dfdx = eye(6);
[dfdx(1,2),dfdx(3,4),dfdx(5,6)] = deal(t);

dfda = zeros(6,6);
[dfda(2,2),dfda(4,4),dfda(6,6)] = deal(1);

dhdn = eye(2,2);

R = zeros(2,2);
[R(1,1),R(2,2)] = deal(sigma_odo);
%[R(2,1),R(1,2)] = deal(small);


% Predict next state
% xk = f(xk-1)
xtemp = zeros(6,1);
xtemp(1) = x(1) + t*x(2);
u1 = normrnd(0,sigma_ax);
xtemp(2) = x(2) + u1;
xtemp(3) = x(3) + t*x(4);
u2 = normrnd(0,sigma_ay);
xtemp(4) = x(4) + u2;
xtemp(5) = x(5) + t*x(6);
u3 = normrnd(0,sigma_atau);
xtemp(6) = x(6) + u3;

x = xtemp

% Predict next state covariance
% Pk = dfdx * Pk-1 * transpose(dfdx) + dfda * Q * transpose(dfda)
P = dfdx * P * transpose(dfdx) + dfda * Q * transpose(dfda);

% Calculate Kalman gain
% Kk = P * transpose(dhdx) [dhdx * P * transpose(dhdx) + dhdn * R * transpose(dhdn)]^-1
dhdx = zeros(2,6);
if((x(2) &lt; 10^(-6)) &amp;&amp; (x(4)&lt; 10^(-6)))
    [dhdx(1,2),dhdx(2,2)] = deal((t*n0)/(2*pi*r));
    [dhdx(1,4),dhdx(2,4)] = deal((t*n0)/(2*pi*r));
else
    [dhdx(1,2),dhdx(2,2)] = deal(((t*n0)/(2*pi*r))*(x(2)/sqrt(x(2)^2+x(4)^2)));
    [dhdx(1,4),dhdx(2,4)] = deal(((t*n0)/(2*pi*r))*(x(4)/sqrt(x(2)^2+x(4)^2)));
end
%[dhdx(1,2),dhdx(2,2)] = deal(((t*n0)/(2*pi*r))*(x(2)/sqrt(x(2)^2+x(4)^2)));
%[dhdx(1,4),dhdx(2,4)] = deal(((t*n0)/(2*pi*r))*(x(4)/sqrt(x(2)^2+x(4)^2)));
dhdx(1,6) = (t*n0*b)/(2*pi*r);
dhdx(2,6) = -(t*n0*b)/(2*pi*r);

Kk = P * transpose(dhdx) * ((dhdx * P * transpose(dhdx) + dhdn * R * transpose(dhdn))^(-1))

% Update state
H = zeros(2,1);
n1 = normrnd(0,sigma_odo);
H(1) = (((t*n0)/(2*pi*r))*sqrt(x(2)^2+x(4)^2))+(((t*n0*b)/(2*pi*r))*x(6)) + n1;
n2 = normrnd(0,sigma_odo);
H(2) = (((t*n0)/(2*pi*r))*sqrt(x(2)^2+x(4)^2))-(((t*n0*b)/(2*pi*r))*x(6)) + n2;

x = x + Kk*(z-H)

% Update state covariance
P = (eye(6)-Kk*dhdx)*P;
</code></pre>

<p>end</p>

<h2>Odometry observation equations</h2>

<p>If you would wonder how I come to the observation equations for the odometry data: 
<a href=""http://i.stack.imgur.com/owQ2F.jpg"" rel=""nofollow""><img src=""http://i.stack.imgur.com/owQ2F.jpg"" alt=""odometry observation equations""></a>
$\ V_{vl} = V{c} + \dot \theta \cdot b \rightarrow V_{vl} = \sqrt{ \dot x^{2} + \dot y^{2}} + \dot \theta \cdot b$</p>

<h1>Problem</h1>

<p>If I try the small-size EKF, using a Matlab user interface, it does seem to drive a straight line, but not under a heading of 0° like I would expect. Eventhough I start with a state vector of $\ x= \begin{bmatrix}0\\0\\0\\0\\0\\0\end{bmatrix}$ meaning starting at position [0,0] in the global coordinate frame, with speed and acceleration of zero and under an angle of 0°.</p>

<p><a href=""http://i.stack.imgur.com/SkMiq.jpg"" rel=""nofollow""><img src=""http://i.stack.imgur.com/SkMiq.jpg"" alt=""enter image description here""></a>
In the top right corner you can see the measurement data which I give as input, which is 5 wheelspeed counts on every wheel, every sampling period. (Simulating straight driving vehicle)
In the top left corner you see a plot of the X and Y coordinate (from state vector) at the end of one predict+update cycle of the filter, labeled with the timecycle.
Bottom left corner is a plot of the angle in the state vector. You see that after 12 cycles the angle is still almost 0° like I would expect.</p>

<p><strong>Could anyone please provide some insights in to what could be wrong here?</strong> </p>

<h1>Solutions I've been thinking on</h1>

<ol>
<li><p>I could use the 'odometry motion model' like explained in <a href=""http://robotics.stackexchange.com/questions/964/extended-kalman-filter-using-odometry-motion-model"">this question</a>. The difference is that the odometry data is inserted in the predict step of the filter. But if I would do this, I see 2 problems: 1) I don't see how to make a small-size version of this for testing purposes, because I don't know which measurements to add in the update-step and 2) for the medium-size version I don't know how to make the observation equations as the state vector doesn't imply velocity and acceleration.</p></li>
<li><p>I could use the 'odometry motion model' and in the update step use the Euler-angle, which can be linked to $\ \theta $. This Euler-angle I can obtain from the Digital Motion Processor (DMP), implemented in the IMU. Then it is no problem that angular velocity is not in the state matrix. But than I still have a problem with the acceleration observation equations. </p></li>
</ol>
","kalman-filter imu sensor-fusion odometry"
"10383","Issue with DriveDistance and RotateDegree","<p>Currently, I'm using Microsoft Robotics Dev Studio, and the Visual Studio C# programming language to write some code that is able to drive the iRobot Create 2 on a particular path. Moreover, when I run the code in simulation, it works fine, but if I connect to the actual iRobot Create 2, the code only executes the driveDistance part, and then stops. The problem is that, how come the simulation works, and real robot does not work? </p>

<p>The following is the code (I edited on ""RoboticsTutorial4.cs"" file. So, if anyone need additional code, you can just go to MRDs sample 4 to see the entire file):</p>

<pre><code>//-----------------------------------------------------------------------
//  This file is part of Microsoft Robotics Developer Studio Code Samples.
//
//  Copyright (C) Microsoft Corporation.  All rights reserved.
//
//  $File: RoboticsTutorial4.cs $ $Revision: 22 $
//-----------------------------------------------------------------------
using Microsoft.Ccr.Core;

using Microsoft.Ccr.Adapters.WinForms;

using Microsoft.Dss.Core;

using Microsoft.Dss.Core.Attributes;

using Microsoft.Dss.ServiceModel.Dssp;

using Microsoft.Dss.ServiceModel.DsspServiceBase;

using System;

using System.Collections.Generic;

using System.Security.Permissions;

using xml = System.Xml;

using drive = Microsoft.Robotics.Services.Drive.Proxy;

using W3C.Soap;

using Microsoft.Robotics.Services.RoboticsTutorial4.Properties;

using Microsoft.Robotics.Services.Drive.Proxy;

using System.ComponentModel;




namespace Microsoft.Robotics.Services.RoboticsTutorial4
{
    [DisplayName(""(User) Robotics Tutorial 4 (C#): Drive-By-Wire"")]

    [Description(""This tutorial demonstrates how to create a service that partners with abstract, base definitions of hardware services."")]

    [DssServiceDescription(""http://msdn.microsoft.com/library/bb483053.aspx"")]

    [Contract(Contract.Identifier)]
    public class RoboticsTutorial4 : DsspServiceBase
    {
        [ServiceState]
        private RoboticsTutorial4State _state = new RoboticsTutorial4State();
        //added
        bool loopBreak = true;

        [ServicePort(""/RoboticsTutorial4"", AllowMultipleInstances=false)]
        private RoboticsTutorial4Operations _mainPort = new RoboticsTutorial4Operations();

        [Partner(""Drive"", Contract = drive.Contract.Identifier, CreationPolicy = PartnerCreationPolicy.UseExisting)]
        private drive.DriveOperations _drivePort = new drive.DriveOperations();
        private drive.DriveOperations _driveNotify = new drive.DriveOperations();

        public RoboticsTutorial4(DsspServiceCreationPort creationPort) :
                base(creationPort)
        {
        }

        #region CODECLIP 02-1
        protected override void Start()
        {
            base.Start();

            WinFormsServicePort.Post(new RunForm(StartForm));

            #region CODECLIP 01-5
            _drivePort.Subscribe(_driveNotify);
            Activate(Arbiter.Receive&lt;drive.Update&gt;(true, _driveNotify, NotifyDriveUpdate));
            #endregion

        }
        #endregion

        #region CODECLIP 02-2
        private System.Windows.Forms.Form StartForm()
        {
            RoboticsTutorial4Form form = new RoboticsTutorial4Form(_mainPort);

            Invoke(delegate()
                {
                    PartnerType partner = FindPartner(""Drive"");
                    Uri uri = new Uri(partner.Service);
                    form.Text = string.Format(
                        Resources.Culture,
                        Resources.Title,
                        uri.AbsolutePath
                    );
                }
            );

            return form;
        }
        #endregion

        #region CODECLIP 02-3
        private void Invoke(System.Windows.Forms.MethodInvoker mi)
        {
            WinFormsServicePort.Post(new FormInvoke(mi));
        }
        #endregion


        /// &lt;summary&gt;
        /// Replace Handler
        /// &lt;/summary&gt;
        [ServiceHandler(ServiceHandlerBehavior.Exclusive)]
        public virtual IEnumerator&lt;ITask&gt; ReplaceHandler(Replace replace)
        {
            _state = replace.Body;
            replace.ResponsePort.Post(DefaultReplaceResponseType.Instance);
            yield break;
        }

        [ServiceHandler(ServiceHandlerBehavior.Concurrent)]
        //stop
        public virtual IEnumerator&lt;ITask&gt; StopHandler(Stop stop)
        {
            drive.SetDrivePowerRequest request = new drive.SetDrivePowerRequest();
            request.LeftWheelPower = 0;
            request.RightWheelPower = 0;

            yield return Arbiter.Choice(

                _drivePort.SetDrivePower(request),
                delegate(DefaultUpdateResponseType response) { },
                delegate(Fault fault)
                {
                    LogError(null, ""Unable to stop"", fault);
                }
            );

            //added
            loopBreak = false;
        }

        //forward
        #region CODECLIP 01-3 
        [ServiceHandler(ServiceHandlerBehavior.Concurrent)]
        //forward
        public virtual IEnumerator&lt;ITask&gt; ForwardHandler(Forward forward)
        {
            if (!_state.MotorEnabled)
            {
                yield return EnableMotor();
            }
            // movement speed
            // This sample sets the power to 75%.
            // Depending on your robotic hardware,
            // you may wish to change these values.
            drive.SetDrivePowerRequest request = new drive.SetDrivePowerRequest();
            request.LeftWheelPower = 0.5;
            request.RightWheelPower = 0.5;

            yield return Arbiter.Choice(
                _drivePort.SetDrivePower(request),
                delegate(DefaultUpdateResponseType response) { },
                delegate(Fault fault)
                {
                    LogError(null, ""Unable to drive forwards"", fault);
                }
            );
        }
        #endregion

        [ServiceHandler(ServiceHandlerBehavior.Concurrent)]
        // backup speed
        public virtual IEnumerator&lt;ITask&gt; BackwardHandler(Backward backward)
        {
            if (!_state.MotorEnabled)
            {
                yield return EnableMotor();
            }

            drive.SetDrivePowerRequest request = new drive.SetDrivePowerRequest();          
            request.LeftWheelPower = -0.6;
            request.RightWheelPower = -0.6;

            yield return Arbiter.Choice(
                _drivePort.SetDrivePower(request),
                delegate(DefaultUpdateResponseType response) { },
                delegate(Fault fault)
                {
                    LogError(null, ""Unable to drive backwards"", fault);
                }
            );
        }

        [ServiceHandler(ServiceHandlerBehavior.Concurrent)]
        // left turn speed
        public virtual IEnumerator&lt;ITask&gt; TurnLeftHandler(TurnLeft turnLeft)
        {
            if (!_state.MotorEnabled)
            {
                yield return EnableMotor();
            }

            drive.SetDrivePowerRequest request = new drive.SetDrivePowerRequest();
            request.LeftWheelPower = -0.5;
            request.RightWheelPower = 0.5;

            yield return Arbiter.Choice(
                _drivePort.SetDrivePower(request),
                delegate(DefaultUpdateResponseType response) { },
                delegate(Fault fault)
                {
                    LogError(null, ""Unable to turn left"", fault);
                }
            );
        }

        [ServiceHandler(ServiceHandlerBehavior.Concurrent)]
        // right turn speed
        public virtual IEnumerator&lt;ITask&gt; TurnRightHandler(TurnRight forward)
        {
            if (!_state.MotorEnabled)
            {
                yield return EnableMotor();
            }

            drive.SetDrivePowerRequest request = new drive.SetDrivePowerRequest();
            request.LeftWheelPower = 0.5;
            request.RightWheelPower = -0.5;

            yield return Arbiter.Choice(
                _drivePort.SetDrivePower(request),
                delegate(DefaultUpdateResponseType response) { },
                delegate(Fault fault)
                {
                    LogError(null, ""Unable to turn right"", fault);
                }
            );
        }

        #region CODECLIP 01-4
        private Choice EnableMotor()
        {
            drive.EnableDriveRequest request = new drive.EnableDriveRequest();
            request.Enable = true;

            return Arbiter.Choice(
                _drivePort.EnableDrive(request),
                delegate(DefaultUpdateResponseType response) { },
                delegate(Fault fault)
                {
                    LogError(null, ""Unable to enable motor"", fault);
                }
            );
        }
        #endregion

        #region CODECLIP 01-6
        private void NotifyDriveUpdate(drive.Update update)
        {
            RoboticsTutorial4State state = new RoboticsTutorial4State();
            state.MotorEnabled = update.Body.IsEnabled;

            _mainPort.Post(new Replace(state));
        }
        #endregion


        #region Test Code (CMD input, Start Path)
        [ServiceHandler(ServiceHandlerBehavior.Concurrent)]
        public virtual IEnumerator&lt;ITask&gt; PathHandler(StartPath path)
        {
            if (!_state.MotorEnabled)
            {
                yield return EnableMotor();
            }


             Boolean option = true;



            while (option)
            {

                double power;
                double distance;
                double degree;
                int Dtime;
                int Rtime;

                char direction;
                string Direction = """";

                Console.WriteLine(""How far do you want to move: "");
                distance = Convert.ToDouble(Console.ReadLine());
                Console.WriteLine(""Please provide power for robot: "");
                power = Convert.ToDouble(Console.ReadLine());
                Console.WriteLine(""What degree do you want: "");
                degree = Convert.ToDouble(Console.ReadLine());
                Console.WriteLine(""Which direction (R or L): "");
                direction = Convert.ToChar(Console.ReadLine());


                Console.WriteLine(""your distance: "" + distance);
                Console.WriteLine(""your robotic power: "" + power);
                Console.WriteLine(""your rotate degree: "" + degree);

                if ((direction == 'R') || (direction == 'r'))
                {
                    Direction = ""Turn Right"";
                    degree = -degree;


                }
                else if ((direction == 'l') || (direction == 'L'))
                {
                    Direction = ""Turn Left"";

                }

                if ((direction == 'f') || (direction == 'F'))
                {
                    Direction = ""Forward"";

                }
                else if ((direction == 'b') || (direction == 'B'))
                {
                    Direction = ""Backward"";
                    power = -power;

                }

                Console.WriteLine(""your direction: "" + Direction);

                loopBreak = true;
                double temp = distance * 2500;
                Dtime = Convert.ToInt32(temp);

                if (degree &gt; 90 &amp;&amp; degree &lt; -90)
                {
                    Rtime = 3000;
                }
                else
                {
                    Rtime = 2000;
                }


                for (int i = 0; i &lt; 3; i++)
                {

                    if (loopBreak == false)
                    {
                        break;
                    }

                    if (i == 0)
                    {

                        yield return Arbiter.Choice(
                            _drivePort.DriveDistance(new DriveDistanceRequest(distance, power)),
                            delegate (DefaultUpdateResponseType response) { },
                            delegate (Fault fault)
                            {
                                LogError(null, ""Unable to enable motor"", fault);
                            }
                        );
                        if (loopBreak == false)
                        {
                            break;
                        }
                        yield return Arbiter.Receive(false, TimeoutPort(Dtime), delegate (DateTime t) { });


                    }
                    if (i == 1)
                    {
                        yield return Arbiter.Choice(
                              _drivePort.RotateDegrees(new RotateDegreesRequest(degree, power)),
                              delegate (DefaultUpdateResponseType response) { },
                              delegate (Fault fault)
                              {
                                  LogError(null, ""Unable to enable motor"", fault);
                              }
                           );
                        if (loopBreak == false)
                        {
                            break;
                        }
                        yield return Arbiter.Receive(false, TimeoutPort(Rtime), delegate (DateTime t) { });

                    }


                    if (i == 2)
                    {

                        yield return Arbiter.Choice(
                           _drivePort.DriveDistance(new DriveDistanceRequest(0, 0)),
                           delegate (DefaultUpdateResponseType response) { },
                           delegate (Fault fault)
                           {
                               LogError(null, ""Unable to enable motor"", fault);
                           }
                       );
                        if (loopBreak == false)
                        {
                            break;
                        }

                        yield return Arbiter.Receive(false, TimeoutPort(Rtime), delegate (DateTime t) { });

                        Console.WriteLine(""Do you want to continue? (Y/N): "");
                        direction = Convert.ToChar(Console.ReadLine());
                        if ((direction == 'y') || (direction == 'Y'))
                        {
                            option = true;
                        }
                        else
                        {
                            option = false;
                        }

                    }

                }

            }

            yield break;

        }
        #endregion
    }
}
</code></pre>
","mobile-robot irobot-create mrds"
"10387","How do I interface with a drone?","<p>I recently bought a <a href=""http://www.snapdeal.com/product/flyers-bay-xdrone-evolution-24/1190637216#bcrumbSearch:drone"" rel=""nofollow"">drone(quadcopter)</a>. I like how the drone works but now I would like to create an application that will allow me to control the drone remotely from my PC or Phone.</p>

<p>How does a computer or phone interface to an aerial vehicle?</p>

<p>Some of my initial thoughts were </p>

<ol>
<li>Get a RF detector and detect what signals are being sent to the drone and replicate it using a new transmitter.</li>
<li>Replace the control circuit in the drone with an Arduino and send corresponding signals to it to fly the drone</li>
</ol>

<p>Both these methods seem to be kind of far fetched and hard, but I'm not sure how this could otherwise be done.</p>
","quadcopter software radio-control wireless"
"10388","How to check which Gazebo/ODE functions are being called?","<p>I'm trying to take a simple event in which the Atlas steps on the ground plane. I want to see which functions ODE calls and the functions ODE uses to determine the constraint forces. I'd like to see this happen while the simulation is running. Is there a way I could do that? I'd like to know what constraint equations and constraint forces ODE is using for that particular case. Thanks.</p>
","ros dynamics gazebo"
"10392","view angle of distance sensor","<p>I need a distance sensor (IR or Optical or any other) with 90 degree view angle to sense a rectangle surface.
in this case sensor must putting at the same level of surface area. please help me to solve this.</p>

<p><a href=""http://i.stack.imgur.com/AiWo0.jpg"" rel=""nofollow""><img src=""http://i.stack.imgur.com/AiWo0.jpg"" alt=""enter image description here""></a></p>
","mobile-robot"
"10394","why my 2 DC motors of same model run at different speeds?","<p>I have 2 wheels for my robot, and they're both powered by the same battery. yet my robot has a differential motion(as both wheels are running at differnt speeds) why is this?</p>
","mobile-robot motor"
"10404","Addressing the sample impoverishment in particle filter","<p>I have implemented a particle filter algorithm for the state estimation of a mobile robot.</p>

<p>There are several external range sensors(transmitters) in the environment which gives information on the distance (radius) of the robot based on the time taken for the receiver on the robot to send back its acknowledgement. So, using three or more such transmitters it will be possible to triangulate the position of the robot.</p>

<p>The particle filter is initialized with 15000 particles and the sensor noise is relatively low (0.02m).</p>

<p>Update Phase: At each iteration a range information from an external sensor is received. This assigns higher weights to the particles along the radius of the external sensor. Not all the particles are equally weighted since the process noise is low. Hence in most of the cases, the particle relatively closer to the robot gets lower weight than an incorrect one that happens to be along the radius. The weight is a pdf.</p>

<p>Resampling Phase: At this stage, the lower weighted particle(the correct one) that has negligible weight gets lost because the higher weighted particle gets picked up.</p>

<p>All this happens at the first iteration and so when the range information from another sensor arrives, the robot is already kidnapped.</p>

<p>Googling around, said that this problem is called as sample impoverishment and the most common approach is to resample only when the particle variance is low. (Effective Sample Size &lt; number of particles / 2)</p>

<p>But, when the particles are assigned negligible weights and there are relatively very few particles with higher weights, the diversity of the particles are lost at resampling phase. So, when the variance is higher resampling is done which removes the lower weighted particle and hence the diversity of the particles is lost. Isnt this completely the opposite of the above idea of ESS?</p>

<p>Is my understanding of sample impoverishment correct? Is there a way this issue can be fixed?</p>

<p>Any pointers or help would be highly appreciated.</p>
","mobile-robot localization particle-filter probability"
"10407","What is the interpretation of unsampled particles in particle filters?","<p>I implemented <a href=""https://sourceforge.net/projects/r-localization/"" rel=""nofollow"">Particle Filters</a> few years back. I was experimenting few things with Particle Filters.
I learned that we can resolve Robot Kidnapping problem by introducing new particles.
What if we left some particles unsampled e.g. 1% of the population</p>

<ul>
<li><p>What is the interpretation of unsampled particles in this context?</p></li>
<li><p>How they can effect our localization output?</p></li>
</ul>

<p><a href=""http://i.stack.imgur.com/RgHYH.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/RgHYH.png"" alt=""enter image description here""></a></p>
","localization algorithm particle-filter"
"10408","Localization of a Robot to find it Coordinates according to the Known Map","<p>I am a third-year electrical engineering student and am working on an intelligent autonomous robot in my summer vacations.</p>

<p>The robot I am trying to make is supposed to be used in rescue operations. The information I would know is the position of the person (the coordinates of the person in a JSON file that can be changed anytime except during the challenge) to be rescued from a building on fire. I would also know the rooms of the building from a map, but I don't know where the robot may be placed inside the building to start the rescue operation.</p>

<p>That means I have to localise the robot placed at an unknown position in a known environment, and then the robot can plan its path to the person who has to be rescued. I can use gyroscope, accelerometer, magnetometer and ultrasonic sensors to do the localising job. I cannot use a GPS module or a camera for this purpose.</p>

<p>The object to be rescued (whose location is known in terms of coordinates &amp; can be changed anytime) is surrounded by walls from 3 sides. Hence, adding more walls in this map.</p>

<p>According to my research particle filter is the best method used for localization of robot. But how can I deal with the landmarks (walls) that are <em>fixed</em> as shown in the map image and <em>that are variable</em> depending on the location of the object to be rescued being provided in the JSON file?</p>

<p>I can do the path planning from a known position to the target position, but I'm not sure how to determine the starting position.</p>

<p>More about JSON file:
(1) json file containing the coordinates of the object to be rescued can change. (2) it won't change during the challenge. (3) json file will be provided to me in an SD card that my robot has to read. I have successfully written the code that will allow the robot to read the json file and hence the coordinates of the object to be rescued.</p>

<p>Here is the map of the building which is known to me.</p>

<p><a href=""http://i.stack.imgur.com/6EIgO.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/6EIgO.png"" alt=""enter image description here""></a></p>
","localization particle-filter"
"10415","Determining the graspable range of a robot arm","<p>I have a 6 DOF robot arm, and I want to do some object grasping experiments. Here, the robot is rigidly mounted to a table, and the object is placed on a different adjacent table. The robot must pick up the object with its gripper parallel to the normal of the object's table, such that it is pointing directly downwards at the point of grasping.</p>

<p>Now, the two tables have adjustable heights. For any given height difference between them. there will be a fixed range of positions over which the robot arm can achieve this perpendicular pose. What I am trying to figure out, is what the optimum relative distance of the tables is such that this range of positions is maximum.</p>

<p>Is there a way to compute this analytically given the robot arm kinematics? Or is there a solution which applies to all robot arms (e.g. it is optimum when the tables are at the same height)?</p>

<p>If it is important, the arm is the Kinova MICO: <a href=""https://www.youtube.com/watch?v=gUrjtUmivKo"" rel=""nofollow"">https://www.youtube.com/watch?v=gUrjtUmivKo</a>.</p>

<p>Thanks!</p>
","robotic-arm kinematics"
"10416","Sensors in Collaborative Robots","<p>I'm currently doing some research on collaborative robotics. One area of interest is the type of sensor(s) used in these kind of machines. </p>

<p>I had a look at some robots by FANUC and Universal Robots and I've noticed that they do not come equipped with sensors; they are sold as an add-on.</p>

<p>Is this inherent with collaborative robots? Do customers need to buy sensors as an add-on - which has advantages and disadvantages. </p>

<p>Thanks for your help.</p>
","sensors industrial-robot"
"10425","IR distance sensor","<p>I am trying to make a IR distance sensor. I have seen <a href=""http://www.instructables.com/id/Simple-IR-proximity-sensor-with-Arduino/"" rel=""nofollow"">this</a> online. My goal however is to see the distance between a IR transmitter and my IR sensor. In the example above he uses the IR led's ambient light and timing to track the distance. Is there a way to find the distance between lets say a IR remote and a sensor? It would only have to be accurate to about 1 meter. I am also open to other ideas of accurately tracking distance between two objects weither that be bluetooth/ir/ultrasonic </p>
","sensors"
"10426","Will this pseudocode work as a basis for a flight controller?","<p>I'm programming a flight controller on an Arduino. I've researched how other people have written theirs but without notes it's often so obfuscated that I've decided it will be easier and better to write my own.</p>

<p>This is my pseudocode thus far, will this work?</p>

<p><em>all of this will happen inside the constant Arduino loop</em></p>

<ul>
<li>Read RX signal</li>
<li>Calculate desired pitch, roll, and yaw angles from RX input</li>
<li>Signal ESCs using PWM in order to match desired pitch, roll, and yaw from RX input</li>
<li>Gather IMU values (using Kalman filter to reduce noise)</li>
<li>Compare filtered IMU values vs. RX input to find errors in desired outcome vs. actual outcome</li>
<li>Use PID algo to settle errors between IMU vs. RX</li>
<li>Rinse and repeat</li>
</ul>

<p>Suggestions are greatly appreciated</p>
","arduino microcontroller uav"
"10427","Is there a portable and accurate sensor to measure the position of the hand relative to the body?","<p>My team has been working on a wearable glove to capture data about hand movements, and use it as a human-computer interface for a variety of applications. One of the major applications is the translation of sign language, shown here: <a href=""https://www.youtube.com/watch?v=7kXrZtdo39k"" rel=""nofollow"">https://www.youtube.com/watch?v=7kXrZtdo39k</a></p>

<p>Right now we can only translate letters and numbers, because the signs for them require the person to hold their hand still in one position ('stationary' signs). I want to be able to translate <em>words</em> as well, which are non-stationary signs. Also the position of the hands really matters when signing words, for example it matters whether the hand is in front of the forehead, eyes, mouth, chest, cheeks, etc.</p>

<p>For this we need a portable and highly accurate position sensor. We have tried getting position from a 9-DOF IMU (accelerometer, gyroscope, magnetometer) but as you might guess, there were many problems with double integration of the noise and accelerometer bias.</p>

<p>So is there a device that can provide <strong>accurate position</strong> information? It should be <strong>portable and wearable</strong> (for example worn in the chest pocket, headband/cap, etc...be creative!).</p>

<p><strong>EDIT (more details):</strong></p>

<p>I'm going to emphasize certain aspects of this design that weren't clear before, based on people's comments:</p>

<ol>
<li>My <em>current</em> problem of position detection is due to errors in double integration of the accelerometer data. So hopefully the solution <s>has some incredibly powerful kalman filter (I think this is unlikely) or </s> uses some other portable device instead of an accelerometer.</li>
<li>I do not need absolute position of the hand in space/on earth. I only need the <em>hand position relative to some stable point on the body</em>, such as the chest or belly. So maybe there can be a device on the hand that can measure position relative to a wearable device on the body. I don't know if such technology exists; I guess it'd use either magnets, ultrasound, bend sensors, or EM waves of some sort. Be creative :)</li>
</ol>
","sensors sensor-error precise-positioning"
"10429","Difference between SLAM and ""3D reconstruction""?","<p>I'm reading this paper: <a href=""http://arxiv.org/abs/1310.2053"" rel=""nofollow"">http://arxiv.org/abs/1310.2053</a> (<strong>The role of RGB-D benchmark datasets: an overview</strong>) and see the following words:</p>

<blockquote>
  <p>Thanks to accurate depth data, currently published papers could
  present a broad range of RGB-D setups addressing well-known problems
  in computer vision in which the Microsoft Kinect ranging from SLAM
  [10, 12, 19, 17, 35, 11] over 3d reconstruction [2, 33, 38, 32, 1]
  over realtime face [18] and hand [30] tracking to motion capturing and
  gait analysis [34, 41, 8, 7, 4]</p>
</blockquote>

<p>I thought of the term <strong>SLAM</strong> and <strong>3D Reconstruction</strong> being the same thing, while the paper says the opposite with a bunch of citations (which still haven't tell the two apart).</p>

<p>In my opinion, <strong>M</strong>apping in <strong>SLAM</strong> is the same term as <strong>3D Reconstruction</strong>, while <strong>L</strong>ocalization is the essential part for <strong>M</strong>apping. So I don't find a difference between <strong>SLAM</strong> and <strong>3D Reconstruction</strong>, am I wrong (or is the author misclassfying)?</p>
","slam 3d-reconstruction"
"10436","What is the difference between ROSberry Pi builds?","<p>I went to go install ROS for my Rassberry Pi and found that there are 5 different variants. What is the difference between them and where can I go to learn about these differences for future updates?</p>

<p>Link to the ROSberryPi downloads I'm talking about:
<a href=""http://wiki.ros.org/ROSberryPi/Setting%20up%20ROS%20on%20RaspberryPi"" rel=""nofollow"">http://wiki.ros.org/ROSberryPi/Setting%20up%20ROS%20on%20RaspberryPi</a></p>
","ros raspberry-pi"
"10438","I-Robot Create 2 Connectors","<p>Am I correct in assuming that the I-Robot Create 2 does not have the 25-pin connector like the original version of I-Robot Create has? Thanks much...Rick</p>
","irobot-create"
"10443","Stereo vision for outdoor obstacle detection","<p>I'm trying to detect obstacles for a distance of up to 10 meters in an outdoor environment. Up to meaning that I also want to be able to detect obstacles that are close to the robot. I am thinking of doing this using stereo vision, but I am unsure if this is in fact even possible (before I buy expensive hardware). So is it possible? Has anyone had any success?</p>

<p>If this isn't possible, then what kind of sensors could give me a decent point cloud for such a range (outdoors)? I need a sensor that will fit a medium size robot. Also it needs to be not overly expensive since I have a limited budget.</p>

<p>Thanks</p>
","sensors stereo-vision"
"10445","NameError: name 'TK' is not defined","<p>I reference the following article.
<a href=""http://www.egr.msu.edu/classes/ece480/capstone/spring15/group02/assets/docs/nsappnote.pdf"" rel=""nofollow"">http://www.egr.msu.edu/classes/ece480/capstone/spring15/group02/assets/docs/nsappnote.pdf</a></p>

<p>I have followed article's code, 
but it appears:</p>

<p><a href=""http://i.stack.imgur.com/L1CxO.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/L1CxO.png"" alt=""enter image description here""></a></p>

<p>How can i solve this problem?</p>
","irobot-create"
"10447","unable to install ros kinetic in ubuntu 16.04","<p>I am trying to install ros kinetic kame in ubuntu 16.04 , but after trying the first step setup your sources. list. 
I am getting  cannot create /etc/apt/sources.list.d/ros-latest.list: Permission denied what to do now</p>
","ros irobot-create"
"10450","Conversion GPS (longitude,latitude) to (x,y) of local reference frame?","<p>I would like to use GPS data as measurement input for an extended kalman filter. Therefore I need to convert from GPS longitude and lattitude to x and y coordinate. I found information about the  <a href=""http://stackoverflow.com/questions/16266809/convert-from-latitude-longitude-to-x-y"">equirectangular projection</a> given these formulas: 
$$\ X = r_{earth} \cdot \lambda \cdot cos(\phi_0) $$
$$\ Y = r_{earth} \cdot \phi $$</p>

<p>However I think these formulas are only for use when the axis x- and y-axis of my local frame are parallel to north and south axis of the earth.
<a href=""http://i.stack.imgur.com/dAVlh.jpg"" rel=""nofollow""><img src=""http://i.stack.imgur.com/dAVlh.jpg"" alt=""enter image description here""></a>
But my vehicle is starting in my local reference frame in the origin and heading straight in y-direction. In whatever compas angle I put my vehicle, this should always be the starting position. 
I can measure the angle $ \alpha $ to north with a compass on my vehicle.</p>

<p>Now what is the relationship between (longitude,lattitude) an (x,y) of my local frame?</p>
","kalman-filter gps"
"10454","How can a DMP be used for simulating physics?","<p>I read a paper from 2015, <a href=""http://h2t.anthropomatik.kit.edu/pdf/Woergoetter2015.pdf"" rel=""nofollow"">""Structural bootstrapping - A novel, generative
mechanism for faster and more efficient acquisition of action-knowledge
""</a>, which introduces a concept called, ""Structural bootstrapping with semantic event chains and dynamic movement primitives,"" which confused me a little bit. </p>

<p>According to my knowledge a robotarm is controlled by a PDDL-like planner. The PDDL file is a ""qualitative physics engine"" which can predict future events. The paper says the ""qualitative physics engine"" consists of dynamic movement primitive (DMP) which are learned from motion capture data. </p>

<p>My question is: How can a DMP be used for simulating physics?</p>
","motion-planning algorithm machine-learning"
"10455","Replacing just the Wheels on Create2","<p>Is it possible to replace just the wheels on the create2 robot? Is it a standard shaft/coupling?</p>
","irobot-create roomba"
"10461","Measure 2 diffrent battery voltages on arduino","<p>Is it possible to measure the voltage of 2 different batteries on arduino? Currently I am able to use a resistor divider / voltage divider of 2x 10K resistors to an analog pin to read the voltage of the battery supplying the arduino.</p>

<p>Currently the system looks like 6v battery -> 5v power regulator to Arduino -> resistor divider attached to 6v (unregulated) battery. GND is common throughout.</p>

<p>How could I measure the voltage of another battery given that it will be on a different circuit? e.g. different ground loop.</p>
","arduino power"
"10462","Name of the linkage (or carriage) in video","<p>I am trying to find the name (nomenclature) of the linkage (or carriage) that is being driven by the dual linear servo (actuator) arrangement in the following Youtube videos:</p>

<ul>
<li><p><a href=""https://www.youtube.com/watch?v=Le9hyRdcnOg"" rel=""nofollow"">Servo Basic Concepts</a></p></li>
<li><p><a href=""https://www.youtube.com/watch?v=P1kTsvfH2ss"" rel=""nofollow"">YouTube - 4 X Linear Servo Application</a></p></li>
</ul>

<p>The linkage (carriage) appears to be able to rotate about a 180 degree arc.</p>

<p>What is this metal linkage (or carriage) system called? 
</p>
","mechanism arm"
"10463","Reward Function for q learning on a robot","<p>I have 2 wheeled differential drive robot which i use pid for low level control to follow line. I implemented q learning which uses samples for 16 iterations then uses them to decide the best position to be on the line so car takes the turn from there. This allows pid to setup and smooth fast following. My question is how can i setup a reward function that improves the performance i.e. lets the q learning to find the best</p>

<p><strong>Edit</strong>
What it tries to learn is this, it has 16 inputs which contains the line positions for the last 15 iterations and this iteration. Line position is between -1 and 1 which -1 means only left most sensor sees the line and 0 means the line is in the center. I want it to learn a line position that when it faces this input again it will set that line position like its the center and take the curve according to that line position. For example error is required position - line position so let say i had 16 0 as input then i calculated the required as 0.4. So after that the car will center itself at 0.4 i hope this helps :)</p>

<p>You asked for my source code i post it below</p>

<pre><code>void MainController::Control(void){

float linePosition = sensors-&gt;ReadSensors();


if(linePosition == -2.0f){       
  lost_line-&gt;FindLine(lastPos[1] - lastPos[0]);
} 
else{
    line_follower-&gt;Follow(linePosition);
    lastPos.push_back(linePosition);
    lastPos.erase(lastPos.begin());

}
</code></pre>

<p>}</p>

<p>My Sensor reading returns a value between -1.0f and 1.0f. 1.0f means Outer Sensor on the right is only the line. I have 8 sensors.</p>

<pre><code>void LineFollower::Follow(float LinePosition){

float requiredPos = Qpredictor.Process(LinePosition,CurrentSpeed);
float error = requiredPos - LinePosition;

float ErrorDer = error -LastError;


float diffSpeed = (KpTerm * error + (KdTerm * ErrorDer));

float RightMotorSpeed = CurrentSpeed - diffSpeed;
float LeftMotorSpeed = CurrentSpeed + diffSpeed;

LastError = error;

driver-&gt;Drive(LeftMotorSpeed,RightMotorSpeed);
</code></pre>

<p>}</p>

<p>Here is the logic for the value for QPredictor(I call the learning part as this).And Finally QPredictor</p>

<pre><code>float Memory[MemorySize][DataVectorLength] =
{
  {0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0},
  {0.3, 0.3, 0.3, 0.3, 0.3, 0.3, 0.3, 0.3, 0.3, 0.3, 0.3, 0.3, 0.3, 0.3, 0.3, 0.3},
  {0.6, 0.6, 0.6, 0.6, 0.6, 0.6, 0.6, 0.6, 0.6, 0.6, 0.6, 0.6, 0.6, 0.6, 0.6, 0.6},
  {0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8},

  {0.000, 0.012, 0.050, 0.113, 0.200, 0.312, 0.450, 0.613, 0.800, 1.000, 1.000, 1.000, 1.000, 1.000, 1.000, 1.000},
  {0.000, 0.000, 0.012, 0.050, 0.113, 0.200, 0.312, 0.450, 0.613, 0.800, 1.000, 1.000, 1.000, 1.000, 1.000, 1.000},
  {0.000, 0.000, 0.000, 0.012, 0.050, 0.113, 0.200, 0.312, 0.450, 0.613, 0.800, 1.000, 1.000, 1.000, 1.000, 1.000},
  {0.000, 0.000, 0.000, 0.000, 0.012, 0.050, 0.113, 0.200, 0.312, 0.450, 0.613, 0.800, 1.000, 1.000, 1.000, 1.000},
  {0.000, 0.000, 0.000, 0.000, 0.000, 0.012, 0.050, 0.113, 0.200, 0.312, 0.450, 0.613, 0.800, 1.000, 1.000, 1.000},
  {0.000, 0.000, 0.000, 0.000, 0.000, 0.000, 0.012, 0.050, 0.113, 0.200, 0.312, 0.450, 0.613, 0.800, 1.000, 1.000},
  {0.000, 0.000, 0.000, 0.000, 0.000, 0.000, 0.000, 0.012, 0.050, 0.113, 0.200, 0.312, 0.450, 0.613, 0.800, 1.000},

  {0.000, 0.025, 0.100, 0.225, 0.400, 0.625, 0.900, 1.000, 1.000, 1.000, 1.000, 1.000, 1.000, 1.000, 1.000, 1.000},

  {0.000, 0.050, 0.200, 0.450, 0.800, 1.000, 1.000, 1.000, 1.000, 1.000, 1.000, 1.000, 1.000, 1.000, 1.000, 1.000},


  {0.000, 0.100, 0.400, 0.900, 1.000, 1.000, 1.000, 1.000, 1.000, 1.000, 1.000, 1.000, 1.000, 1.000, 1.000, 1.000},
  {0.000, 0.000, 0.100, 0.400, 0.900, 1.000, 1.000, 1.000, 1.000, 1.000, 1.000, 1.000, 1.000, 1.000, 1.000, 1.000},
  {0.000, 0.000, 0.000, 0.100, 0.400, 0.900, 1.000, 1.000, 1.000, 1.000, 1.000, 1.000, 1.000, 1.000, 1.000, 1.000},
  {0.000, 0.000, 0.000, 0.000, 0.100, 0.400, 0.900, 1.000, 1.000, 1.000, 1.000, 1.000, 1.000, 1.000, 1.000, 1.000},
  {0.000, 0.000, 0.000, 0.000, 0.000, 0.100, 0.400, 0.900, 1.000, 1.000, 1.000, 1.000, 1.000, 1.000, 1.000, 1.000},
  {0.000, 0.000, 0.000, 0.000, 0.000, 0.000, 0.100, 0.400, 0.900, 1.000, 1.000, 1.000, 1.000, 1.000, 1.000, 1.000},
  {0.000, 0.000, 0.000, 0.000, 0.000, 0.000, 0.000, 0.100, 0.400, 0.900, 1.000, 1.000, 1.000, 1.000, 1.000, 1.000},
  {0.000, 0.000, 0.000, 0.000, 0.000, 0.000, 0.000, 0.000, 0.100, 0.400, 0.900, 1.000, 1.000, 1.000, 1.000, 1.000},
  {0.000, 0.000, 0.000, 0.000, 0.000, 0.000, 0.000, 0.000, 0.000, 0.100, 0.400, 0.900, 1.000, 1.000, 1.000, 1.000},
  {0.000, 0.000, 0.000, 0.000, 0.000, 0.000, 0.000, 0.000, 0.000, 0.000, 0.100, 0.400, 0.900, 1.000, 1.000, 1.000},
  {0.000, 0.000, 0.000, 0.000, 0.000, 0.000, 0.000, 0.000, 0.000, 0.000, 0.000, 0.100, 0.400, 0.900, 1.000, 1.000},
  {0.000, 0.000, 0.000, 0.000, 0.000, 0.000, 0.000, 0.000, 0.000, 0.000, 0.000, 0.000, 0.100, 0.400, 0.900, 1.000}
};


QPredictor::QPredictor(){
for(int i=0;i&lt;MemorySize;i++){
    output[i]=0.0f;
    input[i]=0.0f;
}

state = 0;
PrevState = 0;
}

float QPredictor::Process(float linePosition,float currentBaseSpeed){
for(int i=1;i&lt;DataVectorLength;i++){
    input[i] = input[i-1];
}

input[0] = m_abs(linePosition);


int MinIndex = 0;
float Distance = 10000.0f;

float sum = 0.0f;

for(int i=0;i&lt;MemorySize;i++){
    sum = 0.0f;
    for(int j=0;j&lt;DataVectorLength;j++){
        sum +=m_abs(input[j] - Memory[i][j]);
    }

    if(sum &lt;= Distance){
        MinIndex = i;
        Distance = sum;

    }
}


sum = 0.0f;
for(int i=0;i&lt;DataVectorLength;i++){
    sum += input[i];
}

float eta = 0.95f;

output[MinIndex] = eta * output[MinIndex] + (1 - eta) * sum;

return -m_sgn(linePosition) * output[MinIndex];
}





float QPredictor::rewardFunction(float *inputData,float currentBaseSpeed){
float sum = 0.0f;

for(int i=0;i&lt;DataVectorLength;i++){
    sum += inputData[i];
}

sum /= DataVectorLength;

return sum;
}
</code></pre>

<p>I now only have average Error and currently not using learning because it's not complete without reward function. How can i adjust it according to the dimensions of my Robot ?</p>
","machine-learning"
"10467","NameError: global name 'sendCommandASCII' is not defined","<p>When I execute  following code,</p>

<pre><code>    if k == 'P':         # Passive
     sendCommandASCII('128')
    elif k == 'S':       # Safe
        sendCommandASCII('131')
    elif k == 'F':       # Full
        sendCommandASCII('132')
    elif k == 'C':       # Clean
        sendCommandASCII('135')
    elif k == 'D':       # Dock
        sendCommandASCII('143')
    elif k == 'SPACE':   # Beep
        sendCommandASCII('140 3 1 64 16 141 3')
    elif k == 'R':       # Reset
        sendCommandASCII('7')
    elif k == 'UP':      # Forward
        callbackKey.up = True
        motionChange = True
    elif k == 'DOWN':    # Backward
        callbackKey.down = True
        motionChange = True
    elif k == 'LEFT':    # Counterclockwise
        callbackKey.left = True
        motionChange = True
    elif k == 'RIGHT':   # Clockwise
        callbackKey.right = True
        motionChange = True
</code></pre>

<p>but errors,</p>

<pre><code>Traceback (most recent call last):
  File ""C:\Python27_2\lib\lib-tk\Tkinter.py"", line 1532, in __call__
    return self.func(*args)
  File ""C:/Python27_2/IRobot.py"", line 38, in callbackKey
    sendCommandASCII('131')
NameError: global name 'sendCommandASCII' is not defined
</code></pre>

<p>1532 line program:
<a href=""http://i.stack.imgur.com/19Kcr.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/19Kcr.png"" alt=""enter image description here""></a></p>

<p>How can I solve?</p>
","irobot-create python"
"10468","Inverse Kinematics Computation -- why are alpha angle values not included","<p>Given a desired transform matrix of the end effector relevant to the base frame of the P560:
<img src=""http://i.imgur.com/FyuhvUK.png"" alt=""transform""></p>

<p>John J. Craig, in his book, Introduction to Robotics
Mechanics and Control, computes the inverse kinematic solutions of a Puma 560, with (correct me if wrong) Modified DH parameters and gets the following set of equations for theta angles:
<img src=""http://i.imgur.com/v8bO8yR.png"" alt=""transform""></p>

<p>and I noticed that there are no alpha angles in these calculations.</p>

<p>So my question is why aren't the alpha angle values not used in the calculation for the desired pose with the given end effector transform. Why is it independent of the axes twist angles of the robot?</p>
","inverse-kinematics"
"10469","Heading and Yaw Rate Measurements","<p>I am working in the field of automated vehicles mainly in the domain of passenger and commercial vehicles. I have been studying whatever I can get regarding the measurement the state (relative position, relative velocity, relative heading and roation, a.k.a. yaw rate) of surrounding objects especially other vehicles using sensors.</p>

<p>While everything else is possible to measure precisely using on-board sensors, I have found out that not much literature is available for measuring the vehicle heading and yaw rate <strong><em>of other vehicles</em></strong> which is baffling to me given the extreme precision of laser based sensing (albeit using time stamps).</p>

<p>I am looking for:</p>

<ol>
<li><p>Reference to literature with experiments for estimation of yaw rate and vehicle heading.</p></li>
<li><p>As I can see from the literature available (or the lack thereof), no direct way of measuring yaw rate is available but by using LIDAR or Camera with consecutive time stamps or scans of data. However, this inherently requires the data to be correct. Hence, I would think that due to the inaccuracies involved, this method is not used! Is this correct?</p></li>
<li><p>Are there any commercially available sensors that give accurate heading and yaw rate information <strong><em>of other vehicles</em></strong>?</p></li>
</ol>

<p>Sources and research papers would be most welcome!</p>

<p>Edit: By <strong>this inherently requires the data to be correct</strong> I mean, given the high sensitivity to error in heading or yaw rate at high vehicle speeds, the values computed using sensor information is not accurate enough to be put to use in practice!</p>
","localization"
"10471","What is difference between RoboEarth and KnowRob?","<p>I am not able to clearly differentiate between the two platforms:</p>

<ul>
<li><a href=""http://roboearth.org/"" rel=""nofollow"">RoboEarth</a>, and;</li>
<li><a href=""http://www.knowrob.org/"" rel=""nofollow"">KnowRob</a>.</li>
</ul>
","theory cloud"
"10472","I-Robot Create 2 reset after sleep issue?","<p>I am having issues with bringing the robot out of its sleep or off mode. Seems it goes into sleep mode when there is no activity for about 4 minutes. I am using the i-Robot Create 2 serial cable. When it is in its sleep mode I try removing the cable end plugged into robot and connect jumper wire between pins 5 and 6 on the robot 7 pin connector for a brief time period. This effectively shorts the BRC pin to GND for a short period of time ( less than 1 second). Then I reconnect the serial port cable into the robot 7 pin connector and try giving the robot a command but no go. I have also read that commands 173 and 173 173 can help with this issue but I may be mistaken. Any help on this is very much appreciated !!!! Rick</p>
","irobot-create"
"10474","Odometry motion model for Kalman filter, but is the error zero mean?","<p>I was planning on using the odometry model in the prediction stage of an Extended Kalman Filter.
State transition equations:
$$ f(X_t,a_t) = \begin{bmatrix}
x_{t+1} = x_t + \frac{\delta s_r + \delta s_l}{2} \cdot \cos(\theta_t) +u_1
\\ y_{t+1} = y_t + \frac{\delta s_r + \delta s_l}{2} \cdot \sin(\theta_t) + u_2
\\ \theta_{t+1} = \theta_t + \frac{\delta s_r + \delta s_l}{b} \cdot \sin(\theta_t)+u_3
\end{bmatrix} $$
with $\delta s_r$ and $\delta s_l = \frac{n}{n_0} \cdot 2 \cdot \pi \cdot r$</p>

<blockquote>
  <p>$X_t = \begin{bmatrix} x_t &amp; y_t &amp; \theta_t\end{bmatrix}^T$ state matrix containing XY-coordinate and heading $\theta$ of vehicle in global reference frame</p>
</blockquote>

<hr>

<blockquote>
  <p>$\delta s_r$ and $\delta s_l$ distance travelled by respectively right and left wheel</p>
</blockquote>

<hr>

<blockquote>
  <p>$b$ distance from center of the vehicle to the wheel</p>
</blockquote>

<hr>

<blockquote>
  <p>$n$ encoder pulses count during sampling period t</p>
</blockquote>

<hr>

<blockquote>
  <p>$n_0$ total pulses count in 1 wheelturn</p>
</blockquote>

<hr>

<blockquote>
  <p>$r$ wheel radius</p>
</blockquote>

<hr>

<blockquote>
  <p>$u_1,u_2$ and $u_3$ random noise N(0,$\sigma^2$)</p>
</blockquote>

<hr>

<p>Now I doubt if this noise indeed does have a zero mean?
Wheelslip will always make the estimated distance travelled shorter than the measured distance isn't it?</p>
","kalman-filter odometry noise"
"10479","GPS observation equations for Kalman filter?","<p>In the design of an Extended Kalman Filter for the position estimation of a vehicle, I am searching for the observation equations for inserting GPS data (longitude, latitude) into the update step of the filter.</p>

<p>The state vector of my filter $X_t = \begin{bmatrix} x_t &amp; y_t &amp; \phi_t\end{bmatrix}$ contains the X and Y coordinate of the vehicle in the local reference frame and the angle under which the vehicle is standing relative to the X-axis.</p>

<p><a href=""http://i.stack.imgur.com/h7212.jpg"" rel=""nofollow""><img src=""http://i.stack.imgur.com/h7212.jpg"" alt=""enter image description here""></a></p>

<p>The observation equations should look like this:
$$H(X_t) = \begin{bmatrix} longitude = f(x_t,y_t) \\ latitude = f(x_t,y_t)\end{bmatrix} $$</p>

<p>Can anybody fill them in?</p>
","kalman-filter gps"
"10484","Equations of motion of 3D pendulum-like system","<p>I'm trying to get the equations of motion of a 3D pendulum system (spherical pendulum), however I don't want to describe the system using spherical coordinates (which there is lot of information about). </p>

<p>Instead I want to describe the system using x,y,z coordinates of the mass as well as the euler angles phi, theta, psi (the roll, pitch and yaw of the mass). 
That is, I want to assume that the mass has a position and an orientation in relation to the inertial frame. </p>

<p>Furthermore this is a 3D pendulum system, where the mass, which is symmetric, is actuated (notice that this is a simplification of the system needed to actuate the mass, where only the resulting forces and torques are taken into account):</p>

<ul>
<li>There is a force F acting in the x-axis direction of the mass reference frame</li>
<li>There is a torque T acting about the z-axis of the mass reference frame</li>
<li>There is also the gravitational force acting on the center of mass (in the negative direction of z-axis of the inertial reference frame</li>
</ul>

<p>In order to clear misconceptions about how this forces are generated, think of the mass as a differential drive robot using fans instead of wheels.</p>

<p>The cable connecting the mass to the anchor point is assumed rigid and works as a distance constraint modeled by:
$\|r_{anchor} - r_{mass}\| = cable\_length$</p>

<p>Where $r_{anchor}$ is the position of the anchor point to which the mass is connected through the cable and $r_{mass}$ is the position of the mass. This constraint makes it so that it is similar to having two spherical joints: one at the anchor point and another at the mass. Futhermore the cable is assumed to have no mass. 
It's important to note that this rigid connection (calbe) is meant to be modeled by the distant constraint refered above.</p>

<p><a href=""http://i.stack.imgur.com/5VoWc.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/5VoWc.png"" alt=""enter image description here""></a></p>

<p>I'm looking for help solving this system to obtain its equations of motion.
Thanks in advance</p>
","kinematics dynamics"
"10486","Combine individually working cartesian coordinates","<p><a href=""http://i.stack.imgur.com/cZzjL.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/cZzjL.png"" alt=""The triangles used for angle calculations""></a></p>

<p>I am trying to control a <a href=""http://dobot.cc/"" rel=""nofollow"">Dobot arm</a>.  The arm moves with angles whereas I need to work with cartesian coordinates. From inverse kinematics equations and polar coordinates I have implemented x,y and z coordinates working very well on their own. But I can not combine the coordinates in order to work all together. When I add them up it is not going to the desired place. How can I combine these coordinates? I got some help from (<a href=""https://github.com/maxosprojects/open-dobot"" rel=""nofollow"">https://github.com/maxosprojects/open-dobot</a>) but could not manage to successfully move dobot. </p>

<p>Edit: I've written the codes in Qt and also I've added the triangles used for angle calculations.</p>

<p><code>//foreArmLength=160mm rearArmLEngth=135mm</code></p>

<pre><code>float DobotInverseKinematics::gotoX(float x) //func for x-axis
float h=qSqrt(qPow(lengthRearArm,2)-qPow(x,2)); //height from ground
QList&lt;float&gt; zEffect=gotoZ(h); //trying to find the effect of x movement on z-axis
float cosQ=h/lengthRearArm; //desired joint angle
float joint2=qRadiansToDegrees(qAcos(cosQ));    
//move in range control
if(joint2 != joint2)
{joint2=0;
    qDebug()&lt;&lt; ""joint2NAN"";}
return joint2;

QList&lt;float&gt; DobotInverseKinematics::gotoY(float y) //func for y-axis
QList&lt;float&gt; result ;
float actualDist=lengthForeArm+distToTool; //distance to the end effector
float x=(qSqrt(qPow(actualDist,2)+qPow(y,2)))-actualDist; //calculating x movement caused by y movement
float joint1=qRadiansToDegrees(qAcos(actualDist/(actualDist+x))); //desired joint angle
float joint2=gotoX(x);  //the angle calculation of y movement on x axis
if(joint1 != joint1)
{joint1=0;
    qDebug()&lt;&lt; ""joint1NAN"";}
result.append(joint1);
result.append(joint2);
return result;

QList&lt;float&gt; DobotInverseKinematics::gotoZ(float z) //func. for z-axis
QList&lt;float&gt; result ;
float joint3=qSqrt(qPow(160,2.0)-qPow(z,2.0))/ 160; //desired joint angle
float temp=160-qSqrt(qPow(160,2.0)-qPow(z,2.0));
float joint2=qSqrt(qPow(lengthRearArm,2)-qPow(temp,2.0))/lengthRearArm; //desired joint angle
if(joint3 != joint3)
{joint3=0;
    qDebug()&lt;&lt; ""joint3NAN"";}
joint2=qAcos(joint2)*(180/M_PI);
joint3=qAcos(joint3)*(180/M_PI);
result.append(joint2);
result.append(joint3);
return result;
</code></pre>
","robotic-arm inverse-kinematics c++"
"10487","What type of motor can be hooked up on a bike?","<p>As the title briefly explains, my question is, what type of motor is powerful enough be on a cycle?</p>

<p>My plan is to convert a cycle into an electric bike by mounting a motor and controlling it through either a Raspberry Pi or an Arduino board.</p>
","motor electric"
"10490","Is it possible to use a LiPo charger as a lab bench power supply?","<p>I recently thought about building a lab bench power supply, it comes in cheaper and I love to build things...</p>

<p>But then I also have a LiPo charger an iMax B6AC, that I had bought for my quadcopter, then came the idea of whether I can use the charger as a lab bench power supply...</p>

<p>My questions is, could this work and how could I make it work?</p>
","power"
"10491","Examples of Zeno Behaviour in the Real World","<p>Zeno Behaviour or Zeno Phenomenon can be informally stated as the behavior of a system making an infinite number of jumps in a finite amount of time.</p>

<p>While this is an important Control system problem in ideal systems, can Zeno behavior exist in real systems? Any examples?<br>
If so, why don't noise or external factors deviate a system from achieving Zeno?</p>
","mobile-robot control"
"10492","Why don't cheap toy robotic arms move smoothly?","<p>Why don't cheap toy robotic arms like <a href=""https://www.youtube.com/watch?v=4xP9OtcGpUk"" rel=""nofollow"">this</a> move smoothly?  Why can't it even move itself smoothly, (even without any load)?</p>

<p>In other words - what do real industrial robotic arms have, that cheap toys don't?</p>
","robotic-arm"
"10502","What are good, low cost, actuators for a braille tablet to be controlled by arduino?","<p>I want to basically make a pin matrix controlled either by spring, electromagnets or small motors(spring being the most viable option), something like what’s shown in the image. I'm pretty new to arduino and hardware in general so any input would be appreciated.
I mostly know the arduino end but don't have clue about the hardware part. Plus I don't have the technical expertise, as in I know electromagnets won't be a good option as I have to control individual pins and not clusters. Plus springs have the disadvantage of pushing them back in but other than that a very option. And its not viable to have individual motors for so many pins.</p>

<p><a href=""http://i.stack.imgur.com/Pplxi.jpg"" rel=""nofollow""><img src=""http://i.stack.imgur.com/Pplxi.jpg"" alt=""enter image description here""></a></p>
","arduino motor electronics"
"10503","Running a cycle on brushless outrunner motors?","<p>Is it possible to convert a cycle into an electric bike by using brushless outrunner motors that usually are for RC planes, multicopter, helicopter, etc?</p>

<p>If it is possible, what specs do my motors need to be to provide enough power to bring my cycle to speed?</p>

<p>Will I need a gear system?</p>
","motor power electric"
"10504","How to properly calibrate a magnetometer in IMU for precise yaw?","<p>I'm using <a href=""https://www.sparkfun.com/products/10736"" rel=""nofollow"">Sparkfun Razor IMU 9DOF sensor</a> which incorporates accelerometer, gyroscope, and magnetometer, for giving the Euler's angles (yaw, pitch, and roll). I'm using the firmware at this <a href=""http://cdn.sparkfun.com/datasheets/Sensors/IMU/SF9DOF_AHRS.zip"" rel=""nofollow"">link</a>. It has Processing sketch for calibration of magnetometer, but it doesn't give the precise measurements. Especially, the yaw is imprecise. I'm using this sensor for measuring azimuth and altitude of stellar objects. The altitude is mostly correct, but azimuth (yaw) isn't. </p>

<p>I have several questions:</p>

<ul>
<li>Is there a better way to calibrate magnetometer? Is the calibration sufficient, without using the Madgwick or Kalman filter?</li>
<li>Is there some nonlinearity present in the sensor? Since the yaw offset isn't constant, it changes (around -12 degrees up north to almost correct value at southwest). And if it is, how could I measure that nonlinearity and apply to the yaw measurements?</li>
<li>If I have to use Madgwick or Kalman, do I have to apply them on quaternions? I believe that applying them at the final yaw measurements wouldn't do the job.</li>
</ul>
","kalman-filter imu calibration magnetometer precise-positioning"
"10505","Covariance Check?","<p>I have a localization data estimated and GPS_truth and generated [3x3] covariance matrix along with them. </p>

<p>What i would like to do is to see if the covariance is correct or not? </p>

<p>Can we check it by plotting the covariance? </p>
","localization slam visualization"
"10507","Do DH parameters change for a scaled robot 3d model?","<p>I have the actual <strong>DH parameters</strong> of a robot:</p>

<pre><code>d1 = 0.4865 m
d2 = 0.600 m
d3 = 0.065 m
a1 = 0.150 m
a2 = 0.475 m
</code></pre>

<p>all other di's and ai's are zero.</p>

<p>Can I use <strong><em>these</em></strong> for an inverse kinematics analytic closed form computation or should I measure the <strong><em>virtual distances</em></strong> in the 3d environment?</p>

<p>I am <em>actually</em> asking if the theta angles that will be yeld after the computations <strong>are dependent on the scale</strong> of those distances.</p>

<p>EDIT: Scale factor is unknown</p>
","inverse-kinematics dh-parameters 3d-model"
"10508","Building an RC Airbus A340","<p>I am planning to build a scale version of an Airbus A340 as the title suggests and I have a few questions with the build...</p>

<ol>
<li><p>Are there any template/plans for this build? If so please reply with the links...</p></li>
<li><p>Can I control the plane with a CC3D Revolution and fly it with ground station? If there are any ground telemetry application that will help me, please send me the link...</p></li>
<li><p>How do I make the landing gears? I don't really fancy spending above £50 (I live in London)... I want to make all my landing gears shock absorbing and I want to make the nose gear have a steering mechanism with servos...</p></li>
<li><p>How do I make the outer shell for the EDF motors? </p></li>
</ol>

<p>I am ready to spend as much time and effort with this build, I want the build to not be very costly, this is going to be a hobby. And I have also built a Quadcopter, so I hope I can apply that knowledge to this build...</p>
","servos"
"10511","Accelerometer, gyro, and magnetometer sensor fusion for material resource survey","<p>As a hardware engineer, I have studied quite a lot on sensor spec such as Accel, Gyro and Magnetometer including custom made fluxgate. I have studied matrix and quadarion (complex number) and so on.
I moving into calibration arena, I seen lot of article on calibration but not sure which is best solution to fix offset and mis-alignment axis. Can anyone point to best open source code.
I'm not interested in output results related to flight such as quadchopter and GPS, but more interested in directional math for drilling pipes, where toolface, inclination, azimuth and position is most important. What the best thesis or paper that cover this topic and open-source or example code (in C) for this application. Do I need Kalnman filter or such advance post data capture processing. Any tip how to avoid getting too involved with maths</p>
","control kalman-filter imu calibration precise-positioning"
"10514","Scale factor of a 3d robot model relative to the real measurements of a robot","<p>I have some measurements of a real life robot, and a 3d model of that robot (lets say in Unity) and I want to know the scale factor, plus I dont want to find the 3d models measurements and then divide with the real world ones to find the scale factor, in order not to get more confused with more mathematics than I am already. So, is there a methodology or will I have to do it as I mentioned?</p>
","3d-model"
"10515","Basic Components For Having a 'Follow Me' mode on Quad?","<p>I want to know what all essential components I need to have on a multirotor inorder to have a 'Follow Me' mode (With,me carrying a Device/Reference Piece to track).? </p>
","localization"
"10516","can't find ros package in kinetic driver-base","<p>I can't find the package, in the ros site , I saw it not maintainable.</p>

<p>How can i migrate the package from (jade/indigo) to became a package in kinetic?</p>

<p>That important because it's dependency for my quad-gazebo pacakge</p>
","ros"
"10517","Which kind of motors and how powerfull should they be for a robotic arm","<p>I am building a robotic arm with these specifications:</p>

<ul>
<li>1 meter long</li>
<li>approx. 1kg weight</li>
<li>it is made of 4 motors. (2 at the base. One for rotating the whole arm left and right and another one for rotating up down. 1 motor for rotating the second half of the arm, only up and down. 1 for the claw used for grabbing)</li>
<li>it must be able to lift at least 4kg + 1kg (it's own weight), and have a speed of 180 degrees in 2 seconds = 360 degrees in 1 second resulting 60rpm.</li>
</ul>

<p>Which kind of motor would be best for the project (servo or stepper) and how much torque will it need? (Please also give me an explanation of how I can calculate the torque needed). Could you also give me an example or two of the motors I would need and/or a gearbox for them (models or links).</p>


","motor robotic-arm stepper-motor servos torque"
"10519","Math for dynamic gait","<p>I'm researching dynamic gaits for bipedal robots. Can anyone recommend a reference that reviews and explains the math behind modern approaches?</p>
","walking-robot"
"10520","Once matching features are computed between a stereo pair, how can they be tracked?","<p>I am currently working on a SLAM-like application using a variable baseline stereo rig. Assuming I'm trying to perform visual SLAM using a stereo camera, my initialization routine would involve producing a point cloud of all 'good' features I detect in the first pair of images.   </p>

<p>Once this map is created and the cameras start moving, how do I keep 'track' of the original features that were responsible for this map, as I need to estimate my pose? Along with feature matching between the stereo pair of cameras, do I also have to perform matching between the initial set of images and the second set to see how many features are still visible (and thereby get their coordinates)? Or is there a more efficient way of doing this, through for instance, locating the overlap in the two points clouds?</p>
","slam computer-vision stereo-vision"
"10521","Help finding robot tracks","<p>I have a Robot with tracks. One of the tracks broke and I need to find a replacement, the tracks use the same plastic interconnects/pieces as this:</p>

<p><a href=""http://i.stack.imgur.com/ja4UA.jpg"" rel=""nofollow""><img src=""http://i.stack.imgur.com/ja4UA.jpg"" alt=""enter image description here""></a></p>

<p>They were very popular years back. Does anyone know the brand/name?</p>
","mobile-robot tracks"
"10523","Determining position from a 2D map and LIDAR","<p>We need to determine the 2D position of my robot. To do so, we have a LIDAR at a known high, with an horizontal plane, which gives us the distance to the nearest point for each angular degree (so 360 points for one rotation). The environment is known, so I have a map of the position of every object that the LIDAR is susceptible to hit.</p>

<p>My question is, based on the scatter plot that the LIDAR is returning, how can we retrieve the position of my robot in the map ? We would need the <code>x</code>, <code>y</code> position in the map frame and also the <code>theta</code> angle from the map frame to my robot frame.</p>

<p>We have tried to match objects on map with groups of points based on their distance between each other and by identifying those objects and the way the LIDAR ""sees"" them to retrieve the robot position. But it is still unsuccessful.</p>

<p>To put it in a nutshell, we want to make <code>SLAM</code> without the mapping part. How is it possible, and with what kind of algorithms ?</p>

<p>A first step could be to stop the robot while acquiring data if it seems easier to process.</p>
","localization lidar precise-positioning"
"10525","Shield IMU from magnetic interferences","<p>I experienced some drifting when coming near to magnetic fields with my IMU, so I wondered if it is possible to completely shield the IMU from external influences. Would this be theoretically possible or does the IMU rely on external fields like the earth magnetic field? Are there maybe alternatives to IMUs that are less susceptible to magnetic interferences? I only need the rotational data of the sensor and don't use translational output.</p>
","sensors imu rotation"
"10527","Is it a bad-design decision to implement high number of moving parts in an automation-robot?","<p>I'm currently designing an autonomous robotic system to manipulate clothes using computer vision and complex moving hardware. My current design incorporates quite a number of moving parts. My biggest worry is a frame (140 x 80 x 40 cm) rotates from 0 to 90 degrees every time it manipulates a piece of cloth. Other than this the design involves various other moving parts to achieve successful manipulation of the cloth. It seems like the hardware is capable of achieving the task despite the high number of complex and moving parts. </p>

<p>So the question is, what are the design considerations I should take in designing an automated system. Should I think of a alternative design with less number of parts? Or proceed with the current design if it does the job>? Sorry I am in a position where I can't disclose much information about the project. </p>

<p>Thank you.   </p>
","computer-vision automation"
"10533","How to make a simple Arduino Insect Robot?","<p>I want to make a simple Arduino based programmable insect robot.</p>

<p>I want it to walk on legs and legs will be made of hard aluminum wire. It needs to have 4 legs. I am planning to use Arduino Nano for that. I just had few questions like:</p>

<ol>
<li>How to arrange servos and wire to have motion?</li>
<li>I also want it to turn sideways?</li>
<li>Where can I read good theory on making insect like robots?</li>
</ol>
","arduino"
"10536","What is the difference between CC3D Revolution Mini and CC3D Revolution","<p>I recently came across this doubt... As the title suggests what's the difference between the two flight controller. They have a big price difference and size difference, that's all I know...</p>

<p>Do they all function the same way, so does the two flight controller differ in size?</p>

<p>Answers appreciated,</p>

<p>Sid</p>
","quadcopter microcontroller"
"10537","LIDAR Points as Landmarks","<p>I am currently trying to implement a GraphSLAM/SAM algorithm for LIDAR. From papers I've read, you generate a directed graph from expected LIDAR measurements to landmarks similar to the image below (taken from the Square Root SLAM Paper by Dellaert).</p>

<p><a href=""http://i.stack.imgur.com/qzmIS.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/qzmIS.png"" alt=""Smoothing and Mapping Dellaert""></a></p>

<p>However in practice the point clouds you obtain from LIDAR are similar to this (taken from the KITTI car collected dataset):
<a href=""http://i.stack.imgur.com/NnYra.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/NnYra.png"" alt=""enter image description here""></a></p>

<p>It seems algorithms such as SIFT for 3D point clouds aren't as accurate yet. Is there a commonly used technique to efficiently find features in consecutive point clouds to find landmarks for SLAM algorithms without using >30,000 points in a point cloud?</p>
","localization slam lidar"
"10544","Programmable Wheeled Vehicle","<p>Ok, this may be a simple question, but here goes. I would like to build some type wheeled vehicle that can sustain a tripod mount but also be programmable to follow a programmed path. However, I would like to be able to change this path as well. </p>

<p>This started with trying to mount my Ricoh Theta S on an RC car to create 3D video of a room. I'm really not familiar with any type of robotics but I'm convinced it's possible to create something better... More efficient if you will... I've looked at drones, but for what I would like to do I don't think I can find something as precise as I would like (I could be completely wrong). </p>

<p>Any guidance on this is extremely appreciated.</p>
","mobile-robot"
"10546","Is it possible to implement a robot that moves to spcific locations based on dynamic inputs?","<p>I'd like to build a robot that can move to different places(like different rooms) in a floor based on the input that i'm giving to it dynamically. I've surfed about it and i don't need a line follower. Except this, can you give me a way to implement this ??   Thanks in advance :) </p>
","wheeled-robot"
"10547","How to get live audio from robot?","<p>I am building a robot and I want to be able to hear sounds from it's environment (ideally from my laptop). What is the best way to get live audio from my robot's microphone to my computer?</p>

<p>I have looked into a few solutions for hosting live audio streams using packages such as <code>darkice</code> and <code>icecast</code>. I'm just wondering about better solutions for robotic applications.</p>

<p>Additional details:
- I have access to hardware such as Raspberry Pi, Arduino, etc.</p>
","real-time digital-audio"
"10549","Power solution for Raspberry Pi robot","<p>I am building a Pi car with 4 gear motors (190-250mAh each max). Now I want to use my 10000mAh USB power bank to power up raspberry pi along with the 4 gear motors.</p>

<p>I can power up the raspberry pi directly but I want to use my power bank as the only source of power for the Pi Car. How can I connect both my RPi and motor controller L298N to my USB power bank?</p>
","motor raspberry-pi power"
"10554","What types of motor should I use for a particular application?","<p>I want to create an amateur wire looping machine with Arduino, that has similar functionality than <a href=""https://www.youtube.com/watch?v=5xg7Z0KDzkg"" rel=""nofollow"">this</a> machine. I don't need the automatic wire feeding as for my purposes this part can be done manually. I just want to automate the wire loop creation process, assuming that I already have straight wires.</p>

<p>I'm new to the world of motors, robotics, etc., so please be as descriptive as possible :)</p>

<p>From the video, I can tell that there are two motors:</p>

<ol>
<li>Makes the initial wire bending</li>
<li>Spins to create the loop</li>
</ol>

<p>The wire that I will be working with is galvanized steel of 11 gauge (2.0 - 2.5 mm diameter).</p>

<p>So what type of motors would be recommended for this application, taking into account:</p>

<ul>
<li>They need to be accurate in their positioning for repeat ability</li>
<li>They need to have enough torque (specially the one that creates the loop itself) to work with this type of material</li>
<li>They're as fast as (or close to) the ones in the video</li>
<li>This is not going to be an industrial grade machine that will be running all the time</li>
<li>Ideally, they need to be not that expensive.. I don't want to be bankrupt by the end of this project :)</li>
<li>If links can be included for recommended products, it would be great.</li>
</ul>

<p>Thanks!</p>
","motor"
"10555","3 Axis Gimbal Stabilization System can replace with 3 Axis Accelerometer","<p>i have ""TAROT ZYX-GS 3-Axis Gimbal Stabilization System ZYX13"" sensor that gives me the value of Roll tilt and Pan.The 3 axis accelerometer give me the value of x y and z.so can we use the Gimbal stabiliztion system in place of accelerometer<a href=""http://i.stack.imgur.com/9uUyU.jpg"" rel=""nofollow""><img src=""http://i.stack.imgur.com/9uUyU.jpg"" alt=""enter image description here""></a></p>
","quadcopter sensors accelerometer"
"10556","Quadrature encoder signal from dc motor is very noisy","<p>I'm starting out with robotics, got my first DC gear motor with quadrature encoder (<a href=""https://www.pololu.com/product/2824"" rel=""nofollow"">https://www.pololu.com/product/2824</a>):</p>

<p><a href=""http://i.stack.imgur.com/sxUXp.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/sxUXp.png"" alt=""enter image description here""></a></p>

<p>I ultimately plan to hook it up to a motor driver connected to a Tiva Launchpad. However, since I'm a noob, and curious, I am starting by just playing with it with my breadboard, oscilloscope, and voltage source. E.g., when I plug in the motor power lines into my (variable) voltage source the axis spins nicely as expected between 1 and 12 V.</p>

<p>The problems start when I try to check how the encoder works. To do this, first I plug a a 5V source into the encoder GND/Vcc, and then try to monitor the encoder output.</p>

<p>While the motor is running, I check the Yellow (encoder A output) cable (referencing it to the green (encoder GND) cable).  <a href=""https://youtu.be/uxve5Ou4Qqg"" rel=""nofollow"">I made a video</a> that shows a representative output from one of the lines (no USB on my old oscilloscope so I took a video of it using my phone).</p>

<p>As you would see at the video, the output doesn't look anything like the beautiful square waves you typically see in the documentation. Instead, it is an extremely degraded noisy sin wave (at the correct frequency for the encoder). The amplitude of the sin is not constant, but changes drastically over time. Strangely, sometimes it ""locks in"" and looks like the ideal square wave, for about a second or two, but then it gets all wonky again.</p>

<p>Both of the lines (encoder A and B output) act this way, and they act this way at the same time (e.g., they will both lock in and square up at the same time, for those brief glorious moments of clarity). Both of my motors are the same, so I don't think it's that I have a bad motor.</p>

<p>I have also checked using Vcc=12V, but it made no difference other than changing the amplitude of the output.</p>

<p>Note I already posted this question at reddit:
<a href=""https://www.reddit.com/r/robotics/comments/502vjt/roboredditors_my_quadrature_encoder_output_is/"" rel=""nofollow"">https://www.reddit.com/r/robotics/comments/502vjt/roboredditors_my_quadrature_encoder_output_is/</a></p>
","motor quadrature-encoder"
"10561","Can I use a 3D gimbal system as a simplistic quadcopter IMU(3 axis accelerometer)?","<p>i have 3d gimbal system and i want to use this sensor in place of IMU(3 axsis accelerometer) in Quadcopter </p>
","quadcopter accelerometer gyroscope"
"10562","What is the torque transmission efficiency using a bycicle chain/setup for robot?","<p>For this robot the gear attached to the motor is linked to the gear attached to the wheels by a bicycle chain (I am using a bicycle wheel and transmission set as the parts for the robots movements).</p>

<p>How does using a bicycle chain affect the power transmission efficiency, how does this impact the torque?</p>
","mobile-robot torque gearing"
"10567","Is there any C++ library I could use to program a robotic manipulator involving forward and inverse kinematics?","<p>I came across robotics library (RL), but quite unclear about its real purpose. Is it a FK/IK solver library or simply an graphical simulator?. RL has poor documentation, so its not clear how to use it. Im looking for some C++ library where there APIs to solve FK/IF analytically. Thank you.</p>
","inverse-kinematics c++ forward-kinematics"
"10568","What types of actuators do these industrial bots use?","<p>I have a particular example robot that interests me:</p>

<p><a href=""http://www.scmp.com/tech/innovation/article/1829834/foxconns-foxbot-army-close-hitting-chinese-market-track-meet-30-cent"" rel=""nofollow"">http://www.scmp.com/tech/innovation/article/1829834/foxconns-foxbot-army-close-hitting-chinese-market-track-meet-30-cent</a></p>

<p>See first image, the bigger robot on the left, in particular the shoulder pitch joint that would support the most weight. I'm curious because I know it's really hard to balance strength and precision in these types of robots, and want to know how close a hobbyist could get.</p>

<p>Would they be something similar to this: rotary tables w/ worm gears?</p>

<p><a href=""http://www.velmex.com/products/Rotary_Tables/Motorized-Rotary-Tables.html"" rel=""nofollow"">http://www.velmex.com/products/Rotary_Tables/Motorized-Rotary-Tables.html</a> </p>

<p>Looking for more actuation schemes to research, thanks!</p>
","motor robotic-arm actuator torque"
"10573","Technique to increase POV resolution","<p>I have thought of a technique to increase the resolution of a POV (persistence of vision) display. In an usual POV display, the LEDs are arranged in a strip and spun in a circle. There are two limiting factors to increasing the radial resolution along the circumference of any one circular path that an LED follows. One is, depending on the speed of the POV wheel, the minimum time required (decided by the microcontroller) to change the LED's color in case of a RGB. The other factor is the LED's width, that increases the 'bleeding' of color from one pixel to the neighboring pixel if the LED changes color or brightness too fast. </p>

<p>If one were to fix a slit in the front of an LED, |*|  &lt;-- like so, would this help improve the resolution of the POV display; by doing this one would in effect be reducing the width of the 'pixel' along the circumference on which the led would be traversing.</p>

<p>Thus if one were to use a fast enough microcontroller and a narrow enough slit, one could probably obtain a very high resolution along one dimension at least.</p>

<p>To be clear I've not yet implemented this, and am just looking for any experienced person who can tell if this will work or not.</p>
","microcontroller electronics"
"10580","How can I upload sketches to an Arduino over a raspberry pi?","<p>I am doing robotics project on Raspberry pi and Arduino. The Arduino UNO is connected to raspberry pi. I am using the raspberry pi in Putty (SSH) now. </p>

<p>I want to setup user interface for raspberry pi also most importantly i want to use Arduino IDE to work and load Arduino sketch into system. How to do this?</p>
","arduino raspberry-pi embedded-systems first-robotics linux"
"10581","EKF SLAM 2d laser scanned datasets usage","<p>How to understand the 2d laser scanner scanned data and use it in the implementation of ekf slam, if someone can provide an implementation of EKF SLAM in python with pseudo datasets.</p>
","slam ekf first-robotics"
"10584","Communication in SWARM robotics","<p>Hey so I am trying to research into SWARM robotics, and trying to find helpful information or even articles/papers to read on the process of setting up communication protocols between different robots. For instance using a LAN connection, does each robot need to have a wireless adapter, and how would I begin setting up a network for say 5-10 smaller robots?</p>

<p>More generally could someone help me understand how devices connect and communicate across networks? I understand the basics of IP addressing, but I haven't researched into further complexities.</p>

<p>Any advice or direction is appreciated.</p>
","wireless"
