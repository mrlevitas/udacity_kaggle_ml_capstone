# Machine Learning Engineer Nanodegree
## Capstone Proposal
Roman Levitas
February 2017

## Proposal

### Domain Background

The field chosen for this project is a popular one in the machine learning community but with a twist.
The aim of this investigation will be to build a text classifier.
The twist is that the classifier will train on seemingly unrelated topics to those that it will be tested on.

The data to be analyzed is a subset of the Stack Exchange Data Dump [1] published on December 15, 2016 focusing on 6 areas in particular: biology, cooking, cryptography, diy, robotics, and travel.
The data contains the titles, text, and tags of Stack Exchange questions in each of the 6 respective fields in the form of a comma separated value (CSV) document.

The history of Stack Exchange's question-and-answer format dates back to as recent as 2008 when StackOverflow was created, the originating leg of the Stack Exchange network which allows users to pose questions on the topic of computer science/software development and have answers formulated by other users.
Data aggregated through an internet forum allows knowledge to be crowdsourced and also provides an opportunity as a training set in a text processing model.

Previous online forums meant for sharing knowledge include Yahoo! Answers which launched in 2005, however it showed notable drawback in user community and credibility. Stack Exchange has seen success in avoiding such issues and thus an export of their data is an interesting subject to study.
Their success is in part due to performing text analysis on a user's questions/answers in order to predict questions that users may be interested in answering themselves--a non-trivial machine learning problem as well. [2]

[1] https://archive.org/details/stackexchange
[2] https://kevinmontrose.com/2013/05/22/your-future-on-stack-overflow/

### Problem Statement
The problem at hand is defined by the Kaggle team's competition titled: 'Transfer Learning on Stack Exchange Tags' [3] which aims to "Predict tags from models trained on unrelated topics". Specifically, predicting tags for physics questions after training our model on the questions provided in the 6 different fields mentioned above. This is an example of a classification problem.
Predictions can be compared against the physics questions' actual tags and thus the model can be ranked on the correctness of its categorization.

[3] https://www.kaggle.com/c/transfer-learning-on-stack-exchange-tags

### Datasets and Inputs
The dataset is 6 separate CSV files for biology, cooking, cryptography, diy, robotics, and travel [4]. Each row of the data contains a unique id, the title, text, and tags of questions, all as strings. The export of the data from StackExchange supported html markdown for the text column so our analysis will have to take into account seeing links, formatting, and html tags.

The training set contains ~25,000 entries for diy, ~8,600 for biology, ~10,400 for cryptography, ~2,700 for robotics, ~12,000 for travel, ~15,000 for cooking.
The id's are not sequential and do not match the row number necessarily but they are not unique across the different files either (i.e. id 3 shows up in nearly each file).
A new mapping of ids might be helpful if combining all 6 files into one pool.

The testing data has ~82,000 entries on physics.

The tags (labels) are generated by the user community so it worthwhile investigating their spread independently.


[4] https://www.kaggle.com/c/transfer-learning-on-stack-exchange-tags/data

### Solution Statement
The solution to the problem posed by the Kaggle competition would be to build a classifier and train it on the dataset of 6 fields and see if it can give reasonable assignment of tags on the testing set which is in a different, 7th, field--physics.
Underlying in this approach is the presumption that physics is the unifying thread, but the creators of the competition confess their facetiousness by stating the 6 different areas "all were chosen randomly by a scheming Kaggle employee for a twisted transfer learning competition".
So the investigation is itself an exercise into a problem without a definite answer but if there is a correlation that can be found, it can certainly shed light and illuminate the grey area in question: Is Physics at the heart of eveything?

To begin, the data set will have to be tamed by reducing the text of it's markdown properties.
Classic NLP approaches will be explored, such as removing stopwords, converting case to a convention (lower), removing punctuation, and transforming the titles, text, and tags into data structures that allow for easier processing.
Training a classifier on text brings to mind supervised learning algorithms such as k-nearest-neighbor (kNN), Support Vector Machine (SVM), and Decision Trees.
All 3 of these are worthy candidates and will be explored, optimized, and applied in order to seek the optimal potential correlation.
Bagging and boosting can be considered if the F1 score of the classifiers remains too low.

In order to gauge the general health of the model, the training sets can be be used to validate the model using cross validation train/test splits.

### Benchmark Model
The benchmark model for the classifier will be running the model on unseen physics questions and analyzing their text and title in order to predict tags (actual tags are provided for testing). The results can be evaluated as true positives (tp) and false positives (fp)--together all predicted positives, and true positives with false negatives (fn)--all actual positives.
These categorizations allow for empirical analysis of our model and can be compared across implementations of classification algorithms.

The top ten F1 scores of the competition leaderboard range between 0.2 - 0.29 (excluding an outlier in first place with a score of .67).
To be able to predict tags with an F1 score above 0.2 would be satisfactory, above 0.25 would be great, and above 0.3 would be outstanding.


### Evaluation Metrics
The evaluation metric for this competition is Mean F1-Score [5]. The F1 score measures accuracy using the statistics precision p and recall r. Precision is the ratio of true positives (tp) to all predicted positives (tp + fp). Recall is the ratio of true positives to all actual positives (tp + fn).

The F1 score is given by:
  F1 = 2pr/p+r
where
  p=tp/tp+fp  and  r=tp/tp+fn


In the multi-class and multi-label case, this is the weighted average of the F1 score of each class. [6]

The classifier with the highest F1 score will be selected to compete.

[5] https://www.kaggle.com/c/transfer-learning-on-stack-exchange-tags#evaluation
[6] http://scikit-learn.org/stable/modules/generated/sklearn.metrics.f1_score.html


### Project Design
The proposed project will be to investigate the potential correlation of physics to 6 other areas and see if the knowledge can be generalized. The correlation may be low due to a lack of cross over in the fields or due to a weakness in the algorithm finding the correlation. The latter can be optimized to make sure that it is not the failing link.

Before getting started, it would be wise to examine the provided data and possibly strip it of its HTML formatting so as to prevent analyzing <p></p> tags as something meaningful and apply NLP techniques mentioned in the Solution Statement.

Further, the 3 various classification algorithms to be explored (kNN, SVMs, Decision Trees) provide variety in how they utilize the data--some more prone to over or under fitting or have parameters that can be fine tuned.
For example, for decision trees, pruning can be used to avoid overfitting. For kNN, the value of k should be reasonable for the provided data. GridSearchCV will be utilized in addressing this.

Each of the trained models will be run against a test CSV dataset which has physics questions and their corresponding tags in order to calculate the F1 score in accordance to evaluation metrics and the model's tag predictions. In the case that correlation is found to be extremely weak in testing, optimized tagging classifiers will still have been developed that can run independently in their own field of knowledge.

A theory exists stating that any 2 people can be connected through 6 traversals on a graph and is known as 6 degrees of Separation [7].
Can the same be applied to 6 random topics and have physics be the common connection between them all?
This investigation sets to find out.

[7] https://en.wikipedia.org/wiki/Six_degrees_of_separation
