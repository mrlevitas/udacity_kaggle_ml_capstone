{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "importing biology\n",
      "importing crypto\n",
      "importing robotics\n",
      "importing diy\n",
      "importing cooking\n",
      "importing travel\n",
      "importing physics test set\n"
     ]
    }
   ],
   "source": [
    "# Data Import\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O\n",
    "from time import time \n",
    "\n",
    "print \"importing biology\"\n",
    "biology = pd.read_csv(\"data/biology.csv\")\n",
    "print \"importing crypto\"\n",
    "crypto = pd.read_csv(\"data/crypto.csv\")\n",
    "print \"importing robotics\"\n",
    "robotics = pd.read_csv(\"data/robotics.csv\")\n",
    "print \"importing diy\"\n",
    "diy = pd.read_csv(\"data/diy.csv\")    \n",
    "print \"importing cooking\"\n",
    "cooking = pd.read_csv(\"data/cooking.csv\")\n",
    "print \"importing travel\"\n",
    "travel = pd.read_csv(\"data/travel.csv\")\n",
    "    \n",
    "print \"importing physics test set\"\n",
    "test = pd.read_csv(\"data/test.csv\") \n",
    "\n",
    "df_hash = {\n",
    "    \"cooking\": cooking,\n",
    "    \"crypto\": crypto,\n",
    "    \"robotics\": robotics,\n",
    "    \"biology\": biology,\n",
    "    \"travel\": travel,\n",
    "    \"diy\": diy,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['id' 'title' 'content' 'tags']\n",
      "id                                                         1\n",
      "title      What is the criticality of the ribosome bindin...\n",
      "content    <p>In prokaryotic translation, how critical fo...\n",
      "tags       ribosome binding-sites translation synthetic-b...\n",
      "Name: 0, dtype: object\n",
      "id                                                         2\n",
      "title      How is RNAse contamination in RNA based experi...\n",
      "content    <p>Does anyone have any suggestions to prevent...\n",
      "tags                                        rna biochemistry\n",
      "Name: 1, dtype: object\n",
      "id                                                         3\n",
      "title          Are lymphocyte sizes clustered in two groups?\n",
      "content    <p>Tortora writes in <em>Principles of Anatomy...\n",
      "tags                      immunology cell-biology hematology\n",
      "Name: 2, dtype: object\n",
      "\n",
      "Number of rows by topic\n",
      "biology: 13196\n",
      "cooking: 15404\n",
      "travel: 19279\n",
      "robotics: 2771\n",
      "crypto: 10432\n",
      "diy: 25918\n"
     ]
    }
   ],
   "source": [
    "# Data Exploration\n",
    "print df_hash['biology'].columns.values\n",
    "\n",
    "print df_hash['biology'].iloc[0]\n",
    "print df_hash['biology'].iloc[1]\n",
    "print df_hash['biology'].iloc[2]\n",
    "\n",
    "print \"\\nNumber of rows by topic\"\n",
    "for topic, df in df_hash.iteritems():\n",
    "    print topic + \": \" +  str(len(df.index))\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "# When running this notebook for first time,\n",
    "# uncomment command below, select \"stopwords\" in gui, and follow prompt to download\n",
    "\n",
    "# nltk.download()\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# In Python, searching a set is much faster than searching a list, so convert the stop words to a set\n",
    "stops = set(stopwords.words(\"english\"))  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://www.kaggle.com/c/word2vec-nlp-tutorial#part-1-for-beginners-bag-of-words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "def separate_tags(tags):\n",
    "    return tags.split(\" \")\n",
    "\n",
    "def clean_data(raw_data):\n",
    "    if raw_data:\n",
    "        # remove html tags & code snippets\n",
    "        soup = BeautifulSoup(raw_data, \"html.parser\")\n",
    "\n",
    "        [s.extract() for s in soup(['pre', 'code'])]\n",
    "            \n",
    "        question_text = soup.get_text()\n",
    "        \n",
    "        # remove everything but letters\n",
    "        letters_only = re.sub(\"[^a-zA-Z]\", \" \", question_text) \n",
    "            \n",
    "        # normalize case\n",
    "        words = letters_only.lower().split()   \n",
    "\n",
    "        # remove stopwords         \n",
    "        meaningful_words = [w for w in words if not w in stops] \n",
    "        \n",
    "        # remove permutations of the same word by reducing it to its stem\n",
    "        wordnet_lemmatizer = WordNetLemmatizer()\n",
    "        meaningful_word_stems = map(lambda x: wordnet_lemmatizer.lemmatize(x) , meaningful_words)\n",
    "        return( \" \".join( meaningful_word_stems )) \n",
    "    else:\n",
    "        return \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Data Preprocessing\n",
    "\n",
    "# training set\n",
    "for topic, df in df_hash.iteritems():\n",
    "    df.content = df.content.apply(lambda x: clean_data(x) )\n",
    "    df.title = df.title.apply(lambda x: clean_data(x) )\n",
    "    df.tags = df.tags.apply(lambda y: separate_tags(y) )\n",
    "    # drop rows without tags\n",
    "    df.tags.replace(['', ' ', 'untagged'], np.nan, inplace=True)\n",
    "    df.dropna(subset=['tags'], inplace=True)\n",
    "    \n",
    "\n",
    "# testing set\n",
    "test.content = test.content.apply(lambda x: clean_data(x) )\n",
    "test.title = test.title.apply(lambda x: clean_data(x) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def top_tfidf_feats(row, features, top_n=20):\n",
    "    ''' Get top n tfidf values in row and return them with their corresponding feature names.'''\n",
    "    topn_indicies = np.argsort(row)[::-1][:top_n]\n",
    "    top_feats = [(features[i], row[i]) for i in topn_indicies]\n",
    "    df = pd.DataFrame(top_feats)\n",
    "    df.columns = ['feature', 'tfidf']\n",
    "    return df\n",
    "\n",
    "def plot_tfidf_classfeats_h(dfs):\n",
    "    ''' Plot the data frames returned by the function plot_tfidf_classfeats(). '''\n",
    "\n",
    "    x = np.arange(len(dfs[\"feature\"]))\n",
    "    fig = plt.figure(figsize=(12, 14), facecolor=\"w\")\n",
    "\n",
    "    ax = fig.add_subplot(111)\n",
    "    ax.spines[\"top\"].set_visible(False)\n",
    "    ax.spines[\"right\"].set_visible(False)\n",
    "    ax.set_frame_on(False)\n",
    "    ax.get_xaxis().tick_bottom()\n",
    "    ax.get_yaxis().tick_left()\n",
    "    ax.set_xlabel(\"Mean Tf-Idf Score\", labelpad=16, fontsize=14)\n",
    "    ax.set_title( str(dfs.label), fontsize=16)\n",
    "    ax.barh(x, dfs[\"tfidf\"], align='center', color='#3F5D7D')\n",
    "    ax.set_yticks(x)\n",
    "    ax.set_ylim([-1, len(x) +1])\n",
    "    ax.set_xlim([0, 7])\n",
    "    yticks = ax.set_yticklabels(dfs[\"feature\"])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from wordcloud import WordCloud\n",
    "\n",
    "def print_wordcloud(dfs):\n",
    "    text = ' '.join(str(x) for x in dfs.tags.values) \n",
    "    \n",
    "    wordcloud = WordCloud(font_path='/Library/Fonts/Verdana.ttf',\n",
    "                          relative_scaling = 1.0, max_words=100,\n",
    "                          ).generate(text)\n",
    "    plt.imshow(wordcloud)\n",
    "    plt.axis(\"off\")\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "biology\n",
      "Finding terms with top tf-idf from question's titles\n",
      "Finding most popular tags\n",
      "cooking\n",
      "Finding terms with top tf-idf from question's titles\n",
      "Finding most popular tags\n",
      "travel\n",
      "Finding terms with top tf-idf from question's titles\n",
      "Finding most popular tags\n",
      "robotics\n",
      "Finding terms with top tf-idf from question's titles\n",
      "Finding most popular tags\n",
      "crypto\n",
      "Finding terms with top tf-idf from question's titles\n",
      "Finding most popular tags\n",
      "diy\n",
      "Finding terms with top tf-idf from question's titles\n",
      "Finding most popular tags\n"
     ]
    }
   ],
   "source": [
    "# Data Visualization\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "vect_hash = {}\n",
    "\n",
    "for topic, df in df_hash.iteritems():\n",
    "    if topic == \"test\":\n",
    "        next\n",
    "    else:\n",
    "        vect_hash[topic] = TfidfVectorizer( lowercase=True , stop_words=\"english\" ,min_df=.01, max_df= .95 ) \n",
    "        X = vect_hash[topic].fit_transform(df.title)\n",
    "\n",
    "        dfs = top_tfidf_feats( vect_hash[topic].idf_ , vect_hash[topic].get_feature_names()  ) \n",
    "        dfs.label = topic\n",
    "        print topic\n",
    "        print \"Finding terms with top tf-idf from question's titles\"\n",
    "#         plot_tfidf_classfeats_h(dfs)\n",
    "        print \"Finding most popular tags\"\n",
    "#         print_wordcloud(df)\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "frames = []\n",
    "for topic, df in df_hash.iteritems():\n",
    "    frames.append(df)\n",
    "\n",
    "df_all = pd.concat(frames)\n",
    "# print df_all.tags\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "    \n",
    "class Selector(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, key):\n",
    "        self.key = key\n",
    "\n",
    "    def fit(self, x, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, data_dict):\n",
    "        return data_dict[self.key]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Implementation\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.pipeline import Pipeline, FeatureUnion\n",
    "\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.metrics import classification_report, f1_score, make_scorer, accuracy_score, confusion_matrix\n",
    "\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "parameters = {\n",
    "    \"union__title__tfidf__min_df\" : [ 0.001],\n",
    "    \"union__title__tfidf__max_df\" : [0.9],\n",
    "    \"union__content__tfidf__min_df\" : [ 0.001],\n",
    "    \"union__content__tfidf__max_df\" : [0.9],\n",
    "    \"DT__estimator__max_depth\" : [25],\n",
    "    \"DT__estimator__criterion\" : [\"gini\"],\n",
    "    \"union__transformer_weights\" :  [{\"title\": 0.6, \"content\": 0.4}]\n",
    "}\n",
    "\n",
    "steps = [('union', FeatureUnion(\n",
    "            transformer_list=[\n",
    "                # Pipeline for tfidf vectorization of the question's title\n",
    "                ('title', Pipeline([\n",
    "                    ('selector', Selector(key='title')),\n",
    "                    ('tfidf', TfidfVectorizer(lowercase=True, stop_words=\"english\") )\n",
    "                ])),\n",
    "                # Pipeline for tfidf vectorization of the question's content\n",
    "                ('content', Pipeline([\n",
    "                    ('selector', Selector(key='content')),\n",
    "                    ('tfidf', TfidfVectorizer(lowercase=True, stop_words=\"english\") )\n",
    "                ]))\n",
    "            ])),\n",
    "        (\"DT\", OneVsRestClassifier(DecisionTreeClassifier( random_state = 42)))]\n",
    "\n",
    "pipeline = Pipeline(steps)\n",
    "\n",
    "mlb = MultiLabelBinarizer()\n",
    "Y =  pd.DataFrame(mlb.fit_transform(df_all.tags) )\n",
    "\n",
    "t0 = time()\n",
    "clf = GridSearchCV(pipeline, parameters, cv=3, scoring='f1_weighted')\n",
    "clf.fit( df_all, Y )\n",
    "print(\"done in %0.3fs\" % (time() - t0))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "# print clf.best_estimator_\n",
    "print(\"Best score: %0.3f\" % clf.best_score_)\n",
    "print(\"Best parameters set:\")\n",
    "best_parameters = clf.best_estimator_.get_params()\n",
    "# print best_parameters\n",
    "print \"\\n\\n\"\n",
    "for param_name in sorted(parameters.keys()):\n",
    "    print(\"\\t%s: %r\" % (param_name, best_parameters[param_name]))\n",
    "\n",
    "    \n",
    "# for param_name in sorted(pipeline.get_params().keys()):\n",
    "#     print param_name\n",
    "# print clf.cv_results_.keys()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print \"testing/predicting\"\n",
    "t0 = time()\n",
    "y_prediction =  clf.predict(test)\n",
    "print(\"done in %0.3fs\" % (time() - t0))\n",
    "print y_prediction.shape\n",
    "print type(y_prediction[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "Y_inv = mlb.inverse_transform(y_prediction)\n",
    "# print Y_inv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# index = 1\n",
    "# y = mlb.inverse_transform(Y)\n",
    "# for actual, pred in zip(test.tags, Y_inv):\n",
    "#         print('index: {0} :: {1} => {2}'.format(index, actual, ', '.join(pred)))\n",
    "#         index = index +1\n",
    "\n",
    "df_out = pd.DataFrame(columns=['id', 'tags'])\n",
    "\n",
    "for y, i in zip(Y_inv, range(len(Y_inv))):\n",
    "    temp_hash = {'id': int(i+1),\n",
    "            'tags': ' '.join(map(lambda x:  \"\".join(x) , y))}\n",
    "    df_out = df_out.append(temp_hash,ignore_index=True)\n",
    "\n",
    "    \n",
    "df_out.id = df_out.id.astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "df_out.to_csv(\"test_out.csv\", index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://www.civisanalytics.com/blog/workflows-in-python-using-pipeline-and-gridsearchcv-for-more-compact-and-comprehensive-code/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# f1_score(Y_physics, y_prediction, average='weighted')"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
