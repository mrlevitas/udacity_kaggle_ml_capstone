{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "importing biology\n",
      "importing crypto\n",
      "importing robotics\n",
      "importing diy\n",
      "importing cooking\n",
      "importing travel\n",
      "importing physics test set\n"
     ]
    }
   ],
   "source": [
    "# Data Import\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O\n",
    "from time import time \n",
    "\n",
    "print \"importing biology\"\n",
    "biology = pd.read_csv(\"data/biology.csv\")\n",
    "print \"importing crypto\"\n",
    "crypto = pd.read_csv(\"data/crypto.csv\")\n",
    "print \"importing robotics\"\n",
    "robotics = pd.read_csv(\"data/robotics.csv\")\n",
    "print \"importing diy\"\n",
    "diy = pd.read_csv(\"data/diy.csv\")    \n",
    "print \"importing cooking\"\n",
    "cooking = pd.read_csv(\"data/cooking.csv\")\n",
    "print \"importing travel\"\n",
    "travel = pd.read_csv(\"data/travel.csv\")\n",
    "    \n",
    "print \"importing physics test set\"\n",
    "test = pd.read_csv(\"data/test.csv\") \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_hash = {\n",
    "    \"cooking\": cooking,\n",
    "    \"crypto\": crypto,\n",
    "    \"robotics\": robotics,\n",
    "    \"biology\": biology,\n",
    "    \"travel\": travel,\n",
    "    \"diy\": diy,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['id' 'title' 'content' 'tags']\n",
      "   id                                              title  \\\n",
      "0   1  What is the criticality of the ribosome bindin...   \n",
      "1   2  How is RNAse contamination in RNA based experi...   \n",
      "2   3      Are lymphocyte sizes clustered in two groups?   \n",
      "3   4  How long does antibiotic-dosed LB maintain goo...   \n",
      "4   5        Is exon order always preserved in splicing?   \n",
      "\n",
      "                                             content  \\\n",
      "0  <p>In prokaryotic translation, how critical fo...   \n",
      "1  <p>Does anyone have any suggestions to prevent...   \n",
      "2  <p>Tortora writes in <em>Principles of Anatomy...   \n",
      "3  <p>Various people in our lab will prepare a li...   \n",
      "4  <p>Are there any cases in which the splicing m...   \n",
      "\n",
      "                                                tags  \n",
      "0  ribosome binding-sites translation synthetic-b...  \n",
      "1                                   rna biochemistry  \n",
      "2                 immunology cell-biology hematology  \n",
      "3                                       cell-culture  \n",
      "4            splicing mrna spliceosome introns exons  \n",
      "\n",
      "Number of rows by topic\n",
      "biology: 13196\n",
      "cooking: 15404\n",
      "travel: 19279\n",
      "robotics: 2771\n",
      "crypto: 10432\n",
      "diy: 25918\n",
      "\n",
      "total number of questions:\n",
      "87000\n",
      "testing set data shape:\n",
      "(81926, 3)\n"
     ]
    }
   ],
   "source": [
    "# Data Exploration\n",
    "\n",
    "print df_hash['biology'].columns.values\n",
    "\n",
    "print biology.head(5)\n",
    "\n",
    "print \"\\nNumber of rows by topic\"\n",
    "counter = 0\n",
    "\n",
    "for topic, df in df_hash.iteritems():\n",
    "    topic_length = len(df.index)\n",
    "    print topic + \": \" +  str(topic_length)\n",
    "    counter += topic_length\n",
    "\n",
    "print \"\\ntotal number of questions:\"\n",
    "print counter\n",
    "\n",
    "print \"testing set data shape:\"\n",
    "print test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "# When running this notebook for first time,\n",
    "# uncomment command below, select \"stopwords\" in gui, and follow prompt to download\n",
    "\n",
    "# nltk.download()\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# In Python, searching a set is much faster than searching a list, so convert the stop words to a set\n",
    "stops = set(stopwords.words(\"english\"))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Data Preprocessing methods\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "def separate_tags(tags):\n",
    "    return tags.split(\" \")\n",
    "\n",
    "def clean_data(raw_data):\n",
    "    if raw_data:\n",
    "        # remove html tags & code snippets\n",
    "        soup = BeautifulSoup(raw_data, \"html.parser\")\n",
    "\n",
    "        [s.extract() for s in soup(['pre', 'code'])]\n",
    "            \n",
    "        question_text = soup.get_text()\n",
    "        \n",
    "        # remove everything but letters\n",
    "        letters_only = re.sub(\"[^a-zA-Z]\", \" \", question_text) \n",
    "            \n",
    "        # normalize case\n",
    "        words = letters_only.lower().split()   \n",
    "\n",
    "        # remove stopwords         \n",
    "        meaningful_words = [w for w in words if not w in stops] \n",
    "        \n",
    "        # remove permutations of the same word by reducing it to its stem\n",
    "        wordnet_lemmatizer = WordNetLemmatizer()\n",
    "        meaningful_word_stems = map(lambda x: wordnet_lemmatizer.lemmatize(x) , \n",
    "                                    meaningful_words)\n",
    "        return( \" \".join( meaningful_word_stems )) \n",
    "    else:\n",
    "        return \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Data Preprocessing\n",
    "\n",
    "# training set\n",
    "for topic, df in df_hash.iteritems():\n",
    "    df.content = df.content.apply(lambda x: clean_data(x) )\n",
    "    df.title = df.title.apply(lambda x: clean_data(x) )\n",
    "    df.tags = df.tags.apply(lambda y: separate_tags(y) )\n",
    "    # drop rows without tags\n",
    "    df.tags.replace(['', ' ', 'untagged'], np.nan, inplace=True)\n",
    "    df.dropna(subset=['tags'], inplace=True)\n",
    "    \n",
    "\n",
    "# testing set\n",
    "test.content = test.content.apply(lambda x: clean_data(x) )\n",
    "test.title = test.title.apply(lambda x: clean_data(x) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Data Visualization methods\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def top_tfidf_feats(row, features, top_n=20):\n",
    "    # Get top n tfidf values in row and return them with their corresponding feature names\n",
    "    topn_indicies = np.argsort(row)[::-1][:top_n]\n",
    "    top_feats = [(features[i], row[i]) for i in topn_indicies]\n",
    "    df = pd.DataFrame(top_feats)\n",
    "    df.columns = ['feature', 'tfidf']\n",
    "    return df\n",
    "\n",
    "def plot_tfidf_feats(dfs):\n",
    "    x = np.arange(len(dfs[\"feature\"]))\n",
    "    fig = plt.figure(figsize=(12, 14), facecolor=\"w\")\n",
    "\n",
    "    ax = fig.add_subplot(111)\n",
    "    ax.spines[\"top\"].set_visible(False)\n",
    "    ax.spines[\"right\"].set_visible(False)\n",
    "    ax.set_frame_on(False)\n",
    "    ax.get_xaxis().tick_bottom()\n",
    "    ax.get_yaxis().tick_left()\n",
    "    ax.set_xlabel(\"Mean Tf-Idf Score\", labelpad=16, fontsize=14)\n",
    "    ax.set_title( str(dfs.label), fontsize=16)\n",
    "    ax.barh(x, dfs[\"tfidf\"], align='center', color='#3F5D7D')\n",
    "    ax.set_yticks(x)\n",
    "    ax.set_ylim([-1, len(x) +1])\n",
    "    ax.set_xlim([0, 7])\n",
    "    yticks = ax.set_yticklabels(dfs[\"feature\"])\n",
    "    plt.show()\n",
    "    fig_file_name = str(dfs.label) + '_tfidf.png'\n",
    "    plt.savefig(fig_file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Data Visualization methods II\n",
    "\n",
    "from wordcloud import WordCloud\n",
    "\n",
    "def print_wordcloud(dfs):\n",
    "    text = ' '.join(str(x) for x in dfs.tags.values) \n",
    "    \n",
    "    wordcloud = WordCloud(font_path='/Library/Fonts/Verdana.ttf',\n",
    "                          relative_scaling = 1.0, max_words=100,\n",
    "                          ).generate(text)\n",
    "    plt.imshow(wordcloud)\n",
    "    plt.axis(\"off\")\n",
    "    plt.show()\n",
    "    fig_file_name = str(dfs.label) + '_wordcloud.png'\n",
    "    plt.savefig(fig_file_name, bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Data Visualization\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "vect_hash = {}\n",
    "\n",
    "for topic, df in df_hash.iteritems():\n",
    "    vect_hash[topic] = TfidfVectorizer( lowercase=True , stop_words=\"english\" ,min_df=.01, max_df= .95 ) \n",
    "    X = vect_hash[topic].fit_transform(df.title)\n",
    "\n",
    "    dfs = top_tfidf_feats( vect_hash[topic].idf_ , vect_hash[topic].get_feature_names()  ) \n",
    "    dfs.label = topic\n",
    "    df.label = topic\n",
    "    print topic\n",
    "    print \"Finding terms with top tf-idf from question's titles\"\n",
    "    plot_tfidf_feats(dfs)\n",
    "    print \"Finding most popular tags\"\n",
    "    print_wordcloud(df)        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# combine dataframes into one\n",
    "\n",
    "frames = []\n",
    "for topic, df in df_hash.iteritems():\n",
    "    frames.append(df)\n",
    "\n",
    "df_all = pd.concat(frames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# subset dataframes by key (content or title)\n",
    "\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "    \n",
    "class Selector(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, key):\n",
    "        self.key = key\n",
    "\n",
    "    def fit(self, x, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, data_dict):\n",
    "        return data_dict[self.key]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Implementation\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.pipeline import Pipeline, FeatureUnion\n",
    "\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.metrics import classification_report, f1_score, make_scorer, accuracy_score, confusion_matrix\n",
    "\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "parameters = {\n",
    "    \"union__title__tfidf__min_df\" : [ 0.001, 0.005, 0.01 , 0.05],\n",
    "    \"union__title__tfidf__max_df\" : [0.9, 0.95 , 0.975],\n",
    "    \"union__content__tfidf__min_df\" : [ 0.001, 0.005, 0.01 , 0.05],\n",
    "    \"union__content__tfidf__max_df\" : [0.9, 0.95 , 0.975],\n",
    "    \"DT__estimator__max_depth\" : [5, 10, 25, 30],\n",
    "    \"DT__estimator__criterion\" : [\"gini\", \"entropy\"],\n",
    "    \"union__transformer_weights\" :  [{\"title\": 0.6, \"content\": 0.4},\n",
    "                                    {\"title\": 0.75, \"content\": 0.25}]\n",
    "}\n",
    "steps = \\\n",
    "    [('union', FeatureUnion(\n",
    "        transformer_list=[\n",
    "            # Pipeline for tfidf vectorization of the question's title\n",
    "            ('title', Pipeline([\n",
    "                ('selector', Selector(key='title')),\n",
    "                ('tfidf', TfidfVectorizer(lowercase=True, \n",
    "                                          stop_words=\"english\") )\n",
    "            ])),\n",
    "            # Pipeline for tfidf vectorization of the question's content\n",
    "            ('content', Pipeline([\n",
    "                ('selector', Selector(key='content')),\n",
    "                ('tfidf', TfidfVectorizer(lowercase=True,\n",
    "                                          stop_words=\"english\") )\n",
    "            ]))\n",
    "        ])),\n",
    "    (\"DT\", OneVsRestClassifier(DecisionTreeClassifier( random_state = 42)))]\n",
    "\n",
    "\n",
    "\n",
    "mlb = MultiLabelBinarizer()\n",
    "Y =  pd.DataFrame(mlb.fit_transform(df_all.tags) )\n",
    "\n",
    "t0 = time()\n",
    "pipeline = Pipeline(steps)\n",
    "clf = GridSearchCV(pipeline, parameters, cv=3, scoring='f1_weighted')\n",
    "clf.fit( df_all, Y )\n",
    "print(\"done in %0.3fs\" % (time() - t0))\n",
    "\n",
    "# referenced for pipeline and parameter construction:\n",
    "# www.civisanalytics.com/blog/workflows-in-python-using-pipeline-and-gridsearchcv-for-more-compact-and-comprehensive-code/\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# print best gridsearch score from cross validation and parameters\n",
    "\n",
    "print(\"Best score: %0.3f\" % clf.best_score_)\n",
    "print(\"Best parameters set:\")\n",
    "best_parameters = clf.best_estimator_.get_params()\n",
    "print \"\\n\\n\"\n",
    "for param_name in sorted(parameters.keys()):\n",
    "    print(\"\\t%s: %r\" % (param_name, best_parameters[param_name]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print \"testing/predicting\"\n",
    "\n",
    "t0 = time()\n",
    "y_prediction =  clf.predict(test)\n",
    "print(\"done in %0.3fs\" % (time() - t0))\n",
    "\n",
    "Y_inv = mlb.inverse_transform(y_prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# index = 1\n",
    "# y = mlb.inverse_transform(Y)\n",
    "# for actual, pred in zip(df_all.tags, Y_inv):\n",
    "#         print('index: {0} :: {1} => {2}'.format(index, actual, ', '.join(pred)))\n",
    "#         index = index +1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# write predictions to CSV\n",
    "\n",
    "df_out = pd.DataFrame(columns=['id', 'tags'])\n",
    "\n",
    "for y, t in zip(Y_inv, test.id):\n",
    "    temp_hash = {'id': t,\n",
    "            'tags': ' '.join(map(lambda x:  \"\".join(x) , y))}\n",
    "    df_out = df_out.append(temp_hash,ignore_index=True)\n",
    "\n",
    "    \n",
    "df_out.id = df_out.id.astype(int)\n",
    "df_out.to_csv(\"physics_predictions.csv\", index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# print decision tree visualizations\n",
    "\n",
    "from sklearn import tree\n",
    "import pydot\n",
    "\n",
    "index = 0\n",
    "while index < 5 :\n",
    "    fileStr = \"tree\" + str(index) + \".dot\"\n",
    "    with open(fileStr, \"w\") as output_file:\n",
    "        tree.export_graphviz(clf.best_estimator_.named_steps['DT'].estimators_[index], class_names=mlb.classes_, out_file=output_file)\n",
    "\n",
    "    (graph,) = pydot.graph_from_dot_file(fileStr)\n",
    "    graph.write_png(\"tree\" + str(index) + '.png')\n",
    "    \n",
    "    index += 1\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
